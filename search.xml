<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[JAVA进程被 kill 排查]]></title>
    <url>%2F2021%2F01%2F06%2Fjvm%2Fjvm-problem-command%2F</url>
    <content type="text"><![CDATA[可能会导致 JAVA 进程被 kill 的原因 Java应用程序的问题：发生 OOM 导致进程 Crash JVM自身故障：JVM 或J DK 自身的 Bug 导致进程 Crash 被操作系统 OOM-Killer Java应用程序的问题：发生 OOM 导致进程 Crash一般情况下，出现 OOM 异常，JVM 的 GC 会进行回收，是不会直接导致 JVM 进程退出的。如果出现退出的情况，那就是内存泄漏，由于内存占用越来越大，最后就直接到 crash 了，这种 JVM 的 OOM 导致的异常，比较好排查。排查步骤如下： 1、-XX:+HeapDumpOnOutOfMemoryError 和 -XX:HeapDumpPath=*/java.hprof 2、根据 HeapDumpPath 指定的路径查看是否产生 dump 文件 3、根据 dump 文件进行分析 JVM自身故障：JVM 或 JDK 自身的 Bug 导致进程 Crash当 JVM 出现致命错误时，会生成一个 hs_err_pid_xxx.log 这样的文件，该文件包含了导致 JVM crash 的重要信息，可以通过分析该文件定位到导致 crash 的根源，从而改善以保证系统稳定。当出现 crash 时，该文件默认会生成到工作目录下，然而可以通过 JVM 参数 -XX:ErrorFile 指定生成路径 -XX:ErrorFile=/xxx/xxx/hs_err_pid.log附 Java BUG dataBase 库：https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8134389 被操作系统 OOM-KillerLinux 内核有个机制叫 OOM killer（Out-Of-Memory killer），该机制会监控那些占用内存过大，尤其是瞬间很快消耗大量内存的进程，为了防止内存耗尽而内核会把该进程杀掉。可以去 /var/log/messages 里翻系统报错日志，执行如下命令: dmesg | grep java grep -i ‘killed process’ /var/log/messages]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker 基础备忘]]></title>
    <url>%2F2020%2F12%2F24%2Fdocker%2Fdocker-docker-command%2F</url>
    <content type="text"><![CDATA[Docker 已经不是什么新鲜的事物了，Docker 崛起的核心是因为 Docker 镜像的存在，这个创新使得 Docker 在短短几年内就可以迅速地改变了整个云计算领域的发展历程。Docker 镜像的存在解决了传统 paas 平台对于打包问题的根本难题，使得“压缩包”赋予了一种极其宝贵的能力：本地环境和云端环境的高度一致！ Docker 镜像，其实就是一个压缩包。但是这个压缩包里的内容，比 PaaS 的应用可执行文件 + 启停脚本的组合就要丰富多了。实际上，大多数 Docker 镜像是直接由一个完整操作系统的所有文件和目录构成的，所以这个压缩包里的内容跟你本地开发和测试环境用的操作系统是完全一样的 Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 OverlayFS 类的 Union FS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 containerd。 runc 是一个 Linux 命令行工具，用于根据 OCI容器运行时规范 创建和运行容器。containerd 是一个守护程序，它管理容器生命周期，提供了在一个节点上执行容器和管理镜像的最小功能集。 容器技术与 Docker 架构容器到底是什么玩意？容器技术是一种沙盒技术，就是能够像一个“箱子”一样，把你的应用“装”起来的技术。通过这样一种“箱子”，使的应用与应用之间相互不干扰，另外就是被装进“箱子”的应用，也具备了可以被方便地搬来搬去的灵活性。 容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。对于 Docker 等大多数 Linux 容器来说，Cgroups 技术是用来制造约束的主要手段，而 Namespace 技术则是用来修改进程视图的主要方法。实际上是在创建容器进程时，指定了这个进程所需要启用的一组 Namespace 参数。这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。就是 Linux 容器最基本的实现原理了，所以说，容器，其实是一种特殊的进程而已。 Docker 架构下图是 Docker 官方提供的 Docker 架构图： Docker 在容器的基础上，进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护。使得 Docker 技术比虚拟机技术更为轻便、快捷。 下面的图片比较了 Docker 和传统虚拟化方式的不同之处。传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。 在这个对比图里，我们应该把 Docker 画在跟应用同级别并且靠边的位置。这意味着，用户运行在容器里的应用进程，跟宿主机上的其他进程一样，都由宿主机操作系统统一管理，只不过这些被隔离的进程拥有额外设置过的 Namespace 参数。而 Docker 项目在这里扮演的角色，更多的是旁路式的辅助和管理工作。 隔离与限制使用虚拟化技术作为应用沙盒，就必须要由 Hypervisor 来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程。这就不可避免地带来了额外的资源消耗和占用。而相比之下，容器化后的用户应用，却依然还是一个宿主机上的普通进程，这就意味着这些因为虚拟化而带来的性能损耗都是不存在的；而另一方面，使用 Namespace 作为隔离手段的容器并不需要单独的 Guest OS，这就使得容器额外的资源占用几乎可以忽略不计。所以说，“敏捷”和“高性能”是容器相较于虚拟机最大的优势，但是万事都有两面，有利就有弊，对于 Docker 这种基于 Linux Namespace 的隔离机制，相比于虚拟化技术最大的不足就是：隔离得不彻底。 1、多个容器之间使用的就还是同一个宿主机的操作系统内核。 通过 Mount Namespace 单独挂载其他不同版本的操作系统文件，比如 CentOS 或者 Ubuntu，但这并不能改变共享宿主机内核的事实。这意味着，如果你要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。而相比之下，拥有硬件虚拟化技术和独立 Guest OS 的虚拟机就要方便得多了。最极端的例子是，Microsoft 的云计算平台 Azure，实际上就是运行在 Windows 服务器集群上的，但这并不妨碍你在它上面创建各种 Linux 虚拟机出来 2、在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。 你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改，这显然不符合用户的预期。相比于在虚拟机里面可以随便折腾的自由度，在容器里部署应用的时候，“什么能做，什么不能做”，就是用户必须考虑的一个问题 正是这种隔离不彻底上的问题，使得还需要另外一种技术来保障容器的稳定性，不至于资源都被一个容器全部吃掉，或者因为某个容器的修改导致其他容器也受到影响。这里就需要提到 Cgroups。 Linux Cgroups 的全称是 Linux Control Group，它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等；Linux Cgroups 的设计是比较易用的，简单理解就是给每一个子系统目录加上一组资源限制文件的组合；对于 Docker 等 Linux 容器项目来说，只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。如： 1$ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash Docker 中的三个角色镜像Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。 由于镜像会包括操作系统完整的 root 文件系统，所以我们一般看到的镜像都是比较大的。Docker 在设计时，其充分利用了 Union FS 的技术，将镜像设计为分层存储的架构模式。所以严格来说，镜像并非是像一个 ISO 那样的打包文件，镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。 容器容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 namespace 。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样 容器也是分层存储，每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为 容器存储层。容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡，所以按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 数据卷（Volume）、或者 绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高 仓库集中的存储、分发镜像的服务：Docker Registry。 Dockerfile待补充… 参考链接 《深入剖析Kubernetes：张磊》 https://yeasy.gitbook.io/docker_practice/ https://www.cnblogs.com/bjlhx/p/13202505.html http://dockone.io/article/783 http://merrigrove.blogspot.com/2015/10/visualizing-docker-containers-and-images.html]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>dockerfile</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：BeanFactory的创建]]></title>
    <url>%2F2020%2F12%2F16%2Fspring%2Fspring-series-beanfactory%2F</url>
    <content type="text"><![CDATA[Spring的Ioc容器其实就是一个bean的关系网，依赖于core，bean，context三个组件来构建的。在spring中最核心的就是对于bean的管理。而bean又依托于我们的容器。本文将从顶层分析一下spring中beanFactory的具体创建过程，为后续的bean的生命周期提供一个基础。 BeanFactory的继承体系 从上图可以看到，BeanFactory有三个子类： ListableBeanFactory HierarchicalBeanFactory AutowireCapableBeanFactory （上述三个类的子类体系小伙伴们可以自己对着源码看下，实在太多） 看下上图中最底层的DefaultListableBeanFactory类的定义：12public class DefaultListableBeanFactory extends AbstractAutowireCapableBeanFactory implements ConfigurableListableBeanFactory, BeanDefinitionRegistry, Serializable 这个其实就是BeanFactory的默认实现类，它直接或者间接的实现了所有的接口。其实在看spring源码的时候都会遇到类似的设计模式，对于某一个具体的功能，通常都会定义很多层的接口，层层包装，层层委托。这种做法的好处就是，对于不同的场合都会有特定的接口；这样一来就可以在spring内部对对象的传递和转化操作都会有一些访问限制。 例如ListableBeanFactory接口表示这些Bean是可列表的，而HierarchicalBeanFactory表示的是这些Bean是有继承关系的，也就是每个Bean有可能有父Bean。AutowireCapableBeanFactory接口定义Bean的自动装配规则。这四个接口共同定义了Bean的集合、Bean之间的关系、以及Bean行为。 BeanFactory的创建 在之前的文章中说过了容器的刷新过程。BeanFactory的创建也在wac.refresh()方法中。具体看下到底是通过哪些子类来完成的： 12// 通知子类刷新内部的bean工厂。ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); 1.AbstractApplicationContext中的obtainFreshBeanFactory 下面是obtainFreshBeanFactory的方法逻辑：12345678910protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; //这个是具体创建的方法，由子类实现 refreshBeanFactory(); //获取BeanFactory实例对象（ConfigurableListableBeanFactory类型的） ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (logger.isDebugEnabled()) &#123; logger.debug("Bean factory for " + getDisplayName() + ": " + beanFactory); &#125; return beanFactory;&#125; refreshBeanFactory并未有具体的实现逻辑，这个方法主要是通过委托给子类的refreshBeanFactory方法来实现，在AbstractApplicationContext中refreshBeanFactory是一个抽象模板方法： 1protected abstract void refreshBeanFactory() throws BeansException, IllegalStateException; 2.refreshBeanFactory方法(AbstractRefreshableApplicationContext类中)： 下面只注释与beanFactory创建相关的代码 1234567891011121314151617181920212223protected final void refreshBeanFactory() throws BeansException &#123; //是否已经有BeanFactory了 if (hasBeanFactory()) &#123; //销毁原有的Bean destroyBeans(); //关闭工厂 closeBeanFactory(); &#125; try &#123; //创建一个新的beanFactory DefaultListableBeanFactory beanFactory = createBeanFactory(); beanFactory.setSerializationId(getId()); customizeBeanFactory(beanFactory); loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException ex) &#123; throw new ApplicationContextException("I/O error parsing bean definition source for " + getDisplayName(), ex); &#125;&#125; 这个方法是实现执行这个上下文的底层bean工厂的实际刷新，如果有的话之前有BeanFactory存在，则关闭以前的bean工厂。并为上下文生命周期的下一个阶段初始化一个新鲜的bean工厂。 3.createBeanFactory(AbstractRefreshableApplicationContext类中) 123protected DefaultListableBeanFactory createBeanFactory() &#123; return new DefaultListableBeanFactory(getInternalParentBeanFactory());&#125; 这个方法就是为当前上下文创建一个内部的bean工厂。每次调用refresh()方法是都会创建尝试创建。默认实现是创建一个DefaultListableBeanFactory。并通过getInternalParentBeanFactory（）获取内部bean工厂来作为父级bean工厂。可以在子类中重写，例如自定义DefaultListableBeanFactory的设置。 getInternalParentBeanFactory（AbstractApplicationContext类中） 1234protected BeanFactory getInternalParentBeanFactory() &#123; return (getParent() instanceof ConfigurableApplicationContext) ? ((ConfigurableApplicationContext) getParent()).getBeanFactory() : getParent();&#125; 4.DefaultListableBeanFactory的构造函数 1234567/** * 通过给定的父类创建一个新的DefaultListableBeanFactory容器 * @param parentBeanFactory the parent BeanFactory */public DefaultListableBeanFactory(BeanFactory parentBeanFactory) &#123; super(parentBeanFactory);&#125; super(parentBeanFactory)调用的是AbstractAutowireCapableBeanFactory的构造函数123456789/** * 通过给定的父类构建新的AbstractAutowireCapableBeanFactory * @param parentBeanFactory parent bean factory, or &#123;@code null&#125; if none */public AbstractAutowireCapableBeanFactory(BeanFactory parentBeanFactory) &#123; this(); //设置父工厂 setParentBeanFactory(parentBeanFactory);&#125; this(),还是AbstractAutowireCapableBeanFactory的构造函数：123456789/** * 构建一个新的AbstractAutowireCapableBeanFactory. */public AbstractAutowireCapableBeanFactory() &#123; super(); ignoreDependencyInterface(BeanNameAware.class); ignoreDependencyInterface(BeanFactoryAware.class); ignoreDependencyInterface(BeanClassLoaderAware.class);&#125; super() ; AbstractBeanFactory的构造函数 12345/** * 构建一个新的AbstractBeanFactory. */public AbstractBeanFactory() &#123;&#125;]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一道算法题：Z字形转换]]></title>
    <url>%2F2020%2F12%2F16%2Fleetcode%2Fleetcodetwo%2F</url>
    <content type="text"><![CDATA[题目将字符串 “PAYPALISHIRING” 以Z字形排列成给定的行数：（下面这样的形状） 123P A H NA P L S I I GY I R 之后按逐行顺序依次排列：”PAHNAPLSIIGYIR” 实现一个将字符串进行指定行数的转换的函数: 1string convert(string text, int nRows); convert(“PAYPALISHIRING”, 3) 应当返回 “PAHNAPLSIIGYIR” 。 方案12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public String convert(String s, int numRows) &#123; //计算字符串长度 int len = s.length(); //两个周期之间的列数， int slash = numRows - 2; //计算行的长度 int rowLength = 0; while(len &gt; 0)&#123; //竖列 len = len - numRows; rowLength++; //斜着的一列 for(int i = 0; i &lt; slash &amp;&amp; len &gt; 0; i++)&#123; len--; rowLength++; &#125; &#125; //建立一个多一列的数组用于保存我们的字符串,并且全部初始化为空格了 char result[] = new char[numRows* rowLength]; // 初始化为空格 for (int i = 0; i &lt; result.length; i++) &#123; result[i] = ' '; &#125; // 当前处理的行数 int curColumn = 0; int index = 0; // 下面将字符串写入所谓的矩阵中 while(index &lt; s.length())&#123; //写入列 for(int i = 0; i &lt; numRows &amp;&amp; index &lt; s.length(); i++)&#123; result[rowLength * i + curColumn] = s.charAt(index); index++; &#125; curColumn++; //写入斜线 for(int i = numRows - 2; i &gt; 0 &amp;&amp; index &lt; s.length(); i--)&#123; result[rowLength * i + curColumn] = s.charAt(index); curColumn++; index++; &#125; &#125; // 去空格，定义两个指针循环进行操作 index = 0; // 找第一个是空格的字符位置 while (index &lt; s.length() &amp;&amp; result[index] != ' ') &#123; index++; &#125; int next = index + 1; while (index &lt; s.length()) &#123; // 找不是空格的元素 while (next &lt; result.length &amp;&amp; result[next] == ' ') &#123; next++; &#125; result[index] = result[next]; index++; next++; &#125; return new String(result, 0, index);&#125; 这个题目想了两天，之前的思路是建立一个二维数组，然后填充，最后遍历数组拿到结果，但是对于很多边界问题不太好考虑，放弃。这两种方案的首要核心都是计算列数和对斜列的处理，没有数学功底和抽象思维的程序员真的伤不起。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven 中 repository 是如何工作的]]></title>
    <url>%2F2020%2F12%2F16%2Fmaven%2Fmaven-repository%2F</url>
    <content type="text"><![CDATA[本文中，我们将来看下如何在 maven 项目中定义和解决依赖关系，然后深入研究 maven 存储库如何使这些依赖关系可供使用的。 什么是 maven 依赖项？关于这个相信都不会陌生，就是在 里面申明这样一个配置 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.glmapper.bridge.boot&lt;/groupId&gt; &lt;artifactId&gt;client&lt;/artifactId&gt; &lt;version&gt;$&#123;version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; maven 坐标大多数的 dependency 申明都会包括 groupId、artifactId、version 这些标签项，这样一组 key/value 对的组合成了maven 坐标，来标识一个特定的 dependency，和地图的经纬度坐标一样，通过这个坐标我们就可以精准的指定一个特定的 dependency。 maven 是如何定位和解析 dependencies？maven 的仓库和其他的像 APT、YUM 等不同，它不存在类似于主索引文件这样的东西存在能够去列举出当前仓库中所有可用的 artifacts。maven 使用的是通过给定依赖项的坐标值，然后再根据 maven repository layout 构造出依赖的 URL。 maven repository layout 映射对于一个 maven artifact，它的 URL 大概格式如下： 12/$groupId[0]/../$&#123;groupId[n]/$artifactId/$version/$artifactId-$version.$extension# 如 /com/glmapper/bridge/boot/client/1.0.0/client-1.0.0.jar groupId 的规则groupId 是一个字符串数组，以 . 分隔，如 org.springframework.boot，那实际上实际路径会转换成的文件路径是 /org/springframework/boot/ 。 artifactmaven 的核心功能之一是其处理传递依赖性的能力。也就是说，以递归的方式查找和下载依赖项的依赖项以及它们的依赖项，直到它们全部满足为止。 pom我们把 com.glmapper.bridge.boot 的 groupId 转换成 /com/glmapper/bridge/boot，然后用 $artifactId 和$versionId 构造 URL 的其余部分，像这样: 1/com/glmapper/bridge/boot/client/1.0.0/client-1.0.0.pom jar和 pom 差不多，如下： 1/com/glmapper/bridge/boot/client/1.0.0/client-1.0.0.jar https://blog.packagecloud.io/eng/2017/03/09/how-does-a-maven-repository-work/#maven-metadataxml]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概念基础-Linux 中的 namespace]]></title>
    <url>%2F2020%2F12%2F12%2Flinux%2Fconcept-linux-namespace%2F</url>
    <content type="text"><![CDATA[基本介绍namespace 是 Linux 内核用来隔离内核资源的方式。通过 namespace 可以让一些进程只能看到与自己相关的一部分资源，而另外一些进程也只能看到与它们自己相关的资源，这两拨进程根本就感觉不到对方的存在。具体的实现方式是把一个或多个进程的相关资源指定在同一个 namespace 中。 Linux namespaces 是对全局系统资源的一种封装隔离，使得处于不同 namespace 的进程拥有独立的全局系统资源，改变一个 namespace 中的系统资源只会影响当前 namespace 里的进程，对其他 namespace 中的进程没有影响。 namespace 的作用从前面介绍我们基本明确了 namespace 的作用，那就是“隔离”。Linux 内核实现 namespace 的一个主要目的就是实现轻量级虚拟化(容器)服务；在同一个 namespace 下的进程可以感知彼此的变化，而对外界的进程一无所知。这样就可以让容器中的进程产生错觉，认为自己置身于一个独立的系统中，从而达到隔离的目的。也就是说 linux 内核提供的 namespace 技术为 docker 等容器技术的出现和发展提供了基础条件。 从 docker 实现者的角度考虑该如何实现一个资源隔离的容器。比如是不是可以 通过 chroot 命令切换根目录的挂载点，从而隔离文件系统。 为了在分布式的环境下进行通信和定位，容器必须要有独立的 IP、端口和路由等，这就需要对网络进行隔离。 容器需要一个独立的主机名以便在网络中标识自己 进程间的通信隔离。 用户权限隔离 运行在容器中的应用需要有进程号(PID)，需要与宿主机中的 PID 进行隔离。 也就是说这六种隔离能力是实现一个容器的基础，下面就看下 linux 内核的 namespace 特性提供了什么样的隔离能力： 名称 Flag 隔离的资源 Cgroup CLONE_NEWCGROUP Cgroup root directory(cgroup 的根目录) IPC CLONE_NEWIPC System V IPC, POSIX message queues(信号量、消息队列和共享内存) Network CLONE_NEWNET Network devices,stacks, ports, etc.（网络设备、网络栈、端口） Mount CLONE_NEWNS Mount points（挂载点） PID CLONE_NEWPID Process IDs（进程 ID） Time CLONE_NEWTIME Boot and monotonic clocks（启动和单调时钟） User CLONE_NEWUSER User and group IDs（用户和用户组） UTS CLONE_NEWUTS Hostname and NIS domian name（主机名和 NIS 域名） Cgroup namespace 是后面才增加的，本篇不做过多介绍，将另起篇幅。 namespace 的基本操作查看进程所属的 namespace/proc/[pid]/ns 目录下会包含进程所属的 namespace 信息，使用下面的命令可以查看当前进程所属的 namespace 信息： 1234567891011$ ll /proc/$$/ns总用量 0dr-x--x--x 2 admin admin 0 12月 12 11:19 ./dr-xr-xr-x 9 admin admin 0 12月 12 11:18 ../lrwxrwxrwx 1 admin admin 0 12月 12 11:19 cgroup -&gt; cgroup:[4026537386]lrwxrwxrwx 1 admin admin 0 12月 12 11:19 ipc -&gt; ipc:[4026536810]lrwxrwxrwx 1 admin admin 0 12月 12 11:19 mnt -&gt; mnt:[4026537383]lrwxrwxrwx 1 admin admin 0 12月 12 11:19 net -&gt; net:[4026536400]lrwxrwxrwx 1 admin admin 0 12月 12 11:19 pid -&gt; pid:[4026537385]lrwxrwxrwx 1 admin admin 0 12月 12 11:19 user -&gt; user:[4026531837]lrwxrwxrwx 1 admin admin 0 12月 12 11:19 uts -&gt; uts:[4026537384] 或者 1234567891011$ ll /proc/7370/ns总用量 0dr-x--x--x 2 admin admin 0 12月 12 11:20 ./dr-xr-xr-x 9 admin admin 0 12月 9 18:43 ../lrwxrwxrwx 1 admin admin 0 12月 12 11:20 cgroup -&gt; cgroup:[4026537386]lrwxrwxrwx 1 admin admin 0 12月 12 11:20 ipc -&gt; ipc:[4026536810]lrwxrwxrwx 1 admin admin 0 12月 12 11:20 mnt -&gt; mnt:[4026537383]lrwxrwxrwx 1 admin admin 0 12月 12 11:20 net -&gt; net:[4026536400]lrwxrwxrwx 1 admin admin 0 12月 12 11:20 pid -&gt; pid:[4026537385]lrwxrwxrwx 1 admin admin 0 12月 12 11:20 user -&gt; user:[4026531837]lrwxrwxrwx 1 admin admin 0 12月 12 11:20 uts -&gt; uts:[4026537384] 从输出的 namespace 信息可以看到，namespace 文件都是链接文件; 以 cgroup:[4026537386] 为例： cgroup 是 namespace 的类型 数字（inode number ）标识一个 namespace，可以理解为 namespace 的 ID 如果两个进程的某个 namespace 文件指向同一个链接文件，说明其相关资源在同一个 namespace 中。 参考 https://man7.org/linux/man-pages/man7/namespaces.7.html https://www.cnblogs.com/sparkdev/p/9365405.html https://segmentfault.com/a/1190000011821634 https://segmentfault.com/a/1190000017474527?utm_source=sf-related]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>namestapce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 性能调优监控工具]]></title>
    <url>%2F2020%2F10%2F26%2Fjvm%2Fjvm-self-command%2F</url>
    <content type="text"><![CDATA[本篇主要学习记录下工作中常用的 JDK 自带的一些 JVM 性能调优监控工具，通过了解这些工具，可以在排查问题时给予我们非常大的帮助，将一些隐藏在底下的东西拿到明面上来做分析。 jps(Java Virtual Machine Process Status Tool)jps 主要用来输出 JVM 中运行的进程状态信息。语法格式如下： 1jps [options] [hostid] 如果不指定 hostid 就默认为当前主机或服务器，命令行参数选项说明如下： -q 不输出类名、Jar名和传入main方法的参数12➜ ~ jps -q42060 -m 输出传入 main 方法的参数(与默认 jps 指令返回的信息相同)12➜ ~ jps -m42060 TestSofaBootApplication -l 输出 main 类或 jar 的全限名12➜ ~ jps -l42060 com.glmapper.bridge.boot.TestSofaBootApplication -v 输出传入 JVM 的参数12➜ ~ jps -v42060 TestSofaBootApplication -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:51645,suspend=y,server=n -XX:TieredStopAtLevel=1 -Xverify:none -Dspring.output.ansi.enabled=always -Dcom.sun.management.jmxremote -Dspring.jmx.enabled=true -Dspring.liveBeansView.mbeanDomain -Dspring.application.admin.enabled=true -javaagent:/Users/xxxx/Library/Caches/JetBrains/IntelliJIdea2020.1/captureAgent/debugger-agent.jar -Dfile.encoding=UTF-8 在排查问题时，我们通过都会通过 jps 来看下当前机器运行的进程有哪些，通过不同的参数来快速找到我们目标进程所在的 pid，以便于我们后续的一系列排查操作。 jstack(Java Stack Trace)jstack 主要用来查看某个 Java 进程内的线程堆栈信息。如果 java 程序崩溃生成 core 文件，jstack 工具可以用来获得 core 文件的 java stack 和 native stack 的信息，从而可以轻松地知道 java 程序是如何崩溃和在程序何处发生问题。另外，jstack 工具还可以附属到正在运行的 java 程序中，看到当时运行的 java 程序的 java stack 和 native stack 的信息, 如果现在运行的 java 程序呈现 hung 的状态，jstack 是非常有用的。 下面是 jstack 语法格式： 123jstack [option] pidjstack [option] executable corejstack [option] [server-id@]remote-hostname-or-ip 不管是什么指令，我们都要学会先通过 -h 去查一下 12345678910111213141516➜ ~ jstack -hUsage: jstack [-l] &lt;pid&gt; (连接到正在运行的进程) jstack -F [-m] [-l] &lt;pid&gt; (连接到挂起的进程) jstack [-m] [-l] &lt;executable&gt; &lt;core&gt; (连接到 core 文件) jstack [-m] [-l] [server_id@]&lt;remote server IP or hostname&gt; (连接到远程调试服务器)Options: -F to force a thread dump. Use when jstack &lt;pid&gt; does not respond (process is hung) -m to print both java and native frames (mixed mode) -l long listing. Prints additional information about locks -h or -help to print this help message Options 参数说明如下： 选项 作用 -F 当正常输出的请求不被响应时，强制输出线程堆栈 -m 如果调用到本地方法的话，可以显示 C/C++ 的堆栈 -l 除堆栈外，显示关于锁的附加信息，在发生死锁时可以用 jstack -l pid 来观察锁持有情况 下面我们重点来聊一聊，jstack 中信息到底要怎么看。 jstack 堆栈信息介绍下面是 jstack 输出的一段 tacer 数据 12345&quot;main&quot; #1 prio=5 os_prio=31 tid=0x00007fb93b802000 nid=0x2703 waiting on condition [0x0000700005e5d000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at com.glmapper.bridge.boot.TestJstack.testWaitingOnConditionCondition(TestJstack.java:19) at com.glmapper.bridge.boot.TestJstack.main(TestJstack.java:10) 通过这段数据我们大概能 get 到的点主要包括以下信息： main 线程名 #1 堆栈序号，没有实际含义，可忽略 prio 线程优先级 os_prio 操作系统层次的优先级 tid 线程标识 nid 线程id 线程状态介绍从上面 jstack 输出的信息可以看到线程状态相关的信息，比如 TIMED_WAITING1java.lang.Thread.State: TIMED_WAITING (sleeping) RUNNABLE1java.lang.Thread.State: RUNNABLE 还有一些 &quot;GC task thread#0 (ParallelGC)&quot; os_prio=31 tid=0x00007fcee9004000 nid=0x1f07 runnable 信息，这种是 jvm 用来回收内存的，先不关注，这里主要看下 java.lang.Thread.State; 1234567891011121314151617181920212223242526public enum State &#123; /** * 当线程对象创建时存在的状态，此时线程不可能执行 */ NEW, /** * 当调用thread.start()后，线程变成为 Runnable 状态。只要得到CPU，就可以执行； */ RUNNABLE, /** * 如果进入同步方法或同步代码块，没有获取到锁，则会进入该状态； */ BLOCKED, /** * 执行thread.join()或在锁对象调用obj.wait()等情况就会进该状态，表明线程正处于等待某个资源或条件发生来唤醒自己； */ WAITING, /** * 执行Thread.sleep(long)、thread.join(long)或obj.wait(long)等就会进该状态，与Waiting的区别在于Timed_Waiting的等待有时间限制； */ TIMED_WAITING, /** * 终止 */ TERMINATED;&#125; 再回到上面堆栈信息，可以观察到，当状态是 TIMED_WAITING 时，堆栈中会出现 waiting on condition xxxx 信息，类似的还有： waiting on monitor entry : 在等待获取锁，一般对应 BLOCKED in Object.wait() : 获取锁后又执行obj.wait()放弃锁，一般对应 WAITING 下面就针对这些状态举一些简单的小例子。 线程状态举例及 jstack 分析waiting on condition1、执行代码 123456789101112/** * 产生 waiting on condition */private static void testWaitingOnConditionCondition()&#123; while (true)&#123; try &#123; Thread.sleep(60000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 2、执行结果 12345&quot;main&quot; #1 prio=5 os_prio=31 tid=0x00007fb93b802000 nid=0x2703 waiting on condition [0x0000700005e5d000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at com.glmapper.bridge.boot.TestJstack.testWaitingOnConditionCondition(TestJstack.java:19) at com.glmapper.bridge.boot.TestJstack.main(TestJstack.java:10) 3、结果分析 这里就比较明显的是 main 线程中正在 sleep 方法。不过这里 TIMED_WAITING 后面的括号里还特殊表明了 sleeping，在一些场景下，常见的还有 parking，下面继续看例子。 waiting on condition (parking)1、执行代码 123456789101112private static void testWaitingOnConditionConditionWithParking()&#123; // 提供一个阻塞对了 BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;String&gt;(1); // 先加一个 blockingQueue.add("test-parking"); try &#123; //继续加，这里肯定加不进去，所以会阻塞 blockingQueue.put("test-parking-xxx"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 2、执行结果 123456789&quot;main&quot; #1 prio=5 os_prio=31 tid=0x00007fd6d5008800 nid=0x2803 waiting on condition [0x000070000ffc1000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x000000076af3a938&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.ArrayBlockingQueue.put(ArrayBlockingQueue.java:353) at com.glmapper.bridge.boot.TestJstack.testWaitingOnConditionConditionWithParking(TestJstack.java:113) at com.glmapper.bridge.boot.TestJstack.main(TestJstack.java:13) 3、结果分析 main 线程进入了 waiting on conditon 状态，等待某一个资源，可以看到是在 a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObjec 进行了等待，阻塞住了。 waiting on monitor entry1、执行代码 1234567891011121314151617181920212223242526272829303132333435363738/** * 产生 waiting on monitor entry */private static void testWaitingOnMonitorEntry()&#123; final Object obj = new Object(); final Thread thread = new Thread()&#123; @Override public void run() &#123; // 锁 obj 对象 synchronized (obj)&#123; System.out.println(Thread.currentThread().getName()); try &#123; Thread.sleep(60000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;; final Thread thread1 = new Thread()&#123; @Override public void run() &#123; // 锁 obj 对象 synchronized (obj)&#123; System.out.println(Thread.currentThread().getName()); try &#123; Thread.sleep(60000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;; thread.setName("test-thread"); thread.start(); thread1.setName("test-thread1"); thread1.start();&#125; 2、执行结果 12345678910&quot;test-thread1&quot; #14 prio=5 os_prio=31 tid=0x00007f9563880800 nid=0x5c03 waiting for monitor entry [0x000070000b029000] java.lang.Thread.State: BLOCKED (on object monitor) at com.glmapper.bridge.boot.TestJstack$2.run(TestJstack.java:50) - waiting to lock &lt;0x000000076af261d0&gt; (a java.lang.Object)&quot;test-thread&quot; #13 prio=5 os_prio=31 tid=0x00007f956387f800 nid=0x5a03 waiting on condition [0x000070000af26000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at com.glmapper.bridge.boot.TestJstack$1.run(TestJstack.java:38) - locked &lt;0x000000076af261d0&gt; (a java.lang.Object) 3、结果分析 test-thread 获取到 obj 对象上的锁，因此正在执行 sleep 操作，状态为 TIMED_WAINTING, 而 test-thread1 由于未获取到 obj 对象上的锁，因此处于BLOCKED 状态。 test-thread1 正在 “waiting to lock “，试图在地址为 0x000000076af261d0 所在的对象获取锁，而该锁却被 test-thread 线程占有 [locked ]。test-thread 线程正在 “waiting on condition”，说明正在等待某个条件触发，由 jstack 来看，此线程正在sleep。 object.wait()1、执行代码 1234567891011121314151617181920212223242526272829303132private static void testObjectWait() &#123; final Thread thread = new Thread() &#123; @Override public void run() &#123; synchronized (this) &#123; System.out.println(Thread.currentThread().getName()); try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;; thread.start(); thread.setName("test-object-wait"); try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (thread) &#123; System.out.println(Thread.currentThread().getName()); try &#123; Thread.sleep(30000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; thread.notify(); &#125;&#125; 2、执行结果 1234567891011121314&quot;test-object-wait&quot; #13 prio=5 os_prio=31 tid=0x00007fd43a809000 nid=0xa803 in Object.wait() [0x0000700010926000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x000000076af26140&gt; (a com.glmapper.bridge.boot.TestJstack$3) at java.lang.Object.wait(Object.java:502) at com.glmapper.bridge.boot.TestJstack$3.run(TestJstack.java:73) - locked &lt;0x000000076af26140&gt; (a com.glmapper.bridge.boot.TestJstack$3)&quot;main&quot; #1 prio=5 os_prio=31 tid=0x00007fd43b001800 nid=0x2603 waiting on condition [0x000070000f2e4000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at com.glmapper.bridge.boot.TestJstack.testObjectWait(TestJstack.java:93) - locked &lt;0x000000076af26140&gt; (a com.glmapper.bridge.boot.TestJstack$3) at com.glmapper.bridge.boot.TestJstack.main(TestJstack.java:10) 3、结果分析 由于调用了 object.wait() 方法的时候放弃了锁，所以 test-object-wait 这个线程就出现了 Object.wait() 状态，线程的状态就是 waiting；等待 notify 来进行唤醒。由于 mian 线程在获得 test-object-wait 的线程锁后，调用了 Thread.sleep 方法，所以此时进入了 wating on condition 等待某一个资源，进入到 time_waiting 状态。 小结一般情况我们在做问题排查时，如果系统非常慢，我们需要特别关注 Blocked，Waiting on condition 这些状态。如果系统的 cpu 负载比较高的话，则可以死循环等思路去摸查，此时要关注下 Runable 状态；那如果堆栈中有 Deadlock，那就是产生了死锁。 jstat(JVM统计监测工具)jstat 是 JVM 统计监测工具，其语法格式如下： 1jstat -&lt;option&gt; [-t] [-h&lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;]] vmid 是 Java 虚拟机 ID，在 Linux/Unix 系统上一般就是进程 ID。interval 是采样时间间隔; count 是采样数目。比如下面输出的是 GC 信息，采样时间间隔为 1000ms，采样数为 3： 12345➜ ~ jstat -gc 58950 1000 3 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT10752.0 10752.0 0.0 0.0 65536.0 6554.0 175104.0 0.0 4480.0 785.7 384.0 75.9 0 0.000 0 0.000 0.00010752.0 10752.0 0.0 0.0 65536.0 6554.0 175104.0 0.0 4480.0 785.7 384.0 75.9 0 0.000 0 0.000 0.00010752.0 10752.0 0.0 0.0 65536.0 6554.0 175104.0 0.0 4480.0 785.7 384.0 75.9 0 0.000 0 0.000 0.000 输出信息的列释义： S0C、S1C、S0U、S1U：Survivor 0/1区容量（Capacity）和使用量（Used） EC、EU：Eden区容量和使用量 OC、OU：年老代容量和使用量 PC、PU：永久代容量和使用量 YGC、YGT：年轻代 GC 次数和 GC 耗时 FGC、FGCT：Full GC 次数和 Full GC耗时 GCT：GC 总耗时 jmap(Memory Map)jmap 用来查看堆内存使用状况，一般结合 jhat 使用。其使用语法如下： 123jmap [option] &lt;pid&gt;jmap [option] &lt;executable &lt;core&gt;jmap [option] [server_id@]&lt;remote server IP or hostname&gt; Options 参数说明如下： 选项 作用 打印与 Solaris pmap 相同的信息 -heap 打印 java 堆摘要 -histo[:live] 打印 java 对象堆的直方图;如果指定了“live”子选项，则只计算live对象 -clstats 打印 classloader 统计信息 -finalizerinfo 打印 等待终结 对象的信息 –dump: : 以 hprof 二进制格式dump java heap -F 使用 -dump: or -histo 强制执行 -J 将 直接传递给运行时系统 dump-options 又包括以下几个选项： live ： 只 dump 活动对象;如果未指定，堆中的所有对象将被dump。 format=b ： 二进制格式 file= ： dump 到指定文件 jmap -heap通过指定 pid，可以将当前进程的 heap 信息打印在 console 上，包括使用的 GC 算法、堆配置参数和各代中堆内存使用情况，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253$ jmap -heap 3493Attaching to process ID 3493, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.172-b245using parallel threads in the new generation.(eden 区使用的是并发线程)using thread-local object allocation.(使用线程本地对象分配)Concurrent Mark-Sweep GC (使用 CMS 垃圾收集器)# 堆配置信息Heap Configuration: MinHeapFreeRatio = 40 MaxHeapFreeRatio = 70 MaxHeapSize = 2147483648 (2048.0MB) NewSize = 805306368 (768.0MB) MaxNewSize = 805306368 (768.0MB) OldSize = 1342177280 (1280.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)# 堆使用情况Heap Usage:New Generation (Eden + 1 Survivor Space): capacity = 724828160 (691.25MB) used = 35156456 (33.527809143066406MB) free = 689671704 (657.7221908569336MB) 4.850315970063856% usedEden Space: capacity = 644349952 (614.5MB) used = 19878008 (18.95714569091797MB) free = 624471944 (595.542854309082MB) 3.084970820328392% usedFrom Space: capacity = 80478208 (76.75MB) used = 15278448 (14.570663452148438MB) free = 65199760 (62.17933654785156MB) 18.984577787815553% usedTo Space: capacity = 80478208 (76.75MB) used = 0 (0.0MB) free = 80478208 (76.75MB) 0.0% usedconcurrent mark-sweep generation: capacity = 1342177280 (1280.0MB) used = 166885296 (159.1542205810547MB) free = 1175291984 (1120.8457794189453MB) 12.433923482894897% used55843 interned Strings occupying 6689024 bytes. jmap -clstats通过指定 pid ，可以将当前进程的 classloader 统计信息打印在 console 上，包括类加载器名称、对象是否存活、对象地址、父类加载器、已加载的类大小等信息，如下： 12345678910111213141516171819202122232425$ jmap -clstats 3493Attaching to process ID 3493, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.172-b245finding class loader instances ..done.computing per loader stat ..done.please wait.. computing liveness.......................liveness analysis may be inaccurate ...class_loader classes bytes parent_loader alive? type&lt;bootstrap&gt; 3211 5818395 null live &lt;internal&gt;0x00000000b150ed50 1 1491 0x00000000b0020830 dead sun/reflect/DelegatingClassLoader@0x000000010000a0400x00000000b8715670 1 900 0x00000000b0020830 dead sun/reflect/DelegatingClassLoader@0x000000010000a0400x00000000cb417140 1 1503 0x00000000b0020830 dead sun/reflect/DelegatingClassLoader@0x000000010000a0400x00000000b98b4388 1 1491 null dead sun/reflect/DelegatingClassLoader@0x000000010000a0400x00000000b5a419a0 1 900 0x00000000b0020830 dead sun/reflect/DelegatingClassLoader@0x000000010000a0400x00000000b358df50 1 1493 0x00000000b0020830 dead sun/reflect/DelegatingClassLoader@0x000000010000a0400x00000000b7b277b8 1 1503 0x00000000b0020830 dead sun/reflect/DelegatingClassLoader@0x000000010000a0400x00000000c2527c58 1 1505 0x00000000b0020830 dead sun/reflect/DelegatingClassLoader@0x000000010000a0400x00000000b98b4580 1 1491 0x00000000b0026260 dead sun/reflect/DelegatingClassLoader@0x000000010000a0400x00000000b9b307b8 1 1493 0x00000000b0020830 dead sun/reflect/DelegatingClassLoader@0x000000010000a0400x00000000e236b038 1 900 0x00000000b0020830 dead sun/reflect/DelegatingClassLoader@0x000000010000a0400x00000000b0108400 1 1493 0x00000000b0020830 dead sun/reflect/DelegatingClassLoader@0x000000010000a0400x00000000b010bc00 3 7946 0x00000000b0022f60 live org/jacoco/compass/agent/rt/internal/fastjson/util/ASMClassLoader@0x00000001000eb8300x00000000b358e148 1 1493 0x00000000b0020830 dead sun/reflect/DelegatingClassLoader@0x000000010000a040 jmap -histo使用 jmap -histo pid 可以查看堆内存中的对象数目、大小统计直方图，如下： 1234567891011121314151617181920212223# jmap -histo:live 1493 带上 live 则只统计存活对象$ jmap -histo 1493 num #instances #bytes class name---------------------------------------------- 1: 1314509 144436976 [C 2: 1572864 37748736 org.apache.logging.log4j.core.async.AsyncLoggerConfigDisruptor$Log4jEventWrapper 3: 77458 32776608 [B 4: 1061561 25477464 java.lang.String 5: 731623 23411936 java.util.HashMap$Node 6: 32930 22826616 [I 7: 150340 15546784 [Ljava.util.HashMap$Node; 8: 144895 14968208 [Ljava.lang.Object; 9: 377379 12076128 java.util.concurrent.ConcurrentHashMap$Node 10: 230943 11085264 java.util.HashMap 11: 81124 3893952 java.nio.HeapByteBuffer 12: 3396 3869944 [Ljava.util.concurrent.ConcurrentHashMap$Node; 13: 78418 3764064 java.nio.HeapCharBuffer 14: 75784 3031360 java.util.TreeMap$Entry 15: 72865 2914600 java.util.LinkedHashMap$Entry 16: 166213 2659408 java.util.HashSet 17: 18355 2643120 com.mysql.jdbc.Field 18: 18394 2044336 java.lang.Class 19: 19966 1757008 java.lang.reflect.Method PS: 上图中的 [C [B 指的是 class 的对象类型，下面是常见类型的参考 B byte C char D double F float I int J long Z boolean [ 数组，如 [I 表示 int[] [L+类名 其他对象，如 [Ljava.lang.Object jmap -dump绝大多数情况下，我们不会直接在 console 来打印分析，更常规的做法是 dump 到指定的文件，然后通过一些可视化工具来辅助分析；那执行 dump 到文件一般使用如下指令： 12345jmap -dump:format=b,file=dumpFileName pid # 语法$ jmap -dump:format=b,file=test-dump.bin 85716 # 举例Dumping heap to /Users/guolei.sgl/test-dump.bin ...Heap dump file created 对于 dump 下来的文件，可以通过 jprofile 等图形化工具来分析，如下 也可以通过 jhat 查看，操作方式如下： 1、起 http 服务 12345678910jhat -port 9300 test-dump.binReading from test-dump.bin...Dump file created Wed Oct 28 17:54:24 CST 2020Snapshot read, resolving...Resolving 1151952 objects...Chasing references, expect 230 dots......................................................................................................................................................................................................................................Eliminating duplicate references......................................................................................................................................................................................................................................Snapshot resolved.Started HTTP server on port 9300Server is ready. 2、dump 类概要信息 访问 localhost:9300 查看 dump 概要信息 3、class 详情 点击某个类查看详细信息 小结本文介绍了一些 JDK 自带的一些性能调优监控工具，通过对这些工具的掌握，可以使的我们在实际的开发或者运维中能够快速的去定位和解决一些问题，常见的有 OOM、内存泄漏、线程死锁、CPU 负载高等等；目前社区也有很多好用的工具，例如 Arthas，perfma 等。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yum 指令]]></title>
    <url>%2F2020%2F10%2F26%2Flinux%2Flinux-yum%2F</url>
    <content type="text"><![CDATA[yum(Yellowdog Updater Modified)：是一个基于 RPM 的软件包管理器，能够从指定服务器自动下载RPM包并且安装，可以处理软件之间的依赖关系，一次性安装所有依赖的软件包，无需一个个下载安装。 工作原理示意图yum 客户端及服务器的工作原理如下图所示 yum 的配置文件配置文件所在目录：/etc/yum.repos.d1234567# vim /etc/yum.repos.d/alios.repo----------------------------------------[alios.7u2.base.$basearch]name=aliosbaseurl=http://yum.tbsite.net/alios/7u2/os/$basearch/ # $basearch：系统基础架构，如 x86_64gpgcheck=0---------------------------------------- x.repo 文件相关配置 key 简介： 1、[alios.7u2.base.$basearch]: 仓库的ID，可以取任意名字，只要不和其他的ID冲突即可 2、name=xxx: 用于描述容器含义 3、enabled={1|0}: 是否启用这个仓库，0表示不启用，1表示启用，默认是启用的 4、mirrorlist: 列出这个容器可以使用的镜像站点（如果不想使用，可以注释，本案例中没有体现） 5、baseurl=url: 容器地址，mirrorlist 是由 yum 程序自行找镜像站点，baseurl 则是指定一个固定容器地址 6、gpgcheck={1|0}: 是否进行签名合法性检测，0 表示不启用，1表示启用，如果选择启用 gpg 检查，则需要告知其 key 是什么 7、gpgkey=url: 如果启用 gpg 检测，则需要指定 gpgkey 的路径，即使导入过 gpgkey，这里仍然需要手动为其指定路径，这个路径可以是远程服务器上的，也可以是本地的，只要让本地客户端访问到即可 如果两个仓库里的 RPM 包是一样的，一个在远程服务器上，另一个在本地光盘上，那么本地光盘的访问速度通常会快于远程服务器上。在配置文件中，我们可以定义这样的两个仓库，为其中一个设定优先级 注: gpgme.GpgmeError: (7, 32870, u’\ufffd\ufffd\ufffd\u8c78\ufffd\ufffd\ufffd\u02b5\ufffd\ufffd\ufffd ioctl \ufffd\ufffd\ufffd\ufffd’) #如果执行报这个错，表示当前系统字符编码不支持 Unicode，改一下就好** 配置 yum 数据源默认情况下 yum 使用的源都是国外的地址，那我们如果期望下载速度更快一些时，就可以考虑使用国内的一些源，比如 aliyun 提供的, 以配置 docker yum 数据源为例： 12345$ sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo已加载插件：bestyumcache, fastestmirror, langpacksadding repo from: http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repograbbing file http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.reporepo saved to /etc/yum.repos.d/docker-ce.repo 然后就可以在 /etc/yum.repos.d/ 下看到 docker-ce.repo 1234567891011121314151617181920212223$ sudo cat docker-ce.repo[docker-ce-stable]name=Docker CE Stable - $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stableenabled=1gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-stable-debuginfo]name=Docker CE Stable - Debuginfo $basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/debug-$basearch/stableenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg[docker-ce-stable-source]name=Docker CE Stable - Sourcesbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/source/stableenabled=0gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg# ...省略其他 yum 的一些基本功能yum 基本功能主要包括：查询、删除、更新/升级以及软件组等 查询yum 查询有以下几种姿势，这里挨个举例。 yum search xxx搜索某个软件名称或者描述的重要关键字 1234567891011$ yum search java已加载插件：bestyumcache, branch, fastestmirror, langpacksLoading mirror speeds from cached hostfilekubernetes 579/579================================================================================ N/S matched: java =================================================================================abrt-java-connector.x86_64 : JNI Agent library converting Java exceptions to ABRT problemsaether-javadoc.noarch : Java API documentation for Aetheralicpp-gcc492-netlib-java.x86_64 : alicpp-gcc492-netlib-java-1.1.2.odpsant-antunit-javadoc.noarch : Javadoc for ant-antunitant-contrib-javadoc.noarch : Javadoc for ant-contribant-javadoc.noarch : Javadoc for ant yum info xxx列出软件功能（不太方便透露的信息，以 xxxx 代替了） 123456789101112131415161718$ yum info docker已加载插件：bestyumcache, branch, fastestmirror, langpacksLoading mirror speeds from cached hostfile可安装的软件包名称 ：docker架构 ：x86_64版本 ：xxxx发布 ：xxxx 大小 ：103 M源 ：xxxx简介 ：xxxx网址 ：xxxx协议 ： Commercial描述 ： CodeUrl:git@xxxx : CodeRev:e3xx79d : AoneLog: xxxx : AoneUrl:xxxx : xxx container service docker-xxx; branch: vxxx yum list列出 yum 服务器上面所有的软件名称，这里就不列了（下面一个是按规则搜的） yum list xxxx*找出以 xxx 开头的软件名称 12345$ yum list docker已加载插件：bestyumcache, branch, fastestmirror, langpacksLoading mirror speeds from cached hostfile可安装的软件包docker.x86_64 version-xxx xxxx yum list updates列出 yum 服务器上可提供本机进行升级的软件（返回的信息是 yum list 的子集，这里也不举例了） 安装 or 升级 yum install/update 软件名称 yum install 软件名称 -y #安装过程中免输入y确认 删除 yum remove 软件名称 软件组 yum grouplist //查看容器和本机上可用与安装过的软件组 yum groupinfo group_name //查看group内所有组名称 yum install/remove group_name //安装与删除 除此之外，如果我们想升级所有的软件包，可以通过如下方式搞定 yum -y update 升级所有包，改变软件设置和系统设置,系统版本内核都升级 yum -y upgrade 升级所有包，不改变软件设置和系统设置，系统版本升级，内核不改变 已经上线的用yum -y upgrade 比较稳，全新的用yum -y update 会更好 editing…. 参考 http://yum.baseurl.org/ https://blog.csdn.net/guohaosun/article/details/81481848 https://blog.51cto.com/wuyelan/1546674]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[摄影-2]]></title>
    <url>%2F2020%2F10%2F26%2Fphotograph%2Fphotograph-serise-two%2F</url>
    <content type="text"><![CDATA[1、西湖-鸳鸯 2、西湖-杭菊 3、西湖-板 4、西湖-桥 5、西湖-湖中木板桥 6、西湖-湖面夕阳]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>西湖</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[摄影-1]]></title>
    <url>%2F2020%2F10%2F24%2Fphotograph%2Fphotograph-serise-one%2F</url>
    <content type="text"><![CDATA[1、龙井-茶花 2、南湖-船 3、杭州-楼角 4、蚂蚁-功夫公仔 5、呼伦贝尔-呼伦湖边的小湖 6、杭州-夕阳]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>草原</tag>
        <tag>夕阳</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Top 命令指南]]></title>
    <url>%2F2020%2F08%2F10%2Flinux%2Fbase-operations-top-command%2F</url>
    <content type="text"><![CDATA[top 命令允许用户监视 Linux 上的进程和系统资源使用情况，它是系统管理员工具箱中最有用的工具之一，并且在每个发行版中都预装了它。与 ps 等其他命令不同，它是交互式的，我们可以浏览进程列表、终止进程，等等。本文中，我们将了解如何使用 top 命令。 Getting startedtop 命令非常简单，只需要在终端中输入 top 即可。top 指令将启动一个交互式命令行应用程序，如下所示，输出的上半部分包含有关进程和资源使用情况的统计信息，下半部分包含当前运行的进程的列表。可以使用箭头键和页面向上/向下键浏览列表。如果你想退出，只需按q键。123456789101112131415161718192021222324$ toptop - 21:07:28 up 21 days, 4:31, 1 user, load average: 0.12, 0.06, 0.07Tasks: 33 total, 1 running, 31 sleeping, 0 stopped, 1 zombie%Cpu(s): 0.2 us, 0.5 sy, 0.0 ni, 89.7 id, 0.0 wa, 0.0 hi, 0.0 si, 9.6 stKiB Mem : 33554432 total, 31188884 free, 513100 used, 1852448 buff/cacheKiB Swap: 2097148 total, 2097148 free, 0 used. 31188884 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 52601 root 39 19 1310268 14900 9836 S 0.3 0.0 22:59.21 logagent-collec 1 root 20 0 45416 5244 3968 S 0.0 0.0 5:35.71 systemd 340 root 20 0 64700 21336 17684 S 0.0 0.1 8:33.90 systemd-journal 357 root 20 0 101836 2768 2312 S 0.0 0.0 0:01.13 gssproxy 384 dbus 20 0 28632 2800 2464 S 0.0 0.0 0:00.04 dbus-daemon 432 root 20 0 84760 5852 4984 S 0.0 0.0 0:00.01 sshd 461 agent 20 0 52376 5200 3684 S 0.0 0.0 0:00.01 ilogtail 1690 agent 20 0 2193388 246304 11264 S 0.0 0.7 23:45.88 java 2527 admin 20 0 161744 4268 3704 R 0.0 0.0 0:00.72 top 3245 root 20 0 559140 12412 5860 S 0.0 0.0 64:48.67 logagent 3420 root 20 0 745052 58464 43820 S 0.0 0.2 11:16.32 metricbeat 3447 root 20 0 957796 55548 43708 S 0.0 0.2 10:14.47 metricbeat 5093 root 20 0 1905356 159280 9584 S 0.0 0.5 35:00.14 java 7458 root 20 0 13700 2564 2356 S 0.0 0.0 0:00.00 bash 7464 root 20 0 86268 4436 3740 S 0.0 0.0 0:00.00 sudo # ... 省略其他 top 有许多变体，但在本文的其余部分中，我们将讨论最常见的变体 — props -ng包附带的变体，下面来运行验证体验下： 1234$ top -v procps-ng version 3.3.10Usage: top -hv | -bcHiOSs -d secs -n max -u|U user -p pid(s) -o field -w [cols] 在 top 的界面中发生了相当多的事情，我们将在下一节中对其逐一进行分析。 了解 top 的界面 - the summary area第一小节中 top 的输出界面，我们可以比较明显的看到被分成了两个部分，这个小节中我们将关注在上半部分信息，这部分一般被称之为：summary area 系统时间、正常运行时间和用户会话 系统时间：当前系统的时间(21:07:28) 正常运行：系统运行时长(21 days, 4:31) 活动用户会话个数：1 个 1top - 21:07:28 up 21 days, 4:31, 1 user, 活动用户会话包括 TTY 和 PTY 两种。实际上，如果您通过桌面环境登录到 Linux 系统，然后启动终端模拟器，您将发现将有两个活动会话。 TTY: 通过命令行或桌面环境在系统上物理地运行PTY: 终端模拟器窗口或通过 SSH 如果我们期望得到更多关于活动用户会话的信息，可以通过 who 命令来得到，如下： 12$ whoadmin pts/0 2020-10-31 17:15 (xx.xx.xx.xx) 内存使用情况Memory 部分显示的是关于系统内存使用情况的信息，如下： 12KiB Mem : 33554432 total, 31188208 free, 513488 used, 1852736 buff/cacheKiB Swap: 2097148 total, 2097148 free, 0 used. 31188208 avail Mem Mem 和 Swap 分别显示的是 RAM 和 swap 空间信息；当 RAM 使用率接近满时，RAM 中不经常使用的区域将被写入 Swap 空间，以便稍后需要时检索。但是，由于访问磁盘的速度很慢，过分依赖 Swap 可能会损害系统性能。 关于 Swap 物理内存就是计算机的实际内存大小，由RAM芯片组成的。虚拟内存则是虚拟出来的、使用磁盘代替内存。虚拟内存的出现，让机器内存不够的情况得到部分解决。当程序运行起来由操作系统做具体虚拟内存到物理内存的替换和加载(相应的页与段的虚拟内存管理)。这里的虚拟内存即所谓的 swap; 当用户提交程序，然后产生进程，在机器上运行。机器会判断当前物理内存是否还有空闲允许进程调入内存运行，如果有那么则直接调入内存进行运行；如果没有，那么会根据优先级选择一个进程挂起，把该进程交换到swap中等待，然后把新的进程调入到内存中运行。根据这种换入和换出，实现了内存的循环利用，让用户感觉不到内存的限制。从这也可以看出swap扮演了一个非常重要的角色，就是暂存被换出的进程。 内存与swap之间是按照内存页为单位来交换数据的，一般Linux中页的大小设置为4kb。而内存与磁盘则是按照块来交换数据的 total、free、used 就是这些单词含义所描述的一样，分别是当前对应空间的总大小、空闲大小、已使用大小。avail mem 值指的是可以分配给进程而不会导致更多的交换的内存量。 Linux 内核层面上总是以不同的方式来尝试减少访问磁盘的次数；它在RAM中维护一个“磁盘缓存（disk cache）”，存储磁盘中经常使用的区域，另外，磁盘写被存储到一个“磁盘缓冲区（disk buffer）”，内核最终将它们写到磁盘上。它们消耗的总内存是 buff/cache 值。这看起来像是一件坏事，但实际上不是，原因是缓存使用的内存将在需要时分配给进程。 任务-TasksTasks 部分显示的是有关系统上运行的进程的统计信息 1Tasks: 33 total, 1 running, 31 sleeping, 0 stopped, 1 zombie total 比较好理解，它表示的就是当前系统正在运行的进程总数。但是对于其他几个状态相关的数字，我们需要了解一点 Linux 内核如何处理进程的背景知识。 进程执行是 I/O 限制的工作(如读取磁盘)和 cpu 限制的工作(如执行算术操作)的混合模式。当一个进程执行 I/O 时，CPU 是空闲的，所以 os 在这段时间切换到执行其他进程。此外，该操作系统允许一个给定的进程执行非常短的时间，然后它切换到另一个进程。这就是操作系统“多任务处理”的表现。做所有这些需要我们跟踪流程的“状态”。在 Linux 中，进程可能处于以下状态: 1、Runnable (R): 处于这种状态的进程要么在 CPU 上执行，要么存在于运行队列中，准备执行。 2、Interruptible sleep(S): 处于这种状态的进程在等待事件完成。 3、Uninterruptible sleep (D): 在这种情况下，一个进程正在等待一个 I/O 操作完成。 4、Stopped (T): 这些进程已经被一个作业控制信号(如按 Ctrl+Z)停止，或者因为它们正在被跟踪。 5、Zombie (Z): 僵尸进程 一个进程可以创建许多子进程，当父进程仍然存在时，这些子进程是可以退出的，但是，这些数据结构必须保留下来，直到父进程获得子进程的状态。这种数据结构仍然存在的终止进程称为僵尸进程。D 和 S 状态都是在 top 信息中体现为 sleeping，T 状态体现为 stopped，Z 状态体现为 zombie。 CPU 使用情况CPU 使用情况，显示了在各种任务上花费的 CPU 时间的百分比。 1%Cpu(s): 0.3 us, 0.4 sy, 0.0 ni, 90.3 id, 0.0 wa, 0.0 hi, 0.0 si, 9.0 st us指的是 CPU 在用户空间中执行进程所花费的时间。类似地，sy指的就是运行内核空间进程所花费的时间。Linux 中使用 nice 值来表示进程的优先级，值越高，优先级越低，后面我们会了解到，默认的 nice 值是可以被修改的。在手动设置 nice 的情况下，执行进程所花费的时间显示为 ni 值。ni 后面是 id，它是CPU 保持空闲的时间，大多数操作系统在 CPU 空闲时将其设置为“省电模式”。接下来是 wa值，它是 CPU 等待 I/O 完成所花费的时间。 中断(Interrupt)是向处理器发出的有关需要立即关注的事件的信号；外设通常使用硬件中断来告知系统有关事件的信息，例如键盘上的按键。另一方面，软件中断是由于处理器上执行的特定指令而产生的。在这两种情况下，操作系统都将处理它们，处理硬件中断和软件中断所花费的时间分别由hi和si给出。 在虚拟化环境中，会将一部分 CPU 资源分配给每个虚拟机（VM）。操作系统会检测到何时有工作要做，如果检测到他需要执行但是由于 CPU 在其他 VM 上繁忙而无法执行时，以这种方式浪费的时间就是“窃取”时间，显示为st。 平均负载-Load averageload average 部分表示的是在最近 1、5 和 15 分钟内的系统平均“负载”。 1load average: 0.11, 0.07, 0.07 负载是对系统执行的计算工作量的度量。在Linux上，负载是在任何给定时刻处于 R 和 D 状态的进程数。load average值为您提供了必须等待多长时间才能完成任务的相对度量。这里有几个小例子，我们来直观的理解下这两个概念。 1、在单核心系统上，load average 为 0.4 意味着系统只完成了它能完成的 40% 的工作。load average为 1 意味着系统正好处于满负荷状态——即使添加一点点额外的工作，系统也会过载。一个 load average 为 2.12 的系统意味着它超载了 112% 的工作，超出了它的处理能力。 2、在多核系统上，应该首先用 load average 除以 CPU 核数，以得到类似的度量。 此外，load average 实际上并不是我们大多数人所知道的典型的平均负载。它是一个“指数移动平均”，这意味着以前的 load average 的一小部分被考虑到当前的值（关于这个点，可以通过这篇文章来了解更多技术细节）。 了解 top 的界面 - the task areasummury area 相对简单，通过它我们可以快速了解到当前系统运行的一些摘要统计信息。但是一个细节性的信息，我们只能通过 task area 中来得到。 12345678910111213141516PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 52601 root 39 19 1310268 14900 9836 S 0.3 0.0 22:59.21 logagent-collec 1 root 20 0 45416 5244 3968 S 0.0 0.0 5:35.71 systemd 340 root 20 0 64700 21336 17684 S 0.0 0.1 8:33.90 systemd-journal 357 root 20 0 101836 2768 2312 S 0.0 0.0 0:01.13 gssproxy 384 dbus 20 0 28632 2800 2464 S 0.0 0.0 0:00.04 dbus-daemon 432 root 20 0 84760 5852 4984 S 0.0 0.0 0:00.01 sshd 461 agent 20 0 52376 5200 3684 S 0.0 0.0 0:00.01 ilogtail 1690 agent 20 0 2193388 246304 11264 S 0.0 0.7 23:45.88 java 2527 admin 20 0 161744 4268 3704 R 0.0 0.0 0:00.72 top 3245 root 20 0 559140 12412 5860 S 0.0 0.0 64:48.67 logagent 3420 root 20 0 745052 58464 43820 S 0.0 0.2 11:16.32 metricbeat 3447 root 20 0 957796 55548 43708 S 0.0 0.2 10:14.47 metricbeat 5093 root 20 0 1905356 159280 9584 S 0.0 0.5 35:00.14 java 7458 root 20 0 13700 2564 2356 S 0.0 0.0 0:00.00 bash 7464 root 20 0 86268 4436 3740 S 0.0 0.0 0:00.00 sudo 先来说明下各个列的含义： PID 这是进程ID，一个惟一的正整数，用于标识进程。 USER 这是启动进程的用户的“有效”用户名(映射到用户ID)。Linux 为进程分配一个真实的用户 ID 和一个有效的用户ID；后者允许进程代表另一个用户进行操作。(例如，非 root 用户可以提升到 root 用户来安装软件) PR NI“NI” 字段显示进程的 “nice” 值，“PR” 字段是从内核的角度显示了进程的调度优先级，“nice” 值影响的是进程的优先级。 VIRT, RES, SHR and %MEM VIRT、RES、SHR 这三个字段都与进程的内存消耗有关。VIRT是一个进程所消耗的内存总量。这包括程序代码、进程在内存中存储的数据，以及已经 swap 到磁盘的任何内存区域。RES是进程在 RAM 中消耗的内存，%MEM 表示这个值占总可用 RAM 的百分比。最后，SHR 是与其他进程共享的内存量。 S表示进程状态 TIME+ TIME+ 列表示的是进程自启动以来所使用的总 CPU 时间，精确到百分之一秒。 COMMAND COMMAND 列表示的是当前进程的名称。 top 命令的使用示例到目前为止，我们已经讨论了 top 的界面信息所描述的含义。但是，top 除了显示这个信息之外，它还可以管理进程，并且我们可以控制 top 输出的各个方面。在本节中，我们将举几个例子。（在下面的大多数例子中，你必须在 top 运行时按下一个键。这些按键是区分大小写的，所以如果你在大写锁定状态下按了k，你实际上已经按了一个k，但是这个命令并不会工作） kill 进程如果你想杀死一个进程，只要在top运行时按k。这将出现一个提示，它将询问进程的进程ID并按enter。 1PID to signal/kill [default pid = 384] 当然上面的这段输出的后面是可以手动输入进程 ID，下面的 34444444444444 就是手动输入的进程ID 1PID to signal/kill [default pid = 384] 34444444444444 如果保留此空白，top 将使用一个SIGTERM，它允许进程优雅地终止。如果您想强制终止进程，您可以在这里输入SIGKILL。你也可以在这里输入信号号，例如，SIGTERM 的数字是 384，而 SIGKILL 的数字是。如果你将进程ID留空并直接按enter`，它将终止列表中最顶端的进程。正如前面提到的，我们也可以使用箭头键滚动，并通过这种方式更改想要终止的进程。 排序进程列表使用像 top 这样的工具的一个最常见的原因是找出哪个进程消耗的资源最多。我们可以按以下键排序列表: M：用于按内存使用情况排序 P：来按CPU使用率排序 N：按进程ID排序 T：来按运行时间排序 默认情况下，top 按降序显示所有结果，但是我们可以通过按R键切换到升序。还可以使用 -o 开关对列表进行排序。例如，如果想排序进程的CPU使用量，可以这样做: 1top -o %CPU 显示线程列表而不是进程列表前面已经介绍过 Linux 如何在进程之间切换。我们知道，进程是不共享内存或其他资源的，这使得这种切换相当慢。和其他操作系统一样，Linux 支持一种“轻量级”的替代方案，称为“线程”。“线程”是进程的一部分，“线程”可以共享内存和其他资源的某些区域，同时它们也可以像进程一样并发运行。默认情况下，top在其输出中显示一个进程列表。如果想列出线程代替进程，按 H 即可，此时 “Tasks” 行将显示的是 “Threads”，显示的是线程的数量，而不是进程的数量。 1Threads: 351 total, 2 running, 349 sleeping, 0 stopped, 0 zombie 细心的读者可能会发现， summury area 中的 “Tasks” 行已经改变成 “Threads” 的了，但是在 task area 中，对应的列表中的属性却没有任何更改，那既然进程和线程不同，这怎么可能呢? 原因是在 Linux 内核内部，线程和进程使用相同的数据结构进行处理，因此，每个线程都有自己的ID、状态等等。如果我们要切换回进程视图，则再次按 H 即可。此外，也可以使用 top -H 在默认情况下显示线程。 显示进程完整路径默认情况下，COMMAND 列下的所有进程名显示的都是摘要名，如果我们期望显示当前进程的完成路径，可以通过按 c 来切换视角，或者直接使用 top -c 来启动交互界面。 以树形结构显示父子进程可以通过在 top 交互中按 V 来切到 forest view 视角，即以以树形结构显示父子进程。 12345 432 root 20 0 84760 5852 4984 S 0.0 0.0 0:00.01 `- /usr/sbin/sshd -D 98518 root 20 0 118432 6884 5792 S 0.0 0.0 0:00.00 `- sshd: admin [priv] 98520 admin 20 0 118432 3648 2556 S 0.0 0.0 0:01.32 `- sshd: admin@pts/0 98521 admin 20 0 120656 4936 3768 S 0.0 0.0 0:00.34 `- -bash130138 admin 20 0 161748 4208 3624 R 0.0 0.0 0:00.27 `- top -c 列出用户的进程要列出某个用户的进程，请在top运行时按 u。然后，输入用户名，或者留空以显示所有用户的进程；或者直接通过 top -u xxx 来指定 xxx 用户的所有进程信息。 1234567KiB Swap: 2097148 total, 2097148 free, 0 used. 31179088 avail MemWhich user (blank for all) root # waiting for input PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 45416 5244 3968 S 0.0 0.0 5:37.57 /usr/lib/systemd/systemd --system --deserialize 18 340 root 20 0 72892 30836 27184 S 0.0 0.1 8:36.56 /usr/lib/systemd/systemd-journald 357 root 20 0 101836 2768 2312 S 0.0 0.0 0:01.14 /usr/sbin/gssproxy -D 432 root 20 0 84760 5852 4984 S 0.0 0.0 0:00.01 /usr/sbin/sshd -D 过滤进程如果我们需要处理许多进程，那么简单的排序实际上对我们的帮助并不是很大。那么在这种情况下，我们可以按 o 来激活 top 的过滤模式，然后通过输入一个过滤器表达式来过滤到我们的目前进程。过滤器表达式是指定属性和值之间关系的语句，例如： COMMAND=java: 进程名=java 的 !COMMAND=java: 进程名 !=java 的 %CPU&gt;3.0: CPU &gt; 3.0 的 如果要清除所有过滤条件的话，按 = 即可。 总结 本文主要是对A Guide to the Linux “Top” Command 这篇文章的一些内容翻译，感谢原作者提供的分享 top 命令对于监视和管理 Linux 系统上的进程非常有帮助，本文只是从表面做了一些简单的介绍，还有很多我们没有涉及到的内容；例如，如何在 top 中添加更多的列。更多信息，可以通过运行 man top 查看 man 页面，来进行更深层面的学习。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>top</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊 BeanPostProcessor 不生效]]></title>
    <url>%2F2020%2F06%2F21%2Fspring%2Fspring-series-factory-bean-and-post-processor%2F</url>
    <content type="text"><![CDATA[关于 BeanPostProcessor 各位一定不陌生，在 SpringBoot 源码系列-Bean 的生命周期与扩展 这篇文章中，我有介绍过 bean 的创建流程及相关扩展，就有提到 BeanPostProcessor，包括它的作用时机及如何使用，这篇文章提到的这种属于比较常规的流程，因此在绝大多数场景下，都是符合我们认知的。但是最近在排查一个问题时，发现在某些场景下，BeanPostProcessor 不生效了… 问题描述 代码详见：extention-FactoryBean; clone 之后可以直接运行 DemoApplication 即可，可以观察到 控制台不输出 GlmapperBeanPostProcessor 里面 print out 的字符串。 运行代码，即可观察到具体的执行现场；代码里除了 BeanPostProcessor 之外，另外一个是 FactoryBean，也就是本篇所要聊的重点：FactoryBean getObjectType 为 null 时导致 bean 提前初始化，从而使得作用与目标 bean 的 BeanPostProcessors 都失效了。 下面将基于这个问题，展开进行分析。 bean 生命周期先来看下 ApplicationContext 和 bean 生命周期(仅列出部分关键流程)： 从流程中可以看到：BeanPostProcessor 的注册是在 ApplicationContext 生命周期中完成的，故而当 bean 创建时，如果相应拦截器 BeanPostProcessor 还没有注册，那么其就不会起作用，这个可能有以下两种原因： 1、bean 本身是一个 BeanPostProcessor ，且实现了 PriorityOrdered 或者 Ordered 接口 2、bean 由于某种原因，被提前初始化了，初始化的时候相应拦截器 BeanPostProcessor 还没有注册 关于第一个其实很好理解，不再赘述，本篇主要基于第二个原因进行说明。 bean 由于某种原因，被提前初始化了，初始化的时候相应拦截器 BeanPostProcessor 还没有注册bean 被提前初始化的情况就比较多了，归纳下来都能符合同一个规律：在 创建所有 non-lazy-init bean 这一步之前，也即在创建 BeanFactoryPostProcessor 或者 BeanPostProcessor 的过程中，引发了 bean 的创建，导致其被提前初始化，大体可以分为两种情形： 用户自定义的 BeanFactoryPostProcessor 或者 BeanPostProcessor 中会通过构造函数、属性注入等方式引用到目标 bean 导致其被提前创建 在上述过程中由于 Spring 自身对 FactoryBean 的 typeCheck(类型检测) 机制导致目标 bean 被提前创建 对于第一种情形，比较简单，这个通常是用户的配置导致的，比如我的 TestBeanFactoryPostProcessor 中通过属性注入了目标 bean 导致了其被提前创建，最终拦截器失效(如果去掉相应 TestBeanFactoryPostProcessor 配置，可以看到拦截器是能够成功的 )。 简单代码如下，作用在 TestFacade 上的 BeanFactoryPostProcessor 可能会由于 TestFacade 的提前被创建而失效 123456789public class TestBeanFactoryPostProcessor implements BeanFactoryPostProcessor &#123; @@Autowired private TestFacade testFacade; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; // ... &#125; 如何找到 bean 被提前初始化的时机呢？可以在 org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#createBean(java.lang.String, org.springframework.beans.factory.support.RootBeanDefinition, java.lang.Object[]) 打一个条件断点，通过 beanName 进行匹配，然后顺着 debug 堆栈往回找，就能够看到是在哪里导致了 bean 被提前创建。 对于第二种情形，其实也是通过上述方法先找到被提前创建的源头，只不过这种情形更加隐晦，也更加复杂，这里我们单独在下面的部分中来分析。 关于 isTypeMatch从 Spring 2.x 版本开始，BeanFactory 中就已经有 isTypeMatch 这个方法了 12345678910111213141516/*** Check whether the bean with the given name matches the specified type.* More specifically, check whether a &#123;@link #getBean&#125; call for the given name* would return an object that is assignable to the specified target type.* &lt;p&gt;Translates aliases back to the corresponding canonical bean name.* Will ask the parent factory if the bean cannot be found in this factory instance.* @param name the name of the bean to query* @param typeToMatch the type to match against (as a &#123;@code Class&#125;)* @return &#123;@code true&#125; if the bean type matches,* &#123;@code false&#125; if it doesn't match or cannot be determined yet* @throws NoSuchBeanDefinitionException if there is no bean with the given name* @since 2.0.1* @see #getBean* @see #getType*/boolean isTypeMatch(String name, @Nullable Class&lt;?&gt; typeToMatch) throws NoSuchBeanDefinitionException; 从方法注释可以简单了解到，isTypeMatch 的作用就是：判断 JavaBean 是否匹配指定的类型。他包括两个参数： name：容器中定义的 JavaBean 的名称。 typeToMatch：要匹配的目标类型。 回到案例，我们需要关注的是 isTypeMatch 和我们前面提到的FactoryBean getObjectType 为 null 时导致 bean 提前初始化，从而使得作用与目标 bean 的 BeanPostProcessors 都失效了。有什么关系呢？这里有两个比较关键的信息： 1、FactoryBean getObjectType 为 null 2、目标 bean 的 BeanPostProcessors 都失效了 其实大概能够猜到的是，actoryBean getObjectType 为 null 时，导致了 当前 bean 被提前初始化，而此时 bean 的 BeanPostProcessors 还没有被注册到当前 bean ，从而导致了目标 bean 的 BeanPostProcessors 都失效。 这个也是本篇的结论，但是还是需要来看看具体原因的细节是什么样的。 我们知道，在 Spring 中，当进行 byType (除了用户主动配置 byType 注入以外，使用 @autowired 以及 @Bean 中的方法参数时等都使用的是 byType 注入) 注入时，会通过 org.springframework.beans.factory.ListableBeanFactory#getBeanNamesForType(java.lang.Class&lt;?&gt;, boolean, boolean) 来寻找相应类型的 bean 。 针对 FactoryBean 而言，当判断其类型时，会先创建一个简单的(非完整的，仅仅是调用构造函数) bean ，调用其 getObjectType() ，如果发现返回为 null，那么就会再创造完整的 bean ，然后再通过 getObjectType() 获取类型进行匹配。 详细分析基于上面提到的点，结合本案例，来 debug 看下 FactoryBean typeCheck(类型检测) 机制导致的 BeanPostProcessor 不生效的原因。 这里主要还是看下 isTypeMatch 方法执行是如何触发 bean 提前初始化的。 isTypeMatch 方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798@Overridepublic boolean isTypeMatch(String name, ResolvableType typeToMatch) throws NoSuchBeanDefinitionException &#123; String beanName = transformedBeanName(name); // Check manually registered singletons. Object beanInstance = getSingleton(beanName, false); // 常规情况下，这里 beanInstance 是不为 null 的，但是对于提前加载的 beanInstance == null if (beanInstance != null &amp;&amp; beanInstance.getClass() != NullBean.class) &#123; // 判断类型是不是 FactoryBean if (beanInstance instanceof FactoryBean) &#123; // 返回给定名称是否为工厂解除引用(以工厂解除引用前缀开始)。 &amp;xxxx if (!BeanFactoryUtils.isFactoryDereference(name)) &#123; // 这里拿 FactoryBean#getObjectType Class&lt;?&gt; type = getTypeForFactoryBean((FactoryBean&lt;?&gt;) beanInstance); return (type != null &amp;&amp; typeToMatch.isAssignableFrom(type)); &#125; else &#123; // 实例类型是否匹配 return typeToMatch.isInstance(beanInstance); &#125; &#125; // 处理泛型和代理 else if (!BeanFactoryUtils.isFactoryDereference(name)) &#123; if (typeToMatch.isInstance(beanInstance)) &#123; // 直接匹配暴露实例? return true; &#125; else if (typeToMatch.hasGenerics() &amp;&amp; containsBeanDefinition(beanName)) &#123; // 泛型可能只匹配目标类，而不匹配代理… RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); Class&lt;?&gt; targetType = mbd.getTargetType(); if (targetType != null &amp;&amp; targetType != ClassUtils.getUserClass(beanInstance) &amp;&amp; typeToMatch.isAssignableFrom(targetType)) &#123; // 还要检查原始类匹配，确保它在代理上暴露。 Class&lt;?&gt; classToMatch = typeToMatch.resolve(); return (classToMatch == null || classToMatch.isInstance(beanInstance)); &#125; &#125; &#125; return false; &#125; // 当前 beanName 的 bean 没有被注册过 else if (containsSingleton(beanName) &amp;&amp; !containsBeanDefinition(beanName)) &#123; // null instance registered return false; &#125; // 没有找到单例实例-&gt;检查bean定义。 BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; // 在这个 factory 中没有找到 bean definition -&gt; 委托 parent。 return parentBeanFactory.isTypeMatch(originalBeanName(name), typeToMatch); &#125; // 检索相应的 bean 定义。 RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); Class&lt;?&gt; classToMatch = typeToMatch.resolve(); if (classToMatch == null) &#123; classToMatch = FactoryBean.class; &#125; Class&lt;?&gt;[] typesToMatch = (FactoryBean.class == classToMatch ? new Class&lt;?&gt;[] &#123;classToMatch&#125; : new Class&lt;?&gt;[] &#123;FactoryBean.class, classToMatch&#125;); // Check decorated bean definition, if any: We assume it'll be easier // to determine the decorated bean's type than the proxy's type. // 检查修饰 bean definition(如果有的话):我们假设确定修饰 bean 的类型比确定代理的类型更容易。 BeanDefinitionHolder dbd = mbd.getDecoratedDefinition(); if (dbd != null &amp;&amp; !BeanFactoryUtils.isFactoryDereference(name)) &#123; RootBeanDefinition tbd = getMergedBeanDefinition(dbd.getBeanName(), dbd.getBeanDefinition(), mbd); // 预测指定bean的最终bean类型(已处理bean实例的)。由&#123;@link #getType&#125;和&#123;@link #isTypeMatch&#125;调用。不需要专门处理factorybean，因为它只应该操作原始bean类型。 // 这个实现过于简单，因为它不能处理工厂方法和实例化 awarebeanpostprocessors。对于标准bean，它只能正确地预测bean类型。要在子类中重写，应用更复杂的类型检测。 Class&lt;?&gt; targetClass = predictBeanType(dbd.getBeanName(), tbd, typesToMatch); if (targetClass != null &amp;&amp; !FactoryBean.class.isAssignableFrom(targetClass)) &#123; return typeToMatch.isAssignableFrom(targetClass); &#125; &#125; // 推断出 beanType Class&lt;?&gt; beanType = predictBeanType(beanName, mbd, typesToMatch); if (beanType == null) &#123; return false; &#125; // 检查 bean class 是否是 FactoryBean 类型。本案例就是在这被处理到 返回 false 的 if (FactoryBean.class.isAssignableFrom(beanType)) &#123; if (!BeanFactoryUtils.isFactoryDereference(name) &amp;&amp; beanInstance == null) &#123; // 如果它是FactoryBean，我们希望看到它创建了什么（getObject），而不是工厂类。 beanType = getTypeForFactoryBean(beanName, mbd); if (beanType == null) &#123; return false; &#125; &#125; &#125; // 省略 ........&#125; getTypeForFactoryBean 方法这个步骤会向尝试从 FactoryBean 的 getObjectType 方法去获取类型，如果拿不到，则调用父类的进行初始化 bean 操作 123456789101112131415// 省略 其他...if (fb != null) &#123; // 尝试从实例的这个早期阶段获取 FactoryBean 的对象类型。这里调用的就是 FactoryBean#getObjectType 方法 Class&lt;?&gt; result = getTypeForFactoryBean(fb); // 本案例中这里返回的是 null, 所以会走到 else if (result != null) &#123; return result; &#125; else &#123; // 这里的意思就是没有通过 FactoryBean#getObjectType 快速获取到类型 // 将执行实例当前实例，然后再获取 return super.getTypeForFactoryBean(beanName, mbd); &#125;&#125;// 省略 其他... AbstractBeanFactory#getTypeForFactoryBean调用父类的 getTypeForFactoryBean 方法，执行 bean 的初始化 123456789101112131415@Nullableprotected Class&lt;?&gt; getTypeForFactoryBean(String beanName, RootBeanDefinition mbd) &#123; if (!mbd.isSingleton()) &#123; return null; &#125; try &#123; // 这里开始执行 doGetBean，之前的文章里面有提到，bean 实例化的入口就是 getBean 的时候 FactoryBean&lt;?&gt; factoryBean = doGetBean(FACTORY_BEAN_PREFIX + beanName, FactoryBean.class, null, true); return getTypeForFactoryBean(factoryBean); &#125; catch (BeanCreationException ex) &#123; // 省略日志打印部分 return null; &#125;&#125; 在 doGetBean 中执行链路中，会在 initializeBean 时给当前 bean 注册 BeanPostProcessor，（applyBeanPostProcessorsBeforeInitialization 方法中) ，这里可以比较清晰的看到 BeanPostProcessor 没有作用于 目标 bean 的。 doGetBean -&gt; createBean -&gt; initializeBean -&gt; applyBeanPostProcessorsBeforeInitialization 小结在本篇的案例中，其实比较明显的可以看到测试工程中 GlmapperFactoryBean 的 getObjectType 返回是为 null 的，也正是因为这个原因导致了 BeanPostProcessor 失效。那么如何在实际的开发过程中来规避呢？ 1、FactoryBean 的 getObjectType() 不要返回 null 2、定义 BeanPostProcessor 时，需要特别注意 order 3、在 创建所有 non-lazy-init bean 之前的 getBeanNamesForType 调用，尽量将 eagerInit 传为 false。 关于第三点，前面提到过 getBeanNamesForType 的调用会触发类型检查，但其实这个方法还有些参数，参考如下： String[] getBeanNamesForType(Class&lt;?&gt; type, boolean includeNonSingletons, boolean allowEagerInit);这里有个很重要的参数 allowEagerInit ，可以看到 spring 的注释中对其有非常详细的解释： 123456@param allowEagerInit whether to initialize lazy-init singletons and* objects created by FactoryBeans (or by factory methods with a* &quot;factory-bean&quot; reference) for the type check. Note that FactoryBeans need to be* eagerly initialized to determine their type: So be aware that passing in &quot;true&quot;* for this flag will initialize FactoryBeans and &quot;factory-bean&quot; references. 简单来说这个参数能够控制是否允许 FactoryBean 的提前创建，如果是 false，那么也不会引发上述的 类型检测 。可以看到在 Spring 中在获取 BeanFactoryPostProcessor 以及 BeanPostProcessor 时，也都是传入 false 的。 12345tring[] postProcessorNames = beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false);String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); 当然在一些 @Bean 的方法参数注入、@Autowire 注入等场景下，这个默认都是 true 的，无法改变；但针对平时编码过程中，如果是在比较早期的调用中，可根据情况，尽量传入 false。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是中台？]]></title>
    <url>%2F2020%2F05%2F19%2Fsolution-series-zhongtai%2F</url>
    <content type="text"><![CDATA[本文来自 PoemByte 公众号，作者 kayzhao 没有XX台的时代 - 烟囱式的架构​ 在传统IT企业，项目的架构是什么样的呢？无论项目内部的如何复杂，都可简化分为“前台”和“后台”两部分，也就是垂直的烟囱式架构（业内人士把见招拆招、垂直化发展、未做足够抽象通用的架构称之为烟囱型架构）。什么是前台？所谓前台即包括各种和消费者用户直接交互的界面业务功能，比如web页面（PC端），手机app（无线端或移动端）。什么是后台？后台是面向运营人员的配置管理系统，比如商品管理、物流管理、结算管理。后台为前台提供业务管理等。前台、后台、用户之间的关系，可以用下图简单表示： ​ 起初，项目的发展相对稳定，并不需要快速的去迭代，所以垂直的烟囱式结构并没有什么问题。但在互联网快速发展的今天，企业之间的竞争越来越激烈，只有以用户为中心，快速响应用户的需求，不断迭代和试错，才能让企业在竞争当中立于不败。在传统的前台-后台架构中，各个项目相对独立，许多项目都在重复发明同样的轮子（比如用户中心，支付业务等），即让项目本身越来越臃肿，也让开发效率越来越低。这种时候，为提高开发效率，我们有必要整合出一个中间组织，为所有的项目提供一些公共资源。而这个中间组织，就是人们所说的平台。 垂直烟囱的进化 - 平台化的架构​ 为什么会出现平台化架构，还得从烟囱型架构说起（参考上一节）。但烟囱型架构并非一无是处，在早期业务死活未知的情况下，不过度设计架构，能直接有效的支持到业务。不过，当业务发展起来之后，烟囱越树越多，成长的烦恼就如期而至了。 ​ 第一个问题是人不够，业务响应慢了下来。我们以一个5人研发团队为例来说明一下这个问题。起初团队一个产品都没有，5个人1个月干出一个简单版本的红包系统；几年之后团队增加到10人，但手头要维护10个系统。那么平均人手一个系统，这时候，又来了2个新业务，团队派出3个人去干，大约要干4个月，严重不符合前端业务的响应预期。 ​ 第二个问题是重复建设，同类烟囱系统中80%的功能是类似的，从数据库模型到主要业务逻辑，都是copy-paste加补丁，一步留神又踩到一个坑。 ​ 第三个问题是维护成本高。日常升级包、咨询支持服务，团队疲惫不堪。基于此，80%甚至90%的共性问题，能不能抽象出来呢？核心领域模型是否可以是稳定的呢？从下图可以看出，这是可以做到的。 ​ 在既要支持不断出现的各种业务，又要支持建设新平台。企业便启动了平台化建设，对前后台业务提供统一的能力露出，由能力组装编排内部服务。研发规则运营、统一后台管理服务等。 ​ 总结下来，平台化架构有以下好处：一是快速支撑、响应业务；二是抽象共性，边界清晰。快速支撑，响应业务是以终为始的出发点。架构如果不服务业务，再高大上都是扯淡。技术不是炫技，要服务商业。再谈谈抽象共性的问题，业务平台化要解决业务共性问题，比如天猫、淘宝都有各类营销活动。那么就抽象出一个营销平台来管理营销活动、营销工具的整个的生命周期管理。 中台的架构思想 - 大中台小前台中台的起源 SuperCell ​ SuperCell是一家芬兰的手机游戏公司，这个名字或许有些陌生，但是说起下面几款游戏，大家一定会很熟悉：部落冲突、海岛奇兵、皇室战争等。SuperCell公司就像是一个高产的游戏孵化器，在几年内开发出了10款以上的游戏，但是大部分用于试错的游戏都在研发过程中被腰斩了，最终呈献给用户的几款游戏都是经典中的经典。是什么让SuperCell公司能够如此高效地试错和迭代呢？他们依靠的是强大的平台资源，支撑起各个游戏开发的小团队。他们开发出的游戏看上去风格迥异，却存在许多共同之处。在业务上，共通的东西包括支付系统、用户系统等等，在技术上，共同的东西包括游戏引擎，内部开发工具等等。而这些共通的资源，都可以由一个强大的“中台”来提供。Supercell的中台，指的是公司将游戏开发过程中公共和通用的游戏素材和算法整合起来，并积累了非常科学的研发工具和框架体系，构建了一个功能非常强大的中台。这样强大的中台可以支持若干个小团队在短时间内开发出一款新的游戏。 阿里巴巴 ​ 马云在2015年的一次欧洲之旅（访问SuperCell公司），将中台的思想结合阿里的现状，提出了大中台、小前台的战略架构，从而将中台架构思想引入国内，开启了中台化热潮。 中台的定义​ 中台是什么？简言之，中台是给业务团队提效为目标的，可复用的技术能力及业务能力的集合。有业务能力说明理解业务，能复用说明能提效。从这个定义可以看出，中台更接近是一个解决方案。 中台的分类​ 中台 是 可复用的技术能力和业务能力的集合；与此相对应的，中间件、技术框架、技术平台 是 可复用的技术能力的集合；中台和中间件的共同点就是他们都需要被复用才能发挥价值，并不能出去单打独斗。 ​ 以此类推：业务中台就是可复用的业务技术能力和组织业务能力的集合；数据中台就是可复用的数据技术能力和数据业务能力的集合；算法中台就是可复用的算法技术能力和算法业务能力的集合； ​ 但是，技术中台这种说法有点迷，会让人误解里面都是技术复用，而没有任何业务。如果其实是纯粹的技术复用平台，建议大家在平常交流时还是尽量别用技术中台，直接用中间件、技术平台、技术框架的原有概念来沟通即可，没必要赶时髦。 中台的用户​ 电商交易系统，前台的用户是消费者，后台的用户是电商运营，中台的用户是谁？ ​ 企业管理系统，前台的用户是员工，后台的用户是企业管理员，中台的用户是谁？ ​ 数字政务系统，前台的用户是公务员，后台的用户是政府管理员，中台的用户是谁？ ​ 大中台，小前台。 这种说法的误导性在于，让人以为中台是为前台服务的。但其实中台可以服务任何业务形态。从Supercell这个故事可以看出，中台不会直面消费者或最终用户。中台的作用就是为业务团队服务，让业务团队更好更快的服务最终用户。 中台是必须？从0到1的阶段​ 没有必要搭建中台。从0到1的创业型公司，首要目的是生存下去，以最快的速度打造出产品，证明自身的市场价值。这个时候，让项目野蛮生长才是最好的选择。如果不慌不忙地先去搭建中台，恐怕中台还没搭建好，公司早就饿死了。 从1到N的阶段​ 适合搭建中台。当企业有了一定规模，产品得到了市场的认可，这时候公司的首要目的不再是活下去，而是活的更好。这个时候，趁着项目复杂度还不是特别高，可以考虑把各项目的通用部分下沉，组建中台，以方便后续新项目的尝试和旧项目的迭代。 从N到N+1的阶段​ 搭建中台势在必行。当企业已经有了很大的规模，各种产品、服务、部门错综复杂，这时候做架构调整会比较痛苦。但是长痛不如短痛，为了项目的长期发展，还是需要尽早调整架构，实现平台化，以免日后越来越难以维护。 中台的FAQ中心化/平台化/中台化异同？​ 中心化-&gt;平台化-&gt;中台化，更像是随着组织规模增大，分布式系统下一种架构思想的演进。在业务最早期，业务既量小又简单，一个业务系统、单机或几台机器就支持了。随着业务快速发展，团队增多，带来诸多的效率和稳定性问题，系统架构升级，开始系统拆分，正式进入分布式系统阶段，并由此开启了一段新的架构演进，如下图所示： ​ 中心化重在领域建模，通过对自身领域的抽象建模，对外提供统一标准的数据和服务； ​ 平台化重在业务抽象和架构开放，“业务抽象解决共性的80%问题，系统架构开放性解决20%的个性化问题”，既能对外提供标准的数据和服务，还能通过平台配置，或实现指定服务接口，或平台内部实现业务逻辑控制等方式支持不同业务的运行； ​ 中台化重在建立标准和机制，通过建立业务身份、能力、扩展点等业务领域概念标准，能力管控、流程编排等系统运行时标准，使大家能互联互通、共享共建，以统一的标准进行需求分析、技术开发和复用。 ​ 总的来说：中心主要负责自身单一领域的建设，而平台要负责对多个业务域的支持，而中台则是要覆盖到所有业务域，建立整个业务域的协同标准和机制。所以中台的技术连通性更强，技术生态性更突出。淘系业务系统的发展正是这个过程，业务上从淘宝时期到三淘（淘宝、天猫、一淘）时期到现在的淘系生态，系统上从商品中心、店铺中心等到商品平台、店铺平台到今天的电商业务中台。 小前台到底多小才算小呢？​ 小前台只是个代称，并不一定非得是前台团队， 用一个“快速反应团队”代之较为合适。也就是5人、7人、最多十几个人组成的团队，不宜过大（其实是相对“小”，不用刻意追求数量的少）。过大了惯性也会比较大，掉头就比较不容易，不利于快速反应、创新、试错。 如何下手建设中台化架构？​ 从哪里开始？哪种路径更适合打造一个中台？ ​ ① 直接下手开始做中台，逐步扩展到其他业务 ​ ② 从最擅长的业务（核心业务）入手，做中台的探索 ​ 第一种路径的好处是一开始可以做好中台的规划，技术栈保持一致性。坏处是失败的概率和成本比较高。第二种路径更保险，也是目前来看比较可能结出果实的路径。比如：阿里先有电商业务和互联网金融业务，然后才做了共享业务、星环等中台方案。头条最擅长算法业务，然后才有了算法中台。腾讯在IM领域沉淀了多年，基于IM做中台符合逻辑。 中台架构到底在学习什么？​ 值得我们学习的不是中台本身，而是 Supercell模式。Supercell模式如何实现，马老师已经给了一种路径：大中台，小前台。 但也许这不是唯一路径，但至少是种思路。18年到19年，有种功利化、蹭热度化、浮躁化的氛围，弥漫在中台的圈子里。大家一哄而上，咋咋呼呼的大跃进式的建设大中台，逢人必谈中台，周报写中台，开会说中台，晋升提中台，甚至借中台之名，行平台之实。 参考资料我看中台：https://mp.weixin.qq.com/s/fQ98fe3XH6imxzNhwiNaNA 漫画：什么是中台：https://mp.weixin.qq.com/s/rF7_xJBq4NJP6CmkW3HPpQ 掘金： 数据中台：https://juejin.im/search?query=数据中台 读透《阿里巴巴数据中台实践》，其到底有什么高明之处？：https://juejin.im/post/5d79fedff265da03cd0aac81 知乎：中台如何做到快速响应：https://www.zhihu.com/search?type=content&amp;q=中台如何做到快速响应 业务中台探索和实践：软件的根本问题：https://zhuanlan.zhihu.com/p/59867439 什么是中台？什么不是中台？所有的中台都是业务中台：https://zhuanlan.zhihu.com/p/77097815 如何建设中台？中台建设的组织、支撑技术和方法论：https://zhuanlan.zhihu.com/p/77362869 什么是人力资源中台模式？：https://www.zhihu.com/question/332569121/answer/733808658 中台是什么，到底要解决什么问题？：https://juejin.im/post/5d8093c251882579f24fb9ed 从平台到中台【上】：https://mp.weixin.qq.com/s/dpkteHsQJ4Rwl6YNl2PVeg? 从平台到中台【下】：https://mp.weixin.qq.com/s/TirTQfWo0gX9PUw_okdGjQ? 陈华编著《企业IT架构转型之道：阿里巴巴中台战略思想与架构实战》：https://book.douban.com/subject/27039508/]]></content>
      <categories>
        <category>解决方案</category>
      </categories>
      <tags>
        <tag>解决方案</tag>
        <tag>中台</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ClassLoader 类加载-type checking 对类加载的影响]]></title>
    <url>%2F2020%2F05%2F01%2Fjava-base-classloader-typecheck%2F</url>
    <content type="text"><![CDATA[Type CheckingType Checking (类型检测) 的作用是分析程序在编译或者运行期间，其类型表达是否一致的一个过程。举个例子：如果一个变量被声明为 int 类型，那么他就不能被赋值为实际的值（或者字符串类型、或者其他任何类型）。java 语言的类型检测分为两种： 静态类型检测（static checking）: 问题在程序运行之前被自动找到，也就是在编译阶段完成的检查。静态类型检测更多的是关注在”类型“上。 动态类型检测（dynamic checking）: 问题在运行期间被检测，动态运行检测关注的是在”值“上。 本文主要介绍静态类型检测。java 语言在编译时会做大量的类型检测，只要你声明了一个变量的类型，编译器将会确保只有相应类型的值可以被赋值给这个变量（或者这个值的类型是变量类型的子类型）。比如，如果你声明了如下变量：1int x; 这里可以确保它只保存 int 值。但是，如果将变量声明为 List，则该变量可能包含列表的子类型，包括 ArrayList、LinkedList 等。 Type Checking 对类加载的影响前面提到静态类型检测主要是对类型的检测，而 java 语言中，类型一致表示的是 类全限定名+ClassLoader 一致，所以在做类型检测时就必定会涉及到某些类的 class load 操作。下面我们就从几个方面来分析下类型检测对于类加载的影响。 在 jvm 参数中配置 -verbose:class 可以观察类加载过程 方法的返回类型在下面的例子中， Main 执行过程，check 方法没有被调用，但是该方法返回了一个非 ClassA 的类型，也就是类型 ClassB。那么类型检测就要求就提前加载 ClassA 和 ClassB 类型，加以验证，因此加载顺序如下（ClassA –&gt; ClassB –&gt; ClassC –&gt; ClassD）。 123456789101112131415public class ClassA &#123;&#125;public class ClassB extends ClassA&#123;&#125;public class ClassC &#123;&#125;public class ClassD &#123;&#125;public class Main &#123; static ClassC c; static &#123; c = new ClassC(); &#125; public static void main(String[] args) &#123; new ClassD(); &#125; ClassA check(ClassA a) &#123; return new ClassB(); &#125; 执行查看类加载顺序如下： 12345678[Loaded com.glmapper.bridge.boot.methodreturn.Main from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/][Loaded sun.launcher.LauncherHelper$FXHelper from /Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/rt.jar][Loaded java.lang.Class$MethodArray from /Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/rt.jar][Loaded com.glmapper.bridge.boot.methodreturn.ClassA from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/][Loaded com.glmapper.bridge.boot.methodreturn.ClassB from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/][Loaded java.lang.Void from /Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/rt.jar][Loaded com.glmapper.bridge.boot.methodreturn.ClassC from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/][Loaded com.glmapper.bridge.boot.methodreturn.ClassD from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/] 从这里可以看到，静态域不一定会比非静态域先加载，这里就是因为静态检测提前出发了类的加载导致。 方法参数先来看下下面这段代码，大家可以想一下类加载顺序是什么样的 1234567891011121314public class ClassA &#123;&#125;public class ClassB extends ClassA&#123;&#125;public class ClassC &#123;&#125;public class Main &#123; static ClassC c; static &#123; c = new ClassC(); &#125; public static void main(String[] args) &#123; Main main = new Main(); main.m(new ClassB()); &#125; void m(ClassA a) &#123;&#125;&#125; 按照我们惯性理解，Main 加载之后，会加载 ClassC，然后再加载 ClassA 和 ClassB。但是事实是这样吗？通过 -verbose:class 参数执行结果如下：1234567[Loaded com.glmapper.bridge.boot.paramscheck.Main from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/][Loaded sun.launcher.LauncherHelper$FXHelper from /Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/rt.jar][Loaded java.lang.Class$MethodArray from /Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/rt.jar][Loaded com.glmapper.bridge.boot.paramscheck.ClassA from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/][Loaded com.glmapper.bridge.boot.paramscheck.ClassB from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/][Loaded java.lang.Void from /Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/rt.jar][Loaded com.glmapper.bridge.boot.paramscheck.ClassC from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/] 但是从这里看到，Main 执行时， ClassA ，ClassB 先于 ClassC 加载了。原因是类型检测过程中，会一行行先行的看你的代码，在这个场景中，它发现有 m(ClassA a) 方法，但是代码中传入了 ClassB 这个类型，那么在真正运行 main 方法之前，在运行 Main 的 static 块之前，先行加载了 ClassA 和 ClassB 两个类型，然后验证它们之间的关系。所以看到的类加载顺序是 ClassA -&gt; ClassB -&gt; ClassC ，而非我们概念中的 ClassC -&gt; ClassA -&gt; ClassB。 变量赋值最后一种场景是变量赋值，来看下面的代码片段： 12345678910111213141516171819public class ClassA &#123;&#125;public class ClassB extends ClassA&#123;&#125;public class ClassC &#123;&#125;public class ClassD &#123; ClassA a;&#125;public class Main &#123; static ClassC c; static &#123; c = new ClassC(); &#125; public static void main(String[] args) &#123; ClassD d = new ClassD(); d.a = new ClassB(); &#125;&#125; 启动 main 方法时，在 jvm 参数中配置 -verbose:class 来观察类型加载顺序； 12345678[Loaded com.glmapper.bridge.boot.variableassign.Main from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/][Loaded sun.launcher.LauncherHelper$FXHelper from /Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/rt.jar][Loaded java.lang.Class$MethodArray from /Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/rt.jar][Loaded com.glmapper.bridge.boot.variableassign.ClassA from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/][Loaded com.glmapper.bridge.boot.variableassign.ClassB from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/][Loaded java.lang.Void from /Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/rt.jar][Loaded com.glmapper.bridge.boot.variableassign.ClassC from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/][Loaded com.glmapper.bridge.boot.variableassign.ClassD from file:/glmapper/Documents/glmapper/glmapper-blog-samples/glmapper-blog-sample-typecheck/target/classes/] 是不是又有点出乎意料呢？类型检测发现 Main 中包含了 d.a = new ClassB() 的语句，其中 d.a 的类型不是 ClassB，因此会先于 main 方法执行以及先于 Main 中的 static 块执行进行加载。 类型检测，将类型 ClassA 和 ClassB 的加载“提前”了。 小结本文主要介绍了静态类型检测对于 Class Loader 加载类顺序的影响，了解此逻辑对于在考虑多 class loader 场景处理问题非常有用，对于常规的类似 ClassCastExcetion, LinkageError 等异常排查有一定的意义。]]></content>
      <categories>
        <category>java 基础</category>
      </categories>
      <tags>
        <tag>classloader</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【译】微服务（microservices）]]></title>
    <url>%2F2020%2F04%2F12%2Fsolution-series-microservices%2F</url>
    <content type="text"><![CDATA[原文链接：https://martinfowler.com/articles/microservices.html 目录 微服务体系结构的特征 通过服务拆分实现组件化 围绕业务功能进行组织 产品不是项目 智能端点和轻量级通信 分散治理 分散的数据管理 基础设施自动化 容错设计 演进式设计 微服务是未来的趋势吗? Sidebars-扩展 微服务有多大? Microservices和SOA 多种语言，多种选择 经过实战检验的标准和强制执行的标准 让做正确的事情变得容易 断路器和可随时上线的代码 同步调用的弊端 术语 术语原词 释义 Microservice Architecture 微服务架构 automated deployment 自动部署 centralized management 集中管理 communicating 通信、交互 lightweight mechanisms 轻量级机制 monolithic application 单体应用/集中式应用 load-balancer 负载均衡器 modular structure 模块化结构 Componentization/components 组件化/组件 physical world 客观世界 OO programs 面向对象编程 encapsulation 封装 processes 进程 cross-team 跨团队 boundaries 边界 business capability 业务能力 business area 业务领域 cross-functional 跨职能的 message bus. 消息总线 Smart endpoints 智能端点 dumb pipes 轻量级通信 Decentralized Governance 去中心化治理 Infrastructure Automation 基础设施自动化 Design for failure 容错机制设计 Evolutionary Design 演进/迭代 设计 coarser-grained 粗粒度的 一个新架构术语的定义 The term “Microservice Architecture” has sprung up over(涌现出了) the last few years to describe a particular(特定的) way of designing software applications as suites of independently deployable services. 过去几年中出现了“微服务架构”一词，用以描述将软件应用程序设计为可独立部署的服务套件的特定方法。 While there is no precise(精确) definition of this architectural style，there are certain(某些) common characteristics around organization around business capability(业务功能), automated deployment(自动部署), intelligence in the endpoints, and decentralized(分散的，去中心化的) control of languages and data. 虽然没有对这种架构风格的精确定义，但围绕业务功能的组织，自动部署，端点智能以及在编程语言和数据方面进行去中心化的控制方面存在某些共同特征。 “Microservices” - yet another new term on the crowded streets of software architecture. Although our natural inclination is to pass such things by with a contemptuous glance, this bit of terminology describes a style of software systems that we are finding more and more appealing. “微服务” - 在繁多的软件架构术语中又多了一个新的名词。 虽然我们对于这种新的概念打心底里自然是不削一顾的，但这个术语描述了一种对于我们来说越来越有吸引力的软件系统风格。 We’ve seen many projects use this style in the last few years, and results so far have been positive, so much so that for many of our colleagues this is becoming the default style for building enterprise applications. 我们已经看到许多项目在过去几年中使用了这种架构风格，并且都取得了很不错的结果；以至于对于我们的许多同事来说，这已成为构建企业应用程序的默认架构风格了。 Sadly, however, there’s not much information that outlines what the microservice style is and how to do it. 然而，遗憾的是，现在还没有太多信息可以概述微服务是什么，以及我们该如何实现微服务架构。 In short, the microservice architectural style [1] is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. 简单来说，微服务架构风格[1]就是以开发一组小型服务的方式来开发一个独立的应用系统的，每个单体服务都在自己独立的进程中运行，并以HTTP资源API这种轻量级机制进行通信。 These services are built around business capabilities and independently deployable by fully automated deployment machinery. 这些服务围绕业务功能构建，可通过自动化部署机制进行独立部署。 There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies. 这些服务具有最低限度的集中管理，可以用不同的编程语言编写，也可以使用不同的数据存储技术。 To start explaining the microservice style it’s useful to compare it to the monolithic style: a monolithic application built as a single unit. 在开始介绍微服务风格之前，将它与单体应用进行比较是很有用的。 Enterprise Applications are often built in three main parts: a client-side user interface (consisting of HTML pages and javascript running in a browser on the user’s machine) a database (consisting of many tables inserted into a common, and usually relational, database management system), and a server-side application. 企业级应用通常由三个主要部分构成： 客户端用户界面(由在用户机器上的浏览器中运行的HTML页面和javascript组成) 数据库(由插入到公共(通常是关系)数据库管理系统中的许多表组成) 服务器端应用程序。 The server-side application will handle HTTP requests, execute domain logic, retrieve and update data from the database, and select and populate HTML views to be sent to the browser. 服务器端应用负责处理HTTP请求、执行域逻辑、从数据库检索和更新数据，并选择和填充要发送到浏览器的HTML视图。 This server-side application is a monolith - a single logical executable[2]. Any changes to the system involve building and deploying a new version of the server-side application. 这个服务器端应用是一个整体 - 一个可执行的逻辑程序[2]。 对系统的任何更改都涉及构建和部署新版本的服务器端应用程序。 Such a monolithic server is a natural way to approach building such a system.All your logic for handling a request runs in a single process, allowing you to use the basic features of your language to divide up the application into classes, functions, and namespaces.With some care, you can run and test the application on a developer’s laptop, and use a deployment pipeline to ensure that changes are properly tested and deployed into production. 这种单体服务器是构建上述系统的常规方式。所有请求的逻辑处理都运行在单个进程中，允许使用语言的基本特性将应用程序划分为类、函数和命名空间。通过这样一些设计，你可以在开发人员的笔记本电脑上运行和测试应用程序，并使用部署流程平台来确保变更可以被正确地测试然后再将其部署到生产环境中。 You can horizontally scale the monolith by running many instances behind a load-balancer. 最后，通过负载均衡器运行许多实例，已达到将这个单体应用进行横向扩展的目的。 Monolithic applications can be successful, but increasingly people are feeling frustrations with them - especially as more applications are being deployed to the cloud .Change cycles are tied together - a change made to a small part of the application, requires the entire monolith to be rebuilt and deployed. 单体应用架构可以很成功的实现，但是随着越来越多的应用程序被部署到云上时，人们对它们将会越来越感到失望。因为对于单体架构的应用来说，每当对一个小小的功能进行修改时，都会涉及到整个应用的重新构建和部署，实际上这个局部功能的改动是不应该对整个应用造成影响的。 Over time it’s often hard to keep a good modular structure, making it harder to keep changes that ought to only affect one module within that module. Scaling requires scaling of the entire application rather than parts of it that require greater resource. 随着时间的推移，单体应用也很难保持一个良好的模块化结构，因为把一个模块的变更影响控制在该模块内将会变得非常困难。当对系统进行扩展时，不得不扩展整个应用系统，而不是对需要更多资源的部分应用程序进行扩展。 图1：单应用架构和微服务架构 These frustrations have led to the microservice architectural style: building applications as suites of services. 这一系列的问题导致了微服务架构风格产生：以构建一组服务的方式来构建应用系统。 As well as the fact that services are independently deployable and scalable, each service also provides a firm module boundary, even allowing for different services to be written in different programming languages. They can also be managed by different teams . 除了服务是可独立部署和可伸缩的这一事实之外，每个服务还提供了一个可靠的模块边界，甚至允许用不同的编程语言编写不同的服务。它们也可以由不同的团队管理。 We do not claim that the microservice style is novel or innovative, its roots go back at least to the design principles of Unix. But we do think that not enough people consider a microservice architecture and that many software developments would be better off if they used it. 我们并不认为微服务架构风格是新颖或创新的，它的根源至少可以追溯到Unix的设计原则。但是我们认为目前还没有足够多的人考虑微服务体系架构，如果他们都参与使用这个架构风格的话，许多软件的开发将会变得更好。 Characteristics of a Microservice Architecture-微服务架构的特征 We cannot say there is a formal definition of the microservices architectural style, but we can attempt to describe what we see as common characteristics for architectures that fit the label. 我们不能说微服务体系架构风格有一个正式的定义，但是我们可以尝试去描述我们所看到的符合这个标签的体系结构的一些共同特征。 As with any definition that outlines common characteristics, not all microservice architectures have all the characteristics, but we do expect that most microservice architectures exhibit most characteristics. 与任何概述共同特征的定义一样，并非所有的微服务体系架构都具有所有特征，但我们期望常见的微服务都应该有这些特性。 While we authors have been active members of this rather loose community, our intention is to attempt a description of what we see in our own work and in similar efforts by teams we know of. In particular we are not laying down some definition to conform to. 虽然我们作者是这个相当松散的社区的活跃成员，但我们的意图是尝试描述我们在自己的工作中看到的内容，以及我们所知道的团队在类似的工作中所做的工作。特别是，我们不依赖于那些已经明确过的定义。 Componentization via Services-通过服务拆分实现组件化 For as long as we’ve been involved in the software industry, there’s been a desire to build systems by plugging together components, much in the way we see things are made in the physical world. 只要我们参与到软件行业，就一直希望通过将组件集成在一起来构建系统，就像我们在物理世界中看到事物的方式一样。 During the last couple of decades we’ve seen considerable progress with large compendiums of common libraries that are part of most language platforms. 在过去的几十年中，我们已经看到了作为大多数语言平台一部分的公共库，已经在大量组合方面取得了相当大的进展。 When talking about components we run into the difficult definition of what makes a component. Our definition is that a component is a unit of software that is independently replaceable and upgradeable. 在讨论组件时，我们遇到了一个困惑是组件到底是什么。我们的定义是，组件一个可独立替换和升级的软件单元。 Microservice architectures will use libraries, but their primary way of componentizing their own software is by breaking down into services. 微服务架构会使用库，但他们将自己的软件组件化的主要方式是把它拆分成服务 We define libraries as components that are linked into a program and called using in-memory function calls, while services are out-of-process components who communicate with a mechanism such as a web service request, or remote procedure call. (This is a different concept to that of a service object in many OO programs [3].) 我们将库定义为链接到程序并使用内存内函数调用的组件，而服务是进程外组件，它们通过诸如web服务请求或远程过程调用之类的机制进行通信。（这与许多面向对象程序中的服务对象的概念不同[3]。） One main reason for using services as components (rather than libraries) is that services are independently deployable. 将服务用作组件（而不是库）的一个主要原因是服务可以独立部署。 If you have an application [4] that consists of a multiple libraries in a single process, a change to any single component results in having to redeploy the entire application. 如果您在单个进程中有一个由多个库组成的应用程序[4]，则对任何单个组件的更改都会导致必须重新部署整个应用程序。 But if that application is decomposed into multiple services, you can expect many single service changes to only require that service to be redeployed. 但是，如果将该应用程序分解为多个服务，那你只需要重新部署那个改变的服务就可以。 That’s not an absolute, some changes will change service interfaces resulting in some coordination, but the aim of a good microservice architecture is to minimize these through cohesive service boundaries and evolution mechanisms in the service contracts. 但是这也不是绝对的，比如一些更改将会更改服务接口，从而导致一些协调问题，但是一个好的微服务体系结构的目标是通过服务契约中的内聚服务边界和演进机制将这些更改最小化。 Another consequence of using services as components is a more explicit component interface.Most languages do not have a good mechanism for defining an explicit Published Interface.Often it’s only documentation and discipline that prevents clients breaking a component’s encapsulation, leading to overly-tight coupling between components.Services make it easier to avoid this by using explicit remote call mechanisms. 将服务用作组件的另一个结果是将拥有更清晰的组件接口。大多数语言都没有定义显式发布接口的良好机制。通常只有文档和规则的说明来防止客户端破坏组件的封装，避免组件之间的耦合过于紧密。但是通过使用显式远程调用机制，则更容易避免这种情况。 Using services like this does have downsides. Remote calls are more expensive than in-process calls, and thus remote APIs need to be coarser-grained, which is often more awkward to use. 但是这种方式也有不足的地方。主要是远程调用比进程内调用更昂贵，因此远程api需要是粗粒度的，但这会比较难用。 If you need to change the allocation of responsibilities between components, such movements of behavior are harder to do when you’re crossing process boundaries. 如果您需要更改组件之间的职责分配，那么当你需要跨进程时，这种行为的迁移将更加困难。 At a first approximation, we can observe that services map to runtime processes, but that is only a first approximation. 一种可能是，我们可以观察到服务映射到运行时进程上，但这只是一种可能。 A service may consist of multiple processes that will always be developed and deployed together, such as an application process and a database that’s only used by that service. 服务可以由多个进程组成，这些进程可以同时开发和部署，例如一个应用程序进程和一个只能由这个服务使用的数据库。 Organized around Business Capabilities-围绕业务功能组织 When looking to split a large application into parts, often management focuses on the technology layer, leading to UI teams, server-side logic teams, and database teams. 当希望将大型应用程序分解为多个模块时，管理通常关注于技术层，重要的包括UI团队、服务器端逻辑团队和数据库团队。 When teams are separated along these lines, even simple changes can lead to a cross-team project taking time and budgetary approval. 当团队按照这些原则分开时，即使是简单的更改也可能涉及到跨团队沟通，那么这样项目就会需要增加时间和预算审批等成本。 A smart team will optimise around this and plump for the lesser of two evils - just force the logic into whichever application they have access to. Logic everywhere in other words. This is an example of Conway’s Law[5] in action. 一个优秀的团队将围绕这一点进行改善，并选择两害相权取其轻——只需将逻辑强制应用到他们能够访问的任何应用程序中。换句话说，逻辑无处不在。这是康威定律[5]的一个例子。 Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization’s communication structure.任何设计系统(广义定义)的组织都会产生一个结构是组织通信结构副本的设计。– Melvyn Conway, 1967 图2:康韦定律的作用 The microservice approach to division is different, splitting up into services organized around business capability. 划分微服务方法是不同的，但更倾向于围绕业务功能的组织来进行服务划分。 Such services take a broad-stack implementation of software for that business area, including user-interface, persistant storage, and any external collaborations. 这些服务在商业领域软件上有广泛实施，包括用户界面、持久性存储和任何外部协作。 Consequently the teams are cross-functional, including the full range of skills required for the development: user-experience, database, and project management. 因此，团队是跨职能的，包括开发所需的全方位技能:用户体验、数据库和项目管理。 图3:由团队边界加强的服务边界 One company organised in this way is www.comparethemarket.com. Cross functional teams are responsible for building and operating each product and each product is split out into a number of individual services communicating via a message bus. www.comparethemarket.com 就是采样这样组织形式的一家公司。跨职能团队负责同时构建和运维每个产品，每个产品都被划分为许多单个服务，服务间再通过消息总线进行通信。 Large monolithic applications can always be modularized around business capabilities too, although that’s not the common case.Certainly we would urge a large team building a monolithic application to divide itself along business lines. 大型集中式应用程序也可以围绕业务功能进行模块划分，尽管这种情况并不常见。当然，我们会敦促构建完整应用程序的大型团队沿着业务线进行自我划分。 The main issue we have seen here, is that they tend to be organised around too many contexts. 我们在这里看到的主要问题是，它们往往围绕太多的上下文背景进行组织（依赖太多，包袱太重）。 If the monolith spans many of these modular boundaries it can be difficult for individual members of a team to fit them into their short-term memory.Additionally we see that the modular lines require a great deal of discipline to enforce. The necessarily more explicit separation required by service components makes it easier to keep the team boundaries clear. 如果整个应用会跨越许多这样的模块边界时，这对于团队中的一些成员来说也很难短期内可以掌握。此外，我们看到模块化开发需要大量的规范来遵守执行。但是对于服务组件这种需要进行明确的业务划分的要求的情况下，可以使得保持团队职能边界清晰变得更加容易。 Products not Projects-产品不是项目 Most application development efforts that we see use a project model: where the aim is to deliver some piece of software which is then considered to be completed. On completion the software is handed over to a maintenance organization and the project team that built it is disbanded. 我们看到的大多数应用程序开发工作都使用这样的项目模式：目标是交付一个他们认为完成的软件。接着，软件被移交给运维团队，项目开发团队被解散。 Microservice proponents tend to avoid this model, preferring instead the notion that a team should own a product over its full lifetime. 微服务的支持者倾向于避免这种模式，他们更希望团队应该负责产品的整个生命周期。 A common inspiration for this is Amazon’s notion of “you build, you run it” where a development team takes full responsibility for the software in production.This brings developers into day-to-day contact with how their software behaves in production and increases contact with their users, as they have to take on at least some of the support burden. 一个常见的灵感是Amazon的“你构建，你运行它”的概念，在这个概念中，开发团队需要对产品承担全部责任。这使得开发人员能够在日常中关注到软件运行情况，并增加与用户的联系，同时必须承担一些支持工作。 The product mentality, ties in with the linkage to business capabilities. Rather than looking at the software as a set of functionality to be completed, there is an on-going relationship where the question is how can software assist its users to enhance the business capability. 产品的理念，与业务能力联系在一起。与其将软件看作一组要完成的功能，还不如将软件看作是一种持续的关系，在这种关系中，软件是如何帮助用户增强业务能力。 There’s no reason why this same approach can’t be taken with monolithic applications, but the smaller granularity of services can make it easier to create the personal relationships between service developers and their users. 没有理由不能在单体应用程序中采用相同的方法，但是服务粒度越小，就越容易在服务开发人员和用户之间创建个人关系。 Smart endpoints and dumb pipes-智能端点和轻量级通信 When building communication structures between different processes, we’ve seen many products and approaches that stress putting significant smarts into the communication mechanism itself. 在构建不同进程之间的通信结构时，我们看到许多产品和方法都强调在通信机制本身中投入大量的方案。 A good example of this is the Enterprise Service Bus (ESB), where ESB products often include sophisticated facilities for message routing, choreography, transformation, and applying business rules. 这方面的一个很好的例子是企业服务总线(ESB)，ESB产品通常包含用于消息路由、编排、转换和应用业务规则的复杂工具。 The microservice community favours an alternative approach: smart endpoints and dumb pipes. 在微服务社区则支持另一种方法:智能端点和轻量级通信。 Applications built from microservices aim to be as decoupled and as cohesive as possible - they own their own domain logic and act more as filters in the classical Unix sense - receiving a request, applying logic as appropriate and producing a response. 使用微服务构建的应用程序旨在尽可能地解耦和内聚 - 采用独立的业务逻辑，表现的更像经典Unix意义上的过滤器一样，接受请求、处理业务逻辑，然后返回响应。 These are choreographed using simple RESTish protocols rather than complex protocols such as WS-Choreography or BPEL or orchestration by a central tool. 它们更喜欢简单的REST风格，而不是使用复杂的协议，如WS-Choreography、BPEL或集中式框架。 The two protocols used most commonly are HTTP request-response with resource API’s and lightweight messaging[8]. 最常用的两种协议是带有资源API的HTTP请求-响应协议和轻量级消息传递[8]协议。 第一个是最好的描述是:善于利用网络，而不是限制。 Be of the web, not behind the web– Ian Robinson Microservice teams use the principles and protocols that the world wide web (and to a large extent, Unix) is built on. Often used resources can be cached with very little effort on the part of developers or operations folk. 微服务团队使用构建在互联网(很大程度上是Unix)上的原则和协议。开发人员或操作人员只需很少的精力就可以缓存经常使用的资源。 The second approach in common use is messaging over a lightweight message bus. The infrastructure chosen is typically dumb (dumb as in acts as a message router only) - simple implementations such as RabbitMQ or ZeroMQ don’t do much more than provide a reliable asynchronous fabric - the smarts still live in the end points that are producing and consuming messages; in the services. 第二种常用的方式是通过轻量级消息总线进行消息传递。所选择的基础设施通常是单一的(只负责消息路由)——像RabbitMQ或者ZeroMQ这样的简单的实现，连可靠的异步机制都没有提供——导致仍然需要依赖产生或者消费消息的终端或者服务来处理这类问题。 In a monolith, the components are executing in-process and communication between them is via either method invocation or function call. The biggest issue in changing a monolith into microservices lies in changing the communication pattern. A naive conversion from in-memory method calls to RPC leads to chatty communications which don’t perform well. Instead you need to replace the fine-grained communication with a coarser -grained approach. 在单体应用中，组件在进程内执行，它们之间的通信要么通过方法调用要么通过回调函数。将单体架构变为微服务架构的最大问题在于改变通信模式。 从内存中的方法调用到RPC调用的简单转换会导致通信性能的下降。因此，你需要用更粗粒度的方法替换细粒度的通信。 Decentralized Governance-分散治理 One of the consequences of centralised governance is the tendency to standardise on single technology platforms. Experience shows that this approach is constricting - not every problem is a nail and not every solution a hammer. We prefer using the right tool for the job and while monolithic applications can take advantage of different languages to a certain extent, it isn’t that common. 集中治理的优势在于可以在单一技术平台上实现标准化。经验表明，这种方法是有局限性的——不是每个问题都是很棘手，也不是每个解决方案都是万能的。我们更喜欢使用适合这项工作的工具，虽然单体应用程序可以在一定程度上利用不同的语言，但这种情况并不常见。 Splitting the monolith’s components out into services we have a choice when building each of them. You want to use Node.js to standup a simple reports page? Go for it. C++ for a particularly gnarly near-real-time component? Fine. You want to swap in a different flavour of database that better suits the read behaviour of one component? We have the technology to rebuild him. 把整体式框架中的组件，拆分成不同的服务，这样构建它们时就会有更多的选择性。 您想使用Node.js站立一个简单的报告页面吗？ 去吧。 C ++是否适用于特别近乎实时的组件？做吧。 您想要交换不同风格的数据库，以更好地适应一个组件的读取行为？ 我们同样有重构它的技术。 Of course, just because you can do something, doesn’t mean you should - but partitioning your system in this way means you have the option. 当然，仅仅因为你可以做一些事情，并不意味着你就应该这么做——但是以这种方式对系统进行划分意味着你可以选择。 Teams building microservices prefer a different approach to standards too. Rather than use a set of defined standards written down somewhere on paper they prefer the idea of producing useful tools that other developers can use to solve similar problems to the ones they are facing.These tools are usually harvested from implementations and shared with a wider group, sometimes, but not exclusively using an internal open source model. Now that git and github have become the de facto version control system of choice, open source practices are becoming more and more common in-house . 与其选用一组写在纸上已经定义好的标准，他们更喜欢编写一些有用的工具，来让其他开发者能够使用，以便解决那些和他们所面临的问题相似的问题。这些工具通常源自他们的微服务实施过程，并且被分享到更大规模的组织中，这种分享有时会使用内部开源的模式来进行。现在，git和github已经成为事实上的首选版本控制系统。在企业内部，开源的做法也正在变得越来越普遍。 Netflix is a good example of an organisation that follows this philosophy. Sharing useful and, above all, battle-tested code as libraries encourages other developers to solve similar problems in similar ways yet leaves the door open to picking a different approach if required. Shared libraries tend to be focused on common problems of data storage, inter-process communication and as we discuss further below, infrastructure automation. Netflix公司是遵循上述理念的好例子。将实用且经过实战检验的代码以软件库的形式共享出来，能鼓励其他开发人员以相似的方式来解决相似的问题，当然也为在需要的时候选用不同的方案留了一扇门。共享软件库往往集中在解决这样的常见问题，即数据存储、进程间的通信和下面要进一步讨论的基础设施的自动化。 For the microservice community, overheads are particularly unattractive. That isn’t to say that the community doesn’t value service contracts. Quite the opposite, since there tend to be many more of them. It’s just that they are looking at different ways of managing those contracts. Patterns like Tolerant Reader and Consumer-Driven Contracts are often applied to microservices. These aid service contracts in evolving independently. Executing consumer driven contracts as part of your build increases confidence and provides fast feedback on whether your services are functioning. Indeed we know of a team in Australia who drive the build of new services with consumer driven contracts. They use simple tools that allow them to define the contract for a service. This becomes part of the automated build before code for the new service is even written. The service is then built out only to the point where it satisfies the contract - an elegant approach to avoid the ‘YAGNI’[9]dilemma when building new software. These techniques and the tooling growing up around them, limit the need for central contract management by decreasing the temporal coupling between services. 对于微服务社区来说，管理费用特别缺乏吸引力。 这并不是说社区不重视服务合同。 恰恰相反，因为往往会有更多。只是他们正在寻找管理这些合同的不同方式。像“容错读取”和“消费者驱动的契约”这样的模式，经常被运用到微服务中。这些都有助于服务契约进行独立演进。将执行“ 消费者驱动的契约 ”做为软件构建的一部分，能增强开发团队的信心，并提供所依赖的服务是否正常工作的快速反馈。实际上，我们了解到一个在澳大利亚的团队就是使用“ 消费者驱动的契约 ”来驱动构建多个新服务的。他们使用了一些简单的工具，来针对每一个服务定义契约。甚至在新服务的代码编写之前，这件事就已经成为自动化构建的一部分了。接下来服务仅被构建到刚好能满足契约的程度——这是一个在构建新软件时避免YAGNI [9] 困境的优雅方法。这些技术和工具在契约周边生长出来，由于减少了服务之间在时域(temporal)上的耦合，从而抑制了对中心契约管理的需求。 Perhaps the apogee of decentralised governance is the build it / run it ethos popularised by Amazon. Teams are responsible for all aspects of the software they build including operating the software 24/7. Devolution of this level of responsibility is definitely not the norm but we do see more and more companies pushing responsibility to the development teams. Netflix is another organisation that has adopted this ethos[11]. Being woken up at 3am every night by your pager is certainly a powerful incentive to focus on quality when writing your code. These ideas are about as far away from the traditional centralized governance model as it is possible to be. 也许分散治理治理技术的极盛时期，就是亚马逊的“你构建，你运行”的理念开始普及的时候。 每个团队负责他们构建的软件的全生命周期，包括持续的软件的运维。 把运维的这种能力放到团队的做法目前还不是主流的，但我们确实看到越来越多的公司将运维的职责推向开发团队。 Netflix是另一个采用这种模式的组织[11]。 如果你不想每天凌晨3点被喊起来去改bug，那么你就该在编写代码时投入更多的精力和时间。 但是这些想法与传统的集中治理模式相差甚远。 Decentralized Data Management-分散的数据管理 Decentralization of data management presents in a number of different ways. At the most abstract level, it means that the conceptual model of the world will differ between systems. This is a common issue when integrating across a large enterprise, the sales view of a customer will differ from the support view. Some things that are called customers in the sales view may not appear at all in the support view. Those that do may have different attributes and (worse) common attributes with subtly different semantics. 分散化的数据管理以多种不同的方式呈现。 在最抽象的层面上来看的话，就意味着各个系统对客观世界所构建的概念模型是彼此各不相同的。 这是在大型企业中集成时的常见问题，比如对于客户来说，销售视角和支持视角肯定是不同的。 销售视角中客户的某些内容可能根本不会出现在支持视角中。即使在两个视角中都能看到的事物，那么各自关注的核心信息也是不同的。极端情况下，甚至两个视角中具有相同属性的事物，或许在语义上也会有细的差距。 This issue is common between applications, but can also occur _within_applications, particular when that application is divided into separate components. A useful way of thinking about this is the Domain-Driven Design notion of Bounded Context. DDD divides a complex domain up into multiple bounded contexts and maps out the relationships between them. This process is useful for both monolithic and microservice architectures, but there is a natural correlation between service and context boundaries that helps clarify, and as we describe in the section on business capabilities, reinforce the separations. 上述问题在不同的应用程序之间经常出现，当然应用程序内部也会出现，尤其是当一个应用程序被分成不同组件的情况下。思考这类问题的一个可靠的方法，就是使用领域驱动设计（Domain-Driven Design, DDD）中的“限界上下文”的概念。DDD将一个复杂的领域划分为多个限界上下文，并且将其相互之间的关系用图画出来。这一划分过程对于单体架构和微服务架构两者都是有用的，而且就像前面有关“业务功能”一节中所讨论的那样，在服务和各个限界上下文之间所存在的自然的联动关系，能有助于澄清和强化这种划分。 As well as decentralizing decisions about conceptual models, microservices also decentralize data storage decisions. While monolithic applications prefer a single logical database for persistant data, enterprises often prefer a single database across a range of applications - many of these decisions driven through vendor’s commercial models around licensing. Microservices prefer letting each service manage its own database, either different instances of the same database technology, or entirely different database systems - an approach called Polyglot Persistence. You can use polyglot persistence in a monolith, but it appears more frequently with microservices. 除了关于概念模型的分散决策之外，微服务还分散了数据存储决策。 虽然单体应用程序通常都是使用单个逻辑数据库来存储持久性数据，但企业往往喜欢一系列单体应用共用一个单独的数据库 - 其中许多决策是通过供应商围绕许可的商业模型来实现的（供应商的版权商业模式所驱动）。 微服务体系中更偏向让每个服务实例管理自己的数据库，可以是相同数据库技术的不同实例，也可以是完全不同的数据库系统 – 这种方法称为 Polyglot Persistence（多语言持久化）。在一个单体系统中也能使用多语种持久化，但它在微服务中更常出现。 Decentralizing responsibility for data across microservices has implications for managing updates. The common approach to dealing with updates has been to use transactions to guarantee consistency when updating multiple resources. This approach is often used within monoliths. 跨微服务分散数据责任对管理更新具有影响。处理软件更新的常用方法，是当更新多个资源的时候，需要使用事务来保证一致性。这种方法经常在单块系统中被采用。 Using transactions like this helps with consistency, but imposes significant temporal coupling, which is problematic across multiple services. Distributed transactions are notoriously difficult to implement and as a consequence microservice architectures emphasize transactionless coordination between services, with explicit recognition that consistency may only be eventual consistency and problems are dealt with by compensating operations. 通过使用事务，有助于保持数据一致性。但对时间的消耗是严重的，而当在多个服务之间处理事务时也会出现一致性问题。众所周知，分布式事务很难实现，因此微服务架构强调服务间事务协调，明确认识到一致性可能只是最终的一致性及通过补偿操作来处理问题。 Choosing to manage inconsistencies in this way is a new challenge for many development teams, but it is one that often matches business practice. Often businesses handle a degree of inconsistency in order to respond quickly to demand, while having some kind of reversal process to deal with mistakes. The trade-off is worth it as long as the cost of fixing mistakes is less than the cost of lost business under greater consistency. 对于许多开发团队来说，选择以这种方式管理数据的“不一致性”问题是一个新的挑战，但是这又是一种非常常见的业务实践场景。为了对需求做出快速反应，企业通常会允许一定程度上的数据“不一致性”，但同时也会采用一些恢复的进程来处理这种错误 。只要业务上处理强一致性成本比处理错误的成本少时，那么这种“ 不一致性”地管理数据的权衡就是值得的。 Infrastructure Automation-基础设施高度自动化 Infrastructure automation techniques have evolved enormously over the last few years - the evolution of the cloud and AWS in particular has reduced the operational complexity of building, deploying and operating microservices. 在过去几年里，基础设施自动化技术有了很大的发展——云计算和AWS的发展降低了构建、部署和运维微服务的复杂性。 Many of the products or systems being build with microservices are being built by teams with extensive experience of Continuous Delivery and it’s precursor, Continuous Integration. Teams building software this way make extensive use of infrastructure automation techniques. This is illustrated in the build pipeline shown below. 许多使用微服务构建的产品或系统都是由具有大量 持续交付与其前身持续集成 经验的团队构建的。以这种方式构建软件的团队广泛使用了基础设施自动化技术。如下图的构建流水线所示： 图5:基本构建流程 Since this isn’t an article on Continuous Delivery we will call attention to just a couple of key features here. We want as much confidence as possible that our software is working, so we run lots of automated tests. Promotion of working software ‘up’ the pipeline means we automate deployment to each new environment. 由于这不是一篇关于持续交付的文章，我们将在这里只关注几个关键特性。我们希望我们的软件能够正常工作，所以我们运行了大量的自动化测试。让可工作的软件达到“晋级”(Promotion)状态从而“推上”流水线，就意味着可以在 每一个新的环境中，对软件进行 自动化部署 。 A monolithic application will be built, tested and pushed through these environments quite happlily. It turns out that once you have invested in automating the path to production for a monolith, then deploying _more_applications doesn’t seem so scary any more. Remember, one of the aims of CD is to make deployment boring, so whether its one or three applications, as long as its still boring it doesn’t matter[12]. 对于单体应用来说，可以轻松的在上述的各个环境中进行构建、测试和发布。其结果是，一旦投入到自动化平台， 那么部署更多的应用系统似乎就不再可怕。记住，持续交付的目的之一，是让“部署”工作变得“无聊”。所以不管是一个还是三个应用系统，只要是部署工作，就依旧很“无聊”，那么就没什么可担心的了 [12] 。 Another area where we see teams using extensive infrastructure automation is when managing microservices in production. In contrast to our assertion above that as long as deployment is boring there isn’t that much difference between monoliths and microservices, the operational landscape for each can be strikingly different. 另一个方面，我们发现使用微服务的团队更加依赖于基础设施的自动化。与前面我们对比单体系统和微服务所说的正相反，只要部署工作很无聊，那么在这一点上单块系统和微服务就没什么区别。然而，两者在运维领域的情况却截然不同。 图6:模块部署通常是不同的 Design for failure-“容错”设计 A consequence of using services as components, is that applications need to be designed so that they can tolerate the failure of services. Any service call could fail due to unavailability of the supplier, the client has to respond to this as gracefully as possible. This is a disadvantage compared to a monolithic design as it introduces additional complexity to handle it. The consequence is that microservice teams constantly reflect on how service failures affect the user experience. Netflix’s Simian Army induces failures of services and even datacenters during the working day to test both the application’s resilience and monitoring. 使用各个微服务来替代组件，其结果是各个应用程序需要设计成能够容忍这些服务所出现的故障。如果服务提供方不可用，那么任何对该服务的调用都会出现故障。客户端要尽可能优雅地应对这种情况。与单体应用设计相比，这是一个劣势。因为这会引人额外的复杂性来处理这种情况。这需要微服务团队要时刻考虑到服务故障情况下的用户体验。Netflix公司所研发的开源测试工具Simian Army，可以为每个应用的服务及数据中心提供日常故障检测和恢复。 This kind of automated testing in production would be enough to give most operation groups the kind of shivers usually preceding a week off work. This isn’t to say that monolithic architectural styles aren’t capable of sophisticated monitoring setups - it’s just less common in our experience. 这种在生产环境中所进行的自动化测试，能足以让大多数运维组织兴奋得浑身颤栗，就像在一周的长假即将到来前那样。这并不是说单体架构风格不能构建先进的监控系统——只是根据我们的经验，这在单体系统中并不常见罢了。 Since services can fail at any time, it’s important to be able to detect the failures quickly and, if possible, automatically restore service. Microservice applications put a lot of emphasis on real-time monitoring of the application, checking both architectural elements (how many requests per second is the database getting) and business relevant metrics (such as how many orders per minute are received). Semantic monitoring can provide an early warning system of something going wrong that triggers development teams to follow up and investigate. 因为每个服务都可能在任何时候发生故障，所以下面两件事就变得很重要，即 快速故障检测 和 自动恢复。各个微服务的应用都将大量的精力放到了应用程序的实时监控上，来检查“架构元素指标”（例如数据库每秒收到多少请求）和“业务相关指标”（例如系统每分钟收到多少订单）。当系统某个地方出现问题，监控系统能提供一个预警，来触发开发团队进行后续的跟进和调查工作。 This is particularly important to a microservices architecture because the microservice preference towards choreography and event collaboration leads to emergent behavior. While many pundits praise the value of serendipitous emergence, the truth is that emergent behavior can sometimes be a bad thing. Monitoring is vital to spot bad emergent behavior quickly so it can be fixed. 这对于一个微服务架构是非常重要的，因为微服务之间交互通信随时都可能出现一些紧急的意外情况。尽管许多权威人士对于突发情况的价值持积极态度，但事实上，突发情况有时可能会酿成大的灾难。在能够快速发现有坏处的突发情况并进行修复的方面，监控是至关重要的。 Monoliths can be built to be as transparent as a microservice - in fact, they should be. The difference is that you absolutely need to know when services running in different processes are disconnected. With libraries within the same process this kind of transparency is less likely to be useful. 单体系统也能构建像微服务那样来实现透明的一套监控系统——实际上，它们也应该如此。差别是，绝对需要知道那些运行在不同进程中的服务，在何时断掉了。而如果在同一个进程内使用软件库的话，这种透明的监控系统就用处不大了。 Microservice teams would expect to see sophisticated monitoring and logging setups for each individual service such as dashboards showing up/down status and a variety of operational and business relevant metrics. Details on circuit breaker status, current throughput and latency are other examples we often encounter in the wild. 微服务团队希望在每一个单独的服务中，都能看到良好的监控和日志记录装置。例如显示“运行/宕机”状态的仪表盘，和各种运维和业务相关的指标。另外我们经常在工作中会碰到这样一些细节，即断路器的状态、当前的吞吐率和延迟，以及其他一些例子。 Evolutionary Design-“演进式”设计 Microservice practitioners, usually have come from an evolutionary design background and see service decomposition as a further tool to enable application developers to control changes in their application without slowing down change. Change control doesn’t necessarily mean change reduction - with the right attitudes and tools you can make frequent, fast, and well-controlled changes to software. 微服务的从业者们，通常具有演进式设计的背景，他们把服务分解成进一步的工具，以达到可以让应用开发者在不改变速度情况下，控制他们应用的需求变更。变更控制并不一定意味着要减少变化——在正确的方式和工具的帮助下，能在软件中让变更发生得频繁、快速且有良好的控制。 Whenever you try to break a software system into components, you’re faced with the decision of how to divide up the pieces - what are the principles on which we decide to slice up our application? The key property of a component is the notion of independent replacement and upgradeability[13] - which implies we look for points where we can imagine rewriting a component without affecting its collaborators. Indeed many microservice groups take this further by explicitly expecting many services to be scrapped rather than evolved in the longer term. 每当试图要将软件系统分解为各个组件时，就会面临这样的问题，即如何进行切分——我们决定切分应用系统时应该遵循的原则是什么？首要的因素，组件可以被独立替换和更新的 [13] ——这意味着，需要寻找这些点，即想象着能否在其中一个点上重写该组件，而无须影响该组件的其他合作组件。事实上，许多微服务团队考虑的更多的是，如何明确地预期许多服务将来会报废，而不是守着这些服务做长期迭代。 The Guardian website is a good example of an application that was designed and built as a monolith, but has been evolving in a microservice direction. The monolith still is the core of the website, but they prefer to add new features by building microservices that use the monolith’s API. This approach is particularly handy for features that are inherently temporary, such as specialized pages to handle a sporting event. Such a part of the website can quickly be put together using rapid development languages, and removed once the event is over. We’ve seen similar approaches at a financial institution where new services are added for a market opportunity and discarded after a few months or even weeks. Guardian网站就是这方面的一个优秀的例子。它初期被设计和构建成一个单体架构应用，然而它已经开始向微服务方向进行迭代演进了。原先的单体系统依旧是该网站的核心，但是在添加新特性时，他们愿意以构建微服务的方式来进行添加，而这些微服务会去调用原先那个单体系统的API。当在开发那些本身就带有临时性特点的新特性时， 这种方法就特别方便，例如开发那些报道一个体育赛事的专门页面。当使用一些快速的开发语言时，像这样的网站页面就能被快速地整合起来。而一旦赛事结束，这样页面就可以被删除。在一个金融机构中，我们已经看到了一些相似的做法，即针对一个市场机会，一些新的服务可以被添加进来。然后在几个月甚至几周之后，这些新服务就作废了。 This emphasis on replaceability is a special case of a more general principle of modular design, which is to drive modularity through the pattern of change [14]. You want to keep things that change at the same time in the same module. Parts of a system that change rarely should be in different services to those that are currently undergoing lots of churn. If you find yourself repeatedly changing two services together, that’s a sign that they should be merged. 这种强调可更换性的特点，是模块化设计一般性原则的一个特例，需求变更通过进行模块化的方式实现。大家都愿意将那些能在同时发生变化的东西，放到同一个模块中。系统中那些很少发生变化的部分，应该被放到不同的服务中，以区别于那些当前正在经历大量变动(churn)的部分。如果发现需要同时反复变更两个服务时，这就是它们两个需要被合并的一个信号。 Putting components into services adds an opportunity for more granular release planning. With a monolith any changes require a full build and deployment of the entire application. With microservices, however, you only need to redeploy the service(s) you modified. This can simplify and speed up the release process. The downside is that you have to worry about changes to one service breaking its consumers. The traditional integration approach is to try to deal with this problem using versioning, but the preference in the microservice world is to only use versioning as a last resort. We can avoid a lot of versioning by designing services to be as tolerant as possible to changes in their suppliers. 把组件改成服务，增加了作出更加精细的软件发布计划的机会。对于一个单体系统，任何变化都需要做一次整个应用系统的全量构建和部署。然而，对于微服务来说，只需要重新部署修改过的那些服务就够了。这能简化并加快发布过程。但缺点是：必须要考虑当一个服务发生变化时，依赖它并对其进行消费的其他服务可能将无法工作。传统的集成方法是试图使用版本化来解决这个问题。但在微服务世界中，大家更喜欢将版本化作为最后万不得已的手段来使用 。我们需要在设计服务时尽可能的容忍供应商的变更，以避免提供多个版本。 Are Microservices the Future?-未来的方向是“微服务”吗？ Our main aim in writing this article is to explain the major ideas and principles of microservices. By taking the time to do this we clearly think that the microservices architectural style is an important idea - one worth serious consideration for enterprise applications. We have recently built several systems using the style and know of others who have used and favor this approach. 我们写这篇文章的主要目的是来解释有关微服务的主要思路和原则。在花了一点时间做了这件事后，我们清楚地认识到，微服务架构风格是一个重要的架构方案——在研发企业应用系统时，值得对它进行认真考虑。我们最近已经使用这种风格构建了一些系统，并且了解到其他一些团队也在使用并支持这种方法。 Those we know about who are in some way pioneering the architectural style include Amazon, Netflix, The Guardian, the UK Government Digital Service, realestate.com.au, Forward and comparethemarket.com. The conference circuit in 2013 was full of examples of companies that are moving to something that would class as microservices - including Travis CI. In addition there are plenty of organizations that have long been doing what we would class as microservices, but without ever using the name. (Often this is labelled as SOA - although, as we’ve said, SOA comes in many contradictory forms. [15]) 我们所了解到的实践先驱包括：亚马逊、Netflix、The Guardian、The UK Government Digital Service、realestate.com.au、Forward和comparethemarket.com。在2013年的技术大会圈子里充满了各种各样的正在转向微服务的公司案例——包括Travis CI。另外还有大量的组织，它们长期以来一直在做着我们可以归类为微服务的产品，却从未使用过这个名字（这通常被标记为SOA—— 尽管正如我们所说，SOA会表现出各种自相矛盾的形式 [15] ）。 Despite these positive experiences, however, we aren’t arguing that we are certain that microservices are the future direction for software architectures. While our experiences so far are positive compared to monolithic applications, we’re conscious of the fact that not enough time has passed for us to make a full judgement. 尽管有这些正面的经验，然而并不是说我们确信微服务是软件架构的未来的方向。尽管到目前为止，与单体应用系统相比，我们对于所经历过的微服务架构的评价都是积极的，但是我们也意识到这样的事实，即能供我们做出完整判断的时间还不够长。 Often the true consequences of your architectural decisions are only evident several years after you made them. We have seen projects where a good team, with a strong desire for modularity, has built a monolithic architecture that has decayed over the years. Many people believe that such decay is less likely with microservices, since the service boundaries are explicit and hard to patch around. Yet until we see enough systems with enough age, we can’t truly assess how microservice architectures mature. 通常，架构决策的真正效果只有在做出这些决策几年之后才会表现出来。我们已经看到由带着强烈的模块化愿望的优秀团队所做的一些项目，最终却构建出一个单体架构，并在几年之内不断腐化。许多人认为，这种腐化不太可能与微服务有关，因为服务的边界是明确的，很难往里面塞新的东西。但是，当我们还没看到足够多的系统运行足够长时间时，我们不能肯定微服务构架是成熟的。 There are certainly reasons why one might expect microservices to mature poorly. In any effort at componentization, success depends on how well the software fits into components. It’s hard to figure out exactly where the component boundaries should lie. Evolutionary design recognizes the difficulties of getting boundaries right and thus the importance of it being easy to refactor them. But when your components are services with remote communications, then refactoring is much harder than with in-process libraries. Moving code is difficult across service boundaries, any interface changes need to be coordinated between participants, layers of backwards compatibility need to be added, and testing is made more complicated. 有人觉得微服务或许很难成熟起来，这当然是有原因的。在组件化上所做的任何工作的是否有效，取决于软件与组件的匹配程度。要想准确地搞清楚某个组件的边界的位置是一件困难的事情。 演进式设计承认难以对边界进行正确定位，所以它将工作的重点放到了易于重构上。但是当各个组件成为各个进行远程通信的服务后，比起在单一进程内进行各个软件库之间的调用，重构就变得更加困难。跨服务边界的代码迁移也会变得困难起来。接口的任何变更，都需要在其各个参与者之间进行协调，向后兼容的层次也需要被添加进来，测试也会变得更加复杂。 Another issue is If the components do not compose cleanly, then all you are doing is shifting complexity from inside a component to the connections between components. Not just does this just move complexity around, it moves it to a place that’s less explicit and harder to control. It’s easy to think things are better when you are looking at the inside of a small, simple component, while missing messy connections between services. 另一个问题在于，如果组件并没有清晰的划分，那么这项工作的复杂性将会从组件内部转向组件间。后果是，不仅仅是将复杂性搬了家，它还将复杂性变得不可控。在一个小的、简单的组件内部考虑事情是很容易的，但也不能忽视了服务之间复杂的连接。 Finally, there is the factor of team skill. New techniques tend to be adopted by more skillful teams. But a technique that is more effective for a more skillful team isn’t necessarily going to work for less skillful teams. We’ve seen plenty of cases of less skillful teams building messy monolithic architectures, but it takes time to see what happens when this kind of mess occurs with microservices. A poor team will always create a poor system - it’s very hard to tell if microservices reduce the mess in this case or make it worse. 最后，对于团队技能也是一个因素。新的技术倾向于被掌握更多的技能的团队使用。适用于技术背景好的团队的技术，不一定适用于一个技术薄弱的团队。我们已经看到大量这样的案例，那些技术薄弱的团队构建出了杂乱的单体架构。当这种杂乱发生到微服务身上时，会出现什么情况？这需要花时间来观察 。一个糟糕的团队，总会构建一个糟糕的系统——在这种情况下，很难讲微服务究竟是减少了杂乱，还是让事情变得更糟。 One reasonable argument we’ve heard is that you shouldn’t start with a microservices architecture. Instead begin with a monolith, keep it modular, and split it into microservices once the monolith becomes a problem. (Although this advice isn’t ideal, since a good in-process interface is usually not a good service interface.) 我们听到一个合理的说法，是说不要一上来就以微服务架构做为起点。相反，要用一个单体系统做为起点，并保持其模块化。当这个单体系统出现了问题后，再将其分解为微服务。（尽管这个建议并不理想，因为一个良好的单一进程内的接口，通常不是一个良好的服务接口） So we write this with cautious optimism. So far, we’ve seen enough about the microservice style to feel that it can be a worthwhile road to tread. We can’t say for sure where we’ll end up, but one of the challenges of software development is that you can only make decisions based on the imperfect information that you currently have to hand. 因此，我们持谨慎乐观的态度来撰写此文。到目前为止，我们已经看到足够多的有关微服务风格的项目，并且觉得这是一条值得去探索的道路。我们不能肯定地说，道路的尽头在哪里。但是，软件开发的挑战之一，就是只能基于 “目前手上拥有但还不够完善” 的信息来做出决策。 TIPSTips1 : How big is a microservice?-一个微服务应该有多大？ Although “microservice” has become a popular name for this architectural style, its name does lead to an unfortunate focus on the size of service, and arguments about what constitutes “micro”. In our conversations with microservice practitioners, we see a range of sizes of services. The largest sizes reported follow Amazon’s notion of the Two Pizza Team (i.e. the whole team can be fed by two pizzas), meaning no more than a dozen people. On the smaller size scale we’ve seen setups where a team of half-a-dozen would support half-a-dozen services. 尽管“微服务”已经成为一个流行的名字，但是这个名字确实会不幸地导致大家对服务规模的关注，并且产生了有关什么是“微”的争论。在与微服务从业者的交谈中，我们看到了有关服务的一系列规模。所听到的最大的一个服务的规模，是遵循了亚马逊的“两个比萨团队”（即一个团队可以被两个比萨所喂饱）的理念，这意味着这个团队不会多于12人。对于规模较小的服务，我们已经看到一个6人的团队在支持6个服务。 This leads to the question of whether there are sufficiently large differences within this size range that the service-per-dozen-people and service-per-person sizes shouldn’t be lumped under one microservices label. At the moment we think it’s better to group them together, but it’s certainly possible that we’ll change our mind as we explore this style further. 这引出了一个问题，即“每12人做一个服务”和“每人做一个服务”这样有关服务规模的差距，是否已经大到不能将两者都纳入微服务之下？此时，我们认为最好还是把它们归为一类，但是随着进一步探索这种架构风格，绝对有可能我们会在将来改变主意。 Tips2 : Microservices and SOA-微服务与SOA When we’ve talked about microservices a common question is whether this is just Service Oriented Architecture (SOA) that we saw a decade ago. There is merit to this point, because the microservice style is very similar to what some advocates of SOA have been in favor of. The problem, however, is that SOA means too many different things, and that most of the time that we come across something called “SOA” it’s significantly different to the style we’re describing here, usually due to a focus on ESBs used to integrate monolithic applications. 当我们谈起微服务时，一个常见的问题就会出现：是否微服务仅仅是十多年前所看到的“面向服务的架构”(Service Oriented Architecture, SOA)？这样问是有道理的，因为微服务风格非常类似于一些支持SOA的人所赞成的观点。然而，问题在于SOA这个词儿意味着太多不同的东西。而且大多数时候，我们所遇到的某些被称作”SOA”的事物，明显不同于本文所描述的风格。这通常由于它们专注于ESB，来集成各个单体应用。 In particular we have seen so many botched implementations of service orientation - from the tendency to hide complexity away in ESB’s [6], to failed multi-year initiatives that cost millions and deliver no value, to centralised governance models that actively inhibit change, that it is sometimes difficult to see past these problems. 特别地，我们已经看到如此之多的面向服务的拙劣实现——从将系统复杂性隐藏于ESB中的趋势 [7] ，到花费数百万进行多年却没有交付任何价值的失败项目，到顽固抑制变化发生的中心化技术治理模型——以至于有时觉得其所造成的种种问题真的不堪回首。 Certainly, many of the techniques in use in the microservice community have grown from the experiences of developers integrating services in large organisations. The Tolerant Reader pattern is an example of this. Efforts to use the web have contributed, using simple protocols is another approach derived from these experiences - a reaction away from central standards that have reached a complexity that is, frankly, breathtaking. (Any time you need an ontology to manage your ontologies you know you are in deep trouble.) 当然，在微服务社区投入使用的许多技术，源自各个开发人员将各种服务集成到各个大型组织的经验。“容错读取”(Tolerant Reader)模式就是这样一个例子。对于Web的广泛使用，使得人们不再使用一些中心化的标准，而使用一些简单的协议。坦率地说，这些中心化的标准，其复杂性已经达到令人吃惊的程度。（任何时候，如果需要一个本体（ontology）来管理其他各个本体，那么麻烦就大了） This common manifestation of SOA has led some microservice advocates to reject the SOA label entirely, although others consider microservices to be one form of SOA [7], perhaps service orientation done right. Either way, the fact that SOA means such different things means it’s valuable to have a term that more crisply defines this architectural style. 这种常见的SOA的表现，已使得一些微服务的倡导者完全拒绝将自己贴上SOA的标签。尽管其他人会将微服务看作是SOA的 一种形式 [8] ，也许微服务就是以正确的形式来实现面向服务的SOA 。不管是哪种情况，SOA意味着很多的不同事物，这表明用一个更加干净利落的术语来命名这种架构风格是很有价值的。 Tips3 : Many languages, many options-多种编程语言，多种选择可能 The growth of JVM as a platform is just the latest example of mixing languages within a common platform. It’s been common practice to shell-out to a higher level language to take advantage of higher level abstractions for decades. As is dropping down to the metal and writing performance sensitive code in a lower level one. However, many monoliths don’t need this level of performance optimisation nor are DSL’s and higher level abstractions that common (to our dismay). Instead monoliths are usually single language and the tendency is to limit the number of technologies in use [10]. 做为一个平台，JVM的发展仅仅是一个将各种编程语言混合到一个通用平台的最新例证。近十年以来，在平台外层实现更高层次的编程语言，来利用更高层次的抽象，已经成为一个普遍做法。同样，在平台底层以更低层次的编程语言编写性能敏感的代码也很普遍。然而，许多单体系统并不需要这种级别的性能优化，另外DSL和更高层次的抽象也不常用（这令我们感到失望）。相反，许多单体应用通常就使用单一编程语言，并且有对所使用的技术数量进行 限制 的趋势 [10] 。 Tips4 : Battle-tested standards and enforced standards-”实战检验”的标准与“强制执行”的标准 It’s a bit of a dichotomy that microservice teams tend to eschew the kind of rigid enforced standards laid down by enterprise architecture groups but will happily use and even evangelise the use of open standards such as HTTP, ATOM and other microformats. 微服务的某些做法有点泾渭分明的味道，即他们趋向于避开被那些企业架构组织所制定的硬性实施的标准，而愉快地使用甚至传播一些开放标准，比如 HTTP、ATOM和其他微格式的协议。 The key difference is how the standards are developed and how they are enforced. Standards managed by groups such as the IETF only become standards when there are several live implementations of them in the wider world and which often grow from successful open-source projects. 这里的关键区别是，这些标准是如何被制定以及如何被实施的。像诸如IETF这样的组织所管理的各种标准，只有达到某些条件才能称为标准，即该标准在全球更广阔的地区有一些正在运行的实现案例，而且这些标准经常源自一些成功的开源项目。 These standards are a world apart from many in a corporate world, which are often developed by groups that have little recent programming experience or overly influenced by vendors. 这些标准组成了一个世界，它区别于来自企业世界的许多标准。企业世界中的标准，经常由这样特点的组织来开发，即缺乏用较新技术进行编程的经验，或受到供应商的过度影响。 Tips5 : Make it easy to do the right thing-让做正确的事情变得容易 One side effect we have found of increased automation as a consequence of continuous delivery and deployment is the creation of useful tools to help developers and operations folk. Tooling for creating artefacts, managing codebases, standing up simple services or for adding standard monitoring and logging are pretty common now. The best example on the web is probably Netflix’s set of open source tools, but there are others including Dropwizard which we have used extensively. 那些因实现持续交付和持续集成所增加的自动化工作的副产品，是创建一些对开发和运维人员有用的工具。现在，能完成下面工作的工具已经相当常见了：即创建工件(artefacts)、管理代码库、启动一些简单的服务、或增加标准的监控和日志功能。Web上最好的例子可能是Netflix提供的一套开源工具集，但也有其他一些好工具，包括我们已经广泛使用的Dropwizard。 Tips6 : The circuit breaker and production ready code-“断路器”与“可随时上线的代码” Circuit Breaker appears in Release It!alongside other patterns such as Bulkhead and Timeout. Implemented together, these patterns are crucially important when building communicating applications. This Netflix blog entry does a great job of explaining their application of them. “断路器”(Circuit Breaker )一词与其他一些模式一起出现在《发布！》(Release It! )一书中，例如隔板(Bulkhead)和超时(Timeout)。当构建彼此通信的应用系统时，将这些模式加以综合运用就变得至关重要。Netflix公司的这篇很精彩的博客解释了这些模式是如何应用的。 Tips7 : Synchronous calls considered harmful-同步调用的弊端 Any time you have a number of synchronous calls between services you will encounter the multiplicative effect of downtime. Simply, this is when the downtime of your system becomes the product of the downtimes of the individual components. You face a choice, making your calls asynchronous or managing the downtime. At www.guardian.co.uk they have implemented a simple rule on the new platform - one synchronous call per user request while at Netflix, their platform API redesign has built asynchronicity into the API fabric. 一旦在一些服务之间进行多个同步调用，就会遇到宕机的乘法效应。简而言之，这意味着整个系统的宕机时间，是每一个单独模块各自宕机时间的乘积。此时面临着一个选择：是让模块之间的调用异步，还是去管理宕机时间？在www.guardian.co.uk网站，他们在新平台上实现了一个简单的规则——每一个用户请求都对应一个同步调用。然而在Netflix公司，他们重新设计的平台API将异步性构建到API的机制(fabric)中。 参考 https://www.aliyun.com/jiaocheng/292444.html]]></content>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 实践系列-集成 RocketMQ]]></title>
    <url>%2F2020%2F04%2F05%2Fspringboot%2Fspringboot-series-rocketmq%2F</url>
    <content type="text"><![CDATA[RocketMQ 简介：Apache RocketMQ是一个分布式消息传递和流媒体平台，具有低延迟、高性能和可靠性、万亿级容量和灵活的可伸缩性。它提供了多种功能，具体参考: https://github.com/apache/rocketmq 。 RocketMQ 快速开始官方指导手册快速开始中提到，RocketMQ 安装需要具体以下条件： 64bit OS, 推荐使用 Linux/Unix/Mac 64bit JDK 1.8+ Maven 3.2.x 4g+ free disk for Broker server （这个需要特别关注下） 下载安装和编译12345wget https://archive.apache.org/dist/rocketmq/4.7.0/rocketmq-all-4.7.0-source-release.zipunzip rocketmq-all-4.7.0-source-release.zipcd rocketmq-all-4.7.0/mvn -Prelease-all -DskipTests clean install -Ucd distribution/target/rocketmq-4.7.0/rocketmq-4.7.0 1、启动 Name Server 123&gt; nohup sh bin/mqnamesrv &amp;&gt; tail -f ~/logs/rocketmqlogs/namesrv.logThe Name Server boot success... 2、启动 Broker 1234&gt; nohup sh bin/mqbroker -n localhost:9876 &amp;# nohup sh bin/mqbroker -n localhost:9876 autoCreateTopicEnable=true &amp;&gt; tail -f ~/logs/rocketmqlogs/broker.log The broker[%s, 172.30.30.233:10911] boot success... autoCreateTopicEnable：使用 RocketMQ 进行发消息时，必须要指定 topic，对于 topic 的设置有一个开关 autoCreateTopicEnable，一般在开发测试环境中会使用默认设置 autoCreateTopicEnable = true，但是这样就会导致 topic 的设置不容易规范管理，没有统一的审核等等，所以在正式环境中会在 Broker 启动时设置参数 autoCreateTopicEnable = false。这样当需要增加 topic 时就需要在 web 管理界面上或者通过 admin tools 添加即可 SpringBoot 集成RocketMQ 目前没有提供集成 SpringBoot 的 starter，因此现在接入都是通过引入客户端进行编程。下面来看下 SpringBoot 集成 RocketMQ 的过程。 引入 RocketMQ 客户端依赖github 上目前更新的最新版本是 4.7.0 版本，这里就使用最新版本： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;4.7.0&lt;/version&gt;&lt;/dependency&gt; 提供生产者的自动配置类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * @author: guolei.sgl (glmapper_2018@163.com) 2020/4/5 5:17 PM * @since: **/@Configurationpublic class MQProducerConfiguration &#123; public static final Logger LOGGER = LoggerFactory.getLogger(MQProducerConfiguration.class); @Value("$&#123;rocketmq.producer.groupName&#125;") private String groupName; @Value("$&#123;rocketmq.producer.namesrvAddr&#125;") private String namesrvAddr; @Value("$&#123;rocketmq.producer.maxMessageSize&#125;") private Integer maxMessageSize; @Value("$&#123;rocketmq.producer.sendMsgTimeout&#125;") private Integer sendMsgTimeout; @Value("$&#123;rocketmq.producer.retryTimesWhenSendFailed&#125;") private Integer retryTimesWhenSendFailed; @Bean @ConditionalOnMissingBean public DefaultMQProducer defaultMQProducer() throws RuntimeException &#123; DefaultMQProducer producer = new DefaultMQProducer(this.groupName); producer.setNamesrvAddr(this.namesrvAddr); producer.setCreateTopicKey("AUTO_CREATE_TOPIC_KEY"); //如果需要同一个 jvm 中不同的 producer 往不同的 mq 集群发送消息，需要设置不同的 instanceName //producer.setInstanceName(instanceName); //如果发送消息的最大限制 producer.setMaxMessageSize(this.maxMessageSize); //如果发送消息超时时间 producer.setSendMsgTimeout(this.sendMsgTimeout); //如果发送消息失败，设置重试次数，默认为 2 次 producer.setRetryTimesWhenSendFailed(this.retryTimesWhenSendFailed); try &#123; producer.start(); LOGGER.info("producer is started. groupName:&#123;&#125;, namesrvAddr: &#123;&#125;", groupName, namesrvAddr); &#125; catch (MQClientException e) &#123; LOGGER.error("failed to start producer.", e); throw new RuntimeException(e); &#125; return producer; &#125;&#125; groupName: 发送同一类消息的设置为同一个 group，保证唯一， 默认不需要设置，rocketmq 会使用 ip@pid(pid代表jvm名字) 作为唯一标示。 namesrvAddr：Name Server 地址 maxMessageSize：消息最大限制，默认 4M sendMsgTimeout：消息发送超时时间，默认 3 秒 retryTimesWhenSendFailed：消息发送失败重试次数，默认 2 次 提供消费者的自动配置类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Configurationpublic class MQConsumerConfiguration &#123; public static final Logger LOGGER = LoggerFactory.getLogger(MQConsumerConfiguration.class); @Value("$&#123;rocketmq.consumer.namesrvAddr&#125;") private String namesrvAddr; @Value("$&#123;rocketmq.consumer.groupName&#125;") private String groupName; @Value("$&#123;rocketmq.consumer.consumeThreadMin&#125;") private int consumeThreadMin; @Value("$&#123;rocketmq.consumer.consumeThreadMax&#125;") private int consumeThreadMax; // 订阅指定的 topic @Value("$&#123;rocketmq.consumer.topics&#125;") private String topics; @Value("$&#123;rocketmq.consumer.consumeMessageBatchMaxSize&#125;") private int consumeMessageBatchMaxSize; @Autowired private MQConsumeMsgListenerProcessor mqMessageListenerProcessor; @Bean @ConditionalOnMissingBean public DefaultMQPushConsumer defaultMQPushConsumer() throws RuntimeException &#123; DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(groupName); consumer.setNamesrvAddr(namesrvAddr); consumer.setConsumeThreadMin(consumeThreadMin); consumer.setConsumeThreadMax(consumeThreadMax); consumer.registerMessageListener(mqMessageListenerProcessor); // 设置 consumer 第一次启动是从队列头部开始消费还是队列尾部开始消费 // 如果非第一次启动，那么按照上次消费的位置继续消费 consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); // 设置消费模型，集群还是广播，默认为集群 consumer.setMessageModel(MessageModel.CLUSTERING); // 设置一次消费消息的条数，默认为 1 条 consumer.setConsumeMessageBatchMaxSize(consumeMessageBatchMaxSize); try &#123; // 设置该消费者订阅的主题和tag，如果是订阅该主题下的所有tag，使用*； consumer.subscribe(topics, "*"); // 启动消费 consumer.start(); LOGGER.info("consumer is started. groupName:&#123;&#125;, topics:&#123;&#125;, namesrvAddr:&#123;&#125;",groupName,topics,namesrvAddr); &#125; catch (Exception e) &#123; LOGGER.error("failed to start consumer . groupName:&#123;&#125;, topics:&#123;&#125;, namesrvAddr:&#123;&#125;",groupName,topics,namesrvAddr,e); throw new RuntimeException(e); &#125; return consumer; &#125;&#125; 参数参考上述生产者部分。这里配置只是启动的消费端的监听，具体的消费需要再实现一个 MessageListenerConcurrently 接口。 1234567891011121314151617181920212223242526272829/** * @author: guolei.sgl (glmapper_2018@163.com) 2020/4/5 5:21 PM * @since: **/@Componentpublic class MessageListenerHandler implements MessageListenerConcurrently &#123; private static final Logger LOGGER = LoggerFactory.getLogger(MessageListenerHandler.class); private static String TOPIC = "DemoTopic"; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; if (CollectionUtils.isEmpty(msgs)) &#123; LOGGER.info("receive blank msgs..."); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; MessageExt messageExt = msgs.get(0); String msg = new String(messageExt.getBody()); if (messageExt.getTopic().equals(TOPIC)) &#123; // mock 消费逻辑 mockConsume(msg); &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; private void mockConsume(String msg)&#123; LOGGER.info("receive msg: &#123;&#125;.", msg); &#125;&#125; 使用客户端发送消息使用客户端发送消息的逻辑比较简单，就是拿到 DefaultMQProducer 对象，调用 send 方法，支持同步、异步、oneway 等多种调用方式。 1234567891011121314151617181920@RestControllerpublic class TestController &#123; private static final Logger LOGGER = LoggerFactory.getLogger(TestController.class); private static String TOPIC = "DemoTopic"; private static String TAGS = "glmapperTags"; @Autowired private DefaultMQProducer defaultMQProducer; @RequestMapping("send") public String test() throws Throwable &#123; Message msg = new Message(TOPIC, TAGS, ("Say Hello RocketMQ to Glmapper").getBytes(RemotingHelper.DEFAULT_CHARSET)); // 调用客户端发送消息 SendResult sendResult = defaultMQProducer.send(msg); LOGGER.info("sendResult: &#123;&#125;.",sendResult); return "SUCCESS"; &#125;&#125; 测试这里的测试应用是将生产端和消费端放在一起的，所以配置如下： 123456789101112131415161718spring.application.name=test-rocketserver.port=8008#producerrocketmq.producer.isOnOff=on #该应用是否启用生产者rocketmq.producer.groupName=$&#123;spring.application.name&#125;rocketmq.producer.namesrvAddr=sofa.cloud.alipay.net:9876rocketmq.producer.maxMessageSize=4096rocketmq.producer.sendMsgTimeout=3000rocketmq.producer.retryTimesWhenSendFailed=2#consumerrocketmq.consumer.isOnOff=on #该应用是否启用消费者rocketmq.consumer.groupName=$&#123;spring.application.name&#125;rocketmq.consumer.namesrvAddr=sofa.cloud.alipay.net:9876rocketmq.consumer.topics=DemoTopicrocketmq.consumer.consumeThreadMin=20rocketmq.consumer.consumeThreadMax=64rocketmq.consumer.consumeMessageBatchMaxSize=1 启动程序，查看日志输出: 122020-04-05 22:53:15.141 INFO 46817 --- [ main] c.g.b.b.c.MQProducerConfiguration : producer is started. groupName:test-rocket, namesrvAddr: sofa.cloud.alipay.net:98762020-04-05 22:53:15.577 INFO 46817 --- [ main] c.g.b.b.c.MQConsumerConfiguration : consumer is started. groupName:test-rocket, topics:DemoTopic, namesrvAddr:sofa.cloud.alipay.net:9876 这里看到，生产者和消费者自动配置已经生效并启动完成。通过 curl localhost:8008/send 来触发消息发送: 122020-04-05 22:54:21.654 INFO 46817 --- [nio-8008-exec-1] c.g.b.boot.controller.TestController : sendResult: SendResult [sendStatus=SEND_OK, msgId=1E0FC3A2B6E118B4AAC21983B3C50000, offsetMsgId=64583D7C00002A9F0000000000011788, messageQueue=MessageQueue [topic=DemoTopic, brokerName=sofa.cloud.alipay.net, queueId=6], queueOffset=50].2020-04-05 22:54:21.658 INFO 46817 --- [MessageThread_1] c.g.b.b.p.MessageListenerHandler : receive msg: Say Hello RocketMQ to Glmapper. 看到发送消息的日志和接受消息的日志。 使用 hook 拦截消息RocKetMQ 中提供了两个 hook 接口：SendMessageHook 和 ConsumeMessageHook 接口，可以用于在消息发送之前、之后，消息消费之前、之后对消息进行拦截，官方文档中并没有关于这部分的描述，那么这里我们就来看下如何使用这两个 hook 接口来搞点事情。 SendMessageHook自定义一个 ProducerTestHook ，代码如下： 12345678910111213141516171819public class ProducerTestHook implements SendMessageHook &#123; public static final Logger LOGGER = LoggerFactory.getLogger(ProducerTestHook.class); @Override public String hookName() &#123; return ProducerTestHook.class.getName(); &#125; @Override public void sendMessageBefore(SendMessageContext sendMessageContext) &#123; LOGGER.info("execute sendMessageBefore. sendMessageContext:&#123;&#125;", sendMessageContext); &#125; @Override public void sendMessageAfter(SendMessageContext sendMessageContext) &#123; LOGGER.info("execute sendMessageAfter. sendMessageContext:&#123;&#125;", sendMessageContext); &#125;&#125; 在上面生产者的自动配置类中，将 ProducerTestHook 注册给 producer。 12// 注册 SendMessageHookproducer.getDefaultMQProducerImpl().registerSendMessageHook(new ProducerTestHook()); ConsumeMessageHook自定义一个 ConsumerTestHook ，代码如下：12345678910111213141516171819public class ConsumerTestHook implements ConsumeMessageHook &#123; public static final Logger LOGGER = LoggerFactory.getLogger(ConsumerTestHook.class); @Override public String hookName() &#123; return ConsumerTestHook.class.getName(); &#125; @Override public void consumeMessageBefore(ConsumeMessageContext consumeMessageContext) &#123; LOGGER.info("execute consumeMessageBefore. consumeMessageContext: &#123;&#125;",consumeMessageContext); &#125; @Override public void consumeMessageAfter(ConsumeMessageContext consumeMessageContext) &#123; LOGGER.info("execute consumeMessageAfter. consumeMessageContext: &#123;&#125;",consumeMessageContext); &#125;&#125; 在上面消费者的自动配置类中，将 ConsumerTestHook 注册给 consumer 12// 注册 ConsumeMessageHookconsumer.getDefaultMQPushConsumerImpl().registerConsumeMessageHook(new ConsumerTestHook()); 执行结果如下： 123456execute sendMessageBefore. sendMessageContext:org.apache.rocketmq.client.hook.SendMessageContext@a50ea34execute sendMessageAfter. sendMessageContext:org.apache.rocketmq.client.hook.SendMessageContext@a50ea34sendResult: SendResult [sendStatus=SEND_OK, msgId=0A0FE8F8C02F18B4AAC21C1275FB0000, offsetMsgId=64583D7C00002A9F0000000000011850, messageQueue=MessageQueue [topic=DemoTopic, brokerName=sofa.cloud.alipay.net, queueId=5], queueOffset=50].execute consumeMessageBefore. consumeMessageContext: org.apache.rocketmq.client.hook.ConsumeMessageContext@6482209areceive msg: Say Hello RocketMQ to Glmapper.execute consumeMessageAfter. consumeMessageContext: org.apache.rocketmq.client.hook.ConsumeMessageContext@6482209a 遇到的一些问题集成过程中遇到几个问题记录如下： 1、Broker 启动失败。 我在测试时遇到的情况是，在 Name Server 启动之后，再启动 Boker 时，ssh 连接会直接提示 connect conversation fail. 通过 dmesg | egrep -i -B100 &#39;killed process&#39; 查看进程被 kill 的记录，得到如下日志： 123456[2257026.030741] Memory cgroup out of memory: Kill process 110719 (systemd) score 0 or sacrifice child[2257026.031888] Killed process 100735 (sh) total-vm:15708kB, anon-rss:176kB, file-rss:1800kB, shmem-rss:0kB[2257026.133506] Memory cgroup out of memory: Kill process 110719 (systemd) score 0 or sacrifice child[2257026.133539] Killed process 100745 (vsar) total-vm:172560kB, anon-rss:22936kB, file-rss:1360kB, shmem-rss:0kB[2257026.206872] Memory cgroup out of memory: Kill process 104617 (java) score 3 or sacrifice child[2257026.207742] Killed process 104617 (java) total-vm:9092924kB, anon-rss:4188528kB, file-rss:496kB, shmem-rss:0kB 那这里看到的结论是发生了 OOM，这里是启动时没哟分配到足够的空间导致的(默认配置文件初始内存设置的太大了)。解决办法是：进入到编译之后的 distribution/target/apache-rocketmq/bin 目录，找到 runbroker.sh 和 runserver.sh 两个脚本文件，这两个脚本理解启动时默认指定的参数是非常大的（4g/8g/2g），我线下测试机器总共才 1c2g，所以适当的调整了下参数: runserver.sh 1JAVA_OPT="$&#123;JAVA_OPT&#125; -server -Xms128m -Xmx256m -Xmn256m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m" runbroker.sh 1JAVA_OPT="$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m" 修改后重新启动 namesrv 和 broker ，正常了 1234$ jps98633 Jps55689 BrokerStartup54906 NamesrvStartup 2、No Topic Route Info，xxx 这个在官方的 FAQ 里面有提到，说明遇到的频次一定是很高的。官方给出的方案可以详解这里 http://rocketmq.apache.org/docs/faq/ 第4条。我是通过 If you can’t find this topic, create it on a broker via admin tools command updateTopic or web console. 这个解决的:12345sh mqadmin updateTopic -b localhost:10911 -n localhost:9876 -t DemoTopic # 执行此指令，创建 DemoTopicRocketMQLog:WARN No appenders could be found for logger (io.netty.util.internal.PlatformDependent0).RocketMQLog:WARN Please initialize the logger system properly.create topic to localhost:10911 success.TopicConfig [topicName=DemoTopic, readQueueNums=8, writeQueueNums=8, perm=RW-, topicFilterType=SINGLE_TAG, topicSysFlag=0, order=false] 总结之前在做 SOFATracer 集成消息组件时有看过 RocketMQ 的部分代码，但是在实际操作时还是饶了不少弯路。总体来看，SpringBoot 集成 RocketMQ 还是比较简单的，在此记录一下。如果文中有描述有误的地方，还请各位大佬留言指正。 参考文档 http://rocketmq.apache.org/docs/quick-start/ https://blog.csdn.net/ph3636/article/details/79528638]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决方案系列-基于 SOFAArk 实现应用的动态装载和卸载]]></title>
    <url>%2F2020%2F04%2F03%2Fsolution-series-dynamic-module%2F</url>
    <content type="text"><![CDATA[本篇主要来看下蚂蚁金服开源的 SOFAArk 这个产品。SOFAArk 是一款基于 Java 实现的轻量级类隔离容器，主要提供类隔离和应用(模块)合并部署能力；本文主要基于 telnet 指令的方式进行应用 Biz 的装载和卸载操作。去年在上海 KubeCon 大会上有分享过 《SOFABoot 动态模块实践》，主要是通过 SOFADashboard 来下发指令的，基于 SOFABoot 3.1.4 和 SOFAArk 0.6.0 版本；目前 SOFABoot 已经发布到 3.3.x+ ，SOFAARK 1.1.1 版本，其中 ，SOFAARK 提供了很多新的特性，包括全生命周期的事件机制、卸载优化等。 由于 SOFABoot 3.3.0 版本中部分代码的重构，导致无法兼容 runtime plugin，所以本文是基于修复版的 SOFABoot 3.3.0-poc-ark-SNAPSHOT 来完成，案例工程kc-sofastack-dynamic-demo 分支 support-3.3.0 。 案例描述先看下官方文档里面对于动态部署的描述，是通过变更/监听 zk 节点数据来实现运维命令的下发和接收： 本篇精简一下，使用 telnet 指令直接进行运维操作；案例描述如下：对于一个运行期的一个应用 A，将另外一个应用 B 动态的装载到 A 上（实际是 A 所在的 Ark 容器上），大体描述如下图所示： 下面两张图是具体的运行结果图： 默认 master 运行、无动态 biz 运行时的运行结果 动态安装 biz 之后运行结果 操作步骤1、构建 SOFABoot 3.3.0-poc-ark-SNAPSHOT 版本，安装到本地仓库 123git clone https://github.com/glmapper/sofa-boot.gitgit checkout 3.3.0-poc-ark-SNAPSHOTmvn clean install -DskipTest 2、下载 kc-sofastack-dynamic-demo 案例工程，打包 123git clone https://github.com/sofastack-guides/kc-sofastack-dynamic-demo.gitgit checkout support-3.3.0mvn clean package -DskipTest 3、完成打包之后，案例工程根目录 target 目录下有两个包，dynamic-stock-mng-1.0.0-ark-biz.jar 和 dynamic-provider-1.0.0-ark-biz.jar。dynamic-stock-mng-1.0.0-ark-biz.jar 是 master biz + ark 容器，dynamic-provider-1.0.0-ark-biz.jar 是动态 biz 4、执行 java -jar dynamic-stock-mng-1.0.0-ark-biz.jar，浏览器输入 http://localhost:8080/，运行结果 这里看到的 SUPPORT BY: DEFAULT BIZ 是默认的 BIZ ，也就是 master BIZ 5、通过 telnet 指令安装 动态 biz 12telnet localhost 1234sofa-ark&gt; biz -i file://xxxxx/dynamic-provider-1.0.0-ark-biz.jar #这里的文件路径根据你自己本地实际的包路径修改 通过 biz -a 查看安装结果： 1234sofa-ark&gt;biz -aprovide:1.0.0:activatedstock-mng:1.0.0:activatedbiz count = 2 在次通过浏览器输入 http://localhost:8080/，运行结果 这里 SUPPORT BY 是刚刚安装的 动态 BIZ 所提供的。 原理 来自官网 SOFAArk 包含三个概念，Ark Container, Ark Plugin 和 Ark Biz; 运行时逻辑结构图如下: 概念解释： Ark Container: SOFAArk 容器，负责 Ark 包启动运行时的管理；Ark Plugin 和 Ark Biz 运行在 SOFAArk 容器之上；容器具备管理插件和应用的功能；容器启动成功后，会自动解析 classpath 包含的 Ark Plugin 和 Ark Biz 依赖，完成隔离加载并按优先级依次启动之； Ark Plugin: Ark 插件，满足特定目录格式要求的 Fat Jar，使用官方提供的 Maven 插件 sofa-ark-plugin-maven-plugin 可以将一个或多个普通的 Java jar 打包成一个标准格式的 Ark Plugin；Ark Plugin 会包含一份配置文件，通常包括插件类导入导出配置、资源导入导出配置、插件启动优先级等；运行时，SOFAArk 容器会使用独立的 PluginClassLoader加载插件，并根据插件配置构建类加载索引表、资源加载索引表，使插件和插件之间、插件和应用之间相互隔离； Ark Biz: Ark 应用模块，满足特定目录格式要求的 Fat Jar，使用官方提供的 Maven 插件 sofa-ark-maven-plugin 可以将工程应用打包成一个标准格式的 Ark Biz；Ark Biz 是工程应用以及其依赖包的组织单元，包含应用启动所需的所有依赖和配置；一个 Ark 包中可以包含多个 Ark Biz 包，按优先级依次启动，Biz 之间通过 JVM 服务交互； 运行 Ark 包，Ark Container 优先启动，容器自动解析 Ark 包中含有的 Ark Plugin 和 Ark Biz，并读取他们的配置信息，构建类和资源的加载索引表；然后使用独立的 ClassLoader 加载并按优先级配置依次启动。]]></content>
      <categories>
        <category>解决方案</category>
      </categories>
      <tags>
        <tag>解决方案</tag>
        <tag>SOFA</tag>
        <tag>SOFAArk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决方案系列-集群选主(基于DB)]]></title>
    <url>%2F2020%2F03%2F29%2Fsolution-series-cluster-selector%2F</url>
    <content type="text"><![CDATA[一个业务量很小的系统，所有的代码都放在一个项目中，部署在一台服务器上。所有的服务都由这台服务器提供，这就是常说的单机模式；我们知道单机模式的缺点是：1、处理能力有限，2、存在单点问题。单机模式大致如下图所示： 为了解决这些问题，出现了集群模式。 常见的集群模式方案集群主要的使用场景就是为了分担请求的压力，也就是在几个服务器上部署相同的应用程序，来分担客户端请求。集群主要是通过加机器来解决问题，对于问题本身是不会做任何分解的。（PS：分解了就是分布式）。那么常见的集群模式有哪几种呢？ 主备，冷备模式，主写主读所有的写请求都由 master 负责处理，如果请求打在 slave 上，则 slave 会将请求转发给 master 处理，架构图如下图所示： 冷备模式主要是在请求量不大，且对于数据状态有要求的情况（比如所有的数据其实都是 master 内存存储），提供一个从机用于备用，在 master 出现问题的时候能够及时顶上去。 主备，主写从读基于冷备模式，衍生出主写从读，也就是 master 会将数据同步给各个 slave，然后就可以分担掉读请求的处理压力，架构图如下图所示： 这种模式算是比较常见的，像数据库的主从模式。 对等，广播这种场景是客户端的请求可能需要指定的集群中的某个 server 来处理，常见的比如配置中心，配置中心客户端会选择与集群中的某个 server 建立连接，当通过管控端推送配置指令下来时，因为没法指定到具体的 server 去推送，所以当请求推到集群的某台机器时，该机器会向集群中的其他机器广播此次请求，谁持有这个客户端的连接，谁去处理。架构图如下图所示： 纯对等（无状态）最后一种就是纯对等的，集群中的机器谁处理都行；这种就是广义说的集群模式，通过扩机器解决高并发。 集群选主集群选主指是上述提到的主备模式下进行的，也就是有状态的情况，无状态场景不需要进行选主操作。集群选主很容易，困难的是选主之后的操作（比如如何协调和感知集群中的机器状态），还有集群内机器发生变更之后的操作（如如何分配客户端连接，这里就会涉及到一个非常常见的问题：一致性 hash）。本篇主要介绍集群选主，所以对于这两个问题不做过多的探讨。 集群选主的方式有很多种，本篇只介绍通过 ”争抢锁“ 的选主方式，即在集群中机器启动时会通过争抢某个”非共享“资源，谁抢到谁来当 master。那么基于此，我们可以罗列以下几种常见的”非共享“资源： DB 的唯一键插入 zk 的节点创建 redis 的原子写 … 基于 DB 的唯一键插入实现选主下面以 DB 的唯一键插入 为例来简单介绍下选主的逻辑。 数据库唯一键：unique key，用来保证对应的字段中的数据唯一。 机器表设计&amp;唯一键设置所有集群中的机器启动时都需要将自己的机器信息写到 DB 中。这里创建一个唯一键：uk_master，包括两个字段，分别是 机器状态和一个 master_lock（master 锁）。 123456789101112131415161718-- ------------------------------ Table structure for cluster_servers-- ----------------------------DROP TABLE IF EXISTS `cluster_servers`;CREATE TABLE `cluster_servers` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键', `host_name` varchar(128) NOT NULL COMMENT '主机名', `ip` varchar(16) NOT NULL COMMENT 'ip 地址', `is_master` int(4) NOT NULL COMMENT '是否是 master', `status` varchar(32) NOT NULL COMMENT '机器状态', `master_lock` varchar(128) NOT NULL COMMENT 'master 锁', `heartbeat` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '心跳时间', `gmt_sql_server_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'sql 执行时间', `gmt_modify` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间', `gmt_create` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', PRIMARY KEY (`id`), UNIQUE KEY `uk_master` (`status`,`master_lock`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8; 争抢唯一键这里就是谁更新数据状态成功，谁就是 master，没有争抢成功则是 slave。 123&lt;update id="setMaster" &gt; update cluster_servers set is_master = #&#123;isMaster&#125;, master_lock = #&#123;masterLock&#125;, status = #&#123;status&#125;, heartbeat = CURRENT_TIMESTAMP, gmt_modify = CURRENT_TIMESTAMP where host_name = #&#123;hostName&#125;&lt;/update&gt; 集群选主的过程基于此，集群选主的大致过程可以通过下图描述： 这里的像 slave 的后置任务，比如开启监听 master 状态，这样在 master 出现问题时就会触发新的选举。master 的后置任务一个是开始监听各个 slave 的状态，如果 slave 出现问题，则可以及时的将此 slave 踢出集群，其他则需要根据具体的业务情况来看。 总结本篇主要介绍了集群的几种基本形态，然后基于需要选主的场景进行了简单分析；最后提供了基于 DB 进行集群选主的一种可行性方案，并介绍了大体的选主流程。需要补充一点，基于 DB 选主和维持心跳本身是比较重的，强依赖 DB 的状态，如果 DB 有问题则集群状态可能会出现一些非预期的情况或者导致集群直接不可用，所以大家在选择具体的方式时，还是要结合业务的具体场景来选择。]]></content>
      <categories>
        <category>解决方案</category>
      </categories>
      <tags>
        <tag>解决方案</tag>
        <tag>集群选主</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊 CopyOnWriteArraySet 的迭代删除]]></title>
    <url>%2F2020%2F03%2F16%2Fjava-base-iterator-of-set%2F</url>
    <content type="text"><![CDATA[上周在工程中涉及到一个清理 Set 集合的操作，将满足设定条件的项从 Set 中删除掉。简化版本代码如下： 123456789101112public static void main(String[] args) &#123; Set&lt;String&gt; sets = new CopyOnWriteArraySet&lt;&gt;(); sets.add("1"); sets.add("3"); sets.add("3"); sets.add("4"); Iterator&lt;String&gt; iterator = sets.iterator(); while (iterator.hasNext())&#123; iterator.remove(); &#125; System.out.println(sets); &#125; 这个看起来是个很常规的问题，没有验证就直接发了线下环境，然后就收到了业务方反馈的服务无法正常使用的问题了。 问题现象先来看下上述代码所抛出的异常： 123Exception in thread "main" java.lang.UnsupportedOperationException at java.util.concurrent.CopyOnWriteArrayList$COWIterator.remove(CopyOnWriteArrayList.java:1178) at com.glmapper.bridge.boot.TestMain.main(TestMain.java:21) 关于 UnsupportedOperationException 这个异常没有什么好说的，在集合操作中经常出现，网上也有很多关于这个异常的说明，这里不再赘述。这里我比较关注的是，我使用的是 CopyOnWriteArraySet，迭代器也是 sets 的，但是异常中居然出现了 CopyOnWriteArrayList，查看了 CopyOnWriteArraySet 的类继承关系，和 CopyOnWriteArrayList 也没啥关系。 排查&amp;结果通过查看了 CopyOnWriteArraySet 的代码，发现 CopyOnWriteArraySet 内部其实是持有了一个 CopyOnWriteArrayList 的对象实例，其内部的所有操作都是基于 CopyOnWriteArrayList 这个对象来进行的。 123456789101112public class CopyOnWriteArraySet&lt;E&gt; extends AbstractSet&lt;E&gt; implements java.io.Serializable &#123; // 省略其他代码 private final CopyOnWriteArrayList&lt;E&gt; al; /** * Creates an empty set. */ public CopyOnWriteArraySet() &#123; al = new CopyOnWriteArrayList&lt;E&gt;(); &#125; // 省略其他代码&#125; 关于 CopyOnWriteArrayList 的操作写操作 在 CopyOnWriteArrayList 里处理写操作（包括 add、remove、set 等）是先将原始的数据通过 JDK1.6 的 Arrays.copyof() 来生成一份新的数组。add 的代码如下： 123456789101112131415public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; // 这里是生产新的数组 Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125; 后续的操作都是在新的数据对象上进行写，写完后再将原来的引用指向到当前这个数据对象，这样保证了每次写都是在新的对象上（因为要保证写的一致性，这里要对各种写操作要加一把锁，JDK1.6 在这里用了重入锁）， 读操作 读的时候就是在引用的当前对象上进行读（包括 get，iterator 等），不存在加锁和阻塞，针对 iterator 使用了一个叫 COWIterator 的简化版迭代器，因为不支持写操作，当获取 CopyOnWriteArrayList 的迭代器时，是将迭代器里的数据引用指向当前引用指向的数据对象，无论未来发生什么写操作，都不会再更改迭代器里的数据对象引用，所以迭代器也很安全。 结论因为 CopyOnWriteArraySet 的内部操作都是基于 CopyOnWriteArrayList 的，从异常来看：1java.util.concurrent.CopyOnWriteArrayList$COWIterator.remove(CopyOnWriteArrayList.java:1178) COWIterator 是 CopyOnWriteArrayList 内部提供的一个简化版的迭代器。所以异常里面出现这个就理所应当了。在来看下 COWIterator 这里简化版的迭代器的 remove 方法： 12345678/*** Not supported. Always throws UnsupportedOperationException.* @throws UnsupportedOperationException always; &#123;@code remove&#125;* is not supported by this iterator.*/public void remove() &#123; throw new UnsupportedOperationException();&#125; 这里实际上是直接就会抛出异常的，另外这里在多补充一个关于 HashSet 的迭代器移除，HashSet 其实内部是持有的 HashMap 实例，因此它的迭代器是 HashMap 内部提供的 HashIterator：1234567891011public final void remove() &#123; Node&lt;K,V&gt; p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; removeNode(hash(key), key, null, false, false); expectedModCount = modCount;&#125; 这里其实也可以看到，在对非安全的集合做 remove 操作时会经常遇到的 ConcurrentModificationException 这个异常。]]></content>
      <categories>
        <category>java 基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>set</tag>
        <tag>CopyOnWriteArraySet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 实践系列-Filter 中的异常处理和 Controller 中的异常处理]]></title>
    <url>%2F2020%2F02%2F12%2Fspringboot%2Fspringboot-series-filter-exception%2F</url>
    <content type="text"><![CDATA[本篇主要是记录如何使用 SpringBoot 所提供的 ErrorController 这个接口能力；其内置了一个 BasicErrorController 对异常进行统一的处理，当在 Controller 发生异常的时候会自动把请求 forward 到 /error 这个请求 path 下(/error 是 SpringBoot 提供的一个默认的mapping)。BasicErrorController 提供两种返回错误：1、页面返回；2、json 返回。 背景开发中遇到的一个问题：项目中所有的 rest 请求均是通过 json 形式返回，且自定义了一个统一的数据结构对象，如下： 123456789public class Response&lt;T&gt; &#123; // 数据 private T data; // success 标记 private boolean success; // 异常信息 private String error; // 省略 get set&#125; 这个结构非常常见，相信很多开发者都这么玩过。项目中 rest 请求返回的所有结果都是以 Response 对象形式返回，如下： 1234567@RequestMapping("test")public Response&lt;String&gt; testApi()&#123; Response&lt;String&gt; result = new Response&lt;&gt;(); result.setData("this is glmapper blog"); result.setSuccess(true); return result;&#125; 这基本是最简化版的一个模型；出于安全考虑，现在有个需求是需要对每个请求做校验，比如校验请求中是否携带 token 这种。思路很简单就是通过拦截器或者过滤器的方式来对请求做拦截检验。 其实不管是拦截器还是过滤器，需要考虑的一个问题是，在校验不通过或者校验时产生异常的情况下，怎么把异常信息以项目中规定的统一数据格式返回，即返回 Response。 直接将 Response 写回去利用 ServletResponse 中提供的 PrintWriter，将 Response 以 json 格式直接 print 回去。大概代码如下： 12345678910111213141516171819202122232425262728293031@Overridepublic void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest request = (HttpServletRequest) servletRequest; String requestURI = request.getRequestURI(); // mock 测试异常请求 if (requestURI.contains("testTokenError")) &#123; Response&lt;String&gt; response = new Response&lt;&gt;(); response.setError("token validation fails"); // 回写异常信息 returnResponse((HttpServletResponse)servletResponse,JSONObject.toJSONString(response)); // 返回 return; &#125; chain.doFilter(servletRequest, servletResponse);&#125;private void returnResponse(HttpServletResponse response, String data) &#123; PrintWriter writer = null; response.setCharacterEncoding("UTF-8"); response.setContentType("text/html; charset=utf-8"); try &#123; writer = response.getWriter(); // 通过 PrintWriter 将 data 数据直接 print 回去 writer.print(data); &#125; catch (IOException e) &#123; &#125; finally &#123; if (writer != null) writer.close(); &#125;&#125; 这种方式比较简单和直接，print 异常数据之后直接 return，不在继续过滤器链。 抛出异常，通过 BasicErrorController 方式处理这种方式是利用了 SpringBoot 本身提供的能力，可以更优雅的处理错误信息。代码大致如下： 1、是在 Filter 中就直接抛出一个异常 123456789101112@Overridepublic void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest request = (HttpServletRequest) servletRequest; String requestURI = request.getRequestURI(); // mock 测试异常请求 if (requestURI.contains("testTokenError")) &#123; // 直接返回一个自定义的异常 throw new ValidationException("token validation fails"); &#125; chain.doFilter(servletRequest, servletResponse);&#125; 2、定义一个异常处理的 Controller 这里定义一个 TokenErrorController ，继承自 SpringBoot 提供的 BasicErrorController 这个类，然后重写 error 这个方法（如果是页面的话，重写 errorHtml 这个方法），用于返回自定义的 Response 数据。代码如下： 1234567891011121314151617181920@RestControllerpublic class TokenErrorController extends BasicErrorController &#123; // 重写 error 方法 @Override @RequestMapping(produces = &#123; MediaType.APPLICATION_JSON_VALUE &#125;) public ResponseEntity&lt;Map&lt;String, Object&gt;&gt; error(HttpServletRequest request) &#123; Map&lt;String, Object&gt; body = getErrorAttributes(request, isIncludeStackTrace(request, MediaType.ALL)); HttpStatus status = getStatus(request); // 拿到 body 中的异常 message String message = body.get("message").toString(); // 构建 Response 对象 Response response = new Response(); // 将 message 的 设置到 response response.setError(message); // 返回 return new ResponseEntity(response, status); &#125; // 省略其他无关代码&#125; 这样就可以实现在不改动之前工程任何代码的情况下只处理额外 Filter 中抛出的异常了。需要注意的是，上述是通过 BasicErrorController 来接受了 Filter 抛出的异常信息，然后再通过 BasicErrorController 将异常信息进行包装并且返回。为什么要提一下这个呢？主要是为了和 SpringBoot 中基于 REST 请求层所提供的两个用于处理全局异常的注解区分，这两个注解分别是 @ControllerAdvice 和 @RestControllerAdvice，通过注解的名字其实就能看出，SpringBoot 中，可以通过这两个注解来实现对 @Controller 和 @RestController 标注的类进行全局拦截，因为是 Controller 层面的 AOP 拦截，所以对于 Filter 中抛出的异常，通过 @ControllerAdvice 和 @RestControllerAdvice 两个注解定义的全局异常处理器是没法处理的。 下面就简单介绍下 @ControllerAdvice 和 @RestControllerAdvice 这两个注解的使用。 全局异常处理自定义一个 OtherExcepetion ，然后再使用基于 @RestControllerAdvice 注解编写一个全局异常处理器。 1234567891011@RestControllerAdvicepublic class OtherExceptionHandler &#123; // 这里只处理 OtherException 异常类型 @ExceptionHandler(value = OtherException.class) public Response&lt;String&gt; otherExceptionHandler(HttpServletRequest req, OtherException e)&#123; Response response = new Response(); response.setError(e.getMessage()); return response; &#125; // 当然你也可以定义处理其他异常的 @ExceptionHandler&#125; 这种方式是没法处理 Filter 中异常的，只能处理 Controller 里面抛出的异常。 小结本篇主要记录了在 SpringBoot 中如何保证 Filter 中抛出的异常能和业务一样以指定类型的对象返回，并对 SpringBoot 中提供的基于 Controller 层异常捕获处理进行简单介绍。两者处理异常的思路是不同的： BasicErrorController：接受来自 /error 的异常请求处理，Filter 中抛出的异常先 forward 到 /error，然后处理。 @RestControllerAdvice：通过对于所有 @Controller 注解所标注的类进行 AOP 拦截，能够根据异常类型匹配具体的 ExceptionHandler 进行处理。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>springmvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[菜鸟成长系列-适配器模式]]></title>
    <url>%2F2020%2F01%2F28%2Fjava-design-model-adapter%2F</url>
    <content type="text"><![CDATA[本文为阅读 《JAVA与模式》的个人笔记，文中相关概念及背景描述参考书本。如有不当，请联系指正。 定义：适配器模式把一个类的接口变换成客户端所期待的另一种接口，从而使原本因接口不匹配为无法在一起工作的两个类能够在一起工作 -《JAVA与模式》 适配、包装、转换几个字眼从汉字的字面意思来说是不同的，不同在于这几个词所对应的行为不同；但是他们所期望的目的是差不多一致的，就是将不可用变成可用。所以常说的适配器模式、转换器模式以及包装模式指的都是一个模式。 适配器模式的两种形式在 java 语言体系中，根据适配类型的不同。适配器模式可以分为：类的适配器模式和对象的适配器模式。 类的适配器模式顾名思义，类的适配器模式是把被适配的类的 API 转换成为目标类的 API。 这里涉及到三个角色： 目标角色：客户端期待的接口 1234567891011public interface Target &#123; /** * 源类没有的方法 */ void sampleOperation1(); /** * 源类没有的方法 */ void sampleOperation2();&#125; 源角色：现有需要被适配的接口 12345public class Adaptee &#123; void sampleOperation1()&#123; // todo your biz &#125;&#125; 适配器角色：把源接口转换成目标接口的适配器类。 1234567891011121314public class Adapter extends Adaptee implements Target &#123; @Override public void sampleOperation1() &#123; // todo your biz &#125; /** * 由于源类没有 sampleOperation2 方法，因此适配器类中补充上这个方法 */ @Override public void sampleOperation2() &#123; // todo your biz &#125;&#125; Target 接口申明了两个方法：sampleOperation1 和 sampleOperation2 ，而源角色 Adaptee 是一个具体的类，它只有一个 sampleOperation1 方法，但是没有 sampleOperation2 这个方法。适配器角色 Adapter 扩展了 Adaptee ，同时又实现了目标接口，由于 Adaptee 没有 sampleOperation2 方法，而目标接口又要求这个方法，因此适配器角色 Adapter 实现了这个方法。 适配器模式所达到的效果是：使用一个具体类把源（Adaptee）适配到目标（Target）中，这样一来，如果源以及源的子类都使用此类适配，就行不通了。由于适配器类是源的子类，因此可以在适配器类中置换掉源的一些方法；另外，由于只引进了一个适配器类，因此只有一个路线达到目标类，使问题得到简化。 对象的适配器模式与类的适配器模式一样，对象的适配器模式把被适配的类的 API 转换成为目标类的 API ，与类的适配器模式不同的是，对象的适配器模式不是使用继承关系连接到 Adaptee 类，而是使用委派关系连接到 Adaptee 类。这里我们只需要对前面的 Adapter 做简单的修改即可。 123456789101112131415161718public class ObjAdapter implements Target &#123; private final Adaptee adaptee; public ObjAdapter(Adaptee adaptee)&#123; this.adaptee = adaptee; &#125; @Override public void sampleOperation1() &#123; adaptee.sampleOperation1(); &#125; @Override public void sampleOperation2() &#123; // todo your biz &#125;&#125; 实际上这里所说的委派就是组合。 适配器模式的用意在于将接口不同而功能相同或者相近的两个接口加以转换，这里面包括适配器角色补充了一个源角色没有的方法。需要注意的是，不要误以为适配器模式就是为了补充源角色没有的方法而准备的。 在什么情况下需要使用适配器模式书中提到的有三种场景，但是在实际的工程中，场景要远远多于书中所说的。 1、系统需要使用现有的类，而此类的接口不符合系统的需要 2、想要建立一个可以重复使用的类，用于与一些彼此之间没有太大关联的一些类，包括一些可能在将来引进的类一起工作，这些源类不一定有很复杂的接口 3、在设计里，需要改变多个已有的子类的接口，如果使用类的适配器模式，就需要针对每一个子类做一个适配器，而这不太实际。 与一些模式的区别与桥接器模式的区别桥接器模式的用意是要把实现和它的接口分开，以便于它们可以独立的变化。桥接器模式并不是用来把一个已有的对象接到不匹配的接口上的，当一个客户端只知道一个特定的接口，但是又必须与具有不同接口的类打交道时，就应当使用适配器模式。 与装饰模式的区别一个装饰类也是位于客户端和另一个 Component 对象之间的，在它接到客户端的调用后把调用传给一个或者几个 Component 对象。一个纯粹的装饰类必须与 Component 对象在接口上的完全相同，并增强后者的功能。 与适配器类不同的是，装饰类不能改变它所装饰的 Component 对象的接口。 装饰模式可以这样理解，《开局一把枪，装备全靠打：皮肤、装备加持就是角色的装饰》 小结本篇是在阅读 《JAVA与模式》一书所做的笔记记录，以备后续方便查阅。]]></content>
      <categories>
        <category>架构之路</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一文详解蚂蚁金服分布式链路组件 SOFATracer 的埋点机制]]></title>
    <url>%2F2020%2F01%2F20%2Fsofa-tracer-integration-analysis%2F</url>
    <content type="text"><![CDATA[SOFATracer 是一个用于分布式系统调用跟踪的组件，通过统一的 TraceId 将调用链路中的各种网络调用情况以日志的方式记录下来，以达到透视化网络调用的目的，这些链路数据可用于故障的快速发现，服务治理等。 GITHUB 地址：https://github.com/sofastack/sofa-tracer/pulls （欢迎 star）官方文件地址：https://www.sofastack.tech/projects/sofa-tracer/overview/ 2018 年末时至 2019 年初，SOFA 团队发起过 剖析-sofatracer-框架 的源码解析系列文章。这个系列中，基本对 SOFATracer 所提供的能力及实现原理都做了比较全面的分析，有兴趣的同学可以看下。 从官方文档及 PR 来看，目前 SOFATracer 已经支持了对以下开源组件的埋点支持： Spring MVC RestTemplate HttpClient OkHttp3 JDBC Dubbo(2.6/2.7) SOFARPC Redis MongoDB Spring Message Spring Cloud Stream (基于 Spring Message 的埋点) RocketMQ Spring Cloud FeignClient Hystrix 大多数能力提供在 3.x 版本，2.x 版本从官方 issue 中可以看到后续将不在继续提供新的功能更新；这也是和 SpringBoot 宣布不在继续维护 1.x 版本有关系。 本文将从插件的角度来分析，SOFATracer 是如何实现对上述组件进行埋点的；通过本文，除了了解 SOFATracer 的埋点机制之外，也可以对上述组件的基本扩展机制以及基本原理有一点学习。 标准 Servlet 规范埋点原理SOFATracer 支持对标准 Servlet 规范的 web mvc 埋点，包括普通的 servlet 和 Springmvc 等；基本原理就是基于 Servelt 规范所提供的 javax.servlet.Filter 过滤器接口扩展实现。 过滤器位于 client 和 web 应用程序之间，用于检查和修改两者之间流过的请求和响应信息。在请求到达 Servlet 之前，过滤器截获请求。在响应送给客户端之前，过滤器截获响应。多个过滤器形成一个 FilterChain，FilterChain 中不同过滤器的先后顺序由部署文件 web.xml 中过滤器映射的顺序决定。最先截获客户端请求的过滤器将最后截获 Servlet 的响应信息。 web 应用程序一般作为请求的接收方，在 Tracer 中应用是作为 server 存在的，其在解析 SpanContext 时所对应的事件为 sr (server receive)。 SOFATracer 在 sofa-tracer-springmvc-plugin 插件中解析及产生 span 的过程大致如下： Servlet Filter 拦截到 request 请求 从请求中解析 SpanContext 通过 SpanContext 构建当前 MVC 的 span 给当前 span 设置 tag、log。 在 filter 处理的最后，结束 span。 当然这里面还会设计到其他很多细节，比如给 span 设置哪些 tag 属性、如果处理异步线程透传等等。本篇不展开细节探讨，有兴趣的同学可以自行阅读代码或者和我交流。 Dubbo 埋点原理Dubbo 埋点在 SOFATracer 中实际上提供了两个插件，分别用于支持 Dubbo 2.6.x 和 Dubbo 2.7.x；Duddo 埋点也是基于 Filter ，此Filter 是 Dubbo 提供的 SPI 扩展-调用拦截扩展 机制实现。 像 Dubbo 或者 SOFARpc 等 rpc 框架的埋点，通常需要考虑的点比较多，首先是 rpc 框架分客户端和服务端，所以在埋点时 rpc 的客户端和服务端必须要有所区分；再者就是 rpc 的调用方式包括很多种，如常见的同步调用、异步调用、oneway 等等，调用方式不同，所对应的 span 的结束时机也不同，重要是的基本所有的 rpc 框架都会使用线程池用来发起和处理请求，那么如何保证 tracer 在多线程环境下不串也很重要。 另外 Dubbo 2.6.x 和 Dubbo 2.7.x 在异步回调处理上差异比较大，Dubbo 2.7.x 中提供了 onResponse 方法（后面又升级为 Listener，包括 onResponse 和 onError 两个方法）；而 Dubbo 2.6.x 中则并未提供相应的机制，只能通过对 future 的硬编码处理来完成埋点和上报。 这个问题 zipkin brave 对 Dubbo 2.6.x 的埋点时其实也没有考虑到，在做 SOFATracer 支持 Dubbo 2.6.x 时发现了这个 bug，并做了修复。 SOFATracer 中提供的 DubboSofaTracerFilter 类： 1234@Activate(group = &#123; CommonConstants.PROVIDER, CommonConstants.CONSUMER &#125;, value = "dubboSofaTracerFilter", order = 1)public class DubboSofaTracerFilter implements Filter &#123; // todo trace&#125; SOFATracer 中用于处理 Dubbo 2.6.x 版本中异步回调处理的核心代码： Dubbo 异步处理依赖 ResponseFuture 接口，但是 ResponseFuture 在核心链路上并非是以数据或者 list 的形式存在，所以在链路上只会存在一个 ResponseFuture，因此如果我自定义一个类来实现 ResponseFuture 接口是没法达到预期目的的，因为运行期会存在覆盖 ResponseFuture 的问题。所以在设计上，SOFATracer 会通过 ResponseFuture 构建一个新的 FutureAdapter出来用于传递。 123456789101112boolean ensureSpanFinishes(Future&lt;Object&gt; future, Invocation invocation, Invoker&lt;?&gt; invoker) &#123; boolean deferFinish = false; if (future instanceof FutureAdapter) &#123; deferFinish = true; ResponseFuture original = ((FutureAdapter&lt;Object&gt;) future).getFuture(); ResponseFuture wrapped = new AsyncResponseFutureDelegate(invocation, invoker, original); // Ensures even if no callback added later, for example when a consumer, we finish the span wrapped.setCallback(null); RpcContext.getContext().setFuture(new FutureAdapter&lt;&gt;(wrapped)); &#125; return deferFinish;&#125; http 客户端埋点原理http 客户端埋点包括 HttpClient、OkHttp、RestTemplate 等，此类埋点一般都是基于拦截器机制来实现的，如 HttpClient 使用的 HttpRequestInterceptor、HttpResponseInterceptor；OkHttp 使用的 okhttp3.Interceptor；RestTemplate 使用的 ClientHttpRequestInterceptor。 以 OkHttp 为例，简单分析下 http 客户端埋点的实现原理： 123456789101112@Overridepublic Response intercept(Chain chain) throws IOException &#123; // 获取请求 Request request = chain.request(); // 解析出 SpanContext ，然后构建 Span SofaTracerSpan sofaTracerSpan = okHttpTracer.clientSend(request.method()); // 发起具体的调用 Response response = chain.proceed(appendOkHttpRequestSpanTags(request, sofaTracerSpan)); // 结束 span okHttpTracer.clientReceive(String.valueOf(response.code())); return response;&#125; Datasource 埋点原理和标准 servlet 规范实现一样，所有基于 javax.sql.DataSource 实现的 DataSource 均可以使用 SOFATracer 进行埋点。因为 DataSource 并没有提供像 Servlet 那样的过滤器或者拦截器，所以 SOFATracer 中没法直接通过常规的方式（Filter/SPI扩展拦截/拦截器等）进行埋点，而是使用了代理模式的方式来实现的。 上图为 SOFATracer 中 DataSource 代理类实现的类继承结构体系；可以看出，SOFATracer 中自定义了一个 BaseDataSource 抽象类，该抽象类继承 javax.sql.DataSource 接口，SmartDataSource 作为 BaseDataSource 的唯一子类，也就是 SOFATracer 中所使用的 代理类。所以如果你使用了 sofa-tracer-datasource-plugin 插件的话，可以看到最终运行时的 Datasource 类型是 com.alipay.sofa.tracer.plugins.datasource.SmartDataSource。 1234567public abstract class BaseDataSource implements DataSource &#123; // 实际被代理的 datasource protected DataSource delegate; // sofatracer 中自定义的拦截器，用于对连接操作、db操作等进行拦截埋点 protected List&lt;Interceptor&gt; interceptors; protected List&lt;Interceptor&gt; dataSourceInterceptors;&#125; Interceptor 主要包括以下三种类型： 以 StatementTracerInterceptor 为例 StatementTracerInterceptor 将将会拦截到所有 PreparedStatement 接口的方法，代码如下： 12345678910111213141516171819202122232425262728public class StatementTracerInterceptor implements Interceptor &#123; // tracer 类型为 client private DataSourceClientTracer clientTracer; public void setClientTracer(DataSourceClientTracer clientTracer) &#123; // tracer 对象实例 this.clientTracer = clientTracer; &#125; @Override public Object intercept(Chain chain) throws Exception &#123; // 记录当前系统时间 long start = System.currentTimeMillis(); String resultCode = SofaTracerConstant.RESULT_SUCCESS; try &#123; // 开始一个 span clientTracer.startTrace(chain.getOriginalSql()); // 执行 return chain.proceed(); &#125; catch (Exception e) &#123; resultCode = SofaTracerConstant.RESULT_FAILED; throw e; &#125; finally &#123; // 这里计算执行时间 System.currentTimeMillis() - start // 结束一个 span clientTracer.endTrace(System.currentTimeMillis() - start, resultCode); &#125; &#125;&#125; 总体思路是，Datasource 通过组合的方式自定义一个代理类（实际上也可以理解为适配器模式中的对象适配模型方式），对所有目标对象的方式进行代理拦截，在执行具体的 sql 或者连接操作之前创建 datasource 的 span，在操作结束之后结束 span，并进行上报。 消息埋点消息框架组件包括很多，像常见的 RocketMQ、Kafka 等；处理各个组件自己提供的客户端之外，像 Spring 就提供了很多消息组件的封装，包括Spring Cloud Stream、Spring Integration、Spring Message 等等。SOFATracer 基于 Spring Message 标准实现了对常见消息组件和 Spring Cloud Stream 的埋点支持，同时也提供了基于 RocketMQ 客户端模式的埋点实现。 Spring Messaging 埋点实现原理spring-messaging 模块为集成 messaging api 和消息协议提供支持。这里我们先看一个 pipes-and-filters 架构模型： spring-messaging 的 support 模块中提供了各种不同的 MessageChannel 实现和 channel interceptor 支持，因此在对 spring-messaging 进行埋点时我们自然就会想到去使用 channel interceptor。 12345678910111213141516171819202122232425262728293031// SOFATracer 实现的基于 spring-messaging 消息拦截器public class SofaTracerChannelInterceptor implements ChannelInterceptor, ExecutorChannelInterceptor &#123; // todo trace&#125;// THIS IS ChannelInterceptorpublic interface ChannelInterceptor &#123; // 发送之前 @Nullable default Message&lt;?&gt; preSend(Message&lt;?&gt; message, MessageChannel channel) &#123; return message; &#125; // 发送后 default void postSend(Message&lt;?&gt; message, MessageChannel channel, boolean sent) &#123; &#125; // 完成发送之后 default void afterSendCompletion(Message&lt;?&gt; message, MessageChannel channel, boolean sent, @Nullable Exception ex) &#123; &#125; // 接收消息之前 default boolean preReceive(MessageChannel channel) &#123; return true; &#125; // 接收后 @Nullable default Message&lt;?&gt; postReceive(Message&lt;?&gt; message, MessageChannel channel) &#123; return message; &#125; // 完成接收消息之后 default void afterReceiveCompletion(@Nullable Message&lt;?&gt; message, MessageChannel channel, @Nullable Exception ex) &#123; &#125;&#125; 可以看到 ChannelInterceptor 实现了消息传递全生命周期的管控，通过暴露出来的方法，可以轻松的实现各个阶段的扩展埋点。 RocketMQ 埋点实现原理RocketMQ 本身是提供了对 Opentracing 规范支持的，由于其支持的版本较高，与 SOFATracer 所实现的 Opentracing 版本不一致，所以在一定程度上不兼容；因此 SOFATracer（opentracing 0.22.0 版本）自身又单独提供了 RocketMQ 的插件。 RocketMQ 埋点其实是通过两个 hook 接口来完成，实际上在 RocketMQ 的官方文档中貌似并没有提到这两个点。 12345// RocketMQ 消息消费端 hook 接口埋点实现类public class SofaTracerConsumeMessageHook implements ConsumeMessageHook &#123;&#125;// RocketMQ 消息发送端 hook 接口埋点实现类public class SofaTracerSendMessageHook implements SendMessageHook &#123;&#125; 首先是 SendMessageHook 接口，SendMessageHook 接口提供了两个方法，sendMessageBefore 和 sendMessageAfter，SOFATracer 在实现埋点时，sendMessageBefore 中用来解析和构建 span，sendMessageAfter 中用于拿到结果然后结束 span。 同样的，ConsumeMessageHook 中也提供了两个方法（consumeMessageBefore和consumeMessageAfter），可以提供给 SOFATracer 来从消息中解析出透传的 tracer 信息然后再将 tracer 信息透传到下游链路中去。 redis 埋点原理SOFATracer 中的 redis 埋点是基于 spring data redis 实现的，没有针对具体的 redis 客户端来埋点。另外 redis 埋点部分参考的是开源社区opentracing-spring-cloud-redis-starter中的实现逻辑。 redis 的埋点实现与 Datasource 的锚点实现基本思路是一致的，都是通过一层代理来是实现的拦截。sofa-tracer-redis-plugin 中对所有的 redis 操作都通过 RedisActionWrapperHelper 进行了一层包装，在执行具体的命令前后通过 SOFATracer 自己提供的 API 进行埋点操作。代码如下： 12345678910111213141516171819202122232425public &lt;T&gt; T doInScope(String command, Supplier&lt;T&gt; supplier) &#123; // 构建 span Span span = buildSpan(command); return activateAndCloseSpan(span, supplier);&#125;// 在 span 的生命周期内执行具体命令private &lt;T&gt; T activateAndCloseSpan(Span span, Supplier&lt;T&gt; supplier) &#123; Throwable candidateThrowable = null; try &#123; // 执行命令 return supplier.get(); &#125; catch (Throwable t) &#123; candidateThrowable = t; throw t; &#125; finally &#123; if (candidateThrowable != null) &#123; // ... &#125; else &#123; // ... &#125; // 通过 tracer api 结束一个span redisSofaTracer.clientReceiveTagFinish((SofaTracerSpan) span, "00"); &#125;&#125; 除此之后 mongodb 的埋点也是基于 spring data 实现，埋点的实现思路和 redis 基本相同，这里就不在单独分析。 总结本文对蚂蚁金服分布式链路组件 SOFATracer 的埋点机制做了简要的介绍；从各个组件的埋点机制来看，整体思路就是对组件操作进行包装，在请求或者命令执行的前后进行 span 构建和上报。目前一些主流的链路跟踪组件像 brave 也是基于此思路，区别在于 brave 并非是直接基于 opentracing 规范进行编码，而是其自己封装了一整套 api ，然后通过面向 opentracing api 进行一层适配；另外一个非常流行的 skywalking 则是基于 java agent 实现，埋点实现的机制上与 SOFATracer 和 brave 不同。 参考 SOFATracer spring源码分析之spring-messaging模块详解]]></content>
      <categories>
        <category>SOFA</category>
      </categories>
      <tags>
        <tag>SOFATracer</tag>
        <tag>OpenTracing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 源码系列-自动配置及 starter 机制解析]]></title>
    <url>%2F2020%2F01%2F05%2Fspringboot%2Fspringboot-series-auto-configure%2F</url>
    <content type="text"><![CDATA[一家之言，如有任何错误，请批评指出，不胜感激 本篇主要来讨论研究两个问题：1、什么自动配置，2、如何编写自动配置 在使用 Spring 作为项目开发框架的过程中，当需要集成某个组件时，通常需要大量的 xml 配置才可以让项目工程 run 起来，下面先以 mybatis 为例，来看下如何使用 mybatis-Spring 模块，需要哪些必不可少的依赖和配置。 使用 mybatis-spring任何组件的集成都绕不过两个问题：依赖和配置，关于配置在这篇文章中介绍了配置的一些点，有兴趣的可以看下。 依赖从 mybatis 的官方文当可以了解到，要使用 MyBatis-Spring 模块，需要在类路径下包含 mybatis-spring.jar 文件和相关依赖（如：mysql-connector-java）即可。如果使用 Maven 作为构建工具，则在 pom.xml 中加入以下代码即可： 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;$&#123;latest.version&#125;&lt;/version&gt;&lt;/dependency&gt; bean 配置Spirng 集成 mybatis 通常需要以下 bean 配置： 1、dataSource 1234&lt;bean id="dataSource" class="com.alibaba.druid.pool.DruidDataSource" init-method="init" destroy-method="close"&gt; // 省略其他配置&lt;/bean&gt; 2、sqlSessionFactory 123&lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;property name="dataSource" ref="dataSource" /&gt;&lt;/bean&gt; 3、其他：包扫描和事务配置 1234567891011&lt;!-- DAO 接口所在包名，Spring 会自动查找其下的类，并将其定义为一个 Spring Bean --&gt;&lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;property name="basePackage" value="com.glmapper.bridge.boot.dao" /&gt; &lt;property name="sqlSessionFactoryBeanName" value="sqlSessionFactory"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- (事务管理)transaction manager --&gt;&lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource" /&gt;&lt;/bean&gt; 这些个 bean 是在 Spring 中使用 mybatis 框架时基本必不可少的配置。那么在 SpringBoot 中呢？ SpringBoot 中如何集成 mybatis 的SpringBoot 集成 mybatis 非常简单，加一下下面的 starter ，再在 application.properties 配置下数据库连接配置即可；不需要配置 datasource，sqlSessionFactory 等这些 bean。 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt;&lt;/dependency&gt; 官方文档：https://mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure/ mybatis starter 是如何规避 bean 配置的引用 mybatis-spring-boot-starter 既然可以不用在 xml 中配置 bean ，那肯定是这些 bean 是在 mybatis-spring-boot-starter 中通过某种方式被创建了。 在 SpringBoot 官方文档的描述中，starter 只是用来管理依赖的，一般不会有代码，自动配置的代码一般在 xxxx-autoconfigure 中。mybatis 的自动配置相关代码是在 mybatis-spring-boot-autoconfigure 中。 mybatis-spring-boot-autoconfigure 这依赖中只有简单的几个类，其中最核心的就是 MybatisAutoConfiguration 这个配置类。另外一个 MybatisProperties 是 mybatis spring boot 的属性配置类，就是常见的 mybatis.xxxx。 MybatisAutoConfiguration 自动配置类MybatisAutoConfiguration 的定义及其生效条件： 1.当前 classpath 下必须有 SqlSessionFactory 和 SqlSessionFactoryBean 这两个类 2.存在 DataSource bean 实例 3.有配置类 MybatisProperties 实例 4.在 DataSourceAutoConfiguration 和 MybatisLanguageDriverAutoConfiguration 两个自动配置类之后刷新 123456789101112131415161718@ConditionalOnClass(&#123; SqlSessionFactory.class, SqlSessionFactoryBean.class &#125;)@ConditionalOnSingleCandidate(DataSource.class)@EnableConfigurationProperties(MybatisProperties.class)@AutoConfigureAfter(&#123; DataSourceAutoConfiguration.class, MybatisLanguageDriverAutoConfiguration.class &#125;)public class MybatisAutoConfiguration implements InitializingBean &#123; // 定义 SqlSessionFactory bean @Bean @ConditionalOnMissingBean public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception &#123; // &#125; // check @Override public void afterPropertiesSet() &#123; checkConfigFileExists(); &#125; // 省略其他code&#125; 从上面的代码片段大体可以知道 MybatisAutoConfiguration 所做的事情主要包括以下几点：1、刷新 SqlSessionFactory 和 SqlSessionFactoryBean 两个 bean；2、afterPropertiesSet 中做一些准备或者检验工作（这里就是 check 了 mybatis 的配置文件是否配置了） 关于 DataSource 的 bean ，则是由 DataSourceAutoConfiguration 这个配置类中来定义。 具体代码有兴趣的读者可以自己查阅相关源码，这里就不展开了。 所以整体看来， MybatisAutoConfiguration 及其所依赖的 xxxConfiguration 会帮助用户定义 bean 和解析配置。 mybatis 自动配置的 bean 是如何生效的上面分析到 MybatisAutoConfiguration 及其依赖的配置自动类会帮助创建运行时所需要的 bean，那么这些 bean 是如何被 SpringBoot 框架感知并加载的呢？ 其实一般的项目工程中，如果我们在一个类上打了 @Configuration 注解的话，Spring 会直接能够加载到的（前提是这个类所在的包在启动类的子包下）。但是在框架层面，项目的包和所引入的组件包的包路径肯定是有差异的，所以在一些情况下会刷不到依赖中的 bean。 SpringBoot 中提供了一种类似于 SPI 机制的方式来帮忙加载 EnableAutoConfiguration、ApplicationListner、ApplicationContextInitializer 等类型的 bean。比如 mybatis 自动配置的配置如下： 1234# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\org.mybatis.spring.boot.autoconfigure.MybatisLanguageDriverAutoConfiguration,\org.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration 其处理逻辑在 SpringApplication 类中，具体解析方法如下： 12345678910111213141516171819private &lt;T&gt; List&lt;T&gt; createSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, ClassLoader classLoader, Object[] args, Set&lt;String&gt; names) &#123; List&lt;T&gt; instances = new ArrayList&lt;&gt;(names.size()); for (String name : names) &#123; try &#123; Class&lt;?&gt; instanceClass = ClassUtils.forName(name, classLoader); Assert.isAssignable(type, instanceClass); // 反射拿到构造函数 Constructor&lt;?&gt; constructor = instanceClass.getDeclaredConstructor(parameterTypes); // 创建 bean T instance = (T) BeanUtils.instantiateClass(constructor, args); instances.add(instance); &#125; catch (Throwable ex) &#123; throw new IllegalArgumentException("Cannot instantiate " + type + " : " + name, ex); &#125; &#125; return instances;&#125; 如何编写自己的 starter本小节将结合上面的描述，自定义一个 starter，让你的项目和 xml bean 配置说再见。 场景描述：有两个 bean,一个 parentBean，一个 childBean，parentBean 需要依赖 childBean，parentBean中又要依赖 http 包 原来的 xml 配置： 1234&lt;bean id="parentBean" class="com.glmapper.bridge.boot.service.impl.ParentBean"&gt; &lt;property name="childBean" ref="childBean"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id="childBean" class="com.glmapper.bridge.boot.service.impl.ChildBean"/&gt; 下面考虑的是将这些 bean 作为公共组件提供给其他项目工程用，从框架角度来看，最佳实践是： 提供一个 autoconfigure 模块用于编写自动配置类代码 提供一个 starter，用于提供给外部用户使用 编写 autoconfigure 自动配置类 123456789101112131415161718@Configuration// parentBean 依赖 HttpClient，所以如果没有 HttpClient 则不会刷新当前自动配置类@ConditionalOnClass(HttpClient.class)public class GlmpperAutoConfiguration &#123; // ParentBean bean 定义 @Bean @ConditionalOnMissingBean // 如果当前 Spring 容器中已经存在 parentBean则不会再创建 public ParentBean parentBean()&#123; return new ParentBean(); &#125; // ChildBean bean 定义 @Bean @ConditionalOnMissingBean public ChildBean childBean()&#123; return new ChildBean(); &#125;&#125; 依赖 scope 使用 provided，不直接打在 autoconfigure 依赖中 12345678&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.6&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 编写 spring.factories，在 resources/META-INF/ 新建一个 spring.factories 文件，配置如下： 123# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\ com.glmapper.bridge.boot.autoconfigure.GlmpperAutoConfiguration 编写 starterstarter 里面没有代码，只做依赖管控 1234567891011&lt;dependency&gt; &lt;groupId&gt;com.glmapper.bridge.boot&lt;/groupId&gt; &lt;artifactId&gt;guides-autoconfigure&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.6&lt;/version&gt;&lt;/dependency&gt; starter 里面包括了自动配置的依赖和 httpclient 的依赖，所以用户在引入 starter 之后所有生效条件都满足了，就会在启动时直接刷新。 示例工程: https://github.com/glmapper/springboot-series-guides.git（guides-autoconfigure 模块和 guides-starter 模块） 小结 本篇是介于源码解析和实践系列之间的一篇，作为源码解析的终篇和实践的开篇。 本篇以 mybatis 为例，对 spring 环境和 SpringBoot 环境下的使用方式做了简单对比；以此为切入点，介绍了 SpringBoot 中的自动配置及 starter 最佳实践。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 实践系列-外部化配置优先级问题]]></title>
    <url>%2F2020%2F01%2F03%2Fspringboot%2Fspringboot-series-externalize-prop%2F</url>
    <content type="text"><![CDATA[本文主要针对 spring.profiles.active、spring.config.location 以及 spring.config.additional-location 的作用机制及优先级问题进行实践对比。 本文案例工程已上传 github 仓库：https://github.com/glmapper/springboot-series-guides/tree/master/guides-properties spring.profiles.active除了 application.properties 文件之外，profile-specific 配置也可以通过以下命名方式来定义:application-{profile}.properties。在没有使用 active 指定 profiles 的情况下，Environment 会指定一组默认的 profiles（默认情况下是[default])，换句话说就是，如果没有显示的激活 profiles 配置文件，则默认加载的是 application-default.properties 配置文件。 profile-specific 配置文件的属性与标准 application.properties 从相同的位置加载（一般是 classpath 下）；profile-specific 指定的 properties 配置文件始终覆盖默认配置。 在案例工程中(guides-properties)，resources 下面包括 application.properties 和 application-dev.properties 两份配置文件 application.properties 文件配置 12spring.application.name=appNameInnertestKey=key-default application-dev.properties 文件配置 1testKey=key-dev 通过以下代码在启动时将配置值输出： 12345678910@Value("$&#123;testKey&#125;")private String testKey;@PostConstructprivate void init()&#123; System.out.println("-------------------------------"); System.out.println(testKey); System.out.println("-------------------------------");&#125; 不指定 spring.profiles.active 时通过 java -jar guides-properties/target/guides-properties-0.0.1-SNAPSHOT.jar 启动工程，console 输出如下： 12342020-01-04 00:08:47.279 INFO 11050 --- [ main] com.glmapper.bridge.boot.BootStrap : No active profile set, falling back to default profiles: default-------------------------------key-default------------------------------- 结论是，如果不显示指定 profiles，则使用默认的。 指定 spring.profiles.active 时通过 java -jar -Dspring.profiles.active=dev guides-properties/target/guides-properties-0.0.1-SNAPSHOT.jar 启动工程，console 输出如下： 12342020-01-04 00:08:14.426 INFO 11040 --- [ main] com.glmapper.bridge.boot.BootStrap : The following profiles are active: dev-------------------------------key-dev------------------------------- 结论是，在显示指定 profiles 的情况下，会覆盖默认 application.properties 中的配置值。 spring.config.location在 SpringBoot 2.x 中 spring.config.location 的语义发生了变更(此项配置会导致 classpath 中的 application.properties 不再生效)。原因如下： 12345678910private Set&lt;String&gt; getSearchLocations() &#123; // spring.config.location 直接使用此份文件，不会再处理其他配置文件 if (this.environment.containsProperty(CONFIG_LOCATION_PROPERTY)) &#123; return getSearchLocations(CONFIG_LOCATION_PROPERTY); &#125; Set&lt;String&gt; locations = getSearchLocations(CONFIG_ADDITIONAL_LOCATION_PROPERTY); locations.addAll( asResolvedSet(ConfigFileApplicationListener.this.searchLocations, DEFAULT_SEARCH_LOCATIONS)); return locations;&#125; 在工程的根目录的 conf 目录下新建一个 application-conf.properties 配置文件，内容如下： 1testKey=key-spring.config.location 通过 java -jar -Dspring.config.location=conf/application-conf.properties guides-properties/target/guides-properties-0.0.1-SNAPSHOT.jar 启动工程，发现启动报错，原因是因为 application-conf.properties 中没有 配置 spring.application.name，而 spring.application.name 是在 resources 目录下的 application.properties 中的，所以也间接说明前面提到的，会使 classpath 下的配置失效。新增 spring.application.name 之后，重新启动工程， 12spring.application.name=guides-propertiestestKey=key-spring.config.location 输出结果如下： 12342020-01-04 00:19:12.225 INFO 11147 --- [ main] com.glmapper.bridge.boot.BootStrap : No active profile set, falling back to default profiles: default-------------------------------key-spring.config.location------------------------------- 所以在使用 spring.config.location 指定外部配置文件时，需要此份配置文件需全量满足当前工程运行时所需，因为它不会去与 resources 目录下的配置文件去做 merge 操作。 spring.config.additional-location在使用 spring.config.additional-location 这种方式自定义 locations 时，除了默认 locations 之外，还会使用 spring.config.additional-location 指定的。 additional-location：言外之意就是增量的配置 在工程的根目录的 conf 目录下新建一个 application-addition.properties 配置文件，内容如下：1testKey=key-addition 通过 java -jar -Dspring.config.additional-location=conf/application-addition.properties guides-properties/target/guides-properties-0.0.1-SNAPSHOT.jar 启动工程，输出结果如下：12342020-01-04 00:28:30.048 INFO 11384 --- [ main] com.glmapper.bridge.boot.BootStrap : No active profile set, falling back to default profiles: default-------------------------------key-addition------------------------------- 结论是，会覆盖默认 application.properties 中的配置值。 spring.config.additional-location 与 spring.profiles.active 配置加载关系spring.config.location 不用多说，它就是独立的一份，使用它就不能使用其它的。所以这里只分析 spring.config.additional-location 与 spring.profiles.active 配置加载关系。 同时指定两个配置通过 java -jar -Dspring.profiles.active=dev -Dspring.config.additional-location=conf/application-addition.properties guides-properties/target/guides-properties-0.0.1-SNAPSHOT.jar 启动工程，输出如下： 12342020-01-04 00:32:59.044 INFO 11451 --- [ main] com.glmapper.bridge.boot.BootStrap : The following profiles are active: dev-------------------------------key-dev------------------------------- 为了排除与 -D 参数顺序有关，也使用如下方式再执行一次：java -jar -Dspring.config.additional-location=conf/application-addition.properties -Dspring.profiles.active=dev guides-properties/target/guides-properties-0.0.1-SNAPSHOT.jar，输出结果与前面相同，所以可以得出，spring.profiles.active 的优先级比 spring.config.additional-location 要高。 spring.config.additional-location 指定差异增量配置在 spring.config.additional-location 中增加 additionKey 12testKey=key-additionadditionKey=testAddition 使用 java -jar -Dspring.config.additional-location=conf/application-addition.properties -Dspring.profiles.active=dev guides-properties/target/guides-properties-0.0.1-SNAPSHOT.jar 启动工程，输出如下： 123452020-01-04 11:44:42.227 INFO 12821 --- [ main] com.glmapper.bridge.boot.BootStrap : The following profiles are active: dev-------------------------------key-devtestAddition------------------------------- 结论是 spring.config.additional-location 可以用于提供出 profiles 机制或者默认方式之外的增量配置。 小结在使用外部化配置文件时，执行顺序为： spring.config.location &gt; spring.profiles.active &gt; spring.config.additional-location &gt; 默认的 application.proerties。 其中通过 spring.profiles.active 和 spring.config.additional-location指定的配置文件会与 默认的application.proerties merge 作为最终的配置，spring.config.location 则不会。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 源码系列-配置解析]]></title>
    <url>%2F2019%2F12%2F29%2Fspringboot%2Fspringboot-series-propertysource%2F</url>
    <content type="text"><![CDATA[注：本文基于 SpringBoot 2.1.11 版本 说到配置，你能想到的是什么？ 在日常的开发和运维过程中，可以说配置都是及其重要的，因为它可能影响到应用的正常启动或者正常运行。相信在之前 Spring xml 时代，很多人都会被一堆 xml 配置折腾的够呛，除此之外，还有像数据库连接配置、缓存配置、注册中心配置、消息配置等等，这些相信大家都不会陌生。 配置对于开发人员或者运维人员来说可以比喻成一把”钥匙“，可以通过这把”钥匙“让我们的程序 run 起来，可以通过这把 ”钥匙“ 开启或者关闭应用程序的某一个功能。那么为什么会需要配置，对于一个应用来说，配置的意义又是什么呢？ 配置对于框架组件和应用程序的意义配置对于框架组件和应用程序的意义是什么？我的理解是可以让框架组件和应用程序变得灵活，通过配置可以使得一个框架组件或者一个应用程序在不需要做任何自身代码变更的情况下跑在不同的环境、不同的场景下。例如 Dubbo ，用户可以通过配置使得 Dubbo 将服务注册到不同的注册中心，nacos、zookeeper、SOFARegistry 等等；再比如，我有一个应用程序，在 dev 环境和生产环境需要连接不同的数据库，但是我又不想去在代码里面去做修改来适配不同的环境，那么同样我也可以使用配置的方式来做控制。配置可以让框架组件和应用程序变得灵活、不强耦合在某一个场景或者环境下，它可以有很多种存在形态，如常见的是存在文件中、配置中心中、系统环境变量中，对于 JAVA 程序来说还可以是命令行参数或者 -D 参数。可以说任何优秀的框架或者应用，都离不开配置。 那么作为 Java 语言生态里面最优秀的框架， Spring 是如何管理和使用配置的呢？本篇将以 SpringBoot 中的配置为切入点，来进行详细的剖析。 SpringBoot 中的配置Spring Boot 官方文章中使用了单独的章节和大量的篇幅对配置进行了描述，可以见得，配置对于 SpringBoot 来说，是相当重要的。 Spring Boot 允许用户将配置外部化，以便可以在不同的环境中使用相同的应用程序代码，用户可以使用 properties 文件、YAML 文件、环境变量和命令行参数来具体化配置。属性值可以通过使用 @Value 注释直接注入 bean，可以通过 Spring 的环境抽象访问，也可以通过 @ConfigurationProperties 绑定到结构化对象。 在日常的开发中，对于 SpringBoot 中的配置，可能直接想到的就是 application.properties，实际上，从 SpringBoot 官方文档可以看到，SpringBoot 获取配置的方式有多达 17 种；同时 Spring Boot 也提供了一种非常特殊的 PropertyOrder，来允许用户可以在适当的场景下覆盖某些属性值，下面就是官方文档中描述的属性优先加载顺序: 1.在主目录（当 devtools 被激活，则为 ~/.spring-boot-devtools.properties ）中的 Devtools 全局设置属性。 2.在测试中使用到的 @TestPropertySource 注解。 3.在测试中使用到的 properties 属性，可以是 @SpringBootTest 和用于测试应用程序某部分的测试注解。 4.命令行参数。 5.来自 SPRING_APPLICATION_JSON 的属性（嵌入在环境变量或者系统属性【system propert】中的内联 JSON） 6.ServletConfig 初始化参数。 7.ServletContext 初始化参数。 8.来自 java:comp/env 的 JNDI 属性。 9.Java 系统属性（System.getProperties()）。 10.操作系统环境变量。 11.只有 random.* 属性的 RandomValuePropertySource。 12.在已打包的 fatjar 外部的指定 profile 的应用属性文件（application-{profile}.properties 和 YAML 变量）。 13.在已打包的 fatjar 内部的指定 profile 的应用属性文件（application-{profile}.properties 和 YAML 变量）。 14.在已打包的 fatjar 外部的应用属性文件（application.properties 和 YAML 变量）。 15.在已打包的 fatjar 内部的应用属性文件（application.properties 和 YAML 变量）。 16.在 @Configuration 类上的 @PropertySource 注解。 17.默认属性（使用 SpringApplication.setDefaultProperties 指定）。 相信绝大多数都是你不曾用过的，不用纠结，其实用不到也很正常，但是我们还是需要能够知道它提供的方式有哪些，以便于在适当的场景下掏出来镇楼！ Spring 中对于配置最终都是交给 Environment 对象来管理，也就是我们常说的 Spring 环境。比如可以通过以下方式从 Environment 中获取配置值： 12ConfigurableEnvironment environment = context.getEnvironment();environment.getProperty("key"); 那么 Environment 是如何被构建的呢？Environment 与配置的关系又是什么？ Environment 构建Environment 的构建发生在 prepareEnvironment 中，关于 SpringBoot 启动过程想了解更多，可以参考这篇 SpringBoot系列-启动过程分析。 12345678910111213141516private ConfigurableEnvironment getOrCreateEnvironment() &#123; if (this.environment != null) &#123; return this.environment; &#125; switch (this.webApplicationType) &#123; // 标准的 web 应用 case SERVLET: return new StandardServletEnvironment(); // webflux 应用 case REACTIVE: return new StandardReactiveWebEnvironment(); // 非web应用 default: return new StandardEnvironment(); &#125; &#125; 本篇基于非 web 应用分析，所有主要围绕 StandardEnvironment 这个类展开分析。 Environment 类继承结构体系： systemProperties &amp; systemEnvironment在构建 StandardEnvironment 对象的过程中，会初始化 systemProperties &amp; systemEnvironment 两个 PropertySource。其触发时机是在其父类 AbstractEnvironment 的构造函数中。customizePropertySources 方法在 AbstractEnvironment 中并没有具体的实现，其依赖子类完成，如下：1234567891011121314public AbstractEnvironment() &#123; customizePropertySources(this.propertySources);&#125;// 子类 StandardEnvironment 中的实现逻辑@Overrideprotected void customizePropertySources(MutablePropertySources propertySources) &#123; // 构建 systemProperties 配置 propertySources.addLast( new PropertiesPropertySource(SYSTEM_PROPERTIES_PROPERTY_SOURCE_NAME, getSystemProperties())); // // 构建 systemEnvironment 配置 propertySources.addLast( new SystemEnvironmentPropertySource(SYSTEM_ENVIRONMENT_PROPERTY_SOURCE_NAME, getSystemEnvironment()));&#125; 以我本机为例，来分别看下 systemProperties 和 systemEnvironment 主要是哪些东西 systemProperties systemEnvironment defaultProperties &amp; commandLineArgs在构建完默认的 Environment 完成之后就是配置 Environment ，这里主要就包括默认的 defaultProperties 和命令行参数两个部分。defaultProperties 可以通过以下方式设置： 12345Map&lt;String, Object&gt; defaultProperties = new HashMap&lt;&gt;();defaultProperties.put("defaultKey","defaultValue");SpringApplication springApplication = new SpringApplication(BootStrap.class);springApplication.setDefaultProperties(defaultProperties);springApplication.run(args); 配置 defaultProperties 和命令行参数过程的代码如下： 1234567891011121314151617181920212223protected void configurePropertySources(ConfigurableEnvironment environment, String[] args) &#123; MutablePropertySources sources = environment.getPropertySources(); // 如果 springApplication 设置了则构建 defaultProperties，没有就算了 if (this.defaultProperties != null &amp;&amp; !this.defaultProperties.isEmpty()) &#123; sources.addLast(new MapPropertySource("defaultProperties", this.defaultProperties)); &#125; // 命令行参数 if (this.addCommandLineProperties &amp;&amp; args.length &gt; 0) &#123; // PropertySource 名为 commandLineArgs String name = CommandLinePropertySource.COMMAND_LINE_PROPERTY_SOURCE_NAME; if (sources.contains(name)) &#123; PropertySource&lt;?&gt; source = sources.get(name); CompositePropertySource composite = new CompositePropertySource(name); composite.addPropertySource( new SimpleCommandLinePropertySource("springApplicationCommandLineArgs", args)); composite.addPropertySource(source); sources.replace(name, composite); &#125; else &#123; sources.addFirst(new SimpleCommandLinePropertySource(args)); &#125; &#125;&#125; SpringBoot 打成 fatjar 包后通过命令行传入的参数 包括以下 3 种实现方式 java -jar xxx.jar a b c : 通过 main 方法的参数获取，即 args java -jar xxx.jar -Dp1=a -Dp2=b -Dp3=c : -D 参数方式，会被设置到系统参数中 java -jar xxx.jar –p1=a –p2=b –p3=c : SpringBoot 规范方式，可以通过 @Value(“${p1}”） 获取 配置 Profiles为 application enviroment 配置哪些配置文件是 active 的(或者默认情况下是 active)。在配置文件处理期间，可以通过 spring.profiles.active 配置属性来激活其他配置文件。主要包括两种： 通过 spring.profiles.active 配置 1234567891011121314151617protected Set&lt;String&gt; doGetActiveProfiles() &#123; synchronized (this.activeProfiles) &#123; if (this.activeProfiles.isEmpty()) &#123; // 获取 spring.profiles.active 配置值 // 如：spring.profiles.active=local ，profiles 为 local // 如：spring.profiles.active=local,dev ，profiles 为 local,dev String profiles = getProperty(ACTIVE_PROFILES_PROPERTY_NAME); if (StringUtils.hasText(profiles)) &#123; // 按 ，分割成 String[] 数组 setActiveProfiles(StringUtils.commaDelimitedListToStringArray( StringUtils.trimAllWhitespace(profiles))); &#125; &#125; // 返回，这里还没有解析和 merge 配置 return this.activeProfiles; &#125;&#125; 通过 SpringApplication 对象 setAdditionalProfiles 配置 1234SpringApplication springApplication = new SpringApplication(BootStrap.class);// 设置 devspringApplication.setAdditionalProfiles("dev");springApplication.run(args); 以上两种方式设置的 profiles 会作为最后生效的 activeProfiles。 configurationProperties将 ConfigurationPropertySource 支持附加到指定的 Environment。将 Environment 管理的每个 PropertySource 调整为 ConfigurationPropertySource 类型，并允许 PropertySourcesPropertyResolver 使用 ConfigurationPropertyName 调用解析。附加的解析器将动态跟踪任何来自基础环境属性源的添加或删除（这个也是 SpringCloud Config 的底层支持原理）。 1234567891011121314151617public static void attach(Environment environment) &#123; // 类型检查 Assert.isInstanceOf(ConfigurableEnvironment.class, environment); MutablePropertySources sources = ((ConfigurableEnvironment) environment).getPropertySources(); // 获取名为 configurationProperties 的 PropertySource PropertySource&lt;?&gt; attached = sources.get(ATTACHED_PROPERTY_SOURCE_NAME); // 如果存在先移除，保证每次都是最新的 PropertySource if (attached != null &amp;&amp; attached.getSource() != sources) &#123; sources.remove(ATTACHED_PROPERTY_SOURCE_NAME); attached = null; &#125; if (attached == null) &#123; // 重新将名为 configurationProperties 的 PropertySource 放到属性源中 sources.addFirst(new ConfigurationPropertySourcesPropertySource(ATTACHED_PROPERTY_SOURCE_NAME, new SpringConfigurationPropertySources(sources))); &#125;&#125; 绑定 Environment 到 SpringApplication在 Spring Boot 2.0 中，用于绑定 Environment 属性的机制 @ConfigurationProperties 已经完全彻底修改; 所以相信很多人在迁移 SpringBoot 从 1.x 到 2.x 系列时，或者或少都会踩这块的坑。 新的 API 可以使得 @ConfigurationProperties 直接在你自己的代码之外使用。绑定规则可以参考：Relaxed-Binding-2.0。这里简单演示下： 123456789101112// 绑定 CustomPropList&lt;CustomProp&gt; props = Binder.get(run.getEnvironment()) .bind("glmapper.property", Bindable.listOf(CustomProp.class)) .orElseThrow(IllegalStateException::new);// 配置类@ConfigurationProperties(prefix = "glmapper.property")public class CustomProp &#123; private String name; private int age; // 省略 get&amp;set&#125; 属性配置：123456glmapper: property: - name: glmapper age: 26 - name: slg age: 26 从上面整个构建过程来看，Enviroment 对象构建实际就是 MutablePropertySources 对象填充的过程。Environment 的静态属性和存储容器都是在AbstractEnvironment 中定义的，ConfigurableWebEnvironment 接口提供的 getPropertySources() 方法可以获取到返回的 MutablePropertySources 实例，然后添加额外的 PropertySource。实际上，Environment 的存储容器就是 PropertySource 的子类集合，而 AbstractEnvironment 中使用的实例就是 MutablePropertySources。 那么到这里相比 Environment 与配置的关系就非常清楚了，一句话概括就是：Environment 是所有配置的管理器，是 Spring 对提供配置的统一接口。前面提到 Environment 管理了所有 Spring 的环境配置，这些配置最终是以 MutablePropertySources 对象的形态存在 Environment 中。下图为 MutablePropertySources 类的继承体系： 下面继续来看 PropertySources。 PropertySource &amp; PropertySources从名字就能直观看出，PropertySources 是持有一个或者多个 PropertySource 的类。PropertySources 提供了一组基本管理 PropertySource 的方法。 PropertySource下面看下 PropertySource 的源码： 123456789101112131415161718192021222324252627282930313233343536373839public abstract class PropertySource&lt;T&gt; &#123; protected final Log logger = LogFactory.getLog(getClass()); // 属性名 protected final String name; // 属性源 protected final T source; // 根据指定 name 和 source 构建 public PropertySource(String name, T source) &#123; Assert.hasText(name, "Property source name must contain at least one character"); Assert.notNull(source, "Property source must not be null"); this.name = name; this.source = source; &#125; // 根据指定 name 构建，source 默认为 Object 类型 @SuppressWarnings("unchecked") public PropertySource(String name) &#123; this(name, (T) new Object()); &#125; // 返回当前 PropertySource 的 name public String getName() &#123; return this.name; &#125; // 返回当前 PropertySource 的 source public T getSource() &#123; return this.source; &#125; public boolean containsProperty(String name) &#123; return (getProperty(name) != null); &#125; @Nullable public abstract Object getProperty(String name); // 返回用于集合比较目的的 PropertySource 实现 (ComparisonPropertySource)。 public static PropertySource&lt;?&gt; named(String name) &#123; return new ComparisonPropertySource(name); &#125; // 省略其他两个内部类实现，无实际意义&#125; 一个 PropertySource 实例对应一个 name，例如 systemProperties、enviromentProperties 等。 PropertySource 包括多种类型的实现，主要包括： 1、AnsiPropertySource：Ansi.*，包括 AnsiStyle、AnsiColor、AnsiBackground 等 2、StubPropertySource：在实际的属性源不能在 application context 创建时立即初始化的情况下用作占位符。例如，基于 ServletContext 的属性源必须等待，直到 ServletContext 对象对其封装的 ApplicationContext 可用。在这种情况下，应该使用存根来保存属性源的默认位置/顺序，然后在上下文刷新期间替换存根。 ComparisonPropertySource：继承自 StubPropertySource ，所有属性访问方法强制抛出异常，作用就是一个不可访问属性的空实现。 3、EnumerablePropertySource：可枚举的 PropertySource，在其父类的基础上扩展了 getPropertyNames 方法 CompositePropertySource：source 为组合类型的 PropertySource 实现 CommandLinePropertySource：source 为命令行参数类型的 PropertySource 实现，包括两种命令行参数和 java opts 参数两种。 MapPropertySource：source 为 Map 类型的 PropertySource 实现 PropertiesPropertySource：内部的 Map 实例由 Properties 实例转换而来 JsonPropertySource：内部的 Map 实例由 Json 实例转换而来 SystemEnvironmentPropertySource：内部的 Map 实例由 system env 获取 其他还有 ServletConfigPropertySource、ServletContextPropertySource、AnnotationsPropertySource 等，均可根据名字知晓其 source 来源。 PropertySourcesPropertySources 接口比较简单，如下所示： 1234567891011public interface PropertySources extends Iterable&lt;PropertySource&lt;?&gt;&gt; &#123; // 从 5.1 版本才提供的 default Stream&lt;PropertySource&lt;?&gt;&gt; stream() &#123; return StreamSupport.stream(spliterator(), false); &#125; // check name 为 「name」 的数据源是否存在 boolean contains(String name); // 根据 name」 获取数据源 @Nullable PropertySource&lt;?&gt; get(String name);&#125; 前面在分析 Enviroment 构建中，可以看到整个过程都是以填充 MutablePropertySources 为主线。MutablePropertySources 是 PropertySources 的默认实现，它允许对包含的属性源进行操作，并提供了一个构造函数用于复制现有的 PropertySources 实例。此外，其内部在 addFirst 和 addLast 等方法中提到了 precedence（优先顺序） ，这些将会影响 PropertyResolver 解析给定属性时搜索属性源的顺序。 MutablePropertySources 内部就是对 propertySourceList 的一系列管理操作（增删改成等），propertySourceList 其实就是整个配置系统最底层的存储容器，所以就很好理解，配置解析为什么都是在填充 MutablePropertySources 这个对象了。 12// 配置最终都被塞到这里了private final List&lt;PropertySource&lt;?&gt;&gt; propertySourceList = new CopyOnWriteArrayList&lt;&gt;(); 最后我们再来看下，Spring 中 Environment 属性是如何被访问的。 Environment 属性访问单从 Environment 代码来看，其内部并没有提供访问属性的方法，这些访问属性的方法都由其父类接口 PropertyResolver 提供。 12345678910111213141516171819public interface PropertyResolver &#123; // 判断属性是否存在 boolean containsProperty(String key); // 获取属性 @Nullable String getProperty(String key); // 获取属性，如果没有则提供默认值 String getProperty(String key, String defaultValue); @Nullable &lt;T&gt; T getProperty(String key, Class&lt;T&gt; targetType); &lt;T&gt; T getProperty(String key, Class&lt;T&gt; targetType, T defaultValue); // 获取 Required 属性 String getRequiredProperty(String key) throws IllegalStateException; &lt;T&gt; T getRequiredProperty(String key, Class&lt;T&gt; targetType) throws IllegalStateException; // 解析占位符 String resolvePlaceholders(String text); // 解析 Required占位符 String resolveRequiredPlaceholders(String text) throws IllegalArgumentException;&#125; Environment 中提供默认访问属性的对象实现是 PropertySourcesPropertyResolver，其定义在 AbstractEnvironment 这个抽象类中： 12private final ConfigurablePropertyResolver propertyResolver = new PropertySourcesPropertyResolver(this.propertySources); 那文章最后就来看下 PropertySourcesPropertyResolver 是如何访问配置属性的吧。 1234567891011121314151617181920212223protected &lt;T&gt; T getProperty(String key, Class&lt;T&gt; targetValueType, boolean resolveNestedPlaceholders) &#123; if (this.propertySources != null) &#123; // 遍历所有的 PropertySource for (PropertySource&lt;?&gt; propertySource : this.propertySources) &#123; // 省略日志 // 从 propertySource 中根据指定的 key 获取值 Object value = propertySource.getProperty(key); // 如果值不为空-&gt;选用第一个不为 null 的匹配 key 的属性值 if (value != null) &#123; // 解析占位符替换, 如$&#123;server.port&#125;，底层委托到 PropertyPlaceholderHelper 完成 if (resolveNestedPlaceholders &amp;&amp; value instanceof String) &#123; value = resolveNestedPlaceholders((String) value); &#125; logKeyFound(key, propertySource, value); // 进行一次类型转换，具体由 DefaultConversionService 处理 return convertValueIfNecessary(value, targetValueType); &#125; &#125; &#125; // 省略日志 ... // 没有的话就返回 null return null;&#125; 这里有一点需要注意，就是如果出现多个 PropertySource 中存在同名的 key，则只会返回第一个 PropertySource 对应 key 的属性值。在实际的业务开发中，如果需要自定义一些环境属性，最好要对各个 PropertySource 的顺序有足够的掌握。 小结整体看来，Spring 中对于配置的管理还是比较简单的，从 Environment 到 PropertySource 整个过程没有那么绕，就是单纯的把来自各个地方的配置统一塞到 MutablePropertySources 中，对外又通过 Environment 接口对外提供接口访问。 最后感谢大家一年来的关注和支持，**和 2019 说声再见，和 2020 说声你好！祝大家元旦快乐。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>PropertySource</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 源码解析系列 & SpringBoot 实践系列综述]]></title>
    <url>%2F2019%2F12%2F24%2Fspringboot%2Fspringboot-series-summary%2F</url>
    <content type="text"><![CDATA[源码解析系列： SpringBoot 系列-FatJar技术解析 SpringBoot 系列-启动过程解析 SpringBoot 系列-事件机制解析 SpringBoot 系列-Bean 的生命周期与扩展 SpringBoot 系列-日志框架解析 SpringBoot 系列-资源访问解析 SpringBoot 系列-内嵌Web容器解析 SpringBoot 系列-配置解析 SpringBoot 系列-自动配置及 starter 机制解析 SpringBoot 实践-外部化配置优先级问题 实践系列 SpringBoot 实践-集成 RocketMQ SpringBoot 实践-Kafka简介&amp;集成SpringBoot SpringBoot 实践-集成 Mybatis SpringBoot 实践-集成 Dubbo SpringBoot 实践-集成 REDIS SpringBoot 实践-集成 NOSQL SpringBoot 实践-集成 Oauth2 SpringBoot 实践-集成 Quartz Scheduler SpringBoot 实践-集成 服务链路跟踪组件 SpringBoot 实践-镜像化部署 SpringBoot 实践-使用 K8S 部署 SpringBoot 工程]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git ssh 配置及使用]]></title>
    <url>%2F2019%2F12%2F20%2Ftool-config-git-ssh%2F</url>
    <content type="text"><![CDATA[配置使用 ssh 方式来提交和克隆代码大概可以分为以下几个步骤： 设置 Git 的 user name 和 email：(如果是第一次的话) 检查是否已经有 SSH Key。 生成密钥 添加密钥到 ssh-agent 登陆 github, 添加 ssh 设置 Git 的 user name 和 email1234# 用户名替换成自己的用户名git config --global user.name "glmapper"# 邮箱替换换成自己的邮箱git config --global user.email "glmapper_2018@163.com" 检查是否已经有 SSH Key12# 到 .ssh 目录下cd ~/.ssh ls 列出所有文件，看是否存在 id_isa 和 id_isa.pub 文件（也可以是别的文件名，只要 yourName 和 yourName.pub 成对存在），如果存在的话，证明已经存在 ssh key了，可以直接跳过 生成密钥 这一步骤 1alipaynet_rsa alipaynet_rsa.pub config id_rsa id_rsa.pub known_hosts ssh-rsa-bridge ssh-rsa-bridge.pub 生成秘钥上述因为我已经配置过了，如果没有的话，可参考本节进行相关操作 12# 生成秘钥ssh-keygen -t rsa -C "glmapper_2018@163.com" 如果不需要密码的话，上述执行过程可以一直回车跳过；执行完成之后将会得到两个文件：id_rsa 和 id_rsa.pub。windows 下默认的路径是 C:\Users\Administrator\.ssh , Mac/Linux 默认是 ~/.ssh。 添加密钥到 ssh-agent确保 ssh-agent 是可用的。ssh-agent 是一种控制用来保存公钥身份验证所使用的私钥的程序，ssh-agent 就是一个密钥管理器，运行 ssh-agent 以后，使用ssh-add 将私钥交给 ssh-agent 保管，其他程序需要身份验证的时候可以将验证申请交给 ssh-agent 来完成整个认证过程。 12# start the ssh-agent in the backgroundeval "$(ssh-agent -s)" 执行完之后将会输出如下信息： 1Agent pid 64345 # 64345 为agent 的进程号 添加生成的 SSH key 到 ssh-agent： 1ssh-add ~/.ssh/id_rsa 登陆 Github， 添加 sshhttps://github.com/settings/keys 添加 SSH key，把 id_rsa.pub 文件里的内容复制到这里即可。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 源码系列-Bean 的生命周期与扩展]]></title>
    <url>%2F2019%2F12%2F14%2Fspringboot%2Fspringboot-series-bean-life%2F</url>
    <content type="text"><![CDATA[本篇基于 SpringBoot 2.2.2.RELEASE 版本，Spring 各个版本之间多少存在一些差异，不过主流程基本相同；大家在阅读过程中需关注这点。 继续承接上一篇 SpringBoot 系列-启动过程分析，本篇围绕一个 bean 的生命周期，对 bean 进行一些修改和扩展。本篇将涉及到以下主要内容： 阅读之前 BeanDefinition 解析时机和过程 invokeBeanFactoryPostProcessors 执行过程分析 invokeBeanDefinitionRegistryPostProcessors 执行过程分析 BeanFactoryPostProcessor 对 BeanDefinition 的修改 案例工程中 BeanFactoryPostProcessor 的实现 通过监听 ApplicationEnvironmentPreparedEvent 事件修改属性值 @Value 注入 &amp; @Autowired 注入 Bean 属性注入发生的时机 Bean 属性注入发生的过程 Bean 的实例化过程 BeanPostProcessor 的处理时机 使用 BeanPostProcessor 修改 Bean 使用 InitializingBean 指定 Bean 的 init-method 方法 总结 BeanFactoryPostProcessor 对于 init-method 的影响 附：案例工程地址及参考 阅读之前下面是本篇文章的“主人公” TestBeanService ，定义如下： 12345678910111213141516public class TestBeanService &#123; /** * 依赖注入 */ @Autowired private InjectBeanService injectBeanService; /** * 属性注入 */ @Value("$&#123;spring.application.name&#125;") private String appName; public String test() &#123; return injectBeanService.testInject(); &#125;&#125; TestBeanService 里面包括两个属性，一个是 injectBeanService ，另外一个是 appName，分别通过 @Autowired 和 @Value 注入值。本篇最终希望完成的目标是能够完成了解 Bean 属性注入的过程，以及 Bean 的实例化过程；除此之外，从 Spring 扩展的角度，来对 BeanFactoryPostProcess、BeanPostProcess、ApplicationListener、InitializingBean 以及 initMethod 的执行时机和作用进行分析。 TestBeanService 被解析成 BeanDifinition 的时机与过程 Spring 容器刷新流程非常复杂，当我们想 debug BeanDifinition 加载过程时可能没法很快找到入口，这里可以直接面向 BeanDifinition 的最终去向来 debug。我们知道 BeanFactory 接口本身是不具体注册 BeanDifinition 能力的，这个能力是由 BeanDefinitionRegistry 接口提供。那么就看下 BeanDefinitionRegistry 的 registerBeanDefinition 方法有几个具体的实现，然后在这几个实现出打上断点，执行找到具体的处理入口。 我们将断点打在 DefaultListableBeanFactory#registerBeanDefinition 这个方法入口处，debug 模式运行工程，可以看到断点进入时的情况如下图所示： 这里通过执行堆栈逆向找到 BeanDifinition 的加载入口是容器刷新阶段的 invokeBeanFactoryPostProcessors 方法；这里就详细分析下 testBeanService 这个 beandifition 是怎么被注册到容器中的。 invokeBeanFactoryPostProcessors 执行过程分析invokeBeanFactoryPostProcessors 这个方法实现非常长，但是基本处理过程很简单，存在很多重复的步骤。为了方便理解整个过程，这里还是有必要贴一下代码，代码中会详细标注所做的事情是什么，这个过程是构建 BeanFactory 非常重要一步。掌握这个过程，就可以随意玩转 BeanFactoryPostProcessor 了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141public static void invokeBeanFactoryPostProcessors( ConfigurableListableBeanFactory beanFactory, List&lt;BeanFactoryPostProcessor&gt; beanFactoryPostProcessors) &#123; // Invoke BeanDefinitionRegistryPostProcessors first, if any. Set&lt;String&gt; processedBeans = new HashSet&lt;&gt;(); // 当前 beanFactory 是否是 BeanDefinitionRegistry 类型 // 只有是 BeanDefinitionRegistry 类型，才具备注册 beanDefinition 的能力 if (beanFactory instanceof BeanDefinitionRegistry) &#123; BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; // 普通的 BeanFactoryPostProcessor 集合 List&lt;BeanFactoryPostProcessor&gt; regularPostProcessors = new ArrayList&lt;&gt;(); // BeanDefinitionRegistryPostProcessor 类型处理器集合 List&lt;BeanDefinitionRegistryPostProcessor&gt; registryProcessors = new ArrayList&lt;&gt;(); // 这里 beanFactoryPostProcessors 是在 SharedMetadataReaderFactoryContextInitializer 中加进来的，是 Spring 自己的处理器 for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) &#123; // 如果是 BeanDefinitionRegistryPostProcessor 类型，就加到 registryProcessors if (postProcessor instanceof BeanDefinitionRegistryPostProcessor) &#123; BeanDefinitionRegistryPostProcessor registryProcessor = (BeanDefinitionRegistryPostProcessor) postProcessor; // 执行 BeanDefinitionRegistryPostProcessor 后置处理 registryProcessor.postProcessBeanDefinitionRegistry(registry); registryProcessors.add(registryProcessor); &#125; else &#123; // 否则就放到 regularPostProcessors regularPostProcessors.add(postProcessor); &#125; &#125; // 不要在这里初始化 FactoryBeans：需要保留所有未初始化的常规bean，以使 beanFacotryPostProcessor 对其处理！ // 分离实现 PriorityOrdered，Ordered和其余优先级的 BeanDefinitionRegistryPostProcessor。 List&lt;BeanDefinitionRegistryPostProcessor&gt; currentRegistryProcessors = new ArrayList&lt;&gt;(); // 首先，调用实现 PriorityOrdered 的 BeanDefinitionRegistryPostProcessors。 String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); // 遍历 BeanDefinitionRegistryPostProcessors for (String ppName : postProcessorNames) &#123; // 只处理实现 PriorityOrdered 接口的 BeanDefinitionRegistryPostProcessor if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; // 符合上述条件的 BeanDefinitionRegistryPostProcessor 放到 currentRegistryProcessors 中，供后面使用 currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); // 标记当前 postProcessor 已经处理过了 processedBeans.add(ppName); &#125; &#125; // 排序 sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); // 调用 BeanDefinitionRegistryPostProcessor 后置处理器 invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); // 接下来，调用实现 Ordered的BeanDefinitionRegistryPostProcessors postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) &#123; if (!processedBeans.contains(ppName) &amp;&amp; beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); &#125; &#125; sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); // 最后，调用所有其他 BeanDefinitionRegistryPostProcessor，直到不再出现（保证全部处理完）。 boolean reiterate = true; while (reiterate) &#123; reiterate = false; postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) &#123; if (!processedBeans.contains(ppName)) &#123; currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); reiterate = true; &#125; &#125; sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); &#125; // 现在，调用到目前为止已处理的所有处理器的 postProcessBeanFactory 回调。 invokeBeanFactoryPostProcessors(registryProcessors, beanFactory); invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory); &#125; else &#123; // 调用在上下文实例中注册的工厂处理器。就是前面提到的 SharedMetadataReaderFactoryContextInitializer 中注册的 invokeBeanFactoryPostProcessors(beanFactoryPostProcessors, beanFactory); &#125; // 这里再次拿到所有的 BeanFactoryPostProcessor String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false); // 同样将实现 PriorityOrdered、Order 和普通的 BeanFactoryPostProcessor 分离开 List&lt;BeanFactoryPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;(); List&lt;String&gt; orderedPostProcessorNames = new ArrayList&lt;&gt;(); List&lt;String&gt; nonOrderedPostProcessorNames = new ArrayList&lt;&gt;(); for (String ppName : postProcessorNames) &#123; if (processedBeans.contains(ppName)) &#123; // 跳过-已在上述第一阶段处理过 &#125; else if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123; priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class)); &#125; else if (beanFactory.isTypeMatch(ppName, Ordered.class)) &#123; orderedPostProcessorNames.add(ppName); &#125; else &#123; nonOrderedPostProcessorNames.add(ppName); &#125; &#125; // 首先，调用实现PriorityOrdered的BeanFactoryPostProcessors。 sortPostProcessors(priorityOrderedPostProcessors, beanFactory); // 优先执行实现 PriorityOrdered 接口的 BeanFactoryPostProcessor invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory); // 接下来，调用实现Ordered的BeanFactoryPostProcessors。 List&lt;BeanFactoryPostProcessor&gt; orderedPostProcessors = new ArrayList&lt;&gt;(orderedPostProcessorNames.size()); for (String postProcessorName : orderedPostProcessorNames) &#123; orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); &#125; sortPostProcessors(orderedPostProcessors, beanFactory); // 执行实现 Ordered 接口的 BeanFactoryPostProcessor invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory); // 最后，调用所有其他 BeanFactoryPostProcessor List&lt;BeanFactoryPostProcessor&gt; nonOrderedPostProcessors = new ArrayList&lt;&gt;(nonOrderedPostProcessorNames.size()); for (String postProcessorName : nonOrderedPostProcessorNames) &#123; nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); &#125; // 执行其他没有实现任何优先级接口的 BeanFactoryPostProcessor invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory); // 清除缓存的合并 beanDefinition，因为后处理器可能已经修改了原始元数据 beanFactory.clearMetadataCache();&#125; 上面代码段中大体就是，先处理 BeanDefinitionRegistryPostProcessor 类型的 BeanFactoryPostProcessor ，然后再处理普通的 BeanFactoryPostProcessor；在这里处理过程中，会根据一些排序规则来调整各个 BeanFactoryPostProcessor 的执行顺序。 这里先处理 BeanDefinitionRegistryPostProcessor 类型的 BeanFactoryPostProcessor 是一定的，因为需要在这个阶段去注册 BeanDefinition。在 classpath 下的所有 BeanDefinition 都被注册之后，再执行普通 BeanFactoryPostProcessor 的后置回调，这样就可以覆盖所有的 BeanDefinition。 invokeBeanDefinitionRegistryPostProcessors 执行过程分析在第一次调用 invokeBeanDefinitionRegistryPostProcessors 时，当前的 BeanDefinitionRegistryPostProcessor 只有一个，就是 org.springframework.context.annotation.ConfigurationClassPostProcessor 。 在 ConfigurationClassPostProcessor 类中，会解析 @Configuration、@ComponentScan、@ComponentScans、@Import 等注解。ConfigurationClassPostProcessor 实现了 BeanDefinitionRegistryPostProcessor 接口，而 BeanDefinitionRegistryPostProcessor 接口继承了 BeanFactoryPostProcessor 接口，所以 ConfigurationClassPostProcessor 中需要重写 postProcessBeanDefinitionRegistry() 方法和 postProcessBeanFactory() 方法。而 ConfigurationClassPostProcessor 类的作用就是通过这两个方法去实现的。更多细节可以参考 ConfigurationClassPostProcessor源码解析 这篇文章，写的非常 nice。 invokeBeanDefinitionRegistryPostProcessors 处理的核心过程如下： 1、ConfigurationClassPostProcessor#postProcessBeanDefinitionRegistry：BeanDefinition 触发加载的入口 2、ConfigurationClassPostProcessor#processConfigBeanDefinitions：解析配置类，在此处会解析配置类上的注解(ComponentScan扫描出的类，@Import注册的类，以及@Bean方法定义的类) 3、ComponentScanAnnotationParser#parse：根据注解的属性值来过滤加载 classpath 下的 beanDefinition（默认条件就是 basePackages，默认的 basePackages 为当前启动类的根包） 4、ClassPathBeanDefinitionScanner#doScan：处理 basePackages 下所以的 beanDefinition，被打了 @Service、@Compoment 等注解的类都会被解析到 5、DefaultListableBeanFactory#registerBeanDefinition：将 beanDefinition 注册到 BeanFactory 中（beanDefinitionMap 中） 那么到这里 TestBeanService 的 BeanDefinition 就被注册到 BeanFactory 中了。 BeanFactoryPostProcessor 对 BeanDefinition 的修改在本篇文章所对应的案例工程中，也实现了一个 BeanFactoryPostProcessor ，没有实现任何排序接口。这个 TestBeanServiceBeanFactoryPostProcessor 的作用是将原来的 TestBeanService 修改为 ProxyTestBeanService。代码如下： 1234567891011121314151617181920212223242526272829public class TestBeanServiceBeanFactoryPostProcessor implements BeanFactoryPostProcessor &#123; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; // 根据类型拿到所有的 beanNames Iterable&lt;String&gt; beanNames = getBeanNames(beanFactory, TestBeanService.class); // 这里因为只有一个 TestBeanService ，所以只处理第一个 beanNames.forEach(beanName -&gt; &#123; System.out.println("begin to execute BeanFactoryPostProcessor..."); BeanDefinitionRegistry beanDefinitionRegistry = (BeanDefinitionRegistry) beanFactory; // 先从工程中拿到原始 beanDefinition BeanDefinition beanDefinition = beanFactory.getBeanDefinition(beanName); // 这里构建一个新的 BeanDefinition，类型为 ProxyTestBeanService，ProxyTestBeanService 是 TestBeanService 的子类 RootBeanDefinition proxy = new RootBeanDefinition(ProxyTestBeanService.class); // 这里设置指定的initMethod proxy.setInitMethodName(beanDefinition.getInitMethodName()); // 设置一些属性 proxy.setPropertyValues(beanDefinition.getPropertyValues()); proxy.setPrimary(beanDefinition.isPrimary()); proxy.setRole(BeanDefinition.ROLE_APPLICATION); // 将原始 beanDefinition 移除掉 beanDefinitionRegistry.removeBeanDefinition(beanName); // 将代理的新的 beanDefinition 注册进去 beanDefinitionRegistry.registerBeanDefinition(beanName,proxy); System.out.println("current bean type is : " + proxy.getBeanClass().getTypeName()); return; &#125;); &#125;&#125; 在 invokeBeanFactoryPostProcessors 执行过程分析中已经分析了 BeanFactoryPostProcessor 执行的时机和过程，这里不再赘述。TestBeanServiceBeanFactoryPostProcessor 的作用就是先将原始的 TestBeanService 的 Beandefinition 从容器中移除掉，然后构建一个 ProxyTestBeanService 的 Beandefinition，然后注册到容器中，beanName 没有变，所以通过 BeanFactoryPostProcessor 可以修改最原始的 Bean 信息，也可以通过 BeanFactoryPostProcessor 来动态注册一个新的 Bean。 通过监听 ApplicationEnvironmentPreparedEvent 事件修改属性值上面完成了对 TestBeanService 的 BeanDefinition 的修改，将 TestBeanService 对象换成了 ProxyTestBeanService。前面提到 TestBeanService 中有两个需要注入的值，一个是通过 @Autowired 注入，一个是通过 @Value 注入，先来看 @Value 注入。@Value 注入的值来自 Enviroment，这里关于 Enviroment 和配置解析及构建不多说，本篇中关注的是如何将 @Value 注入的值改变掉。 ApplicationEnvironmentPreparedEvent 事件是在环境准备完成时发送的事件，此时 Enviroment 已经准备好，可以随时为容器刷新提供环境变量支持。那么既然此时容器中的 Enviroment 对象已经 ready ，说明配置的 application.properties、系统参数等均已经被解析好了，而此时目标 Bean 还没有被刷新，其内部需要被注入的属性值还没有被注入，那么此时就可以通过监听这个事件，来对 Enviroment 中已经准备好的值进行修改，以改变实际被注入的值。代码如下： 12345678910111213141516public class ChangeAppNameListener implements ApplicationListener&lt;ApplicationEnvironmentPreparedEvent&gt; &#123; @Override public void onApplicationEvent(ApplicationEnvironmentPreparedEvent event) &#123; ConfigurableEnvironment environment = event.getEnvironment(); // 获取原始 spring.application.name 的值 String applicationName = environment.getProperty("spring.application.name"); System.out.println("origin applicationName is : " + applicationName); // 修改 spring.application.name Properties props = new Properties(); props.put("spring.application.name", "updateAppName"); environment.getPropertySources().addFirst(new PropertiesPropertySource("decrypted_properties", props)); applicationName = environment.getProperty("spring.application.name"); System.out.println("updated applicationName is : " + applicationName); &#125;&#125; @Value 注入 &amp; @Autowired 注入在 Spring 中，无论是 @Value 注入还是 @Autowired 注入，都是由 AutowiredAnnotationBeanPostProcessor 这个后置处理器处理的。 在很多开源的框架中，其内部自定义的注解也大都是通过 BeanPostProcessor 这个后置处理器来处理的。 AutowiredAnnotationBeanPostProcessor 中有个 AutowiredFieldElement 内部类，这个内部类的作用就是注入目标 bean 的属性值的。这里就包括 @Value 的注入和 @Autowired 注入。 Bean 属性注入发生的时机容器刷新及属性注入调用堆栈如下： 从堆栈看出，在容器刷新的最后阶段，会通过 finishBeanFactoryInitialization 这个方法实例化所有剩余的（非延迟初始化）单例 bean；这个过程就是绝大多数 bean 实例化的过程。这个过程中会涉及到以下两个比较重要的点：1、BeanPostProcessor 处理，2、依赖注入。从上面其实也可以看出，依赖注入的发生就是通过 BeanPostProcessor 处理完成的。下图为遍历所有目标属性，依次注入属性的过程： Bean 属性注入发生的过程这里以 @Autowired 注入为例，@Value 注入和 @Autowired 注入过程基本是一样的。@Autowired 注入相比于 @Value 注入，会涉及到初始化另外一个 Bean 的过程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 构建一个依赖描述符对象DependencyDescriptor desc = new DependencyDescriptor(field, this.required);// 设置包含此依赖项的具体类desc.setContainingClass(bean.getClass());// 初始化一个注入的 beanName 集合，用于后面注册到容器中// 这里实际上只有一个，如果有多个实例 bean 存在，则需要通过 Qualifier 指定了Set&lt;String&gt; autowiredBeanNames = new LinkedHashSet&lt;&gt;(1);Assert.state(beanFactory != null, "No BeanFactory available");TypeConverter typeConverter = beanFactory.getTypeConverter();try &#123; // 解析依赖，依赖注入 value = beanFactory.resolveDependency(desc, beanName, autowiredBeanNames, typeConverter);&#125;catch (BeansException ex) &#123; // 抛出注入失败异常 throw new UnsatisfiedDependencyException(null, beanName, new InjectionPoint(field), ex);&#125;synchronized (this) &#123; if (!this.cached) &#123; if (value != null || this.required) &#123; this.cachedFieldValue = desc; // 注册依赖的 bean registerDependentBeans(beanName, autowiredBeanNames); if (autowiredBeanNames.size() == 1) &#123; String autowiredBeanName = autowiredBeanNames.iterator().next(); // 判断容器中是否存在此依赖 bean,并且校验 bean 的类型是否匹配 if (beanFactory.containsBean(autowiredBeanName) &amp;&amp; beanFactory.isTypeMatch(autowiredBeanName, field.getType())) &#123; // 缓存注入值 this.cachedFieldValue = new ShortcutDependencyDescriptor( desc, autowiredBeanName, field.getType()); &#125; &#125; &#125; else &#123; // 没有找到 依赖bean 实例，且 required 为 false this.cachedFieldValue = null; &#125; this.cached = true; &#125;&#125;// value 为解析到的属性值，如果不为空，则通过反射设置给目标 Bean，完成属性的注入if (value != null) &#123; ReflectionUtils.makeAccessible(field); field.set(bean, value);&#125; 属性注入发生在 populateBean（填充 Bean）的过程，在 Bean 属性填充完成之后就是 Bean 的实例化过程。 Bean 的实例化过程这里截取 AbstractAutowireCapableBeanFactory#doCreateBean 方法中的一小段代码，来承接上下文： 1234567891011// 初始化bean实例。Object exposedObject = bean;try &#123; // 填充 Bean populateBean(beanName, mbd, instanceWrapper); // 实例化 Bean exposedObject = initializeBean(beanName, exposedObject, mbd);&#125;catch (Throwable ex) &#123; // 省略异常处理&#125; 这里通过代码就很好的和上一小节的内容关联起来了，即填充 Bean -&gt; 实例化 Bean 。在 Bean 的实例化阶段会涉及到两个比较重要的扩展：1、BeanPostProcessor，2、InitializingBean。 BeanPostProcessor 的处理时机BeanPostProcessor 有两个抽象方法，一个是实例化之前调用，一个是实例化之后调用。InitializingBean 接口只有一个 afterPropertiesSet 方法，afterPropertiesSet 方法的执行介于实例化之前实例化之后调用之间。BeanPostProcessor 的处理时机是在调用 initializeBean 方法中触发的，下面为 initializeBean 方法中的部分代码片段： 123456789101112131415161718Object wrappedBean = bean;if (mbd == null || !mbd.isSynthetic()) &#123; // 实例化之前调用 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName);&#125;try &#123; // 调用 InitializingBean 和指定的 init-method 方法 invokeInitMethods(beanName, wrappedBean, mbd);&#125;catch (Throwable ex) &#123; throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, "Invocation of init method failed", ex);&#125;if (mbd == null || !mbd.isSynthetic()) &#123; // 实例化之后调用 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);&#125; 这里的 bean 对象实际上已经是完整的 bean 了，postProcessBeforeInitialization 和 postProcessAfterInitialization 是相对于是否执行 InitializingBean 的 afterPropertiesSet 和执行 Bean 指定的 initMethod 方法而言的。 使用 BeanPostProcessor 修改 Bean从 initializeBean 方法中可以看出，了，postProcessBeforeInitialization 和 postProcessAfterInitialization 两处回调返回放回的是 wrappedBean，也就意味着我们可以在这两个方法中对容器中的原始 Bean 做一些处理，比如代理一层原始的 Bean，或者修改 Bean 中的一些属性等。 在案例工程中提供了一个 TestBeanServiceProcessor ，其作用是对 TestBeanService 类型的 Bean 做一层代理，使得在执行 TestBeanService 中方法的前后做一些埋点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// TestBeanServiceProcessorpublic class TestBeanServiceProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; // 如果 bean 的类型是 TestBeanService，则将其包装成 TestBeanWrapperService 并返回 if (bean instanceof TestBeanService)&#123; System.out.println("begin to execute postProcessBeforeInitialization."); TestBeanWrapperService testBeanService = new TestBeanWrapperService((TestBeanService)bean); return testBeanService; &#125; return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (bean instanceof TestBeanService)&#123; System.out.println("begin to execute postProcessAfterInitialization."); &#125; return bean; &#125;&#125;// 代理类 TestBeanWrapperService，注意这里代理类也应该是 TestBeanService 类型，否在是后面使用时就会找不到 Bean 实例public class TestBeanWrapperService extends TestBeanService &#123; private final TestBeanService delegate; public TestBeanWrapperService(TestBeanService delegate)&#123; this.delegate = delegate; &#125; /** * 实现对 test 方法执行前后进行拦截 **/ @Override public String test() &#123; try &#123; before(); return delegate.test(); &#125; finally &#123; after(); &#125; &#125; private void before()&#123; System.out.println("before execute test."); &#125; private void after()&#123; System.out.println("after execute test."); &#125;&#125; 使用 InitializingBean如果一个 bean 集成了 InitializingBean 接口，那么就需要重写其 afterPropertiesSet 方法。这里感觉有点漏洞，afterPropertiesSet 动作其实早就完成了，另外因为 afterPropertiesSet 是在 postProcessAfterInitialization 方法之前调用，所以还是可以在 postProcessAfterInitialization 对属性做修改。实际使用过程中需要关注下这个点，一般情况下，我们会在 afterPropertiesSet 中做一些初始化动作，比如启动连接 Zookeeper。 1234567public class TestBeanService implements InitializingBean &#123; // 省略其他代码 @Override public void afterPropertiesSet() throws Exception &#123; System.out.println("begin to execute afterPropertiesSet..."); &#125;&#125; 指定 Bean 的 init-method 方法init-method 方法只能通过 @Bean 或者 xml 方式指定，如果是使用 @Component 或者 @Service 注解标准的 Bean ，则可以通过 @PostConstruct 注解标注方法，对应的是 destroy-method 和 @PreDestroy 。 12345678910111213public class TestBeanService implements InitializingBean&#123; // 省略其他代码 // init 方法 public void init()&#123; System.out.println("begin to execute init..."); &#125;&#125;// 在自动配置类或者 xml 文件中指定 initMethod@Bean(initMethod = "init")public TestBeanService testBeanService()&#123; return new TestBeanService();&#125; 总结本篇围绕 TestBeanService 这个 Bean 展开，对其生命周期，及其生命周期各个阶段扩展点进行了介绍，包括修改注入的属性值、修改其 BeanDefinition、修改 Bean 实例等等，从扩展点的视角来洞悉一个 Bean 的生命周期。 BeanFactoryPostProcessor 对于 init-method 的影响因为 init-method 这个点是后面想起来加上去的，在实际测试过程中，发现 TestBeanService 中指定的 init 方法没有被执行（正常情况下是在 afterPropertiesSet 之后就会执行的）；对于这个 TestBeanService 在案例工程中有两处对其进行了修改，一个是修改其 BeanDefinition ，一个是修改 其 Bean 实例；最终拿到的 bean 的类型是 TestBeanWrapperService，在此之前 Bean 的类型是 ProxyTestBeanService ，无论是TestBeanWrapperService 还是 ProxyTestBeanService 都是 TestBeanService 的子类，init 方法又是 public 的，所以从这个角度来看，不可能不生效。所以基本可以排除因为访问权限问题导致。最后 debug 下面代码发现，mbd.getInitMethodName() 返回的是 null， mbd 是 RootBeanDefinition； PS: BeanDefinition 中 getInitMethodName 方法是在 Spring 5.1 版本之后才有的，之前版本都是 在 AbstractBeanDefinition 这个抽象类中定义。 123456789if (mbd != null &amp;&amp; bean.getClass() != NullBean.class) &#123; // 从当前 bean 的 BeanDefinition 对象中获取 initMethod 方法名 String initMethodName = mbd.getInitMethodName(); if (StringUtils.hasLength(initMethodName) &amp;&amp; !(isInitializingBean &amp;&amp; "afterPropertiesSet".equals(initMethodName)) &amp;&amp; !mbd.isExternallyManagedInitMethod(initMethodName)) &#123; invokeCustomInitMethod(beanName, bean, mbd); &#125;&#125; 问题出在这里，在 TestBeanServiceBeanFactoryPostProcessor 处理时，没有将原始 BeanDefinition 的 initMethod 给新的 ProxyTestBeanService，所以导致后面所有基于此实例化的 bean 的 BeanDefinition 都没有 initMethod 方法。在TestBeanServiceBeanFactoryPostProcessor#postProcessBeanFactory 方法中补充设置 InitMethodName 之后问题解决。 12// 这里设置指定的initMethodproxy.setInitMethodName(beanDefinition.getInitMethodName()); 附：案例工程地址及参考 工程地址：glmapper-blog-bean-lifecycle 参考文档：ConfigurationClassPostProcessor源码解析]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 源码系列-日志详解]]></title>
    <url>%2F2019%2F12%2F14%2Fspringboot%2Fspringboot-series-log%2F</url>
    <content type="text"><![CDATA[Spring Boot 使用 Commons Logging 进行所有内部日志记录，但保留底层日志实现。为 Java Util Logging、Log4J2 和 Logback 提供了默认配置。在每种情况下，loggers 都预先配置为使用 console 输出，并且也提供可选的文件输出。 默认情况下，如果使用 “starters”，则使用 Logback 进行日志记录。还包括适当的 Logback 路由，以确保使用 Java Util 日志记录、Commons 日志记录、Log4J 或 SLF4J 的依赖库都能正常工作。 下面先来看一个最简单的 SpringBoot demo 工程的日志输出，以此来展开日志格式、控制台输出、日志颜色、日志文件配置、日志体系解析等几个方面的介绍。 新建一个 SpringBoot 工程，默认在什么都不加的情况下直接启动，其启动日志大概如下： 123456782019-12-24 20:41:31.866 INFO 87851 --- [ main] com.glmapper.bridge.boot.BootStrap : No active profile set, falling back to default profiles: default2019-12-24 20:41:32.003 INFO 87851 --- [ main] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@314c508a: startup date [Tue Dec 24 20:41:31 CST 2019]; root of context hierarchy2019-12-24 20:41:32.556 INFO 87851 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup2019-12-24 20:41:32.568 INFO 87851 --- [ main] com.glmapper.bridge.boot.BootStrap : Started BootStrap in 1.035 seconds (JVM running for 2.13)2019-12-24 20:41:32.569 INFO 87851 --- [ Thread-4] s.c.a.AnnotationConfigApplicationContext : Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@314c508a: startup date [Tue Dec 24 20:41:31 CST 2019]; root of context hierarchy2019-12-24 20:41:32.571 INFO 87851 --- [ Thread-4] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdownProcess finished with exit code 0 日志格式上面是 Spring Boot 的默认日志输出，从日志格式来看，主要包括以下几项： 日期时间: 例如 2019-12-24 20:41:31.866 (毫秒精度) 日志级别: 例如 INFO (ERROR, WARN, INFO, DEBUG, or TRACE.) 当前进程: 例如 87851 — 分隔符，用于区分实际日志消息的开头。 线程名称: 例如 Thread-4 (用方括号括起来(为了控制台输出可能被截断)). 日志名称: 这通常是源类名(通常是缩写)。 日志信息: 具体的日志消息 比如这条记录： 12019-12-24 20:41:31.866 INFO 87851 --- [ main] com.glmapper.bridge.boot.BootStrap : No active profile set, falling back to default profiles: default 是在 org.springframework.boot.SpringApplication#logStartupProfileInfo 方法中打印的，日志级别为 INFO。 Console 输出SpringBoot 默认会将日志输出到 Console，默认情况下，会记录 error 级别、warn 级别和 info 级别的消息。还可以通过使用 —-debug 参数启动应用程序来使用 “debug” 级别。 1java -jar myapp.jar --debug 也可以在 application.properties 中指定 debug=true 来启用 debug 级别 当启用 debug 级别时，将配置一系列核心日志记录器(嵌入式容器、Hibernate 和 Spring Boot) 以输出更多信息。启用 debug 模式并不会将应用程序配置为记录所有具有 debug 级别的消息。同样的，也可以使用 —-trace 标记来启动 trace 级别模式来启动应用程序。 彩色编码输出如果你的终端支持 ANSI，你可以通过设置 “spring.output.ansi.enable“ 配置项值来指定颜色（前提是官方已经支持的颜色）。颜色编码是通过使用 %clr 转换字来配置的，最简单的就是根据日志级别对输出的日志进行着色，如下面的示例所示: 1%clr(%5p) 下表是官方提供的描述日志级别到颜色的映射关系表: Level Color FATAL Red ERROR Red WARN Yellow INFO Green DEBUG Green TRACE Green 如果你想要使文本变成黄色，可以使用以下设置: 1%clr(%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;)&#123;yellow&#125; 目前支持的颜色和样式包括 : blue、cyan、faint、green、magenta、red、yellow。 文件输出默认情况下，Spring 引导日志只输出到 Console，不会写入到日志文件中。如果希望在 Console 输出之外还写入到日志文件，则需要设置 logging.file 和 logging.path 属性(在 application.properties 中)。下表显示了 logging.* 属性如何一起使用: logging.file logging.path Example Description none none 控制台日志 指定文件 none my.log 写入指定的日志文件，名称可以是精确位置或相对于当前目录。 none 指定文件 /var/log 将 spring.log 写入指定的目录，名称可以是精确位置或相对于当前目录。 日志文件在达到 10 MB 时会进行 Rolling，与 Console 输出一样，默认情况下会记录 ERROR 级别、WARN 级别和 INFO 级别的消息。可以使用 logging.file.max-size 属性更改大小限制。除非已设置 logging.file.max-history 属性，否则以前 Rolling 的文件将无限期归档。 日志系统在应用程序生命周期的早期初始化。因此，在通过 @PropertySource 注释加载的属性文件中是找不到日志属性的。另外，logging 属性独立于实际的logging 基础结构。所以，Spring Boot 不会管理特定的配置密钥（例如 Logback 的 logback.configurationFile）。 日志级别SpringBoot 中所支持的日志系统都可以通过 logging.level.&lt;logger-name&gt;=&lt;level&gt; 在 Spring 环境中设置日志的级别(比如在application.properties 中)。日志级别主要包括 TRACE, DEBUG, INFO, WARN, ERROR, FATAL 和 OFF 几种。除此之外，还可以使用 logging.level.root 配置 root logger 的日志级别。下面的示例展示了如何在 application.properties 中配置日志级别: 123logging.level.root=warnlogging.level.org.springframework.web=debuglogging.level.org.hibernate=error 除了 application.properties 之外，也可以使用环境变量设置日志级别。例如，LOGGING_LEVEL_ORG_SPRINGFRAMEWORK_WEB=DEBUG 将 org.springframework.web 包下的日志打印级别设置为 DEBUG。 上面的方法只适用于包级别的日志记录。由于 Relaxed Binding 总是将环境变量转换为小写形式，因此不可能以这种方式为单个类配置日志记录。如果需要为一个类配置日志记录，可以使用 SPRING_APPLICATION_JSON 变量。 日志 Groups将相关的 loggers 分组在一起通常很有用，这样就可以同时对它们进行配置，Spring Boot 允许在 Spring 环境中定义日志组。例如将 “tomcat” 组添加到 application.properties。 1logging.group.tomcat=org.apache.catalina, org.apache.coyote, org.apache.tomcat 这样，我们就可以通过一行配置来设置一组日志的日志级别： 1logging.level.tomcat=TRACE Spring Boot 包含以下可以开箱即用的预定义日志组: Name Loggers web org.springframework.core.codec, org.springframework.http, org.springframework.web, org.springframework.boot.actuate.endpoint.web, org.springframework.boot.web.servlet.ServletContextInitializerBeans sql org.springframework.jdbc.core, org.hibernate.SQL 自定义日志配置可以通过在类路径中包含适当的库来激活各种日志系统，还可以通过在类路径的根目录中提供适当的配置文件或在 Spring 环境的 logging.config 属性指定的位置提供适当的配置文件来进一步定制日志系统。 比如可以使用 org.springframework.boot.logging.LoggingSystem 配置属性强制 Spring 引导使用指定的日志系统。该值应该是 LoggingSystem 实现的完全限定类名；如果配置为 none 的话，则表示完全禁用 Spring Boot 的日志配置。下表描述了 SpringBoot 中日志系统所对应的日志配置文件： Logging System Customization Logback logback-spring.xml, logback-spring.groovy, logback.xml, or logback.groovy Log4j2 org.springframework.jdbc.core, org.hibernate.SQL JDK (Java Util Logging) logging.properties SpringBoot 官方建议在日志配置中使用 -spring 的配置方式(例如，使用 logback-spring.xml 而不是 logback.xml)。如果使用标准配置位置，Spring 无法完全控制日志初始化。 另外官方文档中有明确提到，JUL(ava Util Logging) 在 FATJAR 场景下存在一些已知的类加载问题，所以要尽量避免在 FATJAR 场景下使用 JUL。 为了辅助对日志系统进行定制，Spring 会将环境变量属性设置成系统属性，如下表所示: Spring Environment System Property Comments logging.exception-conversion-word LOG_EXCEPTION_CONVERSION_WORD 记录异常时使用的 conversion word logging.file LOG_FILE 如果已定义，则在默认日志配置中使用。 logging.file.max-size LOG_FILE_MAX_SIZE 最大日志文件大小(如果启用了LOG_FILE)。(只支持默认的Logback设置) logging.file.max-history LOG_FILE_MAX_HISTORY 要保留的归档日志文件的最大数量(如果启用了LOG_FILE)。(只支持默认的Logback设置。) logging.path LOG_PATH 如果已定义，则在默认日志配置中使用。 logging.pattern.console CONSOLE_LOG_PATTERN 要在控制台(stdout)上使用的日志模式。(只支持默认的Logback设置。) logging.pattern.dateformat LOG_DATEFORMAT_PATTERN 日志日期格式的附加模式。(只支持默认的 Logback 设置。) logging.pattern.file FILE_LOG_PATTERN 最大日志文件大小(如果启用了LOG_FILE)。(只支持默认的Logback设置) logging.pattern.level LOG_LEVEL_PATTERN 呈现日志级别时使用的格式(默认%5p)。(只支持默认的Logback设置。) PID PID 当前进程ID 所有支持的日志系统在解析配置文件时都可以参考系统属性进行配置解析。 如果希望在日志属性中使用占位符，应该使用 SpringBoot 的语法，而不是底层框架的语法。需要注意的是，如果使用 Logback，应该使用:作为属性名及其默认值之间的分隔符，而不是使用:-。 springProfile 配置 允许用户根据激活的 Spring profiles 选择包含或排除配置部分。profile 文件部分在 元素的任何地方都受支持。可以使用 name 属性指定哪个配置文件接受配置。 可以包含简单的 profile 文件名称(例如 dev )或 profile 文件表达式。profile 文件表达式允许一些比较复杂的 profile 文件逻辑，例如: “production &amp; (eu-central | eu-west)”。下面的显示了三个配置文件示例: 1234567891011 &lt;springProfile name="dev"&gt; &lt;!-- 激活 dev 环境的配置 --&gt;&lt;/springProfile&gt;&lt;springProfile name="dev | pre"&gt; &lt;!-- 激活 dev 和 pre 的环境变量 --&gt;&lt;/springProfile&gt;&lt;springProfile name="!prod"&gt; &lt;!-- 所有非 prod 环境的都激活 --&gt;&lt;/springProfile&gt; 环境属性 标记允许用户传递 Spring Environment 中的属性，以便在 Logback 中使用。比如在 Logback 配置中访问 application.properties 文件中的值。 的作用机制与 Logback 的标准 标签类似。但是，不是指定直接 value，而是指定属性的 source（来自Environment）。如果需要将属性存储在 local 范围以外的其他位置，则可以使用 scope 属性来控制。如果需要默认值（如果未在 Environment 中设置该属性），则可以使用 defaultValue 属性配置。以下示例描述了如何传递在 Logback 中使用的属性： 123456&lt;springProperty scope="context" name="fluentHost" source="myapp.fluentd.host" defaultValue="localhost"/&gt;&lt;appender name="FLUENT" class="ch.qos.logback.more.appenders.DataFluentAppender"&gt; &lt;remoteHost&gt;$&#123;fluentHost&#125;&lt;/remoteHost&gt; ...&lt;/appender&gt; 前面基于 SpringBoot 官方文档对于 Logger 的支持描述做了简单的介绍，下面将通过分析源码来深入的掌握上述这些特性。本文以 log4j2 为例进行分析。 在 SpringBoot 系列-事件机制详解 文章中其实有提到过 logging 初始化的时机。这里简单回顾下： 12345# Application Listenersorg.springframework.context.ApplicationListener=\// 省略其他org.springframework.boot.context.logging.ClasspathLoggingApplicationListener,\org.springframework.boot.context.logging.LoggingApplicationListener,\ 这两个 logging 的监听器中，主要作用的是 LoggingApplicationListener ，这个监听器就是 SpringBoot 中日志初始化的入口。 日志初始化入口LoggingApplicationListener 继承了 GenericApplicationListener 这个接口，其父接口是 ApplicationListener，GenericApplicationListener 中扩展了对于事件类型的支持判断。这里主要关心的是 onApplicationEvent 这个回调方法，关于这个方法中所提到的几个事件类型，可以参考 SpringBoot 系列-事件机制详解 这篇文章的介绍。 123456789101112131415161718192021222324@Overridepublic void onApplicationEvent(ApplicationEvent event) &#123; // ApplicationStartingEvent if (event instanceof ApplicationStartingEvent) &#123; onApplicationStartingEvent((ApplicationStartingEvent) event); &#125; // ApplicationEnvironmentPreparedEvent else if (event instanceof ApplicationEnvironmentPreparedEvent) &#123; onApplicationEnvironmentPreparedEvent((ApplicationEnvironmentPreparedEvent) event); &#125; // ApplicationPreparedEvent else if (event instanceof ApplicationPreparedEvent) &#123; onApplicationPreparedEvent((ApplicationPreparedEvent) event); &#125; // ContextClosedEvent else if (event instanceof ContextClosedEvent &amp;&amp; ((ContextClosedEvent) event).getApplicationContext().getParent() == null) &#123; onContextClosedEvent(); &#125; // ApplicationFailedEvent else if (event instanceof ApplicationFailedEvent) &#123; onApplicationFailedEvent(); &#125;&#125; ApplicationStartingEvent 阶段的处理在收到 ApplicationStartingEvent 事件时，SpringBoot 将通过当前应用的 classloader 来构建一个 loggingSystem 对象，然后执行初始化之前的一些准备工作。 123456private void onApplicationStartingEvent(ApplicationStartingEvent event) &#123; // 通过当前应用的 classloader 构建 loggingSystem 对象 this.loggingSystem = LoggingSystem.get(event.getSpringApplication().getClassLoader()); // loggingSystem 初始化之前准备 this.loggingSystem.beforeInitialize();&#125; 这里可以来看下 loggingSystem 是如何被构建出来的，这个过程可以使得我们非常清楚的了解到，为什么通过引入日志框架依赖或者使用 org.springframework.boot.logging.LoggingSystem 配置能够自动的完成日志框架的选择。 123456789101112131415161718public static LoggingSystem get(ClassLoader classLoader) &#123; // SYSTEM_PROPERTY=org.springframework.boot.logging.LoggingSystem // 这里先从系统变量中获取下 org.springframework.boot.logging.LoggingSystem，看下是否用户自己指定了 LoggingSystem 的类型 String loggingSystem = System.getProperty(SYSTEM_PROPERTY); // 如果 org.springframework.boot.logging.LoggingSystem=xx 有配置值 if (StringUtils.hasLength(loggingSystem)) &#123; // 是否配置的是 none if (NONE.equals(loggingSystem)) &#123; // 如果配置的是 none ，则返回 NoOpLoggingSystem return new NoOpLoggingSystem(); &#125; // 根据指定的日志类型通过反射创建 loggingSystem 对象 return get(classLoader, loggingSystem); &#125; return SYSTEMS.entrySet().stream().filter((entry) -&gt; ClassUtils.isPresent(entry.getKey(), classLoader)) .map((entry) -&gt; get(classLoader, entry.getValue())).findFirst() .orElseThrow(() -&gt; new IllegalStateException("No suitable logging system located"));&#125; 上面代码的最后基于 SYSTEMS 一个 Map 结构的数据进行一系列的处理，主要就是通过判断 entry.getKey() 是否在当前 classpath 中存在，如果存在则通过反射构建类型为 entry.getValue() 的对象；SYSTEMS 是 LoggingSystem 抽象类中的一个静态的 MAP 结构变量，其初始化是在静态代码块中完成的： 1234567891011static &#123; Map&lt;String, String&gt; systems = new LinkedHashMap&lt;&gt;(); // 添加 logback 的 LoggingSystem systems.put("ch.qos.logback.core.Appender", "org.springframework.boot.logging.logback.LogbackLoggingSystem");、 // 添加 log4j2 的 LoggingSystem systems.put("org.apache.logging.log4j.core.impl.Log4jContextFactory", "org.springframework.boot.logging.log4j2.Log4J2LoggingSystem"); // 添加 JUL 的 LoggingSystem systems.put("java.util.logging.LogManager", "org.springframework.boot.logging.java.JavaLoggingSystem"); SYSTEMS = Collections.unmodifiableMap(systems);&#125; 这样看来就比较清晰，如果当前 classpath 中存在 logback、log4j2 或者 JUL 的依赖，则就构建对应的 LoggingSystem 对象。LoggingSystem 对象构建之后还会调用 beforeInitialize 方法，假设引入的是 log4j2 的依赖，则最后构建的 LoggingSystem 就是 Log4J2LoggingSystem 。beforeInitialize 是 LoggingSystem 提供的抽象方法，其具体实现是由子类实现。下面在源码分析部分会展开分析。 ApplicationEnvironmentPreparedEvent 阶段的处理接收到 ApplicationEnvironmentPreparedEvent 事件说明 Environment 对象已经构建完成，环境变量都已经初始化完成了。所以这里主要的工作就是初始化日志框架。 123456789101112131415161718192021222324252627private void onApplicationEnvironmentPreparedEvent(ApplicationEnvironmentPreparedEvent event) &#123; // 这里会再 check 一次loggingSystem 是否已经被创建 if (this.loggingSystem == null) &#123; this.loggingSystem = LoggingSystem.get(event.getSpringApplication().getClassLoader()); &#125; // 通过环境和类路径表达的首选项初始化日志系统。 initialize(event.getEnvironment(), event.getSpringApplication().getClassLoader());&#125;// initializeprotected void initialize(ConfigurableEnvironment environment, ClassLoader classLoader) &#123; // Spring 环境转移到系统属性 new LoggingSystemProperties(environment).apply(); // 解析得到 logFile，依赖 logging.file 和 loggin.path 两个配置值 this.logFile = LogFile.get(environment); if (this.logFile != null) &#123; //设置logging.file-&gt;LOG_FILE // loggin.path -&gt; LOG_PATH this.logFile.applyToSystemProperties(); &#125; initializeEarlyLoggingLevel(environment); // 根据 log 的配置文件初始化 日志 initializeSystem(environment, this.loggingSystem, this.logFile); // 绑定 logging.group , 设置 logging.level initializeFinalLoggingLevels(environment, this.loggingSystem); // 注册 logging.register-shutdown-hook 配置的 钩子 registerShutdownHookIfNecessary(environment, this.loggingSystem);&#125; 这个阶段就是根据我们配置的日志相关的属性和配置文件对日志进行一系列的初始化工作，这里所涉及到的属性和配置在文章前面部分均有提及到。 ApplicationPreparedEvent 阶段的处理接收到 ApplicationPreparedEvent 事件表示应用程序已经准备好，这里会注册两个 bean ， 一个是 springBootLoggingSystem，一个是 pringBootLogFile 。 1234567891011private void onApplicationPreparedEvent(ApplicationPreparedEvent event) &#123; ConfigurableListableBeanFactory beanFactory = event.getApplicationContext().getBeanFactory(); // 注册 springBootLoggingSystem bean if (!beanFactory.containsBean(LOGGING_SYSTEM_BEAN_NAME)) &#123; beanFactory.registerSingleton(LOGGING_SYSTEM_BEAN_NAME, this.loggingSystem); &#125; // 注册 pringBootLogFile bean if (this.logFile != null &amp;&amp; !beanFactory.containsBean(LOGFILE_BEAN_NAME)) &#123; beanFactory.registerSingleton(LOGFILE_BEAN_NAME, this.logFile); &#125;&#125; ContextClosedEvent 和 ApplicationFailedEventContextClosedEvent 事件是 Spring 容器关闭时发送的事件，这里主要就是在 Spring 容器关闭时对日志系统做的一些清理操作；ApplicationFailedEvent 是应用启动失败发送的事件，这里也会对日志系统做清理操作。清理方法由各个子 LoggingSystem 提供具体的实现，以 log4j2 为例，log4j2 的清理主要包括注销桥接处理器（前面初始化阶段有提到）、LogContext 置为null、移除 FILTER，基本上就是初始化阶段的逆过程。 LoggingSystem 分析LoggingSystem 是 SpringBoot 对日志框架进行的一层抽象封装，LoggingSystem 使得我们可以很方便地使用一些日志框架，只需要定义对应日志框架的配置文件，比如 Logback、Log4j、Log4j2 等，代码内部便可以直接使用。 上图为 LoggingSystem 的类继承结构，可以看到 LoggingSystem 的实现子类有 Logback（LogbackLoggingSystem）、Log4j2（Log4J2LoggingSystem）以及 JDK 内置的 Log (JavaLoggingSystem)。LoggingSystem 是个抽象类，内部有这几个方法： beforeInitialize：日志系统初始化之前需要处理的事情 initialize：初始化日志系统 cleanUp：日志系统的清除工作 getShutdownHandler：返回一个 Runnable 用于当 jvm 退出的时候处理日志系统关闭后需要进行的操作，默认返回 null setLogLevel：设置 logger 的级别 这几个方法在上面分析启动入口和日志初始化时都有看到，上述几个方法在 LoggingSystem 要么是抽象方法，要么是空实现，均需要有具体的子类来完成的具体日志框架的处理。从类继承结构图看到有一个 AbstractLoggingSystem，日志实现子类都是继承自这个类，而这个类也是一个抽象类，它又是 LoggingSystem 的子类。所以下面就分别看下 AbstractLoggingSystem 和 Log4J2LoggingSystem 两个类是怎么重写上述几个方法的，这也是 SpringBoot 中日志框架处理的核心逻辑。 AbstractLoggingSystem 处理逻辑beforeInitialize 在 AbstractLoggingSystem 中没有具体的处理逻辑，是个空方法，所以主要是看下 initialize 这个方法. 12345678910111213141516171819202122232425262728293031323334353637383940@Overridepublic void initialize(LoggingInitializationContext initializationContext, String configLocation, LogFile logFile) &#123; // 如果指定了日志配置文件，则通过此配置文件进行初始化 if (StringUtils.hasLength(configLocation)) &#123; initializeWithSpecificConfig(initializationContext, configLocation, logFile); return; &#125; // 没有指定配置文件，则使用默认的方式查找配置文件并加载 initializeWithConventions(initializationContext, logFile);&#125;// 通过指定的配置文件初始化private void initializeWithSpecificConfig(LoggingInitializationContext initializationContext, String configLocation, LogFile logFile) &#123; // 这里会处理日志配置文件中的占位符 configLocation = SystemPropertyUtils.resolvePlaceholders(configLocation); // 抽象方法，由具体子类实现（不同的日志框架处理配置文件的方式由其自身决定） loadConfiguration(initializationContext, configLocation, logFile);&#125;// 通过默认方式查找配置文件并初始化private void initializeWithConventions(LoggingInitializationContext initializationContext, LogFile logFile) &#123; // 查找配置文件，以 log4j2 为例，默认会在 classpath 下查找文件名为 // log4j2.properties、log4j2.yaml, log4j2.yml、log4j2.json，log4j2.jsn，log4j2.xml 的文件 String config = getSelfInitializationConfig(); if (config != null &amp;&amp; logFile == null) &#123; // 发生了自初始化，在属性发生变化时重新初始化 reinitialize(initializationContext); return; &#125; if (config == null) &#123; // 查找 Spring 规则方式的配置， // log4j2-spring.properties、log4j2-spring.xml 等 config = getSpringInitializationConfig(); &#125; if (config != null) &#123; loadConfiguration(initializationContext, config, logFile); return; &#125; // 抽象方法，由具体的日志系统实现 loadDefaults(initializationContext, logFile);&#125; initialize 里主要就是找配置文件，然后通过配置文件进行日志系统的初始化，如果找不到就使用日志系统提供的默认方式进行初始化。上面代码中关于如何 load 配置文件和 load 默认都是在子类中实现的。所以下面就看下在 log4j2 的情况下，是怎么玩的。 Log4J2LoggingSystem 处理逻辑Log4J2LoggingSystem 并非是 AbstractLoggingSystem 的直接子类，而是 Slf4JLoggingSystem 的直接子类，Slf4JLoggingSystem 这个抽象类从代码来看其实就是为了做一些桥接处理，这里不展开分析。 beforeInitialize 在 Log4J2LoggingSystem 中的实现12345678910111213@Overridepublic void beforeInitialize() &#123; // 创建、获取 LoggerContext 对象 LoggerContext loggerContext = getLoggerContext(); // 判断当前 LoggerContext 是否已经初始化过了，如果已经初始化过了则直接返回 if (isAlreadyInitialized(loggerContext)) &#123; return; &#125; // 调用父类 Slf4JLoggingSystem 的 beforeInitialize 的方法，父类这个方法主要就是配置JDK Logging 的桥接处理器 super.beforeInitialize(); // 给 loggerContext 添加默认的 FILTER loggerContext.getConfiguration().addFilter(FILTER);&#125; getLoggerContext 是 log4j2 自己构建 LoggerContext 的过程，此处就先 pass。 initialize 在 Log4J2LoggingSystem 中的实现123456789101112131415@Overridepublic void initialize(LoggingInitializationContext initializationContext, String configLocation, LogFile logFile) &#123; // 拿到当前 loggerContext LoggerContext loggerContext = getLoggerContext(); // 判断下是否已经初始化过了 if (isAlreadyInitialized(loggerContext)) &#123; return; &#125; // 移除默认的 FILTER loggerContext.getConfiguration().removeFilter(FILTER); // 调用父类 initialize，就是在找日志配置文件并且初始化 super.initialize(initializationContext, configLocation, logFile); // 标记已经完成初始化 markAsInitialized(loggerContext);&#125; 这里核心 initialize方法 还是使用的父类的处理逻辑，前面也提到 initialize 在 AbstractLoggingSystem 中最核心的是 load 配置配置文件的过程（loadConfiguration/loadDefaults），而这个 load 的过程是子类实现的。所以下面就看下 log4j2 中 load 配置文件的过程。 loadConfiguration：有配置文件的情况 1234567891011121314151617protected void loadConfiguration(String location, LogFile logFile) &#123; Assert.notNull(location, "Location must not be null"); try &#123; LoggerContext ctx = getLoggerContext(); // 拿到资源url URL url = ResourceUtils.getURL(location); // 构建 ConfigurationSource 对象 ConfigurationSource source = getConfigurationSource(url); // 这里会根据配置的类型选择不同的解析器来解析配置文件,比如 // XmlConfigurationFactory、PropertiesConfigurationFactory... // 以指定的 configuration 启动 ctx.start(ConfigurationFactory.getInstance().getConfiguration(ctx, source)); &#125; catch (Exception ex) &#123; throw new IllegalStateException("Could not initialize Log4J2 logging from " + location, ex); &#125;&#125; 简单概括：通过指定的配置文件地址构建 ConfigurationSource 配置资源对象，然后根据配置资源的文件类型选择不同的 ConfigurationFactory 来解析配置文件，最后日志框架根据此配置文件初始化日志系统。 loadDefaults：没有配置文件的情况 1234567891011@Overrideprotected void loadDefaults(LoggingInitializationContext initializationContext, LogFile logFile) &#123; if (logFile != null) &#123; // 使用 classpath:org/springframework/boot/logging/log4j2/log4j2-file.xml loadConfiguration(getPackagedConfigFile("log4j2-file.xml"), logFile); &#125; else &#123; // 使用 classpath:org/springframework/boot/logging/log4j2/log4j2.xml loadConfiguration(getPackagedConfigFile("log4j2.xml"), logFile); &#125;&#125; 简单概括：在没有指定日志配置文件或者没有在 classpath 下找到符合指定日志系统的配置文件时，则使用 SpringBoot 提供的默认的配置文件进行初始化。 日志系统的清理逻辑cleanUp 方法也是由具体的 LoggingSystem 实现，主要作用就是清理 LoggingSystem 资源。 12345678910@Overridepublic void cleanUp() &#123; // 调用父类，移除桥接器 super.cleanUp(); LoggerContext loggerContext = getLoggerContext(); // 标记loggerContext为未初始化状态，并将内部的 externalContext 置为 null markAsUninitialized(loggerContext); // 移除默认的 FILTER loggerContext.getConfiguration().removeFilter(FILTER);&#125; 一些场景分析这里面包括日常开发工作中使用日志的一些常见场景，比如项目中没有任何日志配置的情况、在 resources 目录下配置日志配置文件的情况、已经使用 SpringBoot 无法识别的日志篇日志文件的情况。 没有任何配置文件没有任何配置，通过前面的分析可知，initialize 方法执行时，是找不到任何资源的，所以会走默认的 loadDefaults 方法进行加载，LogbackLoggingSystem 的loadDefaults 方法，由于 logFile 为 null，所以会使用 classpath:org/springframework/boot/logging/log4j2/log4j2.xml 这份配置文件: 123456789101112131415161718192021222324252627282930&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Configuration status="WARN"&gt; &lt;Properties&gt; &lt;Property name="PID"&gt;????&lt;/Property&gt; &lt;Property name="LOG_EXCEPTION_CONVERSION_WORD"&gt;%xwEx&lt;/Property&gt; &lt;Property name="LOG_LEVEL_PATTERN"&gt;%5p&lt;/Property&gt; &lt;Property name="LOG_DATEFORMAT_PATTERN"&gt;yyyy-MM-dd HH:mm:ss.SSS&lt;/Property&gt; &lt;Property name="CONSOLE_LOG_PATTERN"&gt;%clr&#123;%d&#123;$&#123;LOG_DATEFORMAT_PATTERN&#125;&#125;&#125;&#123;faint&#125; %clr&#123;$&#123;LOG_LEVEL_PATTERN&#125;&#125; %clr&#123;$&#123;sys:PID&#125;&#125;&#123;magenta&#125; %clr&#123;---&#125;&#123;faint&#125; %clr&#123;[%15.15t]&#125;&#123;faint&#125; %clr&#123;%-40.40c&#123;1.&#125;&#125;&#123;cyan&#125; %clr&#123;:&#125;&#123;faint&#125; %m%n$&#123;sys:LOG_EXCEPTION_CONVERSION_WORD&#125;&lt;/Property&gt; &lt;Property name="FILE_LOG_PATTERN"&gt;%d&#123;$&#123;LOG_DATEFORMAT_PATTERN&#125;&#125; $&#123;LOG_LEVEL_PATTERN&#125; $&#123;sys:PID&#125; --- [%t] %-40.40c&#123;1.&#125; : %m%n$&#123;sys:LOG_EXCEPTION_CONVERSION_WORD&#125;&lt;/Property&gt; &lt;/Properties&gt; &lt;Appenders&gt; // 打在控制台 &lt;Console name="Console" target="SYSTEM_OUT" follow="true"&gt; &lt;PatternLayout pattern="$&#123;sys:CONSOLE_LOG_PATTERN&#125;" /&gt; &lt;/Console&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Logger name="org.apache.catalina.startup.DigesterFactory" level="error" /&gt; &lt;Logger name="org.apache.catalina.util.LifecycleBase" level="error" /&gt; &lt;Logger name="org.apache.coyote.http11.Http11NioProtocol" level="warn" /&gt; &lt;logger name="org.apache.sshd.common.util.SecurityUtils" level="warn"/&gt; &lt;Logger name="org.apache.tomcat.util.net.NioSelectorPool" level="warn" /&gt; &lt;Logger name="org.eclipse.jetty.util.component.AbstractLifeCycle" level="error" /&gt; &lt;Logger name="org.hibernate.validator.internal.util.Version" level="warn" /&gt; &lt;logger name="org.springframework.boot.actuate.endpoint.jmx" level="warn"/&gt; &lt;Root level="info"&gt; &lt;AppenderRef ref="Console" /&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 这份配置文件中值有一个 Appender，就是默认的 Console，所以没有配置任何日志配置文件时，日志会被打在控制台。 在 resources 目录下配置 log4j2.xml这份配置文件是能够被 SpringBoot 识别的，所以在初始化日志时会使用此份配置文件来进行日志系统的初始化。下面这份配置文件为每种日志级别都配置了一个 appender，所以在使用时，会根据日志级别将日志打在不同的日志目录下。（PS:关于能够识别的日志配置文件参考前面的分析） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration status="OFF"&gt; &lt;Properties&gt; &lt;Property name="logging.path"&gt;./logs&lt;/Property&gt; &lt;/Properties&gt; &lt;appenders&gt; &lt;Console name="Console" target="SYSTEM_OUT"&gt; &lt;!--只接受程序中 INFO 级别的日志进行处理 --&gt; &lt;ThresholdFilter level="INFO" onMatch="ACCEPT" onMismatch="DENY" /&gt; &lt;PatternLayout pattern="[%d&#123;HH:mm:ss.SSS&#125;] %-5level %class&#123;36&#125; %L %M - %msg%xEx%n" /&gt; &lt;/Console&gt; &lt;!--处理DEBUG级别的日志，并把该日志放到logs/debug.log文件中--&gt; &lt;!--打印出DEBUG级别日志，每次大小超过size，则这size大小的日志会自动存入按年份-月份建立的文件夹下面并进行压缩，作为存档--&gt; &lt;RollingFile name="RollingFileDebug" fileName="$&#123;logging.path&#125;/debug.log" filePattern="logs/$$&#123;date:yyyy-MM&#125;/debug-%d&#123;yyyy-MM-dd&#125;-%i.log.gz"&gt; &lt;Filters&gt; &lt;ThresholdFilter level="DEBUG"/&gt; &lt;ThresholdFilter level="INFO" onMatch="DENY" onMismatch="NEUTRAL"/&gt; &lt;/Filters&gt; &lt;PatternLayout pattern="[%d&#123;yyyy-MM-dd HH:mm:ss&#125;] %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;Policies&gt; &lt;SizeBasedTriggeringPolicy size="500 MB"/&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;!--处理INFO级别的日志，并把该日志放到logs/info.log文件中--&gt; &lt;RollingFile name="RollingFileInfo" fileName="$&#123;logging.path&#125;/info.log" filePattern="logs/$$&#123;date:yyyy-MM&#125;/info-%d&#123;yyyy-MM-dd&#125;-%i.log.gz"&gt; &lt;Filters&gt; &lt;!--只接受INFO级别的日志，其余的全部拒绝处理--&gt; &lt;ThresholdFilter level="INFO"/&gt; &lt;ThresholdFilter level="WARN" onMatch="DENY" onMismatch="NEUTRAL"/&gt; &lt;/Filters&gt; &lt;PatternLayout pattern="[%d&#123;yyyy-MM-dd HH:mm:ss&#125;] %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;Policies&gt; &lt;SizeBasedTriggeringPolicy size="500 MB"/&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;!--处理WARN级别的日志，并把该日志放到logs/warn.log文件中--&gt; &lt;RollingFile name="RollingFileWarn" fileName="$&#123;logging.path&#125;/warn.log" filePattern="logs/$$&#123;date:yyyy-MM&#125;/warn-%d&#123;yyyy-MM-dd&#125;-%i.log.gz"&gt; &lt;Filters&gt; &lt;ThresholdFilter level="WARN"/&gt; &lt;ThresholdFilter level="ERROR" onMatch="DENY" onMismatch="NEUTRAL"/&gt; &lt;/Filters&gt; &lt;PatternLayout pattern="[%d&#123;yyyy-MM-dd HH:mm:ss&#125;] %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;Policies&gt; &lt;SizeBasedTriggeringPolicy size="500 MB"/&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;!--处理error级别的日志，并把该日志放到logs/error.log文件中--&gt; &lt;RollingFile name="RollingFileError" fileName="$&#123;logging.path&#125;/error.log" filePattern="logs/$$&#123;date:yyyy-MM&#125;/error-%d&#123;yyyy-MM-dd&#125;-%i.log.gz"&gt; &lt;ThresholdFilter level="ERROR"/&gt; &lt;PatternLayout pattern="[%d&#123;yyyy-MM-dd HH:mm:ss&#125;] %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;Policies&gt; &lt;SizeBasedTriggeringPolicy size="500 MB"/&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/appenders&gt; &lt;loggers&gt; &lt;root level="DEBUG"&gt; &lt;appender-ref ref="Console"/&gt; &lt;appender-ref ref="RollingFileInfo"/&gt; &lt;appender-ref ref="RollingFileWarn"/&gt; &lt;appender-ref ref="RollingFileError"/&gt; &lt;appender-ref ref="RollingFileDebug"/&gt; &lt;/root&gt; &lt;!--log4j2 自带过滤日志--&gt; &lt;Logger name="org.apache.catalina.startup.DigesterFactory" level="error" /&gt; &lt;Logger name="org.apache.catalina.util.LifecycleBase" level="error" /&gt; &lt;Logger name="org.apache.coyote.http11.Http11NioProtocol" level="warn" /&gt; &lt;logger name="org.apache.sshd.common.util.SecurityUtils" level="warn"/&gt; &lt;Logger name="org.apache.tomcat.util.net.NioSelectorPool" level="warn" /&gt; &lt;Logger name="org.crsh.plugin" level="warn" /&gt; &lt;logger name="org.crsh.ssh" level="warn"/&gt; &lt;Logger name="org.eclipse.jetty.util.component.AbstractLifeCycle" level="error" /&gt; &lt;Logger name="org.hibernate.validator.internal.util.Version" level="warn" /&gt; &lt;logger name="org.thymeleaf" level="warn"/&gt; &lt;Logger name="org.springframework" level="warn"/&gt; &lt;/loggers&gt;&lt;/configuration&gt; 在 resources 下配置一个 log4j2-glmapper.xml将上面的配置文件重命名为 log4j2-glmapper.xml ，因为这个命名规则是 SpringBoot 无法默认识别的，所以在日志配置文件加载时和场景一是一样的。如果希望这份配置文件能够被识别，可以使用 logging.config 来指定。 1logging.config=classpath:log4j2-glmapper.xml 小结本篇对 SpringBoot 中的日志进行了系统的介绍和分析，文章主要是了解 SpringBoot 中对于日志系统的处理，所以不会太关注日志系统自身的一些处理逻辑，有兴趣的读者可以自行研究或者联系作者一起沟通。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 实践系列-资源访问]]></title>
    <url>%2F2019%2F12%2F09%2Fspringboot%2Fspringboot-series-access-resource%2F</url>
    <content type="text"><![CDATA[原文链接：https://smarterco.de/java-load-file-from-classpath-in-spring-boot/ 简介当我们创建一个 SpringBoot web 应用时，有时候需要从 classpath 去加载一些文件，这里记录下在 war 和 jar 两种不同文件格式下加载文件的解决方案 The ResourceLoader在 Java 中 ，我们可以使用当前线程的 classLoader 去尝试加载文件，但是 Spring Framework 为我们提供了更加优雅的解决方案，例如 ResourceLoader。 使用 ResourceLoader 时，我们只需要使用 @Autowire 自动注入 ResourceLoader，然后调用 getResource(“somePath”) 方法即可。 在Spring Boot（WAR）中从资源目录/类路径加载文件的示例12345678910111213141516171819202122232425262728293031323334353637@Service("geolocationservice")public class GeoLocationServiceImpl implements GeoLocationService &#123; private static final Logger LOGGER = LoggerFactory.getLogger(GeoLocationServiceImpl.class); private static DatabaseReader reader = null; private ResourceLoader resourceLoader; @Autowired public GeoLocationServiceImpl(ResourceLoader resourceLoader) &#123; this.resourceLoader = resourceLoader; &#125; @PostConstruct public void init() &#123; try &#123; LOGGER.info("GeoLocationServiceImpl: Trying to load GeoLite2-Country database..."); Resource resource = resourceLoader.getResource("classpath:GeoLite2-Country.mmdb"); File dbAsFile = resource.getFile(); // Initialize the reader reader = new DatabaseReader .Builder(dbAsFile) .fileMode(Reader.FileMode.MEMORY) .build(); LOGGER.info("GeoLocationServiceImpl: Database was loaded successfully."); &#125; catch (IOException | NullPointerException e) &#123; LOGGER.error("Database reader cound not be initialized. ", e); &#125; &#125; @PreDestroy public void preDestroy() &#123; if (reader != null) &#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; LOGGER.error("Failed to close the reader."); &#125; &#125; &#125;&#125; 从 SpringBoot FatJar 中加载资源如果我们想从 Spring Boot JAR 中的类路径加载文件，则必须使用 resource.getInputStream() 方法将其作为 InputStream 检索。 如果尝试使用resource.getFile()，则会收到错误消息，因为 Spring 尝试访问文件系统路径，但它无法访问 JAR 中的路径。 123456789101112131415161718192021222324252627282930313233343536373839@Service("geolocationservice")public class GeoLocationServiceImpl implements GeoLocationService &#123; private static final Logger LOGGER = LoggerFactory.getLogger(GeoLocationServiceImpl.class); private static DatabaseReader reader = null; private ResourceLoader resourceLoader; @Inject public GeoLocationServiceImpl(ResourceLoader resourceLoader) &#123; this.resourceLoader = resourceLoader; &#125; @PostConstruct public void init() &#123; try &#123; LOGGER.info("GeoLocationServiceImpl: Trying to load GeoLite2-Country database..."); Resource resource = resourceLoader.getResource("classpath:GeoLite2-Country.mmdb"); InputStream dbAsStream = resource.getInputStream(); // &lt;-- this is the difference // Initialize the reader reader = new DatabaseReader .Builder(dbAsStream) .fileMode(Reader.FileMode.MEMORY) .build(); LOGGER.info("GeoLocationServiceImpl: Database was loaded successfully."); &#125; catch (IOException | NullPointerException e) &#123; LOGGER.error("Database reader cound not be initialized. ", e); &#125; &#125; @PreDestroy public void preDestroy() &#123; if (reader != null) &#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; LOGGER.error("Failed to close the reader."); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 源码系列-启动过程分析]]></title>
    <url>%2F2019%2F12%2F07%2Fspringboot%2Fspringboot-series-started%2F</url>
    <content type="text"><![CDATA[个人博客：glmapper更多请关注 glmapper工作室 微信公众号 SpringBoot 作为目前非常流行的微服务框架，它使得构建独立的 Spring 生产级应用变得非常简单，因此受到很多互联网企业的青睐。 最近在写 SOFATracer 集成 Spring Cloud Stream RocketMQ 的过程中，遇到了一些问题，比如：BeanPostProcessor 不生效，如何在 BeanPostProcessor 不生效的情况下去修改一个 Bean 等，这些问题其实都是和 Bean 的生命周期有关系的，当然也和容器启动的过程有关系。SpringBoot 的启动过程对于我来说其实不算陌生，也可以说是比较熟悉，但是之前没有完整的梳理过这一块的东西，在实际的应用过程成难免再去踩一些坑。另外想到之前也写过一篇 SpringBoot系列- FatJar 启动原理，刚好承接上篇，继续来探索 SpringBoot 中的一些知识点。 注：本篇基于 SpringBoot 2.1.0.RELEASE 版本，SpringBoot 各个版本之间可能存在差异，不过大体流程基本差不多，所以各位看官在实际的工作过程中也 从一份配置文件开始说起Spring 的启动过程实际上就是 Ioc 容器初始化以及载入 Bean 的过程；SpringBoot 的启动过程最核心的容器刷新流程也是复用了 Spring 容器刷新的逻辑。在分析 SpringBoot 启动过程之前，我们先来简单回顾下 Spring web 应用基于 tomcat 容器部署的启动过程。这就需要从一个大家都熟悉的配置文件开始说起： 1234567&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; 在一般的传统 WEB 项目中，项目的启动一般是从 web.xml 文件的载入开始，如果我们的项目中使用了Spring，那么你肯定会在你的 web.xml 文件中看到上面的配置。Spring 正是通过 ContextLoaderListener 监听器作为容器初始化入口的。 ContextLoaderListener 继承了 ContextLoader 类和 ServletContextListener 接口，并且重写了 ServletContextListener 中的contextInitialized 和 contextDestroyed 方法。在 contextInitialized 中，通过调用父类（ContextLoader）的 initWebApplicationContext 方法进行容器创建： 1234@Overridepublic void contextInitialized(ServletContextEvent event) &#123; initWebApplicationContext(event.getServletContext());&#125; 对于上述 Spring 容器引导刷新大概可以分为两个点来做简单的归纳： 1、通过监听 ServletContextEvent 事件，为 web 容器提供一个全局的 ServletContext 上下文环境，并作为后面 spring 容器的宿主环境 2、在 contextInitialized 方法被调用时，spring 开始初始化一个上下文，这个上下文被称为根上下文，也就是 WebApplicationContext（实际的实现类是 XmlWebApplicationContext ）。这个 WebApplicationContext 就是 spring 的 IoC 容器，其对应的 Bean 定义的配置文件由 web.xml 中的 context-param 指定。 关于依赖监听 ServletContextEvent 事件来引导启动的过程大致可以描述为一下过程： 相对于通过监听 ServletContextEvent 事件方式引导刷新 Spring 上下文，SpringBoot 给我的感觉是回归了 java 的本源，即通过 main 方法方式引导启动。由于 SpringBoot 中对于 web 容器也是使用了嵌入式+自动配置的方式，所以在启动入口上差异还是比较大的，当然 SpringBoot 除了支持 fatjar 方式之外，也提供了 war 包方式来保持对原有 Spring 工程的兼容。 本篇文章将承接上一篇《SpringBoot FatJar 启动原理》，来分析下 SpringBoot 的启动过程。希望通过本篇文章，能够让大家了解到与传统基于 servlet 事件引导启动和基于 main 方式启动的不同，从而对 SpringBoot 的整体启动过程有比较清楚的认识。 启动入口在这篇SpringBoot系列- FatJar 启动原理 文章中介绍得到，JarLaunch 最后是构建了一个 MainMethodRunner 实例对象，然后通过反射的方式调用了 BootStrap 类中的 main 方法，这里的 ’BootStrap 类中的 main 方法‘ 实际上就是 SpringBoot 的业务入口，也就是常见的下面的代码片段： 123456@SpringBootApplicationpublic class GlmapperApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GlmapperApplication.class, args); &#125;&#125; 从代码可以非常直观的了解到，启动是通过调用 SpringApplication 的静态方法 run；这个 run 方法内部其实是会构造一个 SpringApplication 的实例，然后再调用这里实例的 run 方法来启动 SpringBoot 的。 1234567891011/*** Static helper that can be used to run a &#123;@link SpringApplication&#125; from the* specified sources using default settings and user supplied arguments.* @param primarySources the primary sources to load* @param args the application arguments (usually passed from a Java main method)* @return the running &#123;@link ApplicationContext&#125;*/public static ConfigurableApplicationContext run(Class&lt;?&gt;[] primarySources, String[] args) &#123; return new SpringApplication(primarySources).run(args);&#125; 因此，如果要分析 SpringBoot 的启动过程，我们需要熟悉 SpringApplication 的构造过程以及 SpringApplication 的 run 方法执行过程即可。 SpringApplication 实例的构建篇幅原因，我们只分析核心的构建流程。 12345678910111213141516public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) &#123; // 资源加载器，默认是 null this.resourceLoader = resourceLoader; // 启动类 bean Assert.notNull(primarySources, "PrimarySources must not be null"); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); // 是否是 web 应用 this.webApplicationType = WebApplicationType.deduceFromClasspath(); // 设置了 ApplicationContextInitializer setInitializers((Collection) getSpringFactoriesInstances( ApplicationContextInitializer.class)); // 设置 ApplicationListener setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); // 启动类 this.mainApplicationClass = deduceMainApplicationClass();&#125; 上面代码段中，需要关注两个点： 1、初始化 ApplicationContextInitializer； 2、初始化 ApplicationListener 要注意的是这里的实例化，并非是通过注解和扫包完成，而是通过一种不依赖 Spring 上下文的加载方法；这种做法是为了能够使得在 Spring 完成启动前做各种配置。Spring 的解决方法是以接口的全限定名作为 key，实现类的全限定名作为 value 记录在项目的 META-INF/spring.factories 文件中，然后通过SpringFactoriesLoader 工具类提供静态方法进行类加载并缓存下来，spring.factories 是 SpringBoot 的核心配置文件。SpringFactoriesLoader 可以理解为 Spring 自己提供的一种 spi 扩展实现。SpringBoot 中提供的默认的 spring.factories 配置如下： 123456789101112131415161718192021222324# PropertySource Loadersorg.springframework.boot.env.PropertySourceLoader=\// ..省略# Run Listenersorg.springframework.boot.SpringApplicationRunListener=\// ..省略# Error Reportersorg.springframework.boot.SpringBootExceptionReporter=\// ..省略# Application Context Initializersorg.springframework.context.ApplicationContextInitializer=\/// ..省略# Application Listenersorg.springframework.context.ApplicationListener=\// ..省略# Environment Post Processorsorg.springframework.boot.env.EnvironmentPostProcessor=\// ..省略# Failure Analyzersorg.springframework.boot.diagnostics.FailureAnalyzer=\// ..省略# FailureAnalysisReportersorg.springframework.boot.diagnostics.FailureAnalysisReporter=\// ..省略 关于 SpringFactoriesLoader 如何加载这些资源这里就不过多分析，有兴趣的读者可以自行查看相关源码。org.springframework.core.io.support.SpringFactoriesLoader#loadSpringFactories run 方法主流程SpringApplication 的 run 方法 SpringBoot 进行 Spring 容器刷新的实际入口方法，这个方法中包括了很多 SpringBoot 自己扩展出来的一些特性机制，比如 SpringApplicationRunListener、打印启动 Banner、统一的异常处理扩展等等。下面就直观的看下代码，然后再逐个分析各个流程的具体细节： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public ConfigurableApplicationContext run(String... args) &#123; // 开启容器启动计时 StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; // SpringBootExceptionReporter 列表，SpringBoot 允许自定义 Reporter Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); // 设置java.awt.headless属性为true还是false // 可详见解释：https://blog.csdn.net/michaelgo/article/details/81634017 configureHeadlessProperty(); // 获取所有 SpringApplicationRunListener ，也是通过 SpringFactoriesLoader 来获取的 SpringApplicationRunListeners listeners = getRunListeners(args); // 发布 starting 事件，在首次启动 run方法时立即调用，可用于非常早的初始化，注意此时容器上下文还没有刷新 listeners.starting(); try &#123; // 构建 ApplicationArguments 对象 ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); // 准备上下文刷新需要的环境属性 -- 详见 prepareEnvironment 过程分析 ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); // spring.beaninfo.ignore，如果为空设置为true configureIgnoreBeanInfo(environment); // 打印 SpringBoot 启动 Banner Banner printedBanner = printBanner(environment); // 创建上下文，这里会根据 webApplicationType 类型来创建不同的 ApplicationContext context = createApplicationContext(); // 加载获取 exceptionReporters exceptionReporters = getSpringFactoriesInstances( SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); // 上下文刷新之前的准备工作 -- 详见 prepareContext 过程分析 prepareContext(context, environment, listeners, applicationArguments, printedBanner); // 刷新上下文 -- 详见 refreshContext 过程分析 refreshContext(context); // 刷新之后回调，SpringBoot 中这个方法是空实现，可以自行扩展 afterRefresh(context, applicationArguments); // 停止计时 stopWatch.stop(); if (this.logStartupInfo) &#123; new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); &#125; // 发布 started 事件 listeners.started(context); // ApplicationRunner 和 CommandLineRunner 调用 callRunners(context, applicationArguments); &#125; catch (Throwable ex) &#123; // 异常处理 handleRunFailure(context, ex, exceptionReporters, listeners); throw new IllegalStateException(ex); &#125; try &#123; // 发布 running 事件 listeners.running(context); &#125; catch (Throwable ex) &#123; // 异常处理 handleRunFailure(context, ex, exceptionReporters, null); throw new IllegalStateException(ex); &#125; return context;&#125; 上面对代码基本都做了一些详细的注释，有几个需要关注的点： 1、prepareEnvironment 的处理过程 2、prepareContext 的处理过程 3、refreshContext 的处理过程 4、listeners 执行时机及顺序 5、异常处理逻辑 关于 Listeners 执行时机及顺序在之前的文章中有做过非常详细的分析，详见：SpringBoot 系列-事件机制详解。下面就对其他的 4 个点做下详细的分析。 分析启动过程，本质上是对其整个容器生命周期有个了解，包括 listeners 执行各个事件的时机、PostProcessor 执行的时机，Enviroment Ready 的时机等等。掌握这些扩展和时机，可以在实际的业务开发中来做很多事情。 prepareEnvironment 的处理过程prepareEnvironment 过程相对来说是比较早的，这里主要就是为上下文刷新提供 Environment。 123456789101112131415161718192021private ConfigurableEnvironment prepareEnvironment( SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) &#123; // Create and configure the environment ConfigurableEnvironment environment = getOrCreateEnvironment(); // 配置 PropertySources 和 Profiles // 1、将参数和一些默认的属性配置到 environment // 2、激活 profiles configureEnvironment(environment, applicationArguments.getSourceArgs()); // 发布 ApplicationEnvironmentPreparedEvent 事件 listeners.environmentPrepared(environment); // 绑定 SpringApplication 环境 bindToSpringApplication(environment); if (!this.isCustomEnvironment) &#123; environment = new EnvironmentConverter(getClassLoader()) .convertEnvironmentIfNecessary(environment, deduceEnvironmentClass()); &#125; // 附加的解析器将动态跟踪底层 Environment 属性源的任何添加或删除 ConfigurationPropertySources.attach(environment); return environment;&#125; 这里面做的事情就是将我们的配置，包括系统配置、application.properties、-D 参数等等统统打包给 environment。在 Spring 中，我们最常见的 xml 中使用的 ${xxx} 或者代码中使用的 @Value(“${xxxx}”) 等，最后都是从 environment 中拿值的。 这里需要关注的一个比较重要的点是发布 ApplicationEnvironmentPreparedEvent 事件，我们可以通过监听这个事件来修改 environment。这里可以参考下 SOFATracer 中 SofaTracerConfigurationListener 是如何利用这个事件来做环境配置处理的。 prepareContext 的处理过程prepareContext 的处理过程中可以利用的点是非常多的，比如 ApplicationContextInitializer 的执行、ApplicationContextInitializedEvent 和 ApplicationPreparedEvent 事件发布。 123456789101112131415161718192021222324252627282930313233private void prepareContext(ConfigurableApplicationContext context, ConfigurableEnvironment environment, SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) &#123; // 设置 environment 给 context，所以需要注意的是，在此之前拿到的 context 中，environment 是没有的。 context.setEnvironment(environment); // 对 ApplicationContext 的后置处理，比如注册 BeanNameGenerator 和 ResourceLoader postProcessApplicationContext(context); // 这里开始执行所有的 ApplicationContextInitializer applyInitializers(context); // 发布 ApplicationContextInitializedEvent 事件 listeners.contextPrepared(context); if (this.logStartupInfo) &#123; logStartupInfo(context.getParent() == null); logStartupProfileInfo(context); &#125; // Add boot specific singleton beans ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); beanFactory.registerSingleton("springApplicationArguments", applicationArguments); if (printedBanner != null) &#123; beanFactory.registerSingleton("springBootBanner", printedBanner); &#125; if (beanFactory instanceof DefaultListableBeanFactory) &#123; // 是否允许 bean 覆盖，这里如果是 false ,则可能会导致 BeanDefinitionOverrideException 异常 ((DefaultListableBeanFactory) beanFactory) .setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); &#125; // Load the sources Set&lt;Object&gt; sources = getAllSources(); Assert.notEmpty(sources, "Sources must not be empty"); load(context, sources.toArray(new Object[0])); // 发布 ApplicationPreparedEvent 事件 listeners.contextLoaded(context);&#125; ApplicationContextInitializer 是 spring 容器刷新之前初始化 Spring ConfigurableApplicationContext 的回调接口，ApplicationContextInitializer 的 initialize 方法执行之前，context 是还没有刷新的。可以看到在 applyInitializers 之后紧接着发布了 ApplicationContextInitializedEvent 事件。其实这两个点都可以对 context 搞一些事情，ApplicationContextInitializer 更纯粹些，它只关注 context；而 ApplicationContextInitializedEvent 事件源中除了 context 之外，还有 springApplication 对象和参数 args。 prepareContext 最后阶段是发布了 ApplicationPreparedEvent 事件，表示上下文已经准备好了，可以随时执行 refresh 了。 refreshContext 的处理过程refreshContext 是 Spring 上下文刷新的过程，这里实际调用的是 AbstractApplicationContext 的 refresh 方法；所以 SpringBoot 也是复用了 Spring 上下文刷新的过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; // 加锁处理 synchronized (this.startupShutdownMonitor) &#123; // 准备刷新此上下文。主要包括占位符的替换及验证所有的 properties prepareRefresh(); // 这里做了很多事情： // 1、让子类刷新内部beanFactory ，创建IoC容器（DefaultListableBeanFactory--ConfigurableListableBeanFactory 的实现类） // 2、加载解析XML文件（最终存储到Document对象中） // 3、读取Document对象，并完成BeanDefinition的加载和注册工作 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 对 beanFactory 进行一些预处理（设置一些公共属性） prepareBeanFactory(beanFactory); try &#123; // 允许在 AbstractApplicationContext的子类中对 BeanFactory 进行后置处理，postProcessBeanFactory()这个方法是个空实现。 postProcessBeanFactory(beanFactory); // 调用 BeanFactoryPostProcessor 后置处理器处理 BeanFactory 实例（BeanDefinition） invokeBeanFactoryPostProcessors(beanFactory); // 注册BeanPostProcessor后置处理器，BeanPostProcessors后置处理器用于拦截bean的创建 // 用于对创建后的bean实例进行处理 registerBeanPostProcessors(beanFactory); // 初始化消息资源 initMessageSource(); // 初始化应用事件广播器 initApplicationEventMulticaster(); // 初始化特殊的bean，这个方法是空实现，让AbstractApplicationContext的子类重写 onRefresh(); // 注册监听器（ApplicationListener） registerListeners(); // 实例化剩余的单例bean（非懒加载方式）， Bean的 IoC、DI 和 AOP 都是发生在此步骤 finishBeanFactoryInitialization(beanFactory); // 完成刷新 // 1、发布 ContextRefreshedEvent 事件 // 2、处理 LifecycleProcessor finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn("Exception encountered during context initialization - " + "cancelling refresh attempt: " + ex); &#125; // 销毁已经创建的单例以避免资源悬空。 destroyBeans(); // 重置 ”active“ 标记 cancelRefresh(ex); throw ex; &#125; finally &#123; // 重置Spring内核中的常用自检缓存，清空单例bean内缓存 resetCommonCaches(); &#125; &#125;&#125; 这个过程涉及到的东西非常多，可扩展的点也非常多，包括 BeanFactoryPostProcessor 处理、BeanPostProcessor 处理、LifecycleProcessor 处理已经 发布 ContextRefreshedEvent 事件等。到这里容器刷新已经完成，容器已经 ready，DI 和 AOP 也已经完成。 BeanFactoryPostProcessor 处理 BeanFactoryPostProcessor 可以对我们的 beanFactory 内所有的 beandefinition（未实例化）数据进行修改，这个过程是在 bean 还没有实例化之前做的。所以在这，我们通过自己去注册一些 beandefinition ，也可以对 beandefinition 做一些修改。关于 BeanFactoryPostProcessor 的用法在很多框架中都有体现，这里以 SOFATracer 中修改 Datasource 为例来说明下。 SOFATracer 中为了对有所基于 jdbc 规范的数据源进行埋点，提供了一个 DataSourceBeanFactoryPostProcessor，用于修改原生 DataSource 来实现一层代理。代码详见：com.alipay.sofa.tracer.boot.datasource.processor.DataSourceBeanFactoryPostProcessor 这里只看核心代码部分，在 postProcessBeanFactory 方法中会根据 Datasource 的类型来创建不同的 DataSourceProxy；创建 DataSourceProxy 的过程就是修改原生 Datasource 的过程。 123456789101112131415161718192021222324252627282930313233343536private void createDataSourceProxy(ConfigurableListableBeanFactory beanFactory, String beanName, BeanDefinition originDataSource, String jdbcUrl) &#123; // re-register origin datasource bean BeanDefinitionRegistry beanDefinitionRegistry = (BeanDefinitionRegistry) beanFactory; // 先把之前已经存在的 Datasource 的 BeanDefinition 移除 beanDefinitionRegistry.removeBeanDefinition(beanName); boolean isPrimary = originDataSource.isPrimary(); originDataSource.setPrimary(false); // 换个 beanName ,重新注册到容器中 beanDefinitionRegistry.registerBeanDefinition(transformDatasourceBeanName(beanName), originDataSource); // 构建代理的 datasource BeanDefinition，类型为 SmartDataSource RootBeanDefinition proxiedBeanDefinition = new RootBeanDefinition(SmartDataSource.class); // 设置 BeanDefinition 相关属性 proxiedBeanDefinition.setRole(BeanDefinition.ROLE_APPLICATION); proxiedBeanDefinition.setPrimary(isPrimary); proxiedBeanDefinition.setInitMethodName("init"); proxiedBeanDefinition.setDependsOn(transformDatasourceBeanName(beanName)); // 获取原生 datasource 的属性值 MutablePropertyValues originValues = originDataSource.getPropertyValues(); MutablePropertyValues values = new MutablePropertyValues(); String appName = environment.getProperty(TRACER_APPNAME_KEY); // 修改和新增属性 Assert.isTrue(!StringUtils.isBlank(appName), TRACER_APPNAME_KEY + " must be configured!"); values.add("appName", appName); values.add("delegate", new RuntimeBeanReference(transformDatasourceBeanName(beanName))); values.add("dbType", DataSourceUtils.resolveDbTypeFromUrl(unwrapPropertyValue(originValues.get(jdbcUrl)))); values.add("database", DataSourceUtils.resolveDatabaseFromUrl(unwrapPropertyValue(originValues.get(jdbcUrl)))); // 将新的 values 设置给代理 BeanDefinition proxiedBeanDefinition.setPropertyValues(values); // 将代理的 datasource BeanDefinition 注册到容器中 beanDefinitionRegistry.registerBeanDefinition(beanName, proxiedBeanDefinition);&#125; 上面这段代码就是 BeanFactoryPostProcessor 一种典型的应用场景，就是修改 BeanDefinition。 BeanFactoryPostProcessor 处理过程代码比较长，这里就不在具体分析处理的流程。需要关注的点是：1、BeanFactoryPostProcessor 的作用，它能做哪些事情；2、它是在容器启动的哪个阶段执行的。 registerBeanPostProcessors 的处理过程registerBeanPostProcessors 是用于注册 BeanPostProcessor 的。BeanPostProcessor 的作用时机相对于 BeanFactoryPostProcessor 来说要晚一些，BeanFactoryPostProcessor 处理的是 BeanDefinition，Bean 还没有实例化；BeanPostProcessor 处理的是 Bean，BeanPostProcessor 包括两个方法，分别用于在 Bean 实例化之前和实例化之后回调。 开篇有提到，在某些场景下会出现 BeanPostProcessor 不生效。对于 Spring 来说，BeanPostProcessor 本身也会被注册成一个 Bean，那么自然就可能会出现，BeanPostProcessor 处理的 bean 在 BeanPostProcessor 本身初始化之前就已经完成了的情况。 registerBeanPostProcessors 大体分为以下几个部分： 注册 BeanPostProcessorChecker。（当一个 bean 在 BeanPostProcessor 实例化过程中被创建时，即当一个bean没有资格被所有 BeanPostProcessor 处理时，它记录一个信息消息） 实现优先排序、排序和其他操作的 BeanPostProcessor 之间进行排序 注册实现 PriorityOrdered 的 BeanPostProcessor 注册实现 Ordered 的 注册所有常规的 BeanPostProcessor 重新注册所有的内部 BeanPostProcessor 将后处理器注册为用于检测内部 bean 的 applicationlistener，将其移动到处理器链的末端(用于获取代理等)。 这里还是以扩展时机为主线，Bean 的 IoC、DI 和 AOP 初始化过程不细究。 LifecycleProcessor 的处理过程LifecycleProcessor 的处理过程是在 finishRefresh 方法中执行，下面先看下 finishRefresh 方法： 123456789101112protected void finishRefresh() &#123; // 清除上下文级的资源缓存(比如扫描的ASM元数据)。 clearResourceCaches(); // 为此上下文初始化 LifecycleProcessor。 initLifecycleProcessor(); // 首先将 refresh 传播到 LifecycleProcessor。 getLifecycleProcessor().onRefresh(); // 发布 ContextRefreshedEvent 事件 publishEvent(new ContextRefreshedEvent(this)); // Participate in LiveBeansView MBean, if active. LiveBeansView.registerApplicationContext(this);&#125; 初始化 initLifecycleProcessor 是从容器中拿到所有的 LifecycleProcessor ，如果业务代码中没有实现 LifecycleProcessor 接口的 bean ，则使用默认的 DefaultLifecycleProcessor。 onRefresh 过程是 最后会调用到 Lifecycle 接口的 start 方法。LifeCycle 定义 Spring 容器对象的生命周期，任何 spring 管理对象都可以实现该接口。然后，当 ApplicationContext 本身接收启动和停止信号(例如在运行时停止/重启场景)时，spring 容器将在容器上下文中找出所有实现了 LifeCycle 及其子类接口的类，并一一调用它们实现的类。spring 是通过委托给生命周期处理器 LifecycleProcessor 来实现这一点的。Lifecycle 接口定义如下： 123456789101112131415161718192021public interface Lifecycle &#123; /** * 启动当前组件 * 1、如果组件已经在运行，不应该抛出异常 * 2、对于容器，这将把开始信号传播到应用的所有组件 */ void start(); /** * 通常以同步方式停止该组件，当该方法执行完成后,该组件会被完全停止。当需要异步停止行为时，考虑实现 SmartLifecycle 和它的 stop * (Runnable) 方法变体。注意，此停止通知在销毁前不能保证到达:在常规关闭时，&#123;@code Lifecycle&#125; bean将首先收到一个停止通知，然后才传播 * 常规销毁回调;然而，在上下文的生命周期内的热刷新或中止的刷新尝试上，只调用销毁方法。对于容器，这将把停止信号传播到应用的所有组件 */ void stop(); /** * 检查此组件是否正在运行。 * 1. 只有该方法返回 false 时，start方法才会被执行。 * 2. 只有该方法返回 true 时，stop(Runnable callback) 或 stop() 方法才会被执行。 */ boolean isRunning();&#125; 至此，容器刷新其实已经就完成了。可以看到 Spring 或者 SpringBoot 在整个启动过程中，有非常多的口子暴露出来，供用户使用，非常灵活。 异常处理逻辑与正常流程类似，异常处理流程同样作为 SpringBoot 生命周期的一个环节，在异常发生时，会通过一些机制来处理收尾过程。异常处理部分 SpringBoot 1.x 版本和 SpringBoot 2.x 版本差异还是比较大的。这里只分析 SpringBoot 2.x 的处理过程。这里直接贴一段代码： 1234567891011121314151617181920212223242526private void handleRunFailure(ConfigurableApplicationContext context, Throwable exception, Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters, SpringApplicationRunListeners listeners) &#123; try &#123; try &#123; // exitCode handleExitCode(context, exception); if (listeners != null) &#123; // failed listeners.failed(context, exception); &#125; &#125; finally &#123; // 这里也是扩展的口子 reportFailure(exceptionReporters, exception); if (context != null) &#123; context.close(); &#125; &#125; &#125; catch (Exception ex) &#123; logger.warn("Unable to close ApplicationContext", ex); &#125; ReflectionUtils.rethrowRuntimeException(exception);&#125; 上述代码片段主要做了以下几件事： handleExitCode： 这里会拿到异常的 exitCode，随后发布一个 ExitCodeEvent 事件，最后交由 SpringBootExceptionHandler 处理。 SpringApplicationRunListeners#failed： 循环遍历调用所有 SpringApplicationRunListener 的 failed 方法 reportFailure：用户可以自定义扩展 SpringBootExceptionReporter 接口来实现定制化的异常上报逻辑 在 SpringApplicationRunListeners#failed 中，业务产生的异常将直接被抛出，而不会影响异常处理的主流程。 总结至此，SpringBoot 启动的主流程已经全部分析完成了。从扩展和扩展时机的角度来看，整个过程中，SpringBoot 提供了非常多的扩展口子，让用户可以在容器启动的各个阶段（无论是启动，环境准备，容器刷新等等）做一些定制化的操作。用户可以利用这些扩展接口来修改 bean 、修改环境变量，给用户极大的空间。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 源码系列-FatJar 启动原理]]></title>
    <url>%2F2019%2F10%2F13%2Fspringboot%2Fspringboot-series-fatjar%2F</url>
    <content type="text"><![CDATA[微信公众号：glmapper工作室掘金专栏：glmapper微 博：疯狂的石头_henu欢迎关注，一起学习、一起分享 之前有写过一篇文章来介绍 JAR 文件和 MENIFEST.MF 文件，详见：聊一聊 JAR 文件和 MANIFEST.MF，在这篇文章中介绍了 JAR 文件的内部结构。本篇将继续延续前面的节奏，来介绍下，在 SpringBoot 中，是如何将一个 FatJar 运行起来的。 FatJar 解压之后的文件目录从 Spring 官网 或者通过 Idea 创建一个新的 SpringBoot 工程，方便起见，建议什么依赖都不加，默认带入的空的 SpringBoot 工程即可。 通过 maven 命令进行打包，打包成功之后得到的构建产物截图如下： 在前面的文章中有提到，jar 包是zip 包的一种变种，因此也可以通过 unzip 来解压 1unzip -q guides-for-jarlaunch-0.0.1-SNAPSHOT.jar -d mock 解压的 mock 目录，使用 tree 指令，看到整个解压之后的 FatJar 的目录结构如下（部分省略）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647.├── BOOT-INF│ ├── classes│ │ ├── application.properties # 用户-配置文件│ │ └── com│ │ └── glmapper│ │ └── bridge│ │ └── boot│ │ └── BootStrap.class # 用户-启动类│ └── lib│ ├── jakarta.annotation-api-1.3.5.jar│ ├── jul-to-slf4j-1.7.28.jar│ ├── log4j-xxx.jar # 表示 log4j 相关的依赖简写│ ├── logback-xxx.jar # 表示 logback 相关的依赖简写│ ├── slf4j-api-1.7.28.jar│ ├── snakeyaml-1.25.jar│ ├── spring-xxx.jar # 表示 spring 相关的依赖简写├── META-INF│ ├── MANIFEST.MF│ └── maven│ └── com.glmapper.bridge.boot│ └── guides-for-jarlaunch│ ├── pom.properties│ └── pom.xml└── org └── springframework └── boot └── loader ├── ExecutableArchiveLauncher.class ├── JarLauncher.class ├── LaunchedURLClassLoader$UseFastConnectionExceptionsEnumeration.class ├── LaunchedURLClassLoader.class ├── Launcher.class ├── MainMethodRunner.class ├── PropertiesLauncher$1.class ├── PropertiesLauncher$ArchiveEntryFilter.class ├── PropertiesLauncher$PrefixMatchingArchiveFilter.class ├── PropertiesLauncher.class ├── WarLauncher.class ├── archive │ ├── # 省略 ├── data │ ├── # 省略 ├── jar │ ├── # 省略 └── util └── SystemPropertyUtils.class 简单来看，FatJar 解压之后包括三个文件夹： 12345678├── BOOT-INF # 存放的是业务相关的，包括业务开发的类和配置文件，以及依赖的jar│ ├── classes│ └── lib├── META-INF # 包括 MANIFEST.MF 描述文件和 maven 的构建信息│ ├── MANIFEST.MF│ └── maven└── org # SpringBoot 相关的类 └── springframework 我们平时在 debug SpringBoot 工程的启动流程时，一般都是从 SpringApplication#run 方法开始 1234567@SpringBootApplicationpublic class BootStrap &#123; public static void main(String[] args) &#123; // 入口 SpringApplication.run(BootStrap.class,args); &#125;&#125; 对于 java 程序来说，我们知道启动入口必须有 main 函数，这里看起来是符合条件的，但是有一点就是，通过 java 指令执行一个带有 main 函数的类时，是不需要有 -jar 参数的，比如新建一个 BootStrap.java 文件，内容为：12345public class BootStrap &#123; public static void main(String[] args) &#123; System.out.println("Hello World"); &#125;&#125; 通过 javac 编译此文件：1javac BootStrap.java 然后就可以得到编译之后的 .class 文件 BootStrap.class ，此时可以通过 java 指令直接执行：1java BootStrap # 输出 Hello World 那么对于 java -jar 呢？这个其实在 java 的官方文档 中是有明确描述的： -jar filename Executes a program encapsulated in a JAR file. The filename argument is the name of a JAR file with a manifest that contains a line in the form Main-Class:classname that defines the class with the public static void main(String[] args) method that serves as your application’s starting point. When you use the -jar option, the specified JAR file is the source of all user classes, and other class path settings are ignored. 简单说就是，java -jar 命令引导的具体启动类必须配置在 MANIFEST.MF 资源的 Main-Class 属性中。 那回过头再去看下之前打包好、解压之后的文件目录，找到 /META-INF/MANIFEST.MF 文件，看下元数据： 1234567891011Manifest-Version: 1.0Implementation-Title: guides-for-jarlaunchImplementation-Version: 0.0.1-SNAPSHOTStart-Class: com.glmapper.bridge.boot.BootStrapSpring-Boot-Classes: BOOT-INF/classes/Spring-Boot-Lib: BOOT-INF/lib/Build-Jdk-Spec: 1.8Spring-Boot-Version: 2.2.0.RELEASECreated-By: Maven Archiver 3.4.0# Main-Class 在这里，指向的是 JarLauncherMain-Class: org.springframework.boot.loader.JarLauncher org.springframework.boot.loader.JarLauncher 类存放在 org/springframework/boot/loader 下面： 12345└── boot └── loader ├── ExecutableArchiveLauncher.class ├── JarLauncher.class # JarLauncher ├── # 省略 这样就基本理清楚了， FatJar 中，org.springframework.boot.loader 下面的类负责引导启动 SpringBoot 工程，作为入口，BOOT-INF 中存放业务代码和依赖，META-INF 下存在元数据描述。 JarLaunch - FatJar 的启动器在分析 JarLaunch 之前，这里插一下，org.springframework.boot.loader 下的这些类是如何被打包在 FatJar 里面的 spring-boot-maven-plugin 打包 spring-boot-loader 过程因为在新建的空的 SpringBoot 工程中并没有任何地方显示的引入或者编写相关的类。实际上，对于每个新建的 SpringBoot 工程，可以在其 pom.xml 文件中看到如下插件：12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 这个是 SpringBoot 官方提供的用于打包 FatJar 的插件，org.springframework.boot.loader 下的类其实就是通过这个插件打进去的； 下面是此插件将 loader 相关类打入 FatJar 的一个执行流程： org.springframework.boot.maven#execute-&gt;org.springframework.boot.maven#repackage -&gt; org.springframework.boot.loader.tools.Repackager#repackage-&gt;org.springframework.boot.loader.tools.Repackager#writeLoaderClasses-&gt;org.springframework.boot.loader.tools.JarWriter#writeLoaderClasses 最终的执行方法就是下面这个方法，通过注释可以看出，该方法的作用就是将 spring-boot-loader 的classes 写入到 FatJar 中。 12345678/** * Write the required spring-boot-loader classes to the JAR. * @throws IOException if the classes cannot be written */@Overridepublic void writeLoaderClasses() throws IOException &#123; writeLoaderClasses(NESTED_LOADER_JAR);&#125; JarLaunch 基本原理基于前面的分析，这里考虑一个问题，能否直接通过 java BootStrap 来直接运行 SpringBoot 工程呢？这样在不需要 -jar 参数和 JarLaunch 引导的情况下，直接使用最原始的 java 指令理论上是不是也可以，因为有 main 方法。 通过 java BootStrap 方式启动BootStrap 类的如下： 123456@SpringBootApplicationpublic class BootStrap &#123; public static void main(String[] args) &#123; SpringApplication.run(BootStrap.class,args); &#125;&#125; 编译之后，执行 java com.glmapper.bridge.boot.BootStrap，然后抛出异常了： 12345678Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/springframework/boot/SpringApplication at com.glmapper.bridge.boot.BootStrap.main(BootStrap.java:13)Caused by: java.lang.ClassNotFoundException: org.springframework.boot.SpringApplication at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 1 more 从异常堆栈来看，是因为找不到 SpringApplication 这个类；这里其实还是比较好理解的，BootStrap 类中引入了 SpringApplication，但是这个类是在 BOOT-INF/lib 下的，而 java 指令在启动时也没有指定 class path 。 这里不再赘述，通过 -classpath + -Xbootclasspath 的方式尝试了下，貌似也不行，如果有通过 java 指令直接运行成功的，欢迎留言沟通。 通过 java JarLaunch 启动再通过 java org.springframework.boot.loader.JarLauncher 方式启动，可以看到是可以的。 那这里基本可以猜到，JarLauncher 方式启动时，一定会通过某种方式将所需要依赖的 JAR 文件作为 BootStrap 的依赖引入进来。下面就来简单分析下 JarLauncher 启动时，作为启动引导类，它做了哪些事情。 基本原理分析JarLaunch 类的定义如下：123456789101112131415161718192021222324252627public class JarLauncher extends ExecutableArchiveLauncher &#123; // BOOT-INF/classes/ static final String BOOT_INF_CLASSES = "BOOT-INF/classes/"; // BOOT-INF/lib/ static final String BOOT_INF_LIB = "BOOT-INF/lib/"; // 空构造函数 public JarLauncher() &#123; &#125; // 带有指定 Archive 的构造函数 protected JarLauncher(Archive archive) &#123; super(archive); &#125; // 是否是可嵌套的对象 @Override protected boolean isNestedArchive(Archive.Entry entry) &#123; if (entry.isDirectory()) &#123; return entry.getName().equals(BOOT_INF_CLASSES); &#125; return entry.getName().startsWith(BOOT_INF_LIB); &#125; // main 函数 public static void main(String[] args) throws Exception &#123; new JarLauncher().launch(args); &#125;&#125; 通过代码，我们很明显可以看到几个关键的信息点： BOOT_INF_CLASSES 和 BOOT_INF_LIB 两个常量对应的是前面解压之后的两个文件目录 JarLaunch 中包含一个 main 函数，作为启动入口 但是单从 main 来看，只是构造了一个 JarLaunch 对象，然后执行其 launch 方法，并没有我们期望看到的构建所需依赖的地方。实际上这部分是在 JarLaunch 的父类 ExecutableArchiveLauncher 的构造函数中来完成的。 12345678910111213141516171819202122232425262728public ExecutableArchiveLauncher() &#123; try &#123; // 构建 archive this.archive = createArchive(); &#125; catch (Exception ex) &#123; throw new IllegalStateException(ex); &#125;&#125;// 构建 Archiveprotected final Archive createArchive() throws Exception &#123; ProtectionDomain protectionDomain = getClass().getProtectionDomain(); CodeSource codeSource = protectionDomain.getCodeSource(); URI location = (codeSource != null) ? codeSource.getLocation().toURI() : null; // 这里就是拿到当前的 classpath // /Users/xxx/Documents/test/glmapper-springboot-study-guides/guides-for-jarlaunch/target/mock/ String path = (location != null) ? location.getSchemeSpecificPart() : null; if (path == null) &#123; throw new IllegalStateException("Unable to determine code source archive"); &#125; File root = new File(path); if (!root.exists()) &#123; throw new IllegalStateException("Unable to determine code source archive from " + root); &#125; // 构建 Archive return (root.isDirectory() ? new ExplodedArchive(root) : new JarFileArchive(root));&#125; PS: 关于 Archive 的概念这里由于篇幅有限，不再展开说明。 通过上面构建了一个 Archive ，然后继续执行 launch 方法： 123456789protected void launch(String[] args) throws Exception &#123; // 注册协议，利用了 java.net.URLStreamHandler 的扩展机制，SpringBoot // 扩展出了一种可以解析 jar in jar 的协议 JarFile.registerUrlProtocolHandler(); // 通过 classpath 来构建一个 ClassLoader ClassLoader classLoader = createClassLoader(getClassPathArchives()); // launch launch(args, getMainClass(), classLoader);&#125; 下面值需要关注下 getMainClass() 方法即可，这里就是获取 MENIFEST.MF 中指定的 Start-Class ，实际上就是我们的工程里面的 BootStrap 类： 12345678910111213141516@Overrideprotected String getMainClass() throws Exception &#123; // 从 archive 中拿到 Manifest Manifest manifest = this.archive.getManifest(); String mainClass = null; if (manifest != null) &#123; // 获取 Start-Class mainClass = manifest.getMainAttributes().getValue("Start-Class"); &#125; if (mainClass == null) &#123; throw new IllegalStateException( "No 'Start-Class' manifest entry specified in " + this); &#125; // 返回 mainClass return mainClass;&#125; 最终是通过构建了一个 MainMethodRunner 实例对象，然后通过反射的方式调用了 BootStrap 类中的 main 方法： 小结本文主要从 JarLaunch 的角度分析了下 SpringBoot 的启动方式，对常规 java 方式和 java -jar 等启动方式进行了简单的演示；同时简单阐述了下 JarLaunch 启动的基本工作原理。对于其中 构建 Archive 、自定义协议 Handler 等未做深入探究，后面也会针对相关点再做单独分析。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 源码系列-内嵌 Tomcat 的实现原理解析]]></title>
    <url>%2F2019%2F10%2F06%2Fspringboot%2Fspringboot-series-server-tomcat%2F</url>
    <content type="text"><![CDATA[对于一个 SpringBoot web 工程来说，一个主要的依赖标志就是有 spring-boot-starter-web 这个 starter ，spring-boot-starter-web 模块在 spring boot 中其实并没有代码存在，只是在 pom.xml 中携带了一些依赖，包括 web、webmvc、tomcat 等： 1234567891011121314151617181920212223242526&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-json&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate.validator&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Spring Boot 默认的 web 服务容器是 tomcat ，如果想使用 Jetty 等来替换 Tomcat ，可以自行参考官方文档来解决。 web、webmvc、tomcat 等提供了 web 应用的运行环境，那 spring-boot-starter 则是让这些运行环境工作的开关（因为 spring-boot-starter 中会间接引入 spring-boot-autoconfigure ）。 WebServer 自动配置在 spring-boot-autoconfigure 模块中，有处理关于 WebServer 的自动配置类 ServletWebServerFactoryAutoConfiguration 。 ServletWebServerFactoryAutoConfiguration代码片段如下： 12345678910@Configuration@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE)@ConditionalOnClass(ServletRequest.class)@ConditionalOnWebApplication(type = Type.SERVLET)@EnableConfigurationProperties(ServerProperties.class)@Import(&#123; ServletWebServerFactoryAutoConfiguration.BeanPostProcessorsRegistrar.class, ServletWebServerFactoryConfiguration.EmbeddedTomcat.class, ServletWebServerFactoryConfiguration.EmbeddedJetty.class, ServletWebServerFactoryConfiguration.EmbeddedUndertow.class &#125;)public class ServletWebServerFactoryAutoConfiguration 两个 Condition 表示当前运行环境是基于 servlet 标准规范的 web 服务： ConditionalOnClass(ServletRequest.class) ： 表示当前必须有 servlet-api 依赖存在 ConditionalOnWebApplication(type = Type.SERVLET) ：仅基于servlet的Web应用程序 @EnableConfigurationProperties(ServerProperties.class)：ServerProperties 配置中包括了常见的 server.port 等配置属性。 通过 @Import 导入嵌入式容器相关的自动配置类，有 EmbeddedTomcat、EmbeddedJetty 和EmbeddedUndertow。 综合来看，ServletWebServerFactoryAutoConfiguration 自动配置类中主要做了以下几件事情： 导入了内部类 BeanPostProcessorsRegistrar，它实现了 ImportBeanDefinitionRegistrar，可以实现ImportBeanDefinitionRegistrar 来注册额外的 BeanDefinition。 导入了 ServletWebServerFactoryConfiguration.EmbeddedTomcat 等嵌入容器先关配置（我们主要关注tomcat 相关的配置）。 注册了ServletWebServerFactoryCustomizer、TomcatServletWebServerFactoryCustomizer 两个WebServerFactoryCustomizer 类型的 bean。 下面就针对这几个点，做下详细的分析。 BeanPostProcessorsRegistrarBeanPostProcessorsRegistrar 这个内部类的代码如下(省略了部分代码)： 1234567891011121314151617181920public static class BeanPostProcessorsRegistrar implements ImportBeanDefinitionRegistrar, BeanFactoryAware &#123; // 省略代码 @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; if (this.beanFactory == null) &#123; return; &#125; // 注册 WebServerFactoryCustomizerBeanPostProcessor registerSyntheticBeanIfMissing(registry, "webServerFactoryCustomizerBeanPostProcessor", WebServerFactoryCustomizerBeanPostProcessor.class); // 注册 errorPageRegistrarBeanPostProcessor registerSyntheticBeanIfMissing(registry, "errorPageRegistrarBeanPostProcessor", ErrorPageRegistrarBeanPostProcessor.class); &#125; // 省略代码&#125; 上面这段代码中，注册了两个 bean，一个 WebServerFactoryCustomizerBeanPostProcessor，一个 errorPageRegistrarBeanPostProcessor；这两个都实现类 BeanPostProcessor 接口，属于 bean 的后置处理器，作用是在 bean 初始化前后加一些自己的逻辑处理。 WebServerFactoryCustomizerBeanPostProcessor：作用是在 WebServerFactory 初始化时调用上面自动配置类注入的那些 WebServerFactoryCustomizer ，然后调用 WebServerFactoryCustomizer 中的 customize 方法来 处理 WebServerFactory。 errorPageRegistrarBeanPostProcessor：和上面的作用差不多，不过这个是处理 ErrorPageRegistrar 的。 下面简单看下 WebServerFactoryCustomizerBeanPostProcessor 中的代码： 12345678910111213141516171819202122232425public class WebServerFactoryCustomizerBeanPostProcessor implements BeanPostProcessor, BeanFactoryAware &#123; // 省略部分代码 // 在 postProcessBeforeInitialization 方法中，如果当前 bean 是 WebServerFactory，则进行 // 一些后置处理 @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; if (bean instanceof WebServerFactory) &#123; postProcessBeforeInitialization((WebServerFactory) bean); &#125; return bean; &#125; // 这段代码就是拿到所有的 Customizers ，然后遍历调用这些 Customizers 的 customize 方法 private void postProcessBeforeInitialization(WebServerFactory webServerFactory) &#123; LambdaSafe .callbacks(WebServerFactoryCustomizer.class, getCustomizers(), webServerFactory) .withLogger(WebServerFactoryCustomizerBeanPostProcessor.class) .invoke((customizer) -&gt; customizer.customize(webServerFactory)); &#125; // 省略部分代码&#125; 自动配置类中注册的两个 Customizer Bean这两个 Customizer 实际上就是去处理一些配置值，然后绑定到 各自的工厂类的。 WebServerFactoryCustomizer将 serverProperties 配置值绑定给 ConfigurableServletWebServerFactory 对象实例上。 1234567891011121314151617181920212223242526272829@Overridepublic void customize(ConfigurableServletWebServerFactory factory) &#123; PropertyMapper map = PropertyMapper.get().alwaysApplyingWhenNonNull(); // 端口 map.from(this.serverProperties::getPort).to(factory::setPort); // address map.from(this.serverProperties::getAddress).to(factory::setAddress); // contextPath map.from(this.serverProperties.getServlet()::getContextPath) .to(factory::setContextPath); // displayName map.from(this.serverProperties.getServlet()::getApplicationDisplayName) .to(factory::setDisplayName); // session 配置 map.from(this.serverProperties.getServlet()::getSession).to(factory::setSession); // ssl map.from(this.serverProperties::getSsl).to(factory::setSsl); // jsp map.from(this.serverProperties.getServlet()::getJsp).to(factory::setJsp); // 压缩配置策略实现 map.from(this.serverProperties::getCompression).to(factory::setCompression); // http2 map.from(this.serverProperties::getHttp2).to(factory::setHttp2); // serverHeader map.from(this.serverProperties::getServerHeader).to(factory::setServerHeader); // contextParameters map.from(this.serverProperties.getServlet()::getContextParameters) .to(factory::setInitParameters);&#125; TomcatServletWebServerFactoryCustomizer相比于上面那个，这个 customizer 主要处理 Tomcat 相关的配置值 1234567891011121314151617181920@Overridepublic void customize(TomcatServletWebServerFactory factory) &#123; // 拿到 tomcat 相关的配置 ServerProperties.Tomcat tomcatProperties = this.serverProperties.getTomcat(); // server.tomcat.additional-tld-skip-patterns if (!ObjectUtils.isEmpty(tomcatProperties.getAdditionalTldSkipPatterns())) &#123; factory.getTldSkipPatterns() .addAll(tomcatProperties.getAdditionalTldSkipPatterns()); &#125; // server.redirectContextRoot if (tomcatProperties.getRedirectContextRoot() != null) &#123; customizeRedirectContextRoot(factory, tomcatProperties.getRedirectContextRoot()); &#125; // server.useRelativeRedirects if (tomcatProperties.getUseRelativeRedirects() != null) &#123; customizeUseRelativeRedirects(factory, tomcatProperties.getUseRelativeRedirects()); &#125;&#125; WebServerFactory用于创建 WebServer 的工厂的标记接口。 类体系结构 上图为 WebServerFactory -&gt; TomcatServletWebServerFactory 的整个类结构关系。 TomcatServletWebServerFactoryTomcatServletWebServerFactory 是用于获取 Tomcat 作为 WebServer 的工厂类实现，其中最核心的方法就是 getWebServer，获取一个 WebServer 对象实例。 1234567891011121314151617181920212223242526@Overridepublic WebServer getWebServer(ServletContextInitializer... initializers) &#123; // 创建一个 Tomcat 实例 Tomcat tomcat = new Tomcat(); // 创建一个 Tomcat 实例工作空间目录 File baseDir = (this.baseDirectory != null) ? this.baseDirectory : createTempDir("tomcat"); tomcat.setBaseDir(baseDir.getAbsolutePath()); // 创建连接对象 Connector connector = new Connector(this.protocol); tomcat.getService().addConnector(connector); // 1 customizeConnector(connector); tomcat.setConnector(connector); tomcat.getHost().setAutoDeploy(false); // 配置 Engine，没有什么实质性的操作，可忽略 configureEngine(tomcat.getEngine()); // 一些附加链接，默认是 0 个 for (Connector additionalConnector : this.additionalTomcatConnectors) &#123; tomcat.getService().addConnector(additionalConnector); &#125; // 2 prepareContext(tomcat.getHost(), initializers); // 返回 webServer return getTomcatWebServer(tomcat);&#125; 1、customizeConnector ： 给 Connector 设置 port、protocolHandler、uriEncoding 等。Connector 构造的逻辑主要是在NIO和APR选择中选择一个协议，然后反射创建实例并强转为 ProtocolHandler 2、prepareContext 这里并不是说准备当前 Tomcat 运行环境的上下文信息，而是准备一个 StandardContext ，也就是准备一个 web app。 准备 Web App Context 容器对于 Tomcat 来说，每个 context 就是映射到 一个 web app 的，所以 prepareContext 做的事情就是将 web 应用映射到一个 TomcatEmbeddedContext ，然后加入到 Host 中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061protected void prepareContext(Host host, ServletContextInitializer[] initializers) &#123; File documentRoot = getValidDocumentRoot(); // 创建一个 TomcatEmbeddedContext 对象 TomcatEmbeddedContext context = new TomcatEmbeddedContext(); if (documentRoot != null) &#123; context.setResources(new LoaderHidingResourceRoot(context)); &#125; // 设置描述此容器的名称字符串。在属于特定父项的子容器集内，容器名称必须唯一。 context.setName(getContextPath()); // 设置此Web应用程序的显示名称。 context.setDisplayName(getDisplayName()); // 设置 webContextPath 默认是 / context.setPath(getContextPath()); File docBase = (documentRoot != null) ? documentRoot : createTempDir("tomcat-docbase"); context.setDocBase(docBase.getAbsolutePath()); // 注册一个FixContextListener监听，这个监听用于设置context的配置状态以及是否加入登录验证的逻辑 context.addLifecycleListener(new FixContextListener()); // 设置 父 ClassLoader context.setParentClassLoader( (this.resourceLoader != null) ? this.resourceLoader.getClassLoader() : ClassUtils.getDefaultClassLoader()); // 覆盖Tomcat的默认语言环境映射以与其他服务器对齐。 resetDefaultLocaleMapping(context); // 添加区域设置编码映射（请参阅Servlet规范2.4的5.4节） addLocaleMappings(context); // 设置是否使用相对地址重定向 context.setUseRelativeRedirects(false); try &#123; context.setCreateUploadTargets(true); &#125; catch (NoSuchMethodError ex) &#123; // Tomcat is &lt; 8.5.39. Continue. &#125; configureTldSkipPatterns(context); // 设置 WebappLoader ，并且将 父 classLoader 作为构建参数 WebappLoader loader = new WebappLoader(context.getParentClassLoader()); // 设置 WebappLoader 的 loaderClass 值 loader.setLoaderClass(TomcatEmbeddedWebappClassLoader.class.getName()); // 会将加载类向上委托 loader.setDelegate(true); context.setLoader(loader); if (isRegisterDefaultServlet()) &#123; addDefaultServlet(context); &#125; // 是否注册 jspServlet if (shouldRegisterJspServlet()) &#123; addJspServlet(context); addJasperInitializer(context); &#125; context.addLifecycleListener(new StaticResourceConfigurer(context)); ServletContextInitializer[] initializersToUse = mergeInitializers(initializers); // 在 host 中 加入一个 context 容器 // add时给context注册了个内存泄漏跟踪的监听MemoryLeakTrackingListener,详见 addChild 方法 host.addChild(context); //对context做了些设置工作，包括TomcatStarter(实例化并set给context), // LifecycleListener,contextValue,errorpage,Mime,session超时持久化等以及一些自定义工作 configureContext(context, initializersToUse); // postProcessContext 方法是空的，留给子类重写用的 postProcessContext(context);&#125; 从上面可以看下，WebappLoader 可以通过 setLoaderClass 和 getLoaderClass 这两个方法可以更改loaderClass 的值。所以也就意味着，我们可以自己定义一个继承 webappClassLoader 的类，来更换系统自带的默认实现。 初始化 TomcatWebServer在 getWebServer 方法的最后就是构建一个 TomcatWebServer。 12345678910111213// org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactoryprotected TomcatWebServer getTomcatWebServer(Tomcat tomcat) &#123; // new 一个 TomcatWebServer return new TomcatWebServer(tomcat, getPort() &gt;= 0);&#125;// org.springframework.boot.web.embedded.tomcat.TomcatWebServerpublic TomcatWebServer(Tomcat tomcat, boolean autoStart) &#123; Assert.notNull(tomcat, "Tomcat Server must not be null"); this.tomcat = tomcat; this.autoStart = autoStart; // 初始化 initialize();&#125; 这里主要是 initialize 这个方法，这个方法中将会启动 tomcat 服务 12345678910111213141516171819202122232425262728293031323334353637383940414243private void initialize() throws WebServerException &#123; logger.info("Tomcat initialized with port(s): " + getPortsDescription(false)); synchronized (this.monitor) &#123; try &#123; // 对全局原子变量 containerCounter+1，由于初始值是-1， // 所以 addInstanceIdToEngineName 方法内后续的获取引擎并设置名字的逻辑不会执行 addInstanceIdToEngineName(); // 获取 Context Context context = findContext(); // 给 Context 对象实例生命周期监听器 context.addLifecycleListener((event) -&gt; &#123; if (context.equals(event.getSource()) &amp;&amp; Lifecycle.START_EVENT.equals(event.getType())) &#123; // 将上面new的connection以service（这里是StandardService[Tomcat]）做key保存到 // serviceConnectors中，并将 StandardService 中的connectors 与 service 解绑(connector.setService((Service)null);)， // 解绑后下面利用LifecycleBase启动容器就不会启动到Connector了 removeServiceConnectors(); &#125; &#125;); // 启动服务器以触发初始化监听器 this.tomcat.start(); // 这个方法检查初始化过程中的异常，如果有直接在主线程抛出， // 检查方法是TomcatStarter中的 startUpException，这个值是在 Context 启动过程中记录的 rethrowDeferredStartupExceptions(); try &#123; // 绑定命名的上下文和classloader， ContextBindings.bindClassLoader(context, context.getNamingToken(), getClass().getClassLoader()); &#125; catch (NamingException ex) &#123; // 设置失败不需要关心 &#125; // ：与Jetty不同，Tomcat所有的线程都是守护线程，所以创建一个非守护线程 // （例：Thread[container-0,5,main]）来避免服务到这就shutdown了 startDaemonAwaitThread(); &#125; catch (Exception ex) &#123; stopSilently(); throw new WebServerException("Unable to start embedded Tomcat", ex); &#125; &#125;&#125; 查找 Context ，实际上就是查找一个Tomcat 中的一个 web 应用，SpringBoot 中默认启动一个 Tomcat ，并且一个 Tomcat 中只有一个 Web 应用（FATJAR 模式下，应用与 Tomcat 是 1：1 关系），所有在遍历 Host 下的 Container 时，如果 Container 类型是 Context ，就直接返回了。 12345678private Context findContext() &#123; for (Container child : this.tomcat.getHost().findChildren()) &#123; if (child instanceof Context) &#123; return (Context) child; &#125; &#125; throw new IllegalStateException("The host does not contain a Context");&#125; Tomcat 启动过程在 TomcatWebServer 的 initialize 方法中会执行 tomcat 的启动。 12// Start the server to trigger initialization listenersthis.tomcat.start(); org.apache.catalina.startup.Tomcat 的 start 方法： 123456public void start() throws LifecycleException &#123; // 初始化 server getServer(); // 启动 server server.start();&#125; 初始化 Server初始化 server 实际上就是构建一个 StandardServer 对象实例，关于 Tomcat 中的 Server 可以参考附件中的说明。 12345678910111213141516171819202122public Server getServer() &#123; // 如果已经存在的话就直接返回 if (server != null) &#123; return server; &#125; // 设置系统属性 catalina.useNaming System.setProperty("catalina.useNaming", "false"); // 直接 new 一个 StandardServer server = new StandardServer(); // 初始化 baseDir （catalina.base、catalina.home、 ~/tomcat.&#123;port&#125;） initBaseDir(); // Set configuration source ConfigFileLoader.setSource(new CatalinaBaseConfigurationSource(new File(basedir), null)); server.setPort( -1 ); Service service = new StandardService(); service.setName("Tomcat"); server.addService(service); return server;&#125; 小结上面对 SpringBoot 中内嵌 Tomcat 的过程做了分析，这个过程实际上并不复杂，就是在刷新 Spring 上下文的过程中将 Tomcat 容器启动起来，并且将当前应用绑定到一个 Context ，然后添加了 Host。下图是程序的执行堆栈和执行内嵌 Tomcat 初始化和启动的时机。 下面总结下整个过程： 通过自定配置注册相关的 Bean ，包括一些 Factory 和 后置处理器等 上下文刷新阶段，执行创建 WebServer，这里需要用到前一个阶段所注册的 Bean 包括创建 ServletContext 实例化 webServer 创建 Tomcat 实例、创建 Connector 连接器 绑定 应用到 ServletContext，并添加相关的生命周期范畴内的监听器，然后将 Context 添加到 host 中 实例化 webServer 并且启动 Tomcat 服务 SpringBoot 的 Fatjar 方式没有提供共享 Tomcat 的实现逻辑，就是两个 FATJAT 启动可以只实例化一个 Tomcat 实例（包括 Connector 和 Host ），从前面的分析知道，每个 web 应用（一个 FATJAT 对应的应用）实例上就是映射到一个 Context ；而对于 war 方式，一个 Host 下面是可以挂载多个 Context 的。 附：Tomcat 组件说明 组件名称 说明 Server 表示整个Servlet 容器，因此 Tomcat 运行环境中只有唯一一个 Server 实例 Service Service 表示一个或者多个 Connector 的集合，这些 Connector 共享同一个 Container 来处理其请求。在同一个 Tomcat 实例内可以包含任意多个 Service 实例，他们彼此独立。 Connector Tomcat 连接器，用于监听和转化 Socket 请求，同时将读取的 Socket 请求交由 Container 处理，支持不同协议以及不同的 I/O 方式。 Container Container 表示能够执行客户端请求并返回响应的一类对象，在 Tomcat 中存在不同级别的容器：Engine、Host、Context、Wrapper Engine Engine 表示整个 Servlet 引擎。在 Tomcat 中，Engine 为最高层级的容器对象，虽然 Engine 不是直接处理请求的容器，确是获取目标容器的入口 Host Host 作为一类容器，表示 Servlet 引擎（即Engine）中的虚拟机，与一个服务器的网络名有关，如域名等。客户端可以使用这个网络名连接服务器，这个名称必须要在 DNS 服务器上注册 Context Context 作为一类容器，用于表示 ServletContext，在 Servlet 规范中，一个 ServletContext 即表示一个独立的 web 应用 Wrapper Wrapper 作为一类容器，用于表示 Web 应用中定义的 Servlet Executor 表示 Tomcat 组件间可以共享的线程池]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ARK 插件基本规则及注意事项]]></title>
    <url>%2F2019%2F08%2F28%2Fsofa-ark-plugin-rule%2F</url>
    <content type="text"><![CDATA[SOFAARK 是一个轻量级的类隔离框架，其有两个基本的能力：解决依赖包冲突和多应用(模块)合并部署。本篇将从解决依赖角度来说明下 SOFARK 插件的基本使用规则。 下图是官方文档中提供的用于描述依赖包冲突的一个场景： 这里通过一个工程来模拟这种场景，然后通过将其中一个打包成插件的方式来解决。 案例工程1234├── ark-main-project├── dependency-one├── dependency-two├── dependency-two-plugin ark-main-project 为一个 简单的springboot 工程 dependency-one 依赖1，可以对应到图中的 dependency A dependency-two 依赖2，可以对应到图中的 dependency B dependency-two-plugin ，dependency-two 的插件包 另外还有一个 dependency-incompatible 工程，用于描述冲突的依赖。 dependency-incompatibledependency-incompatible 有两个版本 1.0 和 2.0 ，1.0 和 2.0 是不兼容的。 1.0 版本中提供了两个方法： 123456789public class IncompatibleUtil &#123; public static String test1()&#123; return "test1"; &#125; public static String test2()&#123; return "test2"; &#125;&#125; 2.0 版本中提供了两个方法： 123456789101112131415public class IncompatibleUtil &#123; public static String test1()&#123; return "test1"; &#125; public static String test3()&#123; return Incompatible.test(); &#125; public static class Incompatible &#123; public static String test() &#123; return "test"; &#125; &#125;&#125; dependency-one12345public class TestOneUtil &#123; public String testOne()&#123; return IncompatibleUtil.test1()+IncompatibleUtil.test2(); &#125;&#125; dependency-two1234567891011// import org.springframework.util.StringUtils;public class TestTwoUtil &#123; public String testTwo(String param)&#123; if (StringUtils.isEmpty(param))&#123; return IncompatibleUtil.test1() + IncompatibleUtil.test3(); &#125; else &#123; return IncompatibleUtil.test1() + IncompatibleUtil.test3(); &#125; &#125;&#125; 这里引入 spring 的依赖查看是否会引入异常 ark-main-projectark-main-project 引入了 dependency-one 和 dependency-two 两个依赖，然后在启动类中分别调用 dependency-one 和 dependency-two 中提供的 api 。 12345678910111213141516171819@SpringBootApplicationpublic class MainApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MainApplication.class,args); test("test"); &#125; public static void test(String param) &#123; if (!StringUtils.isEmpty(param))&#123; TestOneUtil testOneUtil = new TestOneUtil(); System.out.println(testOneUtil.testOne()); TestTwoUtil testTwoUtil = new TestTwoUtil(); System.out.println(testTwoUtil.testTwo(param)); &#125; else &#123; System.out.println("no params"); &#125; &#125;&#125; 由于 dependency-one 和 dependency-two 底层都都依赖了 dependency-incompatible ，且 dependency-incompatible 的两个版本不兼容，所以在启动时会报错。 dependency-two 插件改造根据文档前面那张图的描述，这里需要将其中一个改造成插件的方式，使用独立的 classloader 来加载，从而达到版本兼容。这里改造 dependency-two 。 新建一个 dependency-two-plugin 模块，然后引入 dependency-two 依赖，并且将 冲突的 api 包导出 123456789101112131415161718192021222324252627282930313233&lt;dependencies&gt; &lt;dependency&gt; &lt;artifactId&gt;dependency-two&lt;/artifactId&gt; &lt;groupId&gt;com.glmapper.bridge.boot&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;com.alipay.sofa&lt;/groupId&gt; &lt;artifactId&gt;sofa-ark-plugin-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.6.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;default-cli&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;ark-plugin&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;exported&gt; &lt;packages&gt; &lt;!--导出冲突的 api --&gt; &lt;package&gt;com.glmapper.bridge.boot.two&lt;/package&gt; &lt;/packages&gt; &lt;/exported&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 关于插件的导出，对于 dependency-two 中，ark-main-projet 中使用到的是 TestTwoUtil 这里类，因此仅需要将这个类导出即可。 mvn clean install 安装到本地仓库，然后在 ark-main-project 中引用。 将 ark-main-project 中的 dependency-two 依赖修改为 dependency-two-plugin 。 12345&lt;dependency&gt; &lt;groupId&gt;com.glmapper.bridge.boot&lt;/groupId&gt; &lt;artifactId&gt;dependency-two-plugin&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 因为插件是运行在容器上的，所以也需要将 ark-main-project 改造成 ark 工程，具体可以参考官方文档。改造完成之后，打包 ark-main-project 工程，然后通过 java -jar 启动，运行结果如下，实现了类隔离。 NoClassDefFoundError 异常的发生关于上面 SpringUtils 工具类在插件中和 BIZ 中均加载并且不会报错的解释是，SpringUtils 虽然在插件中和 BIZ 中都被加载了，但是没有报错，是因为没有触发 java 的 type check 机制。 那么还有一种情况会导致出现 java.lang.NoClassDefFoundError 异常，这种情况是在插件中将 spring 相关的包指定不打入插件了，配置如下： 12345678910111213&lt;configuration&gt; &lt;exported&gt; &lt;packages&gt; &lt;package&gt;com.glmapper.bridge.boot.two.*&lt;/package&gt; &lt;/packages&gt; &lt;/exported&gt; &lt;!--不将 spring 的包打进去--&gt; &lt;excludeGroupIds&gt; &lt;excludeGroupId&gt;org.springframework&lt;/excludeGroupId&gt; &lt;excludeGroupId&gt;org.springframework.boot&lt;/excludeGroupId&gt; &lt;excludeGroupId&gt;org.apache.tomcat.embed&lt;/excludeGroupId&gt; &lt;/excludeGroupIds&gt;&lt;/configuration&gt; 那么这样打出的包实际上包的大小会非常小，但是问题在于运行时，插件从当前 /iib 目录下找不到 spring 相关的依赖，就会报 java.lang.NoClassDefFoundError 。 LinkageError 异常的发生ark-main-project 中 dependency-two 中 重新打包，然后执行 没有报错。此时插件中的类和 biz 中的类完全都是独立的。但是会存在一种情况，比如插件中有一个日志工具类，然后在 Biz 使用了这个工具类，则会报错。 在 dependency-two 中增加一个 LoggerUtil 的类， 123456789101112public class LoggerUtil &#123; private static final Logger LOGGER = LoggerFactory.getLogger(LoggerUtil.class); public void info(String message)&#123; LOGGER.info(message); &#125; public static Logger getLogger()&#123; return LOGGER; &#125;&#125; 然后在 ark-main-project 中这样使用 1234567891011121314151617181920212223public class MainApplication &#123; // 使用 LoggerUtil 获取日志对象实例 private static final Logger LOGGER = LoggerUtil.getLogger(); public static void main(String[] args) &#123; SpringApplication.run(MainApplication.class,args); test("test"); &#125; public static void test(String param) &#123; // 记录日志 LOGGER.info("test in biz."); if (!StringUtils.isEmpty(param))&#123; TestOneUtil testOneUtil = new TestOneUtil(); System.out.println(testOneUtil.testOne()); TestTwoUtil testTwoUtil = new TestTwoUtil(); System.out.println(testTwoUtil.testTwo(param)); &#125; else &#123; System.out.println("no params"); &#125; &#125;&#125; 这种情况下就会导致报错 Caused by: java.lang.LinkageError: loader constraint violation: loader (instance of com/alipay/sofa/ark/container/service/classloader/BizClassLoader) previously initiated loading for a different type with name “org/slf4j/Logger” 1private static final Logger LOGGER = LoggerUtil.getLogger(); 单从这段代码来看，报错的原因在于，Logger LOGGER 的对象加载是被 BizClassLoader 加载的，但是 LoggerUtil.getLogger() 返回的对象是由 PluginClassLoader 加载的。 所以在构建插件时，需要尽可能的去规避可能出现引起类型检查的地方： 方法参数检验 变量赋值 方法返回值]]></content>
      <categories>
        <category>SOFA</category>
      </categories>
      <tags>
        <tag>sofa-ark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[响应式编程 Reactor 小记]]></title>
    <url>%2F2019%2F08%2F24%2Fbase-java-reacotr%2F</url>
    <content type="text"><![CDATA[从响应式编程说起响应式编程是一种关注于数据流（data streams）和变化传递（propagation of change）的异步编程方式。 这意味着它可以用既有的编程语言表达静态（如数组）或动态（如事件源）的数据流。 在响应式编程方面，微软跨出了第一步，它在 .NET 生态中创建了响应式扩展库（Reactive Extensions library, Rx）。接着 RxJava 在 JVM 上实现了响应式编程。后来，在 JVM 平台出现了一套标准的响应式 编程规范，它定义了一系列标准接口和交互规范。并整合到 Java 9 中（Flow 类）。 响应式编程通常作为面向对象编程中的“观察者模式”（Observer design pattern）的一种扩展。 响应式流（reactive streams）与“迭代子模式”（Iterator design pattern）也有相通之处， 因为其中也有 Iterable-Iterator 这样的对应关系。主要的区别在于，Iterator 是基于 “拉取”（pull）方式的，而响应式流是基于“推送”（push）方式的。 iterator 是一种“命令式”（imperative）编程范式，即使访问元素的方法是 Iterable 的唯一职责。关键在于，什么时候执行 next() 获取元素取决于开发者。 响应式流中，相对应的角色是 Publisher-Subscriber，但是当有新的值到来的时候 ，却反过来由发布者（Publisher） 通知订阅者（Subscriber），这种“推送”模式是响应式的关键 此外，对推送来的数据的操作是通过一种声明式（declaratively）而不是命令式（imperatively）的方式表达的：开发者通过描述“控制流程”来定义对数据流的处理逻辑。 除了数据推送，对错误处理（error handling）和完成（completion）信号的定义也很完善。一个 Publisher 可以推送新的值到它的 Subscriber（调用 onNext 方法）， 同样也可以推送错误（调用 onError 方法）和完成（调用 onComplete 方法）信号。 错误和完成信号都可以终止响应式流。可以用下边的表达式描述： 1onNext x 0..N [onError | onComplete] 这种方式非常灵活，无论是有/没有值，还是 n 个值（包括有无限个值的流，比如时钟的持续读秒），都可处理。 以上来自 https://projectreactor.io/docs/core/release/reference/ 翻译 Reactive StreamsReactive Streams 是上面提到的一套标准的响应式编程规范。它由四个核心概念构成： 消息发布者：只有一个 subscribe 接口，是订阅者调用的，用来订阅发布者的消息。发布者在订阅者调用 request 之后把消息 push 给订阅者。 123public interface Publisher&lt;T&gt; &#123; public void subscribe(Subscriber&lt;? super T&gt; s);&#125; 订阅者：订阅者包括四个接口，这些接口都由 Publisher 触发调用的。onSubscribe 告诉订阅者订阅成功，并返回了一个 Subscription ；通过 Subscription 订阅者可以告诉发布者发送指定数量的消息（request 完成） ；onNext 是发布者有消息时，调用订阅者这个接口来达到发布消息的目的；onError 通知订阅者，发布者出现了错误；onComplete 通知订阅者消息发送完毕。 123456public interface Subscriber&lt;T&gt; &#123; public void onSubscribe(Subscription s); public void onNext(T t); public void onError(Throwable t); public void onComplete();&#125; 订阅：包括两个接口，请求 n 个消息和取消此次订阅。 123456789public interface Subscription &#123; // request(n)用来发起请求数据,其中n表示请求数据的数量,它必须大于0, // 否则会抛出IllegalArgumentException,并触发onError,request的调用会 // 累加,如果没有终止,最后会触发相应次数的onNext方法. public void request(long n); // cancel相当于取消订阅,调用之后,后续不会再收到订阅,onError 和 // onComplete也不会被触发 public void cancel();&#125; 处理器：Processor 同时继承了 Subscriber 和 Publisher；其代表一个处理阶段。 12public interface Processor&lt;T, R&gt; extends Subscriber&lt;T&gt;, Publisher&lt;R&gt; &#123;&#125; Reactive Streams 通过上面的四个核心概念和相关的函数，对响应式流进行了一个框架性的约定，它没有具体实现。简单来说，它只提供通用的、合适的解决方案，大家都按照这个规约来实现就好了。 Java 的 Reactive Programming 类库主要有三个，分别是 Akka-Streams ，RxJava 和 Project Reactor。Spring 5 开始支持 Reactive Programming，其底层使用的是 Project Reactor。本篇主要是对 Project Reactor 中的一些点进行学习总结。 Project ReactorProject Reactor 是一个基于 Java 8 的实现了响应式流规范 （Reactive Streams specification）的响应式库。 Reactor 引入了实现 Publisher 的响应式类 Flux 和 Mono，以及丰富的操作方式。 一个 Flux 对象代表一个包含 0..N 个元素的响应式序列，而一个 Mono 对象代表一个包含零或者一个（0..1）元素的结果。 Flux 和 MonoFlux 是生产者，即我们上面提到的 Publisher，它代表的是一个包含 0-N 个元素的异步序列，Mono可以看做 Flux 的有一个特例，代表 0-1 个元素，如果不需要生产任何元素，只是需要一个完成任务的信号，可以使用 Mono。 Flux-包含 0-N 个元素的异步序列 先来看这张图，这里是直接从官方文档上贴过来的。就这张图做下说明，先来关注几个点： 从左到右的时间序列轴 1-6 为 Flux enitted（发射）的元素 上面 6 后面的竖线标识已经成功完成了 下面的 1-3 表示转换的结果 ❌ 表示出现了error，对应的是执行了onError operator : 操作符，声明式的可组装的响应式方法，其组装成的链称为“操作链” 那整体来看就是 Flux 产生元数据，通过一系列 operator 操作得到转换结果，正常成功就是 onCompleted，出现错误就是 onError。看下面的一个小例子：123456789101112131415161718192021Flux.just("glmapper","leishu").subscribe(new Subscriber&lt;String&gt;() &#123; @Override public void onSubscribe(Subscription subscription) &#123; // subscription 表示订阅关系 System.out.println("onSubscribe,"+ subscription.getClass()); // subscription 通过 request 来触发 onNext subscription.request(2); &#125; @Override public void onNext(String s) &#123; System.out.println("currrent value is = " + s); &#125; @Override public void onError(Throwable throwable) &#123; System.out.println("it's error."); &#125; @Override public void onComplete() &#123; System.out.println("it's completed."); &#125;&#125;); 执行结果: 1234onSubscribe,class reactor.core.publisher.StrictSubscribercurrrent value is = glmappercurrrent value is = leishuit&apos;s completed. 如果在 onSubscribe 方法中我们不执行 request，则不会有后续任何操作。关于 request 下面看。 Flux 是一个能够发出 0 到 N 个元素的标准的 Publisher，它会被一个 “error” 或 “completion” 信号终止。因此，一个 Flux 的结果可能是一个 value、completion 或 error。 就像在响应式流规范中规定的那样，这三种类型的信号被翻译为面向下游的 onNext，onComplete和onError方法。 Mono-异步的 0-1 结果 这张图也来自官方文档，和上面 Flux 的区别就是，Mono 最多只能 emitted 一个元素。1Mono.just("glmapper").subscribe(System.out::println); 小结通过上面两段小的代码来看，最直观的感受是，Flux 相当于一个 List，Mono 相当于 Optional。其实在编程中所有的结果我们都可以用 List 来 表示，但是当只返回一个或者没有结果时，用 Optional 可能会更精确些。 Optional 相关概念可自行搜索 jdk Optional 另外，Mono 和 Flux 都提供了一些工厂方法，用于创建相关的实例，这里简单罗列一下：123456789101112131415161718192021// 可以指定序列中包含的全部元素。创建出来的 Flux // 序列在发布这些元素之后会自动结束。Flux.just("glmapper", "leishu");// 从一个Iterable 对象中创建 Flux 对象,当然还可以是数组、Stream对象等Flux.fromIterable(Arrays.asList("glmapper","leishu"));// 创建一个只包含错误消息的序列。Flux.error(new IllegalStateException());// 创建一个包含了从 0 开始递增的 Long 对象的序列。其中包含的元素按照指定的间// 隔来发布。除了间隔时间之外，还可以指定起始元素发布之前的延迟时间。Flux.interval(Duration.ofMillis(100)).take(10);// 创建一个不包含任何消息通知的序列。Flux.never();// 创建一个不包含任何元素，只发布结束消息的序列。Flux.empty(); // 创建包含从 start 起始的 count 个数量的 Integer 对象的序列Flux.range(int start, int count);// Mono 同上Mono.empty();Mono.never();Mono.just("glmapper");Mono.error(new IllegalStateException()); 上面的这些静态方法适合于简单的序列生成，当序列的生成需要复杂的逻辑时，则应该使用 generate() 或 create() 方法。 一些概念 Operator：Operator 是一系列函数式的便捷操作，可以链式调用。所有函数调用基本都 是 Reactor 的 Operator ，比如 just，map，flatMap，filter 等。 Processor：上面从 Processor 的接口定义可以看出，它既是一个 Subscriber，又是一个 Publisher；Processor 夹在第一个 Publisher 和最后一个 Subscriber 中间，对数据进行处理。有点类似 stream 里的 map，filter 等方法。具体在数据流转中， Processor 以 Subscriber 的身份订阅 Publisher 接受数据，又以 Publisher 的方式接受其它 Subscriber 的订阅，它从自己订阅的 Publisher 收到数据后，做一些处理，然后转发给订阅它的 Subscriber。 back pressure：背压。对 MQ 有了解的应该清楚，消息积压一般是在消费端，也就是说生产端只负责生产，并不会关心消费端的消费能力，这样就到导致 pressure 积压在消费端，这个是正向的。从上面对 Reactor 中的一些了解，Subscriber 是主动向 Publisher 请求的，这样当消费端消费的速度没有生产者快时，这些消息还是积压在生产端；这种好处就是生产者可以根据实际情况适当的调整生产消息的速度。 Hot VS Cold ：参考 Hot VS Cold 核心调用过程Reactor 的核心调用过程大致可以分为图中的几个阶段 声明：无论是使用 just 或者其他什么方式创建反应式流，这个过程都可以称之为声明，因为此时这些代码不会被实际的执行。 subscribe：当调用 subscribe 时，整个执行过程便进入 subscribe 阶段，经过一系列的调用之后，subscribe 动作会代理给具体的 Flux 来实现。 onSubscribe：onSubscribe 阶段指的是 Subscriber#onSubscribe 方法被依次调用的阶段。这个阶段会让各 Subscriber 知道 subscribe 方法已被触发，真正的处理流程马上就要开始。 request：onSubscribe 阶段是表示订阅动作的方式，让各 Subscriber 知悉，准备开始处理数据。当最终的 Subscriber 做好处理数据的准备之后，它便会调用 Subscription 的 request 方法请求数据。 onNext：通过调用 Subscriber 的 onNext 方法，进行真正的响应式的数据处理。 onComplete：成功的终端状态，没有进一步的事件将被发送。 onError：错误的终端状态（和 onComplete 一样，当发生时，后面的将不会在继续执行）。 消息处理当需要处理 Flux 或 Mono 中的消息时，可以通过 subscribe 方法来添加相应的订阅逻辑。在调用 subscribe 方法时可以指定需要处理的消息类型。可以只处理其中包含的正常消息，也可以同时处理错误消息和完成消息。 通过 subscribe() 方法处理正常和错误消息123Flux.just(1, 2) .concatWith(Mono.error(new IllegalStateException())) .subscribe(System.out::println, System.err::println); 结果：12312java.lang.IllegalStateException 正常的消息处理相对简单。当出现错误时，有多种不同的处理策略: 通过 onErrorReturn() 方法返回一个默认值1234Flux.just(1, 2) .concatWith(Mono.error(new IllegalStateException())) .onErrorReturn(0) .subscribe(System.out::println); 结果：123120 通过 onErrorResume()方法来根据不同的异常类型来选择要使用的产生元素的流12345678910Flux.just(1, 2) .concatWith(Mono.error(new IllegalArgumentException())) .onErrorResume(e -&gt; &#123; if (e instanceof IllegalStateException) &#123; return Mono.just(0); &#125; else if (e instanceof IllegalArgumentException) &#123; return Mono.just(-1); &#125; return Mono.empty(); &#125;).subscribe(System.out::println); 结果：12312-1 通过 retry 操作符来进行重试，重试的动作是通过重新订阅序列来实现的。在使用 retry 操作符时可以指定重试的次数。1234Flux.just(1, 2) .concatWith(Mono.error(new IllegalStateException())) .retry(1) .subscribe(System.out::println); 结果：123456781212Exception in thread &quot;main&quot; reactor.core.Exceptions$ErrorCallbackNotImplemented: java.lang.IllegalStateExceptionCaused by: java.lang.IllegalStateException at com.glmapper.bridge.boot.reactor.SimpleTest.testFluxSub(SimpleTest.java:75) at com.glmapper.bridge.boot.reactor.SimpleTest.main(SimpleTest.java:23) 调度器 Scheduler在 Reactor 中，执行模式以及执行过程取决于所使用的 Scheduler，Scheduler 是一个拥有广泛实现类的抽象接口，Schedulers 类提供的静态方法用于达成如下的执行环境： 当前线程（Schedulers.immediate()） 12345Schedulers.immediate().schedule(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"-"+11);&#125;);// main-11 可重用的单线程（Schedulers.single()）。注意，这个方法对所有调用者都提供同一个线程来使用， 直到该调度器（Scheduler）被废弃。如果你想使用专一的线程，就对每一个调用使用 Schedulers.newSingle()。 12345Schedulers.single().schedule(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"-"+11);&#125;);// single-1-11 弹性线程池（Schedulers.elastic()。它根据需要创建一个线程池，重用空闲线程。线程池如果空闲时间过长 （默认为 60s）就会被废弃。对于 I/O 阻塞的场景比较适用。 Schedulers.elastic() 能够方便地给一个阻塞 的任务分配它自己的线程，从而不会妨碍其他任务和资源。 12345Schedulers.elastic().schedule(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"-"+11);&#125;);// elastic-2-11 固定大小线程池（Schedulers.parallel()）。所创建线程池的大小与 CPU 个数等同 12345Schedulers.parallel().schedule(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"-"+11);&#125;);// parallel-1-11 基于现有的 ExecutorService 创建 Scheduler 123456ExecutorService executorService = Executors.newSingleThreadExecutor();Schedulers.fromExecutorService(executorService).schedule(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"-"+11);&#125;); // pool-4-thread-1-11 基于 newXXX 方法来创建调度器 12345Schedulers.newElastic("test-elastic").schedule(()-&gt;&#123; System.out.println(Thread.currentThread().getName()+"-"+11);&#125;);// test-elastic-4-11 一些操作符默认会使用一个指定的调度器（通常也允许开发者调整为其他调度器）例如， 通过工厂方法 Flux.interval(Duration.ofMillis(100)) 生成的每 100ms 打点一次的 Flux， 默认情况下使用的是 Schedulers.parallel()，下边的代码演示了如何将其装换为 Schedulers.single()1234Flux&lt;String&gt; intervalResult = Flux.interval(Duration.ofMillis(100), Schedulers.newSingle("test")) .map(i -&gt; Thread.currentThread().getName() +"@"+i); intervalResult.subscribe(System.out::println); 结果：123456test-1@0test-1@1test-1@2test-1@3test-1@4// 省略 publishOn 和 subscribeOnReactor 提供了两种在响应式链中调整调度器 Scheduler 的方法：publishOn 和 subscribeOn。 它们都接受一个 Scheduler 作为参数，从而可以改变调度器。但是 publishOn 在链中出现的位置是有讲究的，而 subscribeOn 则无所谓。 publishOn 的用法和处于订阅链（subscriber chain）中的其他操作符一样。它将上游 信号传给下游，同时执行指定的调度器 Scheduler 的某个工作线程上的回调。 它会 改变后续的操作符的执行所在线程 （直到下一个 publishOn 出现在这个链上） subscribeOn 用于订阅（subscription）过程，作用于那个向上的订阅链（发布者在被订阅 时才激活，订阅的传递方向是向上游的）。所以，无论你把 subscribeOn 至于操作链的什么位置， 它都会影响到源头的线程执行环境（context）。 但是，它不会影响到后续的 publishOn，后者仍能够切换其后操作符的线程执行环境。 1234567891011Flux.create(sink -&gt; &#123; sink.next(Thread.currentThread().getName()); sink.complete(); &#125;) .publishOn(Schedulers.single()) .map(x -&gt; String.format("[%s] %s", Thread.currentThread().getName(), x)) .publishOn(Schedulers.elastic()) .map(x -&gt; String.format("[%s] %s", Thread.currentThread().getName(), x)) .subscribeOn(Schedulers.parallel()) .toStream() .forEach(System.out::println); 结果：1[elastic-2] [single-1] parallel-1 上面这段代码使用 create() 方法创建一个新的 Flux 对象，其中包含唯一的元素是当前线程的名称。 接着是两对 publishOn() 和 map()方法，其作用是先切换执行时的调度器，再把当前的线程名称作为前缀添加。 最后通过 subscribeOn()方法来改变流产生时的执行方式。 最内层的线程名字 parallel-1 来自产生流中元素时使用的 Schedulers.parallel()调度器，中间的线程名称 single-1 来自第一个 map 操作之前的 Schedulers.single() 调度器，最外层的线程名字 elastic-2 来自第二个 map 操作之前的 Schedulers.elastic()调度器。 先到这里，剩下的想到再补充… 参考 https://projectreactor.io/docs/core/release/reference/ https://htmlpreview.github.io/?https://github.com/get-set/reactor-core/blob/master-zh/src/docs/index.html#about-doc https://www.ibm.com/developerworks/cn/java/j-cn-with-reactor-response-encode/index.html?lnk=hmhm]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Reactor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊-JAVA 泛型中的通配符 T，E，K，V，？]]></title>
    <url>%2F2019%2F08%2F19%2Fbase-java-generics%2F</url>
    <content type="text"><![CDATA[前言Java 泛型（generics）是 JDK 5 中引入的一个新特性, 泛型提供了编译时类型安全检测机制，该机制允许开发者在编译时检测到非法的类型。 泛型的本质是参数化类型，也就是说所操作的数据类型被指定为一个参数。 泛型带来的好处在没有泛型的情况的下，通过对类型 Object 的引用来实现参数的“任意化”，“任意化”带来的缺点是要做显式的强制类型转换，而这种转换是要求开发者对实际参数类型可以预知的情况下进行的。对于强制类型转换错误的情况，编译器可能不提示错误，在运行的时候才出现异常，这是本身就是一个安全隐患。 那么泛型的好处就是在编译的时候能够检查类型安全，并且所有的强制转换都是自动和隐式的。 12345678910111213141516171819202122232425262728293031public class GlmapperGeneric&lt;T&gt; &#123; private T t; public void set(T t) &#123; this.t = t; &#125; public T get() &#123; return t; &#125; public static void main(String[] args) &#123; // do nothing &#125; /** * 不指定类型 */ public void noSpecifyType()&#123; GlmapperGeneric glmapperGeneric = new GlmapperGeneric(); glmapperGeneric.set("test"); // 需要强制类型转换 String test = (String) glmapperGeneric.get(); System.out.println(test); &#125; /** * 指定类型 */ public void specifyType()&#123; GlmapperGeneric&lt;String&gt; glmapperGeneric = new GlmapperGeneric(); glmapperGeneric.set("test"); // 不需要强制类型转换 String test = glmapperGeneric.get(); System.out.println(test); &#125;&#125; 上面这段代码中的 specifyType 方法中 省去了强制转换，可以在编译时候检查类型安全，可以用在类，方法，接口上。 泛型中通配符我们在定义泛型类，泛型方法，泛型接口的时候经常会碰见很多不同的通配符，比如 T，E，K，V 等等，这些通配符又都是什么意思呢？ 常用的 T，E，K，V，？本质上这些个都是通配符，没啥区别，只不过是编码时的一种约定俗成的东西。比如上述代码中的 T ，我们可以换成 A-Z 之间的任何一个 字母都可以，并不会影响程序的正常运行，但是如果换成其他的字母代替 T ，在可读性上可能会弱一些。通常情况下，T，E，K，V，？ 是这样约定的： ？ 表示不确定的 java 类型 T (type) 表示具体的一个java类型 K V (key value) 分别代表java键值中的Key Value E (element) 代表Element ？ 无界通配符先从一个小例子看起，原文在 这里 。 我有一个父类 Animal 和几个子类，如狗、猫等，现在我需要一个动物的列表，我的第一个想法是像这样的： 1List&lt;Animal&gt; listAnimals 但是老板的想法确实这样的： 1List&lt;? extends Animal&gt; listAnimals 为什么要使用通配符而不是简单的泛型呢？通配符其实在声明局部变量时是没有什么意义的，但是当你为一个方法声明一个参数时，它是非常重要的。 12345678910111213141516171819202122232425static int countLegs (List&lt;? extends Animal &gt; animals ) &#123; int retVal = 0; for ( Animal animal : animals ) &#123; retVal += animal.countLegs(); &#125; return retVal;&#125;static int countLegs1 (List&lt; Animal &gt; animals )&#123; int retVal = 0; for ( Animal animal : animals ) &#123; retVal += animal.countLegs(); &#125; return retVal;&#125;public static void main(String[] args) &#123; List&lt;Dog&gt; dogs = new ArrayList&lt;&gt;(); // 不会报错 countLegs( dogs ); // 报错 countLegs1(dogs);&#125; 当调用 countLegs1 时，就会飘红，提示的错误信息如下： 所以，对于不确定或者不关心实际要操作的类型，可以使用无限制通配符（尖括号里一个问号，即 &lt;?&gt; ），表示可以持有任何类型。像 countLegs 方法中，限定了上届，但是不关心具体类型是什么，所以对于传入的 Animal 的所有子类都可以支持，并且不会报错。而 countLegs1 就不行。 上界通配符 &lt; ? extends E&gt; 上届：用 extends 关键字声明，表示参数化的类型可能是所指定的类型，或者是此类型的子类。 在类型参数中使用 extends 表示这个泛型中的参数必须是 E 或者 E 的子类，这样有两个好处： 如果传入的类型不是 E 或者 E 的子类，编译不成功 泛型中可以使用 E 的方法，要不然还得强转成 E 才能使用 123456private &lt;K extends A, E extends B&gt; E test(K arg1, E arg2)&#123; E result = arg2; arg2.compareTo(arg1); //..... return result;&#125; 类型参数列表中如果有多个类型参数上限，用逗号分开 下界通配符 &lt; ? super E&gt; 下界: 用 super 进行声明，表示参数化的类型可能是所指定的类型，或者是此类型的父类型，直至 Object 在类型参数中使用 super 表示这个泛型中的参数必须是 E 或者 E 的父类。 123456789101112131415private &lt;T&gt; void test(List&lt;? super T&gt; dst, List&lt;T&gt; src)&#123; for (T t : src) &#123; dst.add(t); &#125;&#125;public static void main(String[] args) &#123; List&lt;Dog&gt; dogs = new ArrayList&lt;&gt;(); List&lt;Animal&gt; animals = new ArrayList&lt;&gt;(); new Test3().test(animals,dogs);&#125;// Dog 是 Animal 的子类class Dog extends Animal &#123;&#125; dst 类型 “大于等于” src 的类型，这里的“大于等于”是指 dst 表示的范围比 src 要大，因此装得下 dst 的容器也就能装 src 。 ？ 和 T 的区别 ？和 T 都表示不确定的类型，区别在于我们可以对 T 进行操作，但是对 ？ 不行，比如如下这种 ： 12345// 可以T t = operate();// 不可以？ car = operate(); 简单总结下： T 是一个 确定的 类型，通常用于泛型类和泛型方法的定义，？是一个 不确定 的类型，通常用于泛型方法的调用代码和形参，不能用于定义类和泛型方法。 区别1：通过 T 来 确保 泛型参数的一致性1234567// 通过 T 来 确保 泛型参数的一致性public &lt;T extends Number&gt; voidtest(List&lt;T&gt; dest, List&lt;T&gt; src)//通配符是 不确定的，所以这个方法不能保证两个 List 具有相同的元素类型public voidtest(List&lt;? extends Number&gt; dest, List&lt;? extends Number&gt; src) 像下面的代码中，约定的 T 是 Number 的子类才可以，但是申明时是用的 String ，所以就会飘红报错。 不能保证两个 List 具有相同的元素类型的情况 1234GlmapperGeneric&lt;String&gt; glmapperGeneric = new GlmapperGeneric&lt;&gt;();List&lt;String&gt; dest = new ArrayList&lt;&gt;();List&lt;Number&gt; src = new ArrayList&lt;&gt;();glmapperGeneric.testNon(dest,src); 上面的代码在编译器并不会报错，但是当进入到 testNon 方法内部操作时（比如赋值），对于 dest 和 src 而言，就还是需要进行类型转换。 区别2：类型参数可以多重限定而通配符不行 使用 &amp; 符号设定多重边界（Multi Bounds)，指定泛型类型 T 必须是 MultiLimitInterfaceA 和 MultiLimitInterfaceB 的共有子类型，此时变量 t 就具有了所有限定的方法和属性。对于通配符来说，因为它不是一个确定的类型，所以不能进行多重限定。 区别3：通配符可以使用超类限定而类型参数不行类型参数 T 只具有 一种 类型限定方式： 1T extends A 但是通配符 ? 可以进行 两种限定： 12? extends A? super A Class&lt;T&gt; 和 Class&lt;?&gt; 区别前面介绍了 ？ 和 T 的区别，那么对于，Class&lt;T&gt; 和 &lt;Class&lt;?&gt; 又有什么区别呢？Class&lt;T&gt; 和 Class&lt;?&gt; 最常见的是在反射场景下的使用，这里以用一段发射的代码来说明下。 1234// 通过反射的方式生成 multiLimit // 对象，这里比较明显的是，我们需要使用强制类型转换MultiLimit multiLimit = (MultiLimit)Class.forName("com.glmapper.bridge.boot.generic.MultiLimit").newInstance(); 对于上述代码，在运行期，如果反射的类型不是 MultiLimit 类，那么一定会报 java.lang.ClassCastException 错误。 对于这种情况，则可以使用下面的代码来代替，使得在在编译期就能直接 检查到类型的问题： Class&lt;T&gt; 在实例化的时候，T 要替换成具体类。Class&lt;?&gt; 它是个通配泛型，? 可以代表任何类型，所以主要用于声明时的限制情况。比如，我们可以这样做申明： 1234// 可以public Class&lt;?&gt; clazz;// 不可以，因为 T 需要指定类型public Class&lt;T&gt; clazzT; 所以当不知道定声明什么类型的 Class 的时候可以定义一 个Class&lt;?&gt;。 那如果也想 public Class&lt;T&gt; clazzT; 这样的话，就必须让当前的类也指定 T ， 1234public class Test3&lt;T&gt; &#123; public Class&lt;?&gt; clazz; // 不会报错 public Class&lt;T&gt; clazzT; 小结本文零碎整理了下 JAVA 泛型中的一些点，不是很全，仅供参考。如果文中有不当的地方，欢迎指正。 参考 JAVA泛型通配符T，E，K，V区别，网友回复：一文秒懂]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>泛型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell 脚本简单归纳和实践]]></title>
    <url>%2F2019%2F08%2F01%2Fseiri-shell-record%2F</url>
    <content type="text"><![CDATA[if 条件 OPTION OPTION 解释 [-a file] 如果file存在则为真 ，也可以表示为 and: 条件与if [ -z “condition1” -a -z “condition2” ] [-b file] 如果file存在且是一个块特殊文件则为真 [-c file] 如果file存在且是一个字特殊文件则为真 [-d file] 如果 file 文件存在且是一个目录则为真，d前的!是逻辑非 #表示目录不存在，则执行后面的 then 操作 if [ ! -d lcd_path/par_date ] [-e file] 如果 file文件存在则为真 [-f file] 如果 file 存在且是一个普通文件则为真 [-g file] 如果 file 存在且已经设置了SGID则为真（SUID 是 Set User ID, SGID 是 Set Group ID的意思） [-h file] 如果 file 存在且是一个符号连接则为真 [-k file] 如果 file 存在且已经设置粘制位则为真 [-p file] 如果file存在且是一个名字管道（F如果O）则为真。管道是linux里面进程间通信的一种方式，其他的还有像信号（signal）、信号量、消息队列、共享内存、套接字（socket）等 [-r file] 如果file存在且是可读的则为真 [-s file] 如果file存在且大小不为0则为真 [-t FD] 如果文件描述符FD打开且指向一个终端则为真 [-u file] 如果file存在且设置了SUID（set userID）则为真 [-w file 如果file存在且是可写的则为真 [-x file] 如果file存在且是可执行的则为真 [-O file] 如果file存在且属有效用户ID则为真 [-G file] 如果file存在且属有效用户组则为真 [-L file] 如果file存在且是一个符号连接则为真 [-N file] 如果file存在and has been mod如果ied since it was last read则为真 [-S file] 如果file存在且是一个套接字则为真 [-o optionname] 如果shell选项“optionname”开启则为真 [-z string] “string”的长度为零则为真 [-n string] or [string] “string”的长度为非零non-zero则为真 if 基本判断 [file1 –nt file2] 如果file1 has been changed more recently than file2或者file1 exists and file2 does not则为真 [file1 –ot file2] 如果file1比file2要老，或者file2存在且file1不存在则为真 [file1 –ef file2] 如果file1和file2指向相同的设备和节点号则为真 [sting1==string2] 如果2个字符串相同。“=”may be used instead of “==”for strict posix compliance则为真 [string1!=string2] 如果字符串不相等则为真 [string1&lt;string2] 如果“string1”sorts before“string2”lexicographically in the current locale则为真 [arg1 OP arg2] “OP”is one of –eq,-ne,-lt,-le,-gt or –ge 截取字符串 # 号截取，删除左边字符，保留右边字符。 （非贪婪匹配） 1234var=http://www.glmapper.com# # 号是运算符，*/ 表示从左边开始删除第一个 / 号及左边的所有字符,即删除 http://echo $&#123;var#*//&#125;#结果 www.glmapper.com ## 号截取，删除左边字符，保留右边字符。（贪婪匹配）** 12345var=http://www.glmapper.com# ##*/ 表示从左边开始删除最后（最右边）一个 / 号及左边的所有字符echo $&#123;var##*//&#125;# 结果 www.glmapper.com %号截取，删除右边字符，保留左边字符 （非贪婪匹配）** 1234var=http://www.glmapper.com# %/* 表示从右边开始，删除第一个 / 号及右边的字符echo $&#123;var%/*&#125;# 结果是：http:/ %% 号截取，删除右边字符，保留左边字符 （贪婪匹配）** 1234var=http://www.glmapper.com# %%/* 表示从右边开始，删除最后（最左边）一个 / 号及右边的字符echo $&#123;var%%/*&#125;# 结果 ：http: 从左边第几个字符开始，及字符的个数 1234var=http://www.glmapper.com# 其中的 0 表示左边第一个字符开始，5 表示字符的总个数echo $&#123;var:0:5&#125;# 结果 http: 从左边第几个字符开始，一直到结束 1234var=http://www.glmapper.com# 其中的 7 表示左边第8个字符开始，一直到结束。echo $&#123;var:7&#125;# 结果 www.glmapper.com 从右边第几个字符开始，及字符的个数 1234var=http://www.glmapper.com# 其中的 0-3 表示右边算起第3个字符开始，3 表示字符的个数echo $&#123;var:0-3:3&#125;# 结果 com 从右边第几个字符开始，一直到结束 1234var=http://www.glmapper.com# 表示从右边第 3 个字符开始，一直到结束echo $&#123;var:0-3&#125;# 结果 com 左边的第一个字符是用 0 表示，右边的第一个字符用 0-1 表示 basename basename 命令简介去除文件名的目录部分和后缀部分。basename 命令读取 String 参数，删除以 /(斜杠) 结尾的前缀以及任何指定的 Suffix 参数，并将剩余的基本文件名称写至标准输出。basename 和 dirname 命令通常用于 shell 脚本中的命令替换来指定和指定的输入文件名称有所差异的输出文件名称。**基本语法如下： 12basename NAME [SUFFIX]basename OPTION 基本示例12345basename /usr/bin/sort# 返回 sortbasename /usr/bin/sort/glmapper.txt# 返回 glmapper.txt 创建基本文件名称的规则 如果 String 参数是 //(双斜杠) 或如果 String 参数包含的都是斜杠字符，则将字符串更改为单个 /(斜杠) 12345basename //usr//bin//sort//glmapper.txt# 返回 glmapper.txtbasename ////# 返回 / 从指定字符串除去任何拖尾的 / 字符。 12basename /usr/bin/sort/# 返回 sort 如果在 String 参数中剩余任何 / 字符，则除去字符串的前缀直到（包含）最后一个 / 字符。 如果指定 Suffix 参数，且它和字符串中的剩余的字符相同，则不修改此字符串 12345basename /usr/bin/sort/glmapper.txt glmapper.txt # 返回glmapper.txt basename /usr/bin/sort/glmapper.txt .txt # 返回 glmapper shell 查看当前目录下文件的个数测试准备，test 目录下有 test1、test2 两个文件夹和一个 1.txt 文件。 12345-test├── 1.txt├── test1│ └── test1_1.txt└── test2 查看当前目录下文件的个数 12➜ test ls -l | grep "^-" | wc -l 1 # 1.txt 查看当前目录下文件的个数，包括子目录里的 12➜ test ls -lR| grep "^-" | wc -l 2 # 1.txt test1_1.txt 查看某目录下文件夹（目录）的个数，包括子目录里的 12➜ test ls -lR| grep "^d" | wc -l 2 # test1 test2 说明： 12341、ls -l ：长列表输出该目录下文件信息(注意这里的文件,不同于一般的文件,可能是目录、链接、设备文件等)2、grep "^-" ：这里将长列表输出信息过滤一部分,只保留一般文件,如果只保留目录就是 ^d3、wc -l ： 统计输出信息的行数,已经过滤得只剩一般文件了,统计结果就是一般文件信息的行数, 又一行信息对应一个文件,也就是文件的个数 利用简单的命令组合实现配置文件的获取测试准备，在 1.txt 中 增加两个属性： 12name=glmapperage=26 12345cat /Users/guolei/logs/test/1.txt | sed 's|[[:blank:]]||g' | grep "^name=" | cut -d= -f2# 返回 glmapper cat /Users/guolei/logs/test/1.txt | sed 's|[[:blank:]]||g' | grep "^age=" | cut -d= -f2# 返回 26 函数封装与返回以上面的解析配置文件为例，将其封装成一个函数 12345678function load_param()&#123; # 接受的第一个参数是文件地址 local properties_file=$1 # 接受的第二个参数是属性名 local param=$2 RESULT=`cat $properties_file | sed 's|[[:blank:]]||g' | grep "^$param=" | cut -d= -f2`&#125; 调用函数并且获取返回值 1234load_param 1.txt namePROP_VAL=$RESULTecho $PROP_VAL# 返回 glmapper shell 实现日志文件的归档处理日志归档简单来说就是，每次希望启动，会将前一次程序运行产生的日志和本地运行产生的日志隔离开来，归档结果就是产生类似于如下的日志文件： stdout.log.20170909 stdout.log.20170709 stdout 所以日志文件的归档在生产脚本中是必须要考虑的，否则就到导致每次产生的文件都会被写入同一份日志文件中。下面是实践过程中归纳的一个日志归档函数： 123456789101112131415161718192021# archive logfunction archive_log() &#123; local FILE_STDOUT_LOG=$LOG_ROOT/stdout.log local FILE_STDERR_LOG=$LOG_ROOT/stderr.log if [ ! -e $LOG_ROOT ] ; then mkdir -p $LOG_ROOT fi NOW=`date +%Y%m%d.%H%M%S` # scroll SOFABoot STDOUT log if [ -e $FILE_STDOUT_LOG ] ; then mv $FILE_STDOUT_LOG $FILE_STDOUT_LOG.$NOW fi # scroll SOFABoot STDERR log if [ -e $FILE_STDERR_LOG ] ; then mv $FILE_STDERR_LOG $FILE_STDERR_LOG.$NOW fi FILE_STDOUT_LOG_GLOBAL=$FILE_STDOUT_LOG; FILE_STDERR_LOG_GLOBAL=$FILE_STDERR_LOG;&#125; 一个简单的 SOFABoot 启动脚本deploy.sh 简单的启动脚本： 123456789101112LOG_ROOT= $1;APP_PATH= $2;# 检查 JAVA_HOMEif [ -z "$JAVA_HOME" ]; then echo "JAVA_HOME not set, exit" exit 1fi# 使用前面的那个日志归档函数archive_log# 启动 java 程序java -jar $APP_PATH &gt;&gt; $FILE_STDOUT_LOG_GLOBAL 2&gt;&gt; $FILE_STDOUT_LOG_GLOBAL &amp; 运行： 1sh deploy.sh ./logs app.jar 小结本文记录日常中常遇到的 shell 命令，基础知识部分零碎的参考了网上一些同学的博客，在此做了归纳。也欢迎大家指正。如果你有比较骚气的操作，也欢迎评论席留言，我会验证后更新到文章中来。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个 maven 插件打包问题的排查]]></title>
    <url>%2F2019%2F07%2F23%2Fmaven%2Fmaven-debug%2F</url>
    <content type="text"><![CDATA[最近研究 sofa-ark 的插件机制时，发现当执行完 maven clean install -DskipTests 时，打在 target 目录下的 xxx.jar 与安装到本地仓库的 xxx.jar 大小不一致。 target 目录下的插件大小 .m2 下的插件大小 其实一开始看到这种现象也是懵逼，同一个工程，同一次命令执行，但是得到的两个 jar 包大小差距巨大。那么对于这种问题，我想到的有两点： debug 打包插件执行过程 了解 maven 插件的生命周期 debug 打包插件执行过程这里需要借助 IDEA 中的远程 debug 能力来完成。目前有两个工程，一个是我们的主工程，工程名为上面截图中的 mq-client-ark-plugin ，另一个是打包插件的源码工程，如下图所示： 那么下面就一步一步来完成远程 debug 的配置。 1、使用 mvnDebug 命令开启 debug 模式在主工程 mq-client-ark-plugin 的根目录下执行 mvnDebug install（当然除了 install 之外，也可以是 compile、package、test、deploy 等）。 当执行完 mvnDebug install 后，可以看到这个阻塞监听 8000 端口了。 2、源码工程配置远程 debug在 idea 主界面找下下图的工具菜单，选择 Edit Configurations...打开配置面板之后，左上角 + 选择 Remote填写相关远程 debug 参数 Host : 远程目标主机地址，因为之前 主工程也是本地启动的，所以这里就是 localhost Port : 远程目标主机开启的远程 debug 端口 开启远程 debug 参数：-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8000 配置完成之后，执行 debug ，可以看下已经连接到了目标机器： 在来看主工程这里，在源码工程没有执行上面的 debug 按钮之前，一直都是阻塞的，执行之后 maven 执行的生命周期开始了: 如上图，因为在源码工程中打了断点，所以当执行到 sofa-ark-maven-plugin 插件时阻塞了。 从 maven 执行的生命周期找出问题根源上面已经搞定了对目标插件源码的 debug 模式的开启，那么下面就是对插件代码进行 debug 操作。节省篇幅，这里直接将断点放在目标代码行位置： 分析这段代码 1、获取到项目的 Artifact ,此时 Artifact 的 file 为： 2、重新设置的 File 3、重新设置了 artifact 如果单从上面 debug 来看，其实很难解释开篇的那个问题。那么这里在回过头来看下 主工程的 maven 执行日志： 如上图中圈红的部分，代表 maven install 所经历的所有阶段。可以看到 sofa-ark-plugin-maven-plugin 是在 maven-install-plugin 后面，那这意味着什么呢？ 我们知道在 target 目录下得到的 xxx.jar 是打包阶段的产物，而 .m2 下面的是 install 的产物。 当然这里没有涉及到 deploy ，deploy 是 install 之后的操作，比如发布到远程仓库。 现在再来看，因为 sofa-ark-plugin-maven-plugin 在执行 install 插件之前将 目标文件给替换了，所以导致打包生成的 target 目录下的 xxx.jar 和 安装到本地仓库的 xxx.jar 不一致。 小结本文记录了日常的一个问题排查过程，包括两个小点，一个是如何去 debug maven 的插件，另外一个是简单了解下 maven 打包的生命周期。 关于 maven 打包的生命周期的代码没有具体研究过，不过这里可以大概猜测下，就是 maven 在执行命令时，有个类似于中央控制器的东西，通过解析 maven 命令得到一个 LifeCycle 或者 一个 Pipeline （LifeCycle 或者 Pipeline 实际上就是组装了一系列的插件）。然后 LifeCycle 或者 Pipeline 启动执行，遍历插件，依次执行插件的 execute 方法。]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊 JAR 文件和 MANIFEST.MF]]></title>
    <url>%2F2019%2F06%2F30%2Ftopic-jar-manifest-intro%2F</url>
    <content type="text"><![CDATA[在 JAVA 语言这个圈子里面摸爬滚打，除了对于语言层面和框架层面的学习之外，有一些东西它一直存在，但是确没有对它们有足够的重视，因为都觉得它是理所当然，比如 JAR 是个什么？ 提到 JAR，最先可能想到的就是依赖，比如 fastjson.jar ，它可以作为依赖在项目中来引用，但是不能通过 java -jar 来执行，这种就是非可执行的 JAR。另外一种，比如我们项目打包之后生成的 JAR （当然也可能是 war），我们可以通过 java -jar 来运行程序，我们把它称之为可执行的 JAR。 JAR 作用大体可以分为以下几种： 用于发布和使用类库 作为应用程序和扩展的构建单元 作为组件、applet 或者插件程序的部署单位 用于打包与组件相关联的辅助资源 基本概念JAR 文件是一种归档文件，以 ZIP 格式构建，以 .jar 为文件扩展名。用户可以使用 JDK 自带的 jar 命令创建或提取 JAR 文件。也可以使用其他 zip 压缩工具，不过压缩时 zip 文件头里的条目顺序很重要，因为 MANIFEST 文件常需放在首位。JAR 文件内的文件名是 Unicode 文本。 JAR 文件（Java 归档，英语：Java Archive）是一种软件包文件格式，通常用于聚合大量的 Java 类文件、相关的元数据和资源（文本、图片等）文件到一个文件，以便分发 Java 平台应用软件或库。 以上来自维基百科 JAR 文件格式提供了许多优势和功能，其中很多是传统的压缩格式如 ZIP 或者 TAR 所没有提供的。它们包括： 安全性：可以对 JAR 文件内容加上数字化签名。这样，能够识别签名的工具就可以有选择地为您授予软件安全特权，这是其他文件做不到的，它还可以检测代码是否被篡改过。 减少下载时间：如果一个 applet 捆绑到一个 JAR 文件中，那么浏览器就可以在一个 HTTP 事务中下载这个 applet 的类文件和相关的资源，而不是对每一个文件打开一个新连接。 压缩：JAR 格式允许您压缩文件以提高存储效率。 传输平台扩展。Java 扩展框架 (Java Extensions Framework) 提供了向 Java 核心平台添加功能的方法，这些扩展是用 JAR 文件打包的 (Java 3D 和 JavaMail 就是由 Sun 开发的扩展例子 )。 包密封：存储在 JAR 文件中的包可以选择进行 密封，以增强版本一致性和安全性。密封一个包意味着包中的所有类都必须在同一 JAR 文件中找到。 包版本控制：一个 JAR 文件可以包含有关它所包含的文件的数据，如厂商和版本信息。 可移植性：处理 JAR 文件的机制是 Java 平台核心 API 的标准部分。 JAR 文件格式这里分别给出两个 JAR 的解压之后的示例 普通的 JAR 解压之后的文件目录以 fastjson 为例：123456789101112131415161718192021222324.├── META-INF│ ├── LICENSE.txt│ ├── MANIFEST.MF│ ├── NOTICE.txt│ ├── maven│ │ └── com.alibaba│ │ └── fastjson│ │ ├── pom.properties│ │ └── pom.xml│ └── services│ ├── javax.ws.rs.ext.MessageBodyReader│ ├── javax.ws.rs.ext.MessageBodyWriter│ ├── javax.ws.rs.ext.Providers│ └── org.glassfish.jersey.internal.spi.AutoDiscoverable└── com └── alibaba └── fastjson ├── JSON.class ├── JSONArray.class ├── JSONAware.class ├── JSONException.class ├── JSONObject.class ....省略 可执行的 jar (以 SpringBoot 的 FAT JAR 为例）这个 jar 是从 start.spring.io 上下载下来的一个最简单的 demo 打包来的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859├── BOOT-INF│ ├── classes│ │ ├── application.properties│ │ └── com│ │ └── example # 应用的.class 文件目录│ │ └── demo│ │ └── DemoApplication.class│ └── lib # 这里存放的是应用的 Maven 依赖的jar包文件│ ├── javax.annotation-api-1.3.2.jar│ ├── jul-to-slf4j-1.7.26.jar│ ├── log4j-api-2.11.2.jar│ ├── log4j-to-slf4j-2.11.2.jar│ ├── logback-classic-1.2.3.jar│ ├── logback-core-1.2.3.jar│ ├── slf4j-api-1.7.26.jar│ ├── snakeyaml-1.23.jar│ ├── spring-aop-5.1.8.RELEASE.jar│ ├── spring-beans-5.1.8.RELEASE.jar│ ├── spring-boot-2.1.6.RELEASE.jar│ ├── spring-boot-autoconfigure-2.1.6.RELEASE.jar│ ├── spring-boot-starter-2.1.6.RELEASE.jar│ ├── spring-boot-starter-logging-2.1.6.RELEASE.jar│ ├── spring-context-5.1.8.RELEASE.jar│ ├── spring-core-5.1.8.RELEASE.jar│ ├── spring-expression-5.1.8.RELEASE.jar│ └── spring-jcl-5.1.8.RELEASE.jar├── META-INF│ ├── MANIFEST.MF│ └── maven│ └── com.example│ └── demo│ ├── pom.properties│ └── pom.xml└── org └── springframework └── boot └── loader #存放的是 Spring boot loader 的 class 文件 ├── ExecutableArchiveLauncher.class ├── JarLauncher.class ├── LaunchedURLClassLoader$UseFastConnectionExceptionsEnumeration.class ├── LaunchedURLClassLoader.class ├── Launcher.class ├── MainMethodRunner.class ├── PropertiesLauncher$1.class ├── PropertiesLauncher$ArchiveEntryFilter.class ├── PropertiesLauncher$PrefixMatchingArchiveFilter.class ├── PropertiesLauncher.class ├── WarLauncher.class ├── archive │ ├── Archive$Entry.class │ ├── ... ├── data │ ├── RandomAccessData.class │ ├── ... ├── jar │ ├── AsciiBytes.class │ ├── ... └── util └── SystemPropertyUtils.class META-INF大多数 JAR 文件包含一个 META-INF 目录，它用于存储包和扩展的配置数据，如安全性和版本信息。Java 2 平台（标准版【J2SE】）识别并解释 META-INF 目录中的下述文件和目录，以便配置应用程序、扩展和类装载器： MANIFEST.MF：这个 manifest 文件定义了与扩展和包相关的数据。 通过 MAVEN 插件打包进来的文件比如： maven services ： 存储所有服务提供程序配置文件 其他的还有一些不常看到的： INDEX.LIST ：这个文件由 jar工具的新选项 -i生成，它包含在应用程序或者扩展中定义的包的位置信息。它是 JarIndex 实现的一部分，并由类装载器用于加速类装载过程。 .SF：这是 JAR 文件的签名文件 .DSA：与签名文件相关联的签名程序块文件，它存储了用于签名 JAR 文件的公共签名。 LICENSE.txt ：证书信息 NOTICE.txt ： 公告信息 可执行的 JAR 可以执行的 JAR 与 普通的 JAR 最直接的区别就是能否通过 java -jar 来执行。 一个 可执行的 jar文件是一个自包含的 Java 应用程序，它存储在特别配置的 JAR 文件中，可以由 JVM 直接执行它而无需事先提取文件或者设置类路径。要运行存储在非可执行的 JAR 中的应用程序，必须将它加入到您的类路径中，并用名字调用应用程序的主类。但是使用可执行的 JAR 文件，我们可以不用提取它或者知道主要入口点就可以运行一个应用程序。可执行 JAR 有助于方便发布和执行 Java 应用程序 一个可执行的 JAR 必须通过 menifest 文件的头引用它所需要的所有其他从属 JAR。如果使用了 -jar选项，那么环境变量 CLASSPATH 和在命令行中指定的所有类路径都被 JVM 所忽略。 MANIFEST.MF 文件当我们用 JAR 命令打完包后，会在根目录下面创建 META-INF 目录，该目录下面会有一些对该 JAR 包信息的描述，其中肯定会有一个 MANIFEST.MF 文件，该文件包含了该 JAR 包的版本、创建人和类搜索路径等信息。 FASTJSON jar 中的 MANIFEST.MF 文件 12345Manifest-Version: 1.0 # 用来定义manifest文件的版本Archiver-Version: Plexus Archiver # 详见 http://codehaus-plexus.github.io/plexus-archiver/Built-By: wenshao # 构建者Created-By: Apache Maven 3.5.0 # # 声明该文件的生成者，一般该属性是由 jar 命令行工具生成的Build-Jdk: 1.8.0_162 # 基于构建的 JDK 版本 SpringBoot demo 的 MANIFEST.MF 文件 12345678910Manifest-Version: 1.0Implementation-Title: demo # 定义了扩展实现的标题Implementation-Version: 0.0.1-SNAPSHOT # 定义扩展实现的版本Start-Class: com.example.demo.DemoApplication # 启动类Spring-Boot-Classes: BOOT-INF/classes/ # 编译之后的 class 文件目录Spring-Boot-Lib: BOOT-INF/lib/ # 当前工程依赖的 jar 包目录Build-Jdk-Spec: 1.8 # 指定的 JDK 版本Spring-Boot-Version: 2.1.6.RELEASE # SpringBoot 版本Created-By: Maven Archiver 3.4.0 Main-Class: org.springframework.boot.loader.JarLauncher # Main 函数 在 Java 平台中， MANIFEST 文件是 JAR 归档中所包含的特殊文件，MANIFEST 文件被用来定义扩展或文件打包相关数据。 MANIFEST 文件作为一个元数据文件，它包含了不同部分中的 k-v 对数据。 如果一个 JAR 文件被当作可执行文件，则其中的 MANIFEST 文件需要指出该程序的主类文件，如上面案例中的 SpringBoot demo 的那个 jar 中的MANIFEST 文件所示 MANIFEST 作用从 MANIFEST 文件中提供的信息大概可以了解到其基本作用 JAR 包基本信息描述 Main-Class 指定程序的入口，这样可以直接用java -jar xxx.jar来运行程序 Class-Path 指定jar包的依赖关系，class loader会依据这个路径来搜索class 获取 MANIFEST.MFJDK 中提供了可以获取 jar 包中 MANIFEST.MF 文件信息的工具，可以通过 java.util.jar 这个类库来获取。 12345678910111213JarFile jar = new JarFile(new File("/Users/glmapper/Documents/test/demo/target/demo-0.0.1-SNAPSHOT.jar"));Manifest manifest = jar.getManifest();Attributes mainAttributes = manifest.getMainAttributes();for(Map.Entry&lt;Object, Object&gt; attrEntry : mainAttributes.entrySet())&#123; System.out.println("main\t"+attrEntry.getKey()+":"+attrEntry.getValue());&#125;Map&lt;String, Attributes&gt; entries = manifest.getEntries();for(Map.Entry&lt;String, Attributes&gt; entry : entries.entrySet()) &#123; Attributes values = entry.getValue(); for (Map.Entry&lt;Object, Object&gt; attrEntry : values.entrySet()) &#123; System.out.println(attrEntry.getKey() + ":" + attrEntry.getValue()); &#125;&#125; 执行结果为：12345678910main Implementation-Title:demomain Implementation-Version:0.0.1-SNAPSHOTmain Start-Class:com.example.demo.DemoApplicationmain Spring-Boot-Classes:BOOT-INF/classes/main Spring-Boot-Lib:BOOT-INF/lib/main Build-Jdk-Spec:1.8main Spring-Boot-Version:2.1.6.RELEASEmain Created-By:Maven Archiver 3.4.0main Manifest-Version:1.0main Main-Class:org.springframework.boot.loader.JarLauncher Jar 文件和 Manifest 在 java 中的定义下面为 JarFile 的定义，从代码就可以看出，前面我们所介绍的 Jar 是以 ZIP 格式构建一种归档文件，因为它是 ZipFile 的子类。 1234567891011121314151617181920public class JarFile extends ZipFile &#123; private SoftReference&lt;Manifest&gt; manRef; private JarEntry manEntry; private JarVerifier jv; private boolean jvInitialized; private boolean verify; //指示是否存在Class-Path属性（仅当hasCheckedSpecialAttributes为true时才有效） private boolean hasClassPathAttribute; // 如果清单检查特殊属性，则为 true private volatile boolean hasCheckedSpecialAttributes; // 在SharedSecrets中设置JavaUtilJarAccess static &#123; SharedSecrets.setJavaUtilJarAccess(new JavaUtilJarAccessImpl()); &#125; /** * The JAR manifest file name.（JAR清单文件名） */ public static final String MANIFEST_NAME = "META-INF/MANIFEST.MF"; // 省略其他&#125; 下面是 Manifest 类的定义，用来描述 JAR 的 清单文件。从其属性中也很好的观察到，其存储的就是 K-V 键值对数据。1234567public class Manifest implements Cloneable &#123; // manifest main attributes private Attributes attr = new Attributes(); // manifest entries private Map&lt;String, Attributes&gt; entries = new HashMap&lt;&gt;(); // 省略其他&#125; 小结JAR 格式远远超出了一种压缩格式，它有许多可以改进效率、安全性和组织 Java 应用程序的功能。因为这些功能已经建立在核心平台 – 包括编译器和类装载器 – 中了，所以开发人员可以利用 JAR 文件格式的能力简化和改进开发和部署过程。 附：常见的 jar工具用法 功能 命令 用一个单独的文件创建一个 JAR 文件 jar cf jar-file input-file… 用一个目录创建一个 JAR 文件 jar cf jar-file dir-name 创建一个未压缩的 JAR 文件 jar cf0 jar-file dir-name 更新一个 JAR 文件 jar uf jar-file input-file… 查看一个 JAR 文件的内容 jar tf jar-file 提取一个 JAR 文件的内容 jar xf jar-file 从一个 JAR 文件中提取特定的文件 jar xf jar-file archived-file… 运行一个打包为可执行 JAR 文件的应用程序 java -jar app.jar 参考 JAR 文件揭密 JAR) JAR File Specification]]></content>
      <categories>
        <category>java 基础</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程-关于 CAS 的几个问题]]></title>
    <url>%2F2019%2F04%2F29%2Fcas-several-qa%2F</url>
    <content type="text"><![CDATA[CAS 相关基础知识CAS的全称是Compare And Swap ,即比较交换。CAS 中一般会设计到3个参数: 内存值 V 旧的预期值A 要修改的新值B 当且仅当预期值 A 和内存值 V 相同时，将内存值V修改为 B，否则什么都不做。 这里关于 CPU 指令对于 CAS 的支持不深入研究,有兴趣的可以自行了解。 CAS 几个问题很多书籍和文章中都有提出它存在的几个问题： 1、循环时间长开销很大 2、只能保证一个共享变量的原子操作 3、ABA 问题 下面就这三个问题展开来聊一下。 1、关于“循环时间长开销很大”的疑惑与验证自旋 CAS 如果长时间不成功，会给 CPU 带来非常大的开销。但是真的是这样吗？到底多大的并发量才造成 CAS 的自旋次数会增加呢？另外，对于当前的机器及JDK，在无锁，无CAS 的情况下，是否对于结果的影响是真的那么明显呢？对于这个问题，下面做了一个简单的测试，但是测试结果也只是针对在我本地环境下，各位看官可以拉一下代码，在自己电脑上 run 一下，把机器信息、JDK版本以及测试结果留言到评论区。 本文案例可以这里获取：glmapper-blog-sample-cas 这里我是用了一个很简单的案例，就是整数自增。使用了两种方式去测试的，一种是无锁，也不用 CAS 操作，另外一种是基于 CAS 的方式。（关于加锁的方式没有验证，有时间再补充吧~） 计数器类计数器里面有两个方法，一种是CAS 自旋方式，一种是直接自增。代码如下：123456789101112131415161718public class Counter &#123; public AtomicInteger safeCount = new AtomicInteger(0); public int unsafe = 0; // 使用自旋的方式 public void safeCount()&#123; for (;;)&#123; int i = safeCount.get(); boolean success = safeCount.compareAndSet(i,++i); if (success)&#123; break; &#125; &#125; &#125; // 普通方式自增 public void unsafeCount()&#123; unsafe++; &#125;&#125; 模拟并发这里我们模拟使用 1000 个线程，执行 30 次来看下结果，包括总耗时和结果的正确性。 CAS 方式 12345678910111213141516171819public static int testSafe() throws InterruptedException &#123; // 记录开始时间 long start = System.currentTimeMillis(); // 实例化一个 Counter 计数器对象 Counter counter = new Counter(); CountDownLatch countDownLatch = new CountDownLatch(testCounts); for (int i =0 ;i &lt; testCounts;i++)&#123; new Thread(()-&gt;&#123; // 调用 safeCount 方法 counter. safeCount(); countDownLatch.countDown(); &#125;).start(); &#125; countDownLatch.await(); // 结束时间 long end = System.currentTimeMillis(); safeTotalCostTime += (end-start); return counter.safeCount.get();&#125; 普通方式 12345678910111213141516171819public static int testUnSafe() throws InterruptedException &#123; // 记录开始时间 long start = System.currentTimeMillis(); // 实例化一个 Counter 计数器对象 Counter counter = new Counter(); CountDownLatch countDownLatch = new CountDownLatch(testCounts); for (int i =0 ;i&lt; testCounts;i++)&#123; new Thread(()-&gt;&#123; // 调用 unsafeCount 方法 counter.unsafeCount(); countDownLatch.countDown(); &#125;).start(); &#125; countDownLatch.await(); // 结束时间 long end = System.currentTimeMillis(); unsafeTotalCostTime += (end-start); return counter.unsafe;&#125; main 方法 12345678910111213141516171819202122public static void main(String[] args) throws InterruptedException &#123; // 执行 300 次 for (int i =0 ;i&lt; 300;i++)&#123; // 普通方式 int unSafeResult = testUnSafe(); // cas 方式 int safeResult = testSafe(); // 结果验证，若果正确就将成功次数增加 if (unSafeResult == testCounts)&#123; totalUnSafeCount++; &#125; // 同上 if (safeResult == testCounts)&#123; totalSafeCount++; &#125; &#125; System.out.println(&quot;test count = &quot; + testCounts); System.out.println(&quot;非安全计数器正确个数 = &quot; + totalUnSafeCount); System.out.println(&quot;非安全计数器耗时 = &quot; + unsafeTotalCostTime); System.out.println(&quot;安全计数器正确个数 = &quot; + totalSafeCount); System.out.println(&quot;安全计数器耗时 = &quot; + safeTotalCostTime);&#125; 我的机器信息如下： MacBook Pro (Retina, 15-inch, Mid 2015) 处理器：2.2 GHz Intel Core i7 内存：16 GB 1600 MHz DDR3 下面是一些测试数据。 1000(线程数) * 300(次数)测试结果如下：12345test count = 1000非安全计数器正确个数 = 300非安全计数器耗时 = 27193安全计数器正确个数 = 300安全计数器耗时 = 26337 居然发现不使用 CAS 的方式居然比使用自旋 CAS 的耗时要高出将近 1s。另外一个意外的点，我尝试了好几次，不使用 CAS 的情况得到的结果正确率基本也是 4 个 9 以上的比率，极少数会出现计算结果错误的情况。 3000(线程数) * 30(次数)测试结果如下：12345test count = 3000非安全计数器正确个数 = 30非安全计数器耗时 = 7816安全计数器正确个数 = 30安全计数器耗时 = 8073 这里看到在耗时上已经很接近了。这里需要考虑另外一个可能影响的点是，因为 testUnSafe 是 testSafe 之前执行的，“JVM 和 机器本身热身” 影响耗时虽然很小，但是也存在一定的影响。 5000(线程数) * 30(次数)测试结果如下：12345test count = 5000非安全计数器正确个数 = 30非安全计数器耗时 = 23213安全计数器正确个数 = 30安全计数器耗时 = 14161 随着并发量的增加，这里奇怪的是，普通自增方式所消耗的时间要高于CAS方式消耗的时间将近 8-9s 。 当尝试 10000 次时，是的你没猜错，抛出了 OOM 。但是从执行的结果来看，并没有说随着并发量的增大，普通方式错误的概率会增加，也没有出现预想的 CAS 方式的耗时要比 普通模式耗时多。 由于测试样本数据比较单一，对于测试结果没法做结论，欢迎大家将各自机器的结果提供出来，以供参考。另外就是，最近看到很多面试的同学，如果有被问道这个问题，还是需要谨慎考虑下。关于是否“打脸”还是“被打脸”还需要更多的测试结果。 CAS 到底是怎么操作的 CPU 指令 Unsafe 类 2、ABA 问题的简单复现网上关于 CAS 讨论另外一个点就是 CAS 中的 ABA 问题，相信大多数同学在面试时如果被问到 CAS ，那么 ABA 问题也会被问到，然后接着就是怎么避免这个问题，是的套路就是这么一环扣一环的。 我相信 90% 以上的开发人员在实际的工程中是没有遇到过这个问题的，即使遇到过，在特定的情况下也是不会影响到计算结果。但是既然这个问题会被反复提到，那就一定有它导致 bug 的场景，找了一个案例供大家参考：CAS下ABA问题及优化方案 。 这里先不去考虑怎么去规避这个问题，我们想怎么去通过简单的模拟先来复现这个 ABA 问题。其实这个也很简单，如果你对线程交叉、顺序执行了解的话。 如何实现多线程的交叉执行这个点实际上也是一个在面试过程中很常见的一个基础问题，我在提供的代码中给了三种实现方式，有兴趣的同学可以拉代码看下。 下面以 lock 的方式来模拟下这个场景，代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class ConditionAlternateTest&#123; private static int count = 0; // 计数器 public AtomicInteger safeCount = new AtomicInteger(0); // lock private Lock lock = new ReentrantLock(); // condition 1/2/3 用于三个线程触发执行的条件 Condition c1 = lock.newCondition(); Condition c2 = lock.newCondition(); Condition c3 = lock.newCondition(); // 模拟并发执行 CountDownLatch countDownLatch = new CountDownLatch(1); // 线程1 ，A Thread t1 = new Thread(()-&gt; &#123; try &#123; lock.lock(); while (count % 3 != 0)&#123; c1.await(); &#125; safeCount.compareAndSet(0, 1); System.out.println("thread1:"+safeCount.get()); count++; // 唤醒条件2 c2.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;); // 线程2 ，B Thread t2 = new Thread(()-&gt; &#123; try &#123; lock.lock(); while (count % 3 != 1)&#123; c2.await(); &#125; safeCount.compareAndSet(1, 0); System.out.println("thread2:"+safeCount.get()); count++; // 唤醒条件3 c3.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;); // 线程2 ，A Thread t3 = new Thread(()-&gt; &#123; try &#123; lock.lock(); while (count % 3 != 2)&#123; c3.await(); &#125; safeCount.compareAndSet(0, 1); System.out.println("thread3:"+safeCount.get()); count++; // 唤醒条件1 c1.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;); // 启动启动线程 public void threadStart() &#123; t3.start(); t1.start(); t2.start(); countDownLatch.countDown(); &#125; public static void main(String[] args) throws InterruptedException &#123; ConditionAlternateTest test = new ConditionAlternateTest(); test.threadStart(); test.countDownLatch.await(); &#125;&#125; 执行结果： 123thread1:1thread2:0thread3:1 上面线程交叉的案例实际上并不是严格意义上的 ABA 问题的复现，这里仅是模拟下产生的一个最简单的过程。如果大家有好的案例，也可以分享一下。 ABA 问题解决常见实践：“版本号”的比对，一个数据一个版本，版本变化，即使值相同，也不应该修改成功。 java 中提供了 AtomicStampedReference 这个类来解决这个 ABA 问题。AtomicStampedReference 原子类是一个带有时间戳的对象引用，在每次修改后，AtomicStampedReference 不仅会设置新值而且还会记录更改的时间。当 AtomicStampedReference 设置对象值时，对象值以及时间戳都必须满足期望值才能写入成功，这也就解决了反复读写时，无法预知值是否已被修改的窘境。 实现代码这里就不贴了，基于前面的代码改造，下面贴一下运行结果：123thread1,第一次修改;值为=1thread2,已经改回为原始值;值为=0thread3,第二次修改;值为=1 3、只能保证一个共享变量的原子操作当对一个共享变量执行操作时，我们可以使用 CAS 的方式来保证原子操作，但是对于对多个变量操作时，循环 CAS 就无法保证操作的原子性了，那么这种场景下，我们就需要使用加锁的方式来解决。]]></content>
      <categories>
        <category>java 基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>cas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 源码系列-事件机制详解]]></title>
    <url>%2F2019%2F04%2F13%2Fspringboot%2Fspringboot-series-event%2F</url>
    <content type="text"><![CDATA[微信公众号：glmapper工作室掘金专栏：glmapper微 博：疯狂的石头_henu欢迎关注，一起学习、一起分享 在这篇文章中聊一聊 Spring 中的扩展机制（一）中对Spring中的事件机制进行了分析。那么对于 SpringBoot 来说，它在 Spring 的基础上又做了哪些拓展呢？本篇将来聊一聊 SpringBoot 中的事件。 在 SpringBoot 的启动过程中，会通过 SPI 机制去加载 spring.factories 下面的一些类，这里面就包括了事件相关的类。 SpringApplicationRunListener 123# Run Listenersorg.springframework.boot.SpringApplicationRunListener=\org.springframework.boot.context.event.EventPublishingRunListener ApplicationListener 1234567891011# Application Listenersorg.springframework.context.ApplicationListener=\org.springframework.boot.ClearCachesApplicationListener,\org.springframework.boot.builder.ParentContextCloserApplicationListener,\org.springframework.boot.context.FileEncodingApplicationListener,\org.springframework.boot.context.config.AnsiOutputApplicationListener,\org.springframework.boot.context.config.ConfigFileApplicationListener,\org.springframework.boot.context.config.DelegatingApplicationListener,\org.springframework.boot.context.logging.ClasspathLoggingApplicationListener,\org.springframework.boot.context.logging.LoggingApplicationListener,\org.springframework.boot.liquibase.LiquibaseServiceLocatorApplicationListener SpringApplicationRunListener 类是 SpringBoot 中新增的类。SpringApplication 类 中使用它们来间接调用 ApplicationListener。另外还有一个新增的类是SpringApplicationRunListeners，SpringApplicationRunListeners 中包含了多个 SpringApplicationRunListener。 SpringApplicationRunListenerSpringApplicationRunListener 接口规定了 SpringBoot 的生命周期，在各个生命周期广播相应的事件，调用实际的 ApplicationListener 类。通过对 SpringApplicationRunListener 的分析，也可以对 SpringBoot 的整个启动过程的理解会有很大帮助。 先来看下SpringApplicationRunListener 接口的代码： 123456789101112131415161718public interface SpringApplicationRunListener &#123; //当run方法首次启动时立即调用。可用于非常早期的初始化。 void starting(); //在准备好环境后，但在创建ApplicationContext之前调用。 void environmentPrepared(ConfigurableEnvironment environment); //在创建和准备好ApplicationContext之后，但在加载源之前调用。 void contextPrepared(ConfigurableApplicationContext context); //在加载应用程序上下文后但刷新之前调用 void contextLoaded(ConfigurableApplicationContext context); //上下文已刷新，应用程序已启动，但尚未调用commandlinerunner和applicationrunner。 void started(ConfigurableApplicationContext context); //在运行方法完成之前立即调用，此时应用程序上下文已刷新， //并且所有commandlinerunner和applicationrunner都已调用。 //2.0 才有 void running(ConfigurableApplicationContext context); //在运行应用程序时发生故障时调用。2.0 才有 void failed(ConfigurableApplicationContext context, Throwable exception);&#125; SpringApplicationRunListeners上面提到，SpringApplicationRunListeners 是SpringApplicationRunListener的集合，里面包括了很多SpringApplicationRunListener实例；SpringApplication 类实际上使用的是 SpringApplicationRunListeners 类，与 SpringApplicationRunListener 生命周期相同，调用各个周期的 SpringApplicationRunListener 。然后广播相应的事件到 ApplicationListener。 代码详见：SpringApplicationRunListeners. EventPublishingRunListenerEventPublishingRunListener 类是 SpringApplicationRunListener接口的实现类 ，它具有广播事件的功能。其内部使用 ApplicationEventMulticaster在实际刷新上下文之前发布事件。下面来看下 EventPublishingRunListener 类生命周期对应的事件。 ApplicationStartingEventApplicationStartingEvent 是 SpringBoot 启动开始的时候执行的事件，在该事件中可以获取到 SpringApplication 对象，可做一些执行前的设置，对应的调用方法是 starting()。 ApplicationEnvironmentPreparedEventApplicationEnvironmentPreparedEvent 是SpringBoot 对应 Enviroment 已经准备完毕时执行的事件，此时上下文 context 还没有创建。在该监听中获取到 ConfigurableEnvironment 后可以对配置信息做操作，例如：修改默认的配置信息，增加额外的配置信息等。对应的生命周期方法是 environmentPrepared(environment)；SpringCloud 中，引导上下文就是在这时初始化的。 ApplicationContextInitializedEvent当 SpringApplication 启动并且准备好 ApplicationContext，并且在加载任何 bean 定义之前调用了 ApplicationContextInitializers 时发布的事件。对应的生命周期方法是contextPrepared() ApplicationPreparedEventApplicationPreparedEvent 是SpringBoot上下文 context 创建完成是发布的事件；但此时 spring 中的 bean 还没有完全加载完成。这里可以将上下文传递出去做一些额外的操作。但是在该监听器中是无法获取自定义 bean 并进行操作的。对应的生命周期方法是 contextLoaded()。 ApplicationStartedEvent这个事件是在 2.0 版本才引入的；具体发布是在应用程序上下文刷新之后，调用任何 ApplicationRunner 和 CommandLineRunner 运行程序之前。 ApplicationReadyEvent这个和 ApplicationStartedEvent 很类似，也是在应用程序上下文刷新之后之后调用，区别在于此时ApplicationRunner 和 CommandLineRunner已经完成调用了，也意味着 SpringBoot 加载已经完成。 ApplicationFailedEventSpringBoot 启动异常时执行的事件，在异常发生时，最好是添加虚拟机对应的钩子进行资源的回收与释放，能友善的处理异常信息。 demo 及各个事件的执行顺序下面的各个事件对应的demo及打印出来的执行顺序。 GlmapperApplicationStartingEventListener 123456public class GlmapperApplicationStartingEventListener implements ApplicationListener&lt;ApplicationStartingEvent&gt; &#123; @Override public void onApplicationEvent(ApplicationStartingEvent applicationStartingEvent) &#123; System.out.println("execute ApplicationStartingEvent ..."); &#125;&#125; GlmapperApplicationEnvironmentPreparedEvent 123456public class GlmapperApplicationEnvironmentPreparedEvent implements ApplicationListener&lt;ApplicationEnvironmentPreparedEvent&gt; &#123; @Override public void onApplicationEvent(ApplicationEnvironmentPreparedEvent applicationEnvironmentPreparedEvent) &#123; System.out.println("execute ApplicationEnvironmentPreparedEvent ..."); &#125;&#125; GlmapperApplicationContextInitializedEvent 123456public class GlmapperApplicationContextInitializedEvent implements ApplicationListener&lt;ApplicationContextInitializedEvent&gt; &#123; @Override public void onApplicationEvent(ApplicationContextInitializedEvent applicationContextInitializedEvent) &#123; System.out.println("execute applicationContextInitializedEvent ..."); &#125;&#125; GlmapperApplicationPreparedEvent 123456public class GlmapperApplicationPreparedEvent implements ApplicationListener&lt;ApplicationPreparedEvent&gt; &#123; @Override public void onApplicationEvent(ApplicationPreparedEvent applicationPreparedEvent) &#123; System.out.println(&quot;execute ApplicationPreparedEvent ...&quot;); &#125;&#125; GlmapperApplicationStartedEvent 123456public class GlmapperApplicationStartedEvent implements ApplicationListener&lt;ApplicationStartedEvent&gt; &#123; @Override public void onApplicationEvent(ApplicationStartedEvent applicationStartedEvent) &#123; System.out.println("execute ApplicationStartedEvent ..."); &#125;&#125; GlmapperApplicationReadyEvent 123456public class GlmapperApplicationReadyEvent implements ApplicationListener&lt;ApplicationReadyEvent&gt; &#123; @Override public void onApplicationEvent(ApplicationReadyEvent applicationReadyEvent) &#123; System.out.println("execute ApplicationReadyEvent ..."); &#125;&#125; 执行结果 SpringBoot 中的事件体系这里围绕 SpringApplicationRunListener 这个类来说。在实现类 EventPublishingRunListener 中，事件发布有两种模式： 通过 SimpleApplicationEventMulticaster 进行事件广播 所有监听器交给相应的 Context 所以EventPublishingRunListener 不仅负责发布事件，而且在合适的时机将 SpringApplication 所获取的监听器和应用上下文作关联。 SimpleApplicationEventMulticasterSimpleApplicationEventMulticaster是 Spring 默认的事件广播器。来看下它是怎么工作的： 1234567891011121314@Overridepublic void multicastEvent(final ApplicationEvent event, @Nullable ResolvableType eventType) &#123; ResolvableType type = (eventType != null ? eventType : resolveDefaultEventType(event)); for (final ApplicationListener&lt;?&gt; listener : getApplicationListeners(event, type)) &#123; Executor executor = getTaskExecutor(); if (executor != null) &#123; // 异步的 executor.execute(() -&gt; invokeListener(listener, event)); &#125; else &#123; invokeListener(listener, event); &#125; &#125;&#125; 从上面的代码段可以看出，它是通过遍历注册的每个监听器，并启动来调用每个监听器的 onApplicationEvent 方法。 下面再来看下 SimpleApplicationEventMulticaster 的类集成结构：这里的 AbstractApplicationContext 下面来聊，这个类实际上就负责了事件体系的初始化工作。 事件体系的初始化事件体系的初始化对应在 SpringBoot启动过程的 refreshContext这个方法；refreshContext具体调用 AbstractApplicationContext.refresh()方法，最后调用 initApplicationEventMulticaster() 来完成事件体系的初始化,代码如下： 用户可以为容器定义一个自定义的事件广播器，只要实现 ApplicationEventMulticaster 就可以了，Spring 会通过 反射的机制将其注册成容器的事件广播器，如果没有找到配置的外部事件广播器，Spring 就是默认使用 SimpleApplicationEventMulticaster 作为事件广播器。 事件注册事件注册是在事件体系初始化完成之后做的事情，也是在 AbstractApplicationContext.refresh() 方法中进行调用的。 这里干了三件事： 首先注册静态指定的 listeners；这里包括我们自定义的那些监听器。 调用 DefaultListableBeanFactory 中 getBeanNamesForType 得到自定义的 ApplicationListener bean 进行事件注册。 广播早期的事件。 事件广播事件发布伴随着 SpringBoot 启动的整个生命周期。不同阶段对应发布不同的事件，上面我们已经对各个事件进行了分析，下面就具体看下发布事件的实现： org.springframework.context.support.AbstractApplicationContext#publishEvent earlyApplicationEvents 中的事件是广播器未建立的时候保存通知信息，一旦容器建立完成，以后都是直接通知。 广播事件最终还是通过调用 ApplicationEventMulticaster 的 multicastEvent 来实现。而 multicastEvent 也就就是事件执行的方法。 事件执行上面 SimpleApplicationEventMulticaster 小节已经初步介绍了 multicastEvent 这个方法。补充一点， 如果有可用的 taskExecutor 会使用并发的模式执行事件，但是实际上 SimpleApplicationEventMulticaster 并没有提供线程池实现，默认请况下是使用同步的方式执行事件（org.springframework.core.task.SyncTaskExecutor），所以如果需要异步配置的话，需要自己去实现线程池。 SpringBoot 启动过程中的事件阶段这里回到 SpringApplication的run方法，看下 SpringBoot 在启动过程中，各个事件阶段做了哪些事情。 starting -&gt; ApplicationStartingEvent这里 debug 到 starting 方法，追踪到 multicastEvent，这里 type为 ApplicationStartingEvent；对应的事件如下： LoggerApplicationListener：配置日志系统。使用logging.config环境变量指定的配置或者缺省配置 BackgroundPreinitializer：尽早触发一些耗时的初始化任务，使用一个后台线程 DelegatingApplicationListener：监听到事件后转发给环境变量context.listener.classes指定的那些事件监听器 LiquibaseServiceLocatorApplicationListener：使用一个可以和 SpringBoot 可执行jar包配合工作的版本替换 liquibase ServiceLocator listeners.environmentPrepared-&gt;ApplicationEnvironmentPreparedEvent AnsiOutputApplicationListener：根据spring.output.ansi.enabled参数配置AnsiOutput ConfigFileApplicationListener：EnvironmentPostProcessor，从常见的那些约定的位置读取配置文件，比如从以下目录读取application.properties,application.yml等配置文件： classpath: file:. classpath:config file:./config/ 也可以配置成从其他指定的位置读取配置文件。 ClasspathLoggingApplicationListener：对环境就绪事件ApplicationEnvironmentPreparedEvent/应用失败事件ApplicationFailedEvent做出响应，往日志DEBUG级别输出TCCL(thread context class loader)的 classpath。 FileEncodingApplicationListener：如果系统文件编码和环境变量中指定的不同则终止应用启动。具体的方法是比较系统属性file.encoding和环境变量spring.mandatory-file-encoding是否相等(大小写不敏感)。 listeners.contextPrepared-&gt;ApplicationContextInitializedEvent 相关监听器参考上面的描述。 listeners.contextLoaded-&gt;ApplicationPreparedEvent 相关监听器参考上面的描述。 refresh-&gt;ContextRefreshedEvent ConditionEvaluationReportLoggingListener：实际上实现的是 ApplicationContextInitializer接口，其目的是将 ConditionEvaluationReport 写入到日志，使用DEBUG级别输出。程序崩溃报告会触发一个消息输出，建议用户使用调试模式显示报告。它是在应用初始化时绑定一个ConditionEvaluationReportListener事件监听器，然后相应的事件发生时输出ConditionEvaluationReport报告。 ClearCachesApplicationListener：应用上下文加载完成后对缓存做清除工作，响应事件ContextRefreshedEvent。 SharedMetadataReaderFactoryContextInitializer： 向context注册了一个BeanFactoryPostProcessor：CachingMetadataReaderFactoryPostProcessor实例。 ResourceUrlProvider：handling mappings处理 started-&gt;ApplicationStartedEvent相关监听器参考上面的描述。 running-&gt;ApplicationReadyEvent相关监听器参考上面的描述。 BackgroundPreinitializer&amp;DelegatingApplicationListener这两个贯穿了整个过程，这里拎出来单独解释下： BackgroundPreinitializer：对于一些耗时的任务使用一个后台线程尽早触发它们开始执行初始化，这是SpringBoot的缺省行为。这些初始化动作也可以叫做预初始化。可以通过设置系统属性spring.backgroundpreinitializer.ignore为true可以禁用该机制。该机制被禁用时，相应的初始化任务会发生在前台线程。 DelegatingApplicationListener：监听应用事件，并将这些应用事件广播给环境属性context.listener.classes指定的那些监听器。 小结到此，SpringBoot 中的事件相关的东西就结束了。本文从SpringApplicationRunListener这个类说起，接着介绍 SpringBoot 启动过程的事件以及事件的生命周期。最后介绍了 SpringBoot中的内置的这些 监听器在启动过程中对应的各个阶段。 新年伊始，祝大家新年快乐！ 参考 https://blog.csdn.net/andy_zhang2007/article/details/84105284]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper 客户端之 Curator]]></title>
    <url>%2F2019%2F04%2F13%2Fzookeeper-client-curator%2F</url>
    <content type="text"><![CDATA[原文链接：ZooKeeper 客户端之 Curator ZooKeeper 是一个分布式的、开放源码的分布式应用程序协调服务，是 Google 的 Chubby 一个开源的实现。它是集群的管理者，监视着集群中各个节点的状态，根据节点提交的反馈进行下一步合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 Curator 是 Netflix 公司开源的一套 Zookeeper 客户端框架，解决了很多 Zookeeper 客户端非常底层的细节开发工作，包括连接重连、反复注册 Watcher 和 NodeExistsException 异常等等。Curator 包含了几个包： curator-framework：对 Zookeeper 的底层 api 的一些封装 curator-client：提供一些客户端的操作，例如重试策略等 curator-recipes：封装了一些高级特性，如：Cache 事件监听、选举、分布式锁、分布式计数器、分布式Barrier 等 Curator 和 zookeeper 的版本问题目前 Curator 有 2.x.x 和 3.x.x 两个系列的版本，支持不同版本的 Zookeeper。其中Curator 2.x.x 兼容 Zookeeper的 3.4.x 和 3.5.x。而 Curator 3.x.x 只兼容 Zookeeper 3.5.x，并且提供了一些诸如动态重新配置、watch删除等新特性。 12Curator 2.x.x - compatible with both ZooKeeper 3.4.x and ZooKeeper 3.5.xCurator 3.x.x - compatible only with ZooKeeper 3.5.x and includes support for new 如果跨版本会有兼容性问题，很有可能导致节点操作失败，当时在使用的时候就踩了这个坑，抛了如下的异常： 1KeeperErrorCode = Unimplemented for /*** Curator API这里就不对比与原生 API 的区别了，Curator 的 API 直接通过 org.apache.curator.framework.CuratorFramework 接口来看，并结合相应的案例进行使用，以备后用。 为了可以直观的看到 Zookeeper 的节点信息，可以考虑弄一个 zk 的管控界面，常见的有 zkui 和 zkweb。 zkui：https://github.com/DeemOpen/zkui zkweb：https://github.com/zhitom/zkweb 我用的 zkweb ，虽然界面上看起来没有 zkui 精简，但是在层次展示和一些细节上感觉比 zkui 好一点 环境准备之前写的一个在 Linux 上安装部署 Zookeeper 的笔记，其他操作系统请自行谷歌教程吧。 本文案例工程已经同步到了 github，传送门。 PS : 目前还没有看过Curator的具体源码，所以不会涉及到任何源码解析、实现原理的东西；本篇主要是实际使用时的一些记录，以备后用。如果文中错误之处，希望各位指出。 Curator 客户端的初始化和初始化时机在实际的工程中，Zookeeper 客户端的初始化会在程序启动期间完成。 初始化时机在 Spring 或者 SpringBoot 工程中最常见的就是绑定到容器启动的生命周期或者应用启动的生命周期中： 监听 ContextRefreshedEvent 事件，在容器刷新完成之后初始化 Zookeeper 监听 ApplicationReadyEvent/ApplicationStartedEvent 事件，初始化 Zookeeper 客户端 除了上面的方式之外，还有一种常见的是绑定到 bean 的生命周期中 实现 InitializingBean 接口 ，在 afterPropertiesSet 中完成 Zookeeper 客户端初始化 关于 SpringBoot中的事件机制可以参考之前写过的一篇文章：SpringBoot-SpringBoot中的事件机制。 Curator 初始化这里使用 InitializingBean 的这种方式，代码如下： 12345678910111213141516171819202122232425262728public class ZookeeperCuratorClient implements InitializingBean &#123; private CuratorFramework curatorClient; @Value("$&#123;glmapper.zookeeper.address:localhost:2181&#125;") private String connectString; @Value("$&#123;glmapper.zookeeper.baseSleepTimeMs:1000&#125;") private int baseSleepTimeMs; @Value("$&#123;glmapper.zookeeper.maxRetries:3&#125;") private int maxRetries; @Value("$&#123;glmapper.zookeeper.sessionTimeoutMs:6000&#125;") private int sessionTimeoutMs; @Value("$&#123;glmapper.zookeeper.connectionTimeoutMs:6000&#125;") private int connectionTimeoutMs; @Override public void afterPropertiesSet() throws Exception &#123; // custom policy RetryPolicy retryPolicy = new ExponentialBackoffRetry(baseSleepTimeMs, maxRetries); // to build curatorClient curatorClient = CuratorFrameworkFactory.builder().connectString(connectString) .sessionTimeoutMs(sessionTimeoutMs).connectionTimeoutMs(connectionTimeoutMs) .retryPolicy(retryPolicy).build(); curatorClient.start(); &#125; public CuratorFramework getCuratorClient() &#123; return curatorClient; &#125;&#125; glmapper.zookeeper.xxx 是本例中需要在配置文件中配置的 zookeeper 的一些参数，参数解释如下： baseSleepTimeMs：重试之间等待的初始时间 maxRetries：最大重试次数 connectString：要连接的服务器列表 sessionTimeoutMs：session 超时时间 connectionTimeoutMs：连接超时时间 另外，Curator 客户端初始化时还需要指定重试策略，RetryPolicy 接口是 Curator 中重试连接(当zookeeper失去连接时使用)策略的顶级接口，其类继承体系如下图所示： RetryOneTime：只重连一次 RetryNTime：指定重连的次数N RetryUtilElapsed：指定最大重连超时时间和重连时间间隔，间歇性重连直到超时或者链接成功 ExponentialBackoffRetry：基于 “backoff”方式重连，和 RetryUtilElapsed 的区别是重连的时间间隔是动态的。 BoundedExponentialBackoffRetry： 同 ExponentialBackoffRetry的区别是增加了最大重试次数的控制 除上述之外，在一些场景中，需要对不同的业务进行隔离，这种情况下，可以通过设置 namespace 来解决，namespace 实际上就是指定zookeeper的根路径，设置之后，后面的所有操作都会基于该根目录。 Curator 基础 API 使用检查节点是否存在checkExists 方法返回的是一个 ExistsBuilder 构造器，这个构建器将返回一个 Stat 对象，就像调用了 org.apache.zookeeper.ZooKeeper.exists()一样。null 表示它不存在，而实际的 Stat 对象表示存在。 123456public void checkNodeExist(String path) throws Exception &#123; Stat stat = curatorClient.checkExists().forPath(path); if (stat != null)&#123; throw new RuntimeException("path = "+path +" has bean exist."); &#125;&#125; 建议在实际的应用中，操作节点时对所需操作的节点进行 checkExists。 新增节点 非递归方式创建节点 12curatorClient.create().forPath(&quot;/glmapper&quot;);curatorClient.create().forPath(&quot;/glmapper/test&quot;); 先创建/glmapper，然后再在/glmapper 下面创建 /test ，如果直接使用 /glmapper/test 没有先创建 /glmapper 时，会抛出异常： 1org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /glmapper/test 如果需要在创建节点时指定节点中数据，则可以这样： 1curatorClient.create().forPath(&quot;/glmapper&quot;,&quot;data&quot;.getBytes()); 指定节点类型(EPHEMERAL 临时节点) 1curatorClient.create().withMode(CreateMode.EPHEMERAL).forPath(&quot;/glmapper&quot;,&quot;data&quot;.getBytes()); 递归方式创建节点 递归方式创建节点有两个方法，creatingParentsIfNeeded 和 creatingParentContainersIfNeeded。在新版本的 zookeeper 这两个递归创建方法会有区别； creatingParentContainersIfNeeded() 以容器模式递归创建节点，如果旧版本 zookeeper，此方法等于creatingParentsIfNeeded()。 在非递归方式情况下，如果直接创建 /glmapper/test 会报错，那么在递归的方式下则是可以的 1curatorClient.create().creatingParentContainersIfNeeded().forPath(&quot;/glmapper/test&quot;); 在递归调用中，如果不指定 CreateMode，则默认PERSISTENT，如果指定为临时节点，则最终节点会是临时节点，父节点仍旧是PERSISTENT 删除节点 非递归删除节点 1curatorClient.delete().forPath(&quot;/glmapper/test&quot;); 指定具体版本 1curatorClient.delete().withVersion(-1).forPath(&quot;/glmapper/test&quot;); 使用 guaranteed 方式删除，guaranteed 会保证在session有效的情况下，后台持续进行该节点的删除操作，直到删除掉 1curatorClient.delete().guaranteed().withVersion(-1).forPath(&quot;/glmapper/test&quot;); 递归删除当前节点及其子节点 1curatorClient.delete().deletingChildrenIfNeeded().forPath(&quot;/glmapper/test&quot;); 获取节点数据获取节点数据 1byte[] data = curatorClient.getData().forPath(&quot;/glmapper/test&quot;); 根据配置的压缩提供程序对数据进行解压缩处理 1byte[] data = curatorClient.getData().decompressed().forPath(&quot;/glmapper/test&quot;); 读取数据并获得Stat信息 12Stat stat = new Stat();byte[] data = curatorClient.getData().storingStatIn(stat).forPath(&quot;/glmapper/test&quot;); 更新节点数据设置指定值 1curatorClient.setData().forPath(&quot;/glmapper/test&quot;,&quot;newData&quot;.getBytes()); 设置数据并使用配置的压缩提供程序压缩数据 1curatorClient.setData().compressed().forPath(&quot;/glmapper/test&quot;,&quot;newData&quot;.getBytes()); 设置数据，并指定版本 1curatorClient.setData().withVersion(-1).forPath(&quot;/glmapper/test&quot;,&quot;newData&quot;.getBytes()); 获取子列表1List&lt;String&gt; childrenList = curatorClient.getChildren().forPath(&quot;/glmapper&quot;); 事件Curator 也对 Zookeeper 典型场景之事件监听进行封装，这部分能力实在 curator-recipes 包下的。 事件类型在使用不同的方法时会有不同的事件发生 12345678910111213141516171819202122232425public enum CuratorEventType&#123; //Corresponds to &#123;@link CuratorFramework#create()&#125; CREATE, //Corresponds to &#123;@link CuratorFramework#delete()&#125; DELETE, //Corresponds to &#123;@link CuratorFramework#checkExists()&#125; EXISTS, //Corresponds to &#123;@link CuratorFramework#getData()&#125; GET_DATA, //Corresponds to &#123;@link CuratorFramework#setData()&#125; SET_DATA, //Corresponds to &#123;@link CuratorFramework#getChildren()&#125; CHILDREN, //Corresponds to &#123;@link CuratorFramework#sync(String, Object)&#125; SYNC, //Corresponds to &#123;@link CuratorFramework#getACL()&#125; GET_ACL, //Corresponds to &#123;@link CuratorFramework#setACL()&#125; SET_ACL, //Corresponds to &#123;@link Watchable#usingWatcher(Watcher)&#125; or &#123;@link Watchable#watched()&#125; WATCHED, //Event sent when client is being closed CLOSING&#125; 事件监听一次性监听方式：Watcher利用 Watcher 来对节点进行监听操作，可以典型业务场景需要使用可考虑，但一般情况不推荐使用。 1234567891011 byte[] data = curatorClient.getData().usingWatcher(new Watcher() &#123; @Override public void process(WatchedEvent watchedEvent) &#123; System.out.println("监听器 watchedEvent：" + watchedEvent); &#125; &#125;).forPath("/glmapper/test");System.out.println("监听节点内容：" + new String(data));// 第一次变更节点数据curatorClient.setData().forPath("/glmapper/test","newData".getBytes());// 第二次变更节点数据curatorClient.setData().forPath("/glmapper/test","newChangedData".getBytes()); 上面这段代码对 /glmapper/test 节点注册了一个 Watcher 监听事件，并且返回当前节点的内容。后面进行两次数据变更，实际上第二次变更时，监听已经失效，无法再次获得节点变动事件了。测试中控制台输出的信息如下： 12监听节点内容：datawatchedEvent：WatchedEvent state:SyncConnected type:NodeDataChanged path:/glmapper/test CuratorListener 方式CuratorListener 监听，此监听主要针对 background 通知和错误通知。使用此监听器之后，调用inBackground 方法会异步获得监听，对于节点的创建或修改则不会触发监听事件。 123456789101112CuratorListener listener = new CuratorListener()&#123; @Override public void eventReceived(CuratorFramework client, CuratorEvent event) throws Exception &#123; System.out.println("event : " + event); &#125; &#125;;// 绑定监听器curatorClient.getCuratorListenable().addListener(listener);// 异步获取节点数据curatorClient.getData().inBackground().forPath("/glmapper/test");// 更新节点数据curatorClient.setData().forPath("/glmapper/test","newData".getBytes()); 测试中控制台输出的信息如下： 12event : CuratorEventImpl&#123;type=GET_DATA, resultCode=0, path=&apos;/glmapper/test&apos;, name=&apos;null&apos;, children=null, context=null, stat=5867,5867,1555140974671,1555140974671,0,0,0,0,4,0,5867, data=[100, 97, 116, 97], watchedEvent=null, aclList=null&#125; 这里只触发了一次监听回调，就是 getData 。 Curator 引入的 Cache 事件监听机制Curator 引入了 Cache 来实现对 Zookeeper 服务端事件监听，Cache 事件监听可以理解为一个本地缓存视图与远程 Zookeeper 视图的对比过程。Cache 提供了反复注册的功能。Cache 分为两类注册类型：节点监听和子节点监听。 NodeCache 监听数据节点本身的变化。对节点的监听需要配合回调函数来进行处理接收到监听事件之后的业务处理。NodeCache 通过 NodeCacheListener 来完成后续处理。 12345678910111213141516String path = "/glmapper/test";final NodeCache nodeCache = new NodeCache(curatorClient,path);//如果设置为true则在首次启动时就会缓存节点内容到Cache中。 nodeCache.start(true);nodeCache.start();nodeCache.getListenable().addListener(new NodeCacheListener() &#123;@Overridepublic void nodeChanged() throws Exception &#123;System.out.println("触发监听回调，当前节点数据为：" + new String(nodeCache.getCurrentData().getData()));&#125;&#125;);curatorClient.setData().forPath(path,"1".getBytes());curatorClient.setData().forPath(path,"2".getBytes());curatorClient.setData().forPath(path,"3".getBytes());curatorClient.setData().forPath(path,"4".getBytes());curatorClient.setData().forPath(path,"5".getBytes());curatorClient.setData().forPath(path,"6".getBytes()); 注意：在测试过程中，nodeCache.start()，NodeCache 在先后多次修改监听节点的内容时，出现了丢失事件现象，在用例执行的5次中，仅一次监听到了全部事件；如果 nodeCache.start(true)，NodeCache 在先后多次修改监听节点的内容时，不会出现丢失现象。 NodeCache不仅可以监听节点内容变化，还可以监听指定节点是否存在。如果原本节点不存在，那么Cache就会在节点被创建时触发监听事件，如果该节点被删除，就无法再触发监听事件。 PathChildrenCache PathChildrenCache 不会对二级子节点进行监听，只会对子节点进行监听。 123456789101112131415161718192021222324252627String path = "/glmapper";PathChildrenCache pathChildrenCache = new PathChildrenCache(curatorClient,path,true);// 如果设置为true则在首次启动时就会缓存节点内容到Cache中。 nodeCache.start(true);pathChildrenCache.start(PathChildrenCache.StartMode.POST_INITIALIZED_EVENT);pathChildrenCache.getListenable().addListener(new PathChildrenCacheListener() &#123; @Override public void childEvent(CuratorFramework curatorFramework, PathChildrenCacheEvent event) throws Exception &#123; System.out.println("-----------------------------"); System.out.println("event:" + event.getType()); if (event.getData()!=null)&#123; System.out.println("path:" + event.getData().getPath()); &#125; System.out.println("-----------------------------"); &#125;&#125;);zookeeperCuratorClient.createNode("/glmapper/test","data".getBytes(),CreateMode.PERSISTENT);Thread.sleep(1000);curatorClient.setData().forPath("/glmapper/test","1".getBytes());Thread.sleep(1000);curatorClient.setData().forPath("/glmapper/test","2".getBytes());Thread.sleep(1000);zookeeperCuratorClient.createNode("/glmapper/test/second","data".getBytes(),CreateMode.PERSISTENT);Thread.sleep(1000);curatorClient.setData().forPath("/glmapper/test/second","1".getBytes());Thread.sleep(1000);curatorClient.setData().forPath("/glmapper/test/second","2".getBytes());Thread.sleep(1000); 注意：在测试过程中发现，如果连续两个操作之间不进行一定时间的间隔，会导致无法监听到下一次事件。因此只会监听子节点，所以对二级子节点 /second 下面的操作是监听不到的。测试中控制台输出的信息如下： 123456789101112131415-----------------------------event:CHILD_ADDEDpath:/glmapper/test----------------------------------------------------------event:INITIALIZED----------------------------------------------------------event:CHILD_UPDATEDpath:/glmapper/test----------------------------------------------------------event:CHILD_UPDATEDpath:/glmapper/test----------------------------- TreeCache TreeCache 使用一个内部类TreeNode来维护这个一个树结构。并将这个树结构与ZK节点进行了映射。所以TreeCache 可以监听当前节点下所有节点的事件。 1234567891011121314151617181920212223String path = "/glmapper";TreeCache treeCache = new TreeCache(curatorClient,path);treeCache.getListenable().addListener((client,event)-&gt; &#123; System.out.println("-----------------------------"); System.out.println("event:" + event.getType()); if (event.getData()!=null)&#123; System.out.println("path:" + event.getData().getPath()); &#125; System.out.println("-----------------------------");&#125;);treeCache.start();zookeeperCuratorClient.createNode("/glmapper/test","data".getBytes(),CreateMode.PERSISTENT);Thread.sleep(1000);curatorClient.setData().forPath("/glmapper/test","1".getBytes());Thread.sleep(1000);curatorClient.setData().forPath("/glmapper/test","2".getBytes());Thread.sleep(1000);zookeeperCuratorClient.createNode("/glmapper/test/second","data".getBytes(),CreateMode.PERSISTENT);Thread.sleep(1000);curatorClient.setData().forPath("/glmapper/test/second","1".getBytes());Thread.sleep(1000);curatorClient.setData().forPath("/glmapper/test/second","2".getBytes());Thread.sleep(1000); 测试中控制台输出的信息如下： 12345678910111213141516171819202122232425262728-----------------------------event:NODE_ADDEDpath:/glmapper----------------------------------------------------------event:NODE_ADDEDpath:/glmapper/test----------------------------------------------------------event:NODE_UPDATEDpath:/glmapper/test----------------------------------------------------------event:NODE_UPDATEDpath:/glmapper/test----------------------------------------------------------event:NODE_ADDEDpath:/glmapper/test/second----------------------------------------------------------event:NODE_UPDATEDpath:/glmapper/test/second----------------------------------------------------------event:NODE_UPDATEDpath:/glmapper/test/second----------------------------- 事务操作 CuratorFramework 的实例包含 inTransaction( ) 接口方法，调用此方法开启一个 ZooKeeper 事务。 可以复合create、 setData、 check、and/or delete 等操作然后调用 commit() 作为一个原子操作提交。 123456789101112131415161718192021// 开启事务 CuratorTransaction curatorTransaction = curatorClient.inTransaction();Collection&lt;CuratorTransactionResult&gt; commit = // 操作1 curatorTransaction.create().withMode(CreateMode.EPHEMERAL).forPath("/glmapper/transaction") .and() // 操作2 .delete().forPath("/glmapper/test") .and() // 操作3 .setData().forPath("/glmapper/transaction", "data".getBytes()) .and() // 提交事务 .commit();Iterator&lt;CuratorTransactionResult&gt; iterator = commit.iterator();while (iterator.hasNext())&#123; CuratorTransactionResult next = iterator.next(); System.out.println(next.getForPath()); System.out.println(next.getResultPath()); System.out.println(next.getType());&#125; 这里debug看了下Collection信息，面板如下： 异步操作前面提到的增删改查都是同步的，但是 Curator 也提供了异步接口，引入了 BackgroundCallback 接口用于处理异步接口调用之后服务端返回的结果信息。BackgroundCallback 接口中一个重要的回调值为 CuratorEvent，里面包含事件类型、响应吗和节点的详细信息。 在使用上也是非常简单的，只需要带上 inBackground() 就行，如下： 1curatorClient.getData().inBackground().forPath("/glmapper/test"); 通过查看 inBackground 方法定义可以看到，inBackground 支持自定义线程池来处理返回结果之后的业务逻辑。 1public T inBackground(BackgroundCallback callback, Executor executor); 这里就不贴代码了。 小结本文主要围绕 Curator 的基本 API 进行了学习记录，对于原理及源码部分没有涉及。这部分如果有时间在慢慢研究吧。另外像分布式锁、分布式自增序列等实现停留在理论阶段，没有实践，不敢妄论，用到再码吧。 参考 http://www.cnblogs.com/felixzh/p/5869212.html https://my.oschina.net/roccn/blog/918209]]></content>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 实践系列-Kafka简介&集成SpringBoot]]></title>
    <url>%2F2019%2F03%2F07%2Fspringboot%2Fspringboot-series-kafka-introduction%2F</url>
    <content type="text"><![CDATA[近期在做 SOFA 与 SpringCloud 的集成，希望通过一系列的 DEMO 工程去帮助大家更好的使用 SOFA 和 SpringCloud；同时也希望大家一起来参与共建和 star。 GitHub传送门：spring-cloud-sofastack-samples Kafka 简介 官方网站：https://kafka.apache.org/ 功能提供Apache Kafka™ 是 一个分布式数据流平台，从官方文档的解释来看，其职能大体如下： Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system。发布和订阅数据流，与消息队列或企业级消息系统很像。 Store streams of records in a fault-tolerant durable way。具有很强容灾性的存储数据流 Process streams of records as they occur。及时的处理数据流。 作为一个后端司机，大多数情况下都是把 Kafka 作为一个分布式消息队列来使用的，分布式消息队列可以提供应用解耦、流量消峰、消息分发等功能，已经是大型互联网服务架构不可缺少的基础设置了。 基本概念topic 和 partitionKafka 对数据提供的核心抽象，topic 是发布的数据流的类别或名称。topic 在 Kafka 中，支持多订阅者； 也就是说，topic 可以有零个、一个或多个消费者订阅写到相应 topic 的数据。对应每一个 topic，Kafka 集群会维护像一个如下这样的分区的日志：每个 Partition 都是一个有序的、不可变的并且不断被附加的记录序列，也就是一个结构化提交日志（commit log）。为了保证唯一标性识 Partition 中的每个数据记录，Partition 中的记录每个都会被分配一个叫做偏移（offset）顺序的ID号。通过一个可配置的保留期，Kafka 集群会保留所有被发布的数据，不管它们是不是已经被消费者处理。例如，如果保留期设置为两天，则在发布记录后的两天内，数据都可以被消费，之后它将被丢弃以释放空间。 Kafka 的性能是不为因为数据量大小而受影响的，因此长时间存储数据并不成问题。 事实上，在每个消费者上保留的唯一元数据是消费者在日志中的偏移位置，这个偏移由消费者控制：通常消费者会在读取记录时线性地提高其偏移值（offset++），但实际上，由于偏移位置由消费者控制，它可以以任何顺序来处理数据记录。 例如，消费者可以重置为较旧的偏移量以重新处理来自过去的数据，或者跳过之前的记录，并从“现在”开始消费。 这种特征的组合意味着 Kafka 消费者非常轻量级，随意的开启和关闭并不会对其他的消费者有大的影响。 日志中的 Partition 有几个目的： 保证日志的扩展性，topic 的大小不受单个服务器大小的限制。每个单独的 Partition 大小必须小于托管它的服务器磁盘大小，但 topic 可能有很多 Partition，因此它可以处理任意数量的海量数据。 作为并行处理的单位 (知乎-Partition：Kafka可以将主题划分为多个分区（Partition），会根据分区规则选择把消息存储到哪个分区中，只要如果分区规则设置的合理，那么所有的消息将会被均匀的分布到不同的分区中，这样就实现了负载均衡和水平扩展。另外，多个订阅者可以从一个或者多个分区中同时消费数据，以支撑海量数据处理能力) kafka中的topic为什么要进行分区 原贴：kafka中的topic为什么要进行分区 ，由于不能转载，此处不摘抄原文~ 生产者生产者将数据发布到他们选择的 topic ， 生产者负责选择要吧数据分配给 topic 中哪个 Partition。这可以通过循环方式（round-robin）简单地平衡负载，或者可以根据某些语义进行分区（例如基于数据中的某些关键字）来完成。 消费者消费者们使用消费群组(consumer group )名称来标注自己，几个消费者共享一个 group，每一个发布到 topic 的数据会被传递到每个消费群组(consumer group )中的一个消费者实例。 消费者实例可以在不同的进程中或不同的机器上。 如果所有的消费者实例具有相同的 consumer group，则记录将在所有的消费者实例上有效地负载平衡 如果所有的消费者实例都有不同的 consumer group，那么每个记录将被广播给所有的消费者进程，每个数据都发到了所有的消费者。 上图解释源自《Kafka 官方文档》 介绍： 如上图，一个两个服务器节点的Kafka集群， 托管着4个分区(P0-P3)，分为两个消费者群. 消费者群A有2个消费者实例，消费者群B有4个. 然而，更常见的是，我们发现主题具有少量的消费者群，每个消费者群代表一个“逻辑订户”。每个组由许多消费者实例组成，保证可扩展性和容错能力。这可以说是“发布-订阅”语义，但用户是一组消费者而不是单个进程。 在Kafka中实现消费的方式，是通过将日志中的分区均分到消费者实例上，以便每个实例在任何时间都是“相应大小的一块”分区的唯一消费者。维护消费者组成员资格的过程，由卡夫卡协议动态处理。 如果新的实例加入组，他们将从组中的其他成员接管一些分区; 如果一个实例消失，其分区将被分发到剩余的实例。 Kafka仅提供单个分区内的记录的顺序，而不是主题中的不同分区之间的总顺序。 每个分区排序结合按键分区，足以满足大多数应用程序的需求。 但是，如果您需要使用总顺序，则可以通过仅具有一个分区的主题来实现，尽管这仅意味着每个消费者组只有一个消费者进程。 Kafka 作为消息系统消息系统传统上有两种模式: 队列和发布-订阅。 队列模式中，消费者池可以从服务器读取，每条记录只会被某一个消费者消费 允许在多个消费者实例上分配数据处理，但是一旦数据被消费之后，数据就没有了 发布订阅模式中，记录将广播给所有消费者 允许将数据广播到多个进程，但无法缩放和扩容，因为每个消息都发送给每个订阅用户 本篇只介绍 Kafka 作为消息队列的一些基本概念，更多介绍请参考官方文档。 Kafka 安装这里来看下如何安装 kafka，下载地址：https://kafka.apache.org/downloads。本篇使用的版本是 kafka_2.12-1.1.1。 获取包文件 1&gt; wget http://mirrors.shu.edu.cn/apache/kafka/1.1.1/kafka_2.12-1.1.1.tgz 解压压缩包 1&gt; tar -zxvf kafka_2.12-1.1.1.tgz 修改配置文件 12&gt; cd kafka_2.12-1.1.1/config&gt; vim server.properties 我这里主要修改项包括以下几个： 12345678# The id of the broker. This must be set to a unique integer for each broker.broker.id=0listeners=PLAINTEXT://192.168.0.1:9092advertised.listeners=PLAINTEXT://192.168.0.1:9092# zookeeper 地址，可以多个zookeeper.connect=192.168.0.6:2181 Kafka 服务启动需要依赖 Zookeeper ，所以在配置文件中需要指定 Zookeeper 集群地址。Kafka 自己的安装包中解压之后是包括 Zookeeper 的，可以通过以下的方式来启动一个单节点 Zookeeper 实例： 1&gt; sh zookeeper-server-start.sh -daemon config/zookeeper.properties 这里我是指定了之前部署的一台ZK机器，所以可以直接将ZK地址指到已部署好的地址。Zookeeper 安装可以参考： Linux 下安装 Zookeeper 通过上述操作，下面就可以直接来启动Kafka 服务了： 1&gt; sh kafka-server-start.sh config/server.properties SpringBoot 集成 Kafka构建一个简单的 Kafka Producer 工具依赖 依赖引入 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;1.3.5.RELEASE&lt;/version&gt;&lt;!--$NO-MVN-MAN-VER$--&gt;&lt;/dependency&gt; producer 为了可以把 Kafka 封装已提供给其他模块使用，大家可以将 Kafka 的生产端工具类使用 SpringBoot 的自动配置机制进行包装，如下：123456789@Configurationpublic class KafkaProducerAutoConfiguration &#123; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; @Bean public KafkaSender kafkaSender()&#123; return new KafkaSender(kafkaTemplate); &#125;&#125; KafkaSender 123456789101112public class KafkaSender &#123; private KafkaTemplate&lt;String, String&gt; kafkaTemplate; public KafkaSender(KafkaTemplate&lt;String, String&gt; kafkaTemplate) &#123; this.kafkaTemplate = kafkaTemplate; &#125; /** * send message */ public void sendMessage(String topic, String message) &#123; kafkaTemplate.send(topic, message); &#125;&#125; 自动配置 12org.springframework.boot.autoconfigure.EnableAutoConfiguration=\io.sofastack.cloud.core.kafka.configuration.KafkaProducerAutoConfiguration 工程模块如下： 案例测试在测试工程中引入依赖，这个依赖就是上面工程打包来的： 1234&lt;dependency&gt; &lt;groupId&gt;io.sofastack.cloud&lt;/groupId&gt; &lt;artifactId&gt;sofastack-cloud-core-kafka&lt;/artifactId&gt;&lt;/dependency&gt; 在 resources 目录下新建 application.properties 配置文件 123456789101112131415161718192021222324#============== kafka ===================# 指定kafka 代理地址，可以多个,这里的192.168.0.1是上面Kafka 启动配置文件中对应的# 注：网上一些帖子中说 Kafka 这里的配置只能是主机名，不支持 ip，没有验证过，# 如果您在验证时出现问题，可以尝试本机绑定下 hostspring.kafka.bootstrap-servers= 192.168.0.1:9092#=============== provider =======================spring.kafka.producer.retries=0# 每次批量发送消息的数量spring.kafka.producer.batch-size=16384spring.kafka.producer.buffer-memory=33554432# 指定消息key和消息体的编解码方式spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializerspring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer#=============== consumer =======================# 指定默认消费者group idspring.kafka.consumer.group-id=test-consumer-groupspring.kafka.consumer.auto-offset-reset=earliestspring.kafka.consumer.enable-auto-commit=truespring.kafka.consumer.auto-commit-interval=100ms# 指定消息key和消息体的编解码方式spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializerspring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializerspring.application.name=kafka-testlogging.path=./logs 启动类中模拟发送消息 123456789101112131415161718@SpringBootApplication@PropertySource("classpath:application-kafka.properties")public class ProviderApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext run = SpringApplication.run(ProviderApplication.class, args); // 这里通过容器获取，正常使用情况下，可以直接使用 Autowired 注入 KafkaSender bean = run.getBean(KafkaSender.class); for (int i = 0; i &lt; 3; i++) &#123; //调用消息发送类中的消息发送方法 bean.sendMessage(KafkaContants.TRADE_TOPIC, "send a test message"); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 编写消费者，在 SpringBoot 工程中，消费者实现非常简单 123456789101112@Componentpublic class KafkaReceiver &#123; // 配置监听的主体，groupId 和配置文件中的保持一致 @KafkaListener(topics = &#123; KafkaContants.TRADE_TOPIC &#125;, groupId = "test-consumer-group") public void listen(ConsumerRecord&lt;?, ?&gt; record) &#123; Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value()); if (kafkaMessage.isPresent()) &#123; Object message = kafkaMessage.get(); System.out.println(message); &#125; &#125;&#125; 启动工程后，可以在控制台看下消费者打印的信息： 这里保持应用正常运行，再通过服务端来手动发送消息，看下是当前消费者能够正确监听到对应的 topic 并消费。1&gt; sh kafka-console-producer.sh --broker-list 192.168.0.1:9092 --topic trading 执行上述命令之后，命令行将会等待输入，这里输入先后输入 glmapper 和 sofa : 然后再看下应用程序控制台输入结果如下： 参考 Introduction 《Kafka 官方文档》介绍]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 下安装 Zookeeper]]></title>
    <url>%2F2019%2F03%2F04%2Fzookeeper-on-linux%2F</url>
    <content type="text"><![CDATA[安装 Zookeeper目前 Curator 有 2.x.x 和 3.x.x 两个系列的版本，支持不同版本的 Zookeeper。其中 Curator 2.x.x 兼容 Zookeeper的 3.4.x 和 3.5.x。而 Curator 3.x.x 只兼容 Zookeeper 3.5.x。 Curator 2.x.x - compatible with both ZooKeeper 3.4.x and ZooKeeper 3.5.xCurator 3.x.x - compatible only with ZooKeeper 3.5.x and includes support for new 选择使用 3.4.x 版本 Zookeeper。 下载 Zookeeper ，选择相应的版本，这里以 3.4.13 版本为例： 1&gt; wget http://apache.fayea.com/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz 执行上述命令进行下载，下载完成之后对文件进行解压。 解压文件 1&gt; tar -zxvf zookeeper-3.4.13.tar.gz 创建数据和日志目录 123&gt; cd zookeeper-3.4.13&gt; mkdir data&gt; mkdir logs 配置文件修改首先将默认的 zoo_sample.cfg 命名为 zoo.cfg 12&gt; cd conf&gt; cp zoo_sample.cfg zoo.cfg 编辑 zoo.cfg ， 将数据目录和日志目录路径修改为上述步骤中创建的两个文件夹： 123# 配置dataDir 和 dataLogDirdataDir=/home/admin/server/zookeeper-3.4.13/datadataLogDir=/home/admin/server/zookeeper-3.4.13/logs 启动 zookeeper到你安装的zookeeper的bin目录下，如：/home/admin/server/zookeeper-3.4.13/bin1&gt; cd /home/admin/server/zookeeper-3.4.13/bin 执行 start 启动 &gt; zkServer.sh start 上述是简单的在 linux 环境下安装配置 Zookeeper 的过程，对于在实际的生成环境，请根据自己项目需求进行更加细化的配置。 安装 Zookeeper 可视化工具为了可以直观的看到 zookeeper 的节点信息，可以考虑部署一个 zookeeper 的管控界面，常见的有 zkui 和 zkweb。 zkui zkweb zkui 界面更加简单一点，zkweb 在一些细节展示上更加有优势，这里推荐使用 zkweb。具体部署方式见官方文档。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud-网关 Gateway 入门体验]]></title>
    <url>%2F2019%2F01%2F19%2Fspringboot%2Fspringcloud-gateway-predicate-project%2F</url>
    <content type="text"><![CDATA[网关服务核心是将进入的请求正确合理的路由到下层具体的服务进行业务处理，从它的功能来看，网关服务的核心就是路由信息的构建。 Spring Cloud Gateway 作为 Spring Cloud 生态系统中的网关，目标是替代 Netflix Zuul，其不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全、监控、埋点和限流等。下面是官方提供的一个工作原理图： 客户端发送请求到 Spring Cloud Gateway，Gateway Handler Mapping 确定请求与路由匹配，则会将请求交给Gateway Web Handler 处理。在代理前后可以执行多个过滤器。最后代理到具体的服务。 几个概念 Route：Gateway 中的基本元素，它有自己的 ID、URI 、 Predicate 集合和 Filter 集合 Predicate：判断请求的 Url 是否匹配当前的 Route Filter ：匹配通过之后对请求和响应的处理及修饰 Spring-Cloud-Gateway 构建路由的数据流向： RouteDefinition 模型是对 Route 模型中 route 的定义以及描述，Spring-Cloud-Gateway 最终会通过RouteDefinition 来构建起 Route 实例信息。其中 RouteDefinition 代码包含两个数组分别是PredicateDefinition，FilterDefinition。 内置的 PredicateSpring Cloud Gateway 是通过 Spring WebFlux 的 HandlerMapping 做为底层支持来匹配到转发路由，Spring Cloud Gateway 内置了很多 Predicates 工厂，这些 Predicates 工厂通过不同的 HTTP 请求参数来匹配，多个 Predicates 工厂可以组合使用。下面是内置的Predicates： 组件 备注 After Route Predicate Factory 此谓词匹配当前日期时间之后发生的请求。 Before Route Predicate Factory 此谓词匹配在当前日期时间之前发生的请求。 Between Route Predicate Factory 此谓词匹配datetime1之后和datetime2之前发生的请求。 datetime2参数必须在datetime1之后。 Cookie Route Predicate Factory Cookie Route Predicate Factory有两个参数，cookie名称和正则表达式。此谓词匹配具有给定名称且值与正则表达式匹配的cookie。 Header Route Predicate Factory Header Route Predicate Factory有两个参数，标题名称和正则表达式。与具有给定名称且值与正则表达式匹配的标头匹配。 Host Route Predicate Factory Host Route Predicate Factory采用一个参数：主机名模式。该模式是一种Ant样式模式“.”作为分隔符。此谓词匹配与模式匹配的Host标头。 Method Route Predicate Factory Method Route Predicate Factory采用一个参数：要匹配的HTTP方法。 Path Route Predicate Factory 匹配请求的path Query Route Predicate Factory Query Route Predicate Factory有两个参数：一个必需的参数和一个可选的正则表达式。 RemoteAddr Route Predicate Factory RemoteAddr Route Predicate Factory采用CIDR符号（IPv4或IPv6）字符串的列表（最小值为1），例如， 192.168.0.1/16（其中192.168.0.1是IP地址，16是子网掩码）。 工程代码本篇将通过一个简单的 gateway 工程来演示如何使用上面的 Predicate 来实现路由。 新建工程这里新建一个 glmapper-cloud-gateway 工程，具体细节如下 依赖引入首先在当前工程的pom文件中引入spring cloud gateway 的依赖：spring-cloud-starter-gateway 1234567891011&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置文件配置文件部分，除了常规的端口，应用名之外；关于spring cloud 的路由规则也可以通过配置文件进行配置，下面先以最简单的 path的方式来演示，最终达到的目标是，当输入：http://localhost:8866/gateway 时，请求信息将会被路由到 http://localhost:8086/hello(这个是一个eureka client，对外提供rest服务，工程详见glmapper-eureka-provider)。 12345678910111213141516eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ #eureka server 的地址server: port: 8866spring: application: name: glmapper-cloud-gateway #应用名 cloud: gateway: routes: - id: glmapper uri: http://localhost:8086/hello #目标地址 predicates: - Path=/gateway #路由规则 启动应用&amp;验证这里直接启动这个工程，SpringCloud Gateway 不需要额外的注解来开启网关服务，所以这里省略启动类的代码。先后启动 glmapper-eureka-server 、glmapper-eureka-provider、glmapper-cloud-gateway。 在浏览器中输入：http://localhost:8866/gateway ，输出的结果如下： 1Hello Glmapper! Now Port is 8086 And hostname is HelloGlmapperService 这里输出的实际上是 http://localhost:8086/hello 提供的资源，说明我们的路由规则已经生效。 内置 Predicate 规则配置上面已经罗列了所有的 spring cloud gateway 一些内置的 Predicate ，下面将来使用这些规则来演示下。 时间匹配Predicate 支持设置一个时间，在请求进行转发的时候，可以通过判断在这个时间之前或者之后进行转发。在上面的列表中可以看出，基于时间的匹配支持某时间节点之前、之后，还支持介于两个时间之间的某个时间段内的匹配。基于某个时间段内的匹配规则常见的场景是限时抢购。 After Route Predicate 12345678910server: port: 8080spring: cloud: gateway: routes: - id: glmapper #自定义的路由ID uri: http://www.glmapper.com #目标服务地址 predicates: - After=2019-01-10T00:00:00+08:00[Asia/Shanghai] #通过时间匹配 2019年1月10日 After Route Predicate 是指在这个时间之后的请求都转发到目标地址。请求时间在 2019年1月10日日00点00分00秒之后的所有请求都转发到地址 http://www.glmapper.com。+08:00是指时间和UTC时间相差八个小时，时间地区为 Asia/Shanghai。 Before Route Predicate 12345678910server: port: 8080spring: cloud: gateway: routes: - id: glmapper #自定义的路由ID uri: http://www.glmapper.com #目标服务地址 predicates: - Before=2019-01-10T00:00:00+08:00[Asia/Shanghai] #通过时间匹配 2019年1月10日 Before Route Predicate 与 After Route Predicate 刚好相反，在某个时间之前的请求的请求都进行转发。 Between Route Predicate 12345678910server: port: 8080spring: cloud: gateway: routes: - id: glmapper #自定义的路由ID uri: http://www.glmapper.com #目标服务地址 predicates: - Between=2019-01-10T00:00:00+08:00[Asia/Shanghai], 2019-01-10T06:00:00+08:00[Asia/Shanghai] 在2019年1月10 零点至6点之间的请求将会被路由到 http://www.glmapper.com ，其他的请求将不会被路由。 Cookie 或者 Header Cookie Route Predicate 12345678spring: cloud: gateway: routes: - id: glmapper uri: http://localhost:8086/hello predicates: - Cookie=name,glmapper 这里，如果我的请求信息中存在 cookie name 为 glmapper，值匹配到 glmapper 的串，那么请求将会被路由。 PS：这里在配置的时候要注意下 routes 后面格式缩进，否则会抛出一些异常，如： Property: spring.cloud.gateway.routes[0].uri Value: null Reason: 不能为null Property: spring.cloud.gateway.routes[0].predicates Value: [] Reason: 不能为空 当cookie的值不满足时，访问时404 Header Route Predicate 12345678910spring: application: name: glmapper-cloud-gateway cloud: gateway: routes: - id: glmapper uri: http://localhost:8086/hello predicates: - Header=X-Request-Id, \d+ 上面这段配置用于配置 Header 中 X-Request-Id值数字的请求： 同样，如果是非数字的话将会返回 404。 域名匹配12345678910spring: application: name: glmapper-cloud-gateway cloud: gateway: routes: - id: glmapper uri: http://localhost:8086/hello predicates: - Host=**.glmapper.com 上面这段配置用于匹配 host 为 xxx.glmapper.com 域名的请求： 关于其他的内置 Predicate 均可在官方文档中有实例参考，这里就不一一罗列了。 组合匹配最后我们来将上面的一些进行组合，假设我需要在 2019.1.10 0点至2019.1.10 6点之间，cookie中带有name=glmapper，header 的 X-Request-Id 为数字，域名是 xx.glmapper.com ，path为 /gateway ，请求方式为GET，参数名为queryParam 的请求路由到 http://localhost:8086/hello。那么具体配置如下 12345678910111213141516spring: application: name: glmapper-cloud-gateway cloud: gateway: routes: - id: glmapper uri: http://localhost:8086/hello predicates: - Host=**.glmapper.com - Path=/gateway - Method=GET - Header=X-Request-Id,\d+ - Query=queryParam - Cookie=name,glmapper - Between=2019-01-10T00:00:00+08:00[Asia/Shanghai], 2019-01-10T06:00:00+08:00[Asia/Shanghai] 还是通过curl 命令来执行以下： 参考 Spring Cloud Gateway]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>Gateway</tag>
        <tag>网关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud-断路器 Hystrix]]></title>
    <url>%2F2019%2F01%2F09%2Fspringboot%2Fspringcloud-hystrix-project%2F</url>
    <content type="text"><![CDATA[简介Hystrix 是 Netflix 的一个开源项目，它能够在服务失效的情况下，通过隔离系统依赖服务的方式，防止服务级联失败，造成服务雪崩。同时Hystrix 还提供了失败回滚机制，使得系统能够更快的从异常中恢复。Hystrix 为服务间调用提供了保护和控制。 Hystrix 具有的功能如下： 当通过客户端调用服务出现高延迟或者调用失败时，能够为系统提供保护机制 在复杂的分布式场景下，可以防止服务雪崩效应 提供快速失败（Fail Fast） 同时能够快速恢复 提供失败回滚和优雅的服务降级机制 提供近实时的监控、报警和运维控制手段 Hystrix 在实际应用过程中的使用方式很丰富，可以通过注解，也可以通过集成 HystrixCommand 和HystrixObservableCommand 。本篇将通过案例简单说明下说用方式。 环境准备 类别 值 JDK 1.8.0_162 SOFABoot/SpringBoot 3.0.0/2.0.x.RELEASE SpringCloud Finchley.RC1 IDE IDEA 工程背景本节将会创建一个 sofa-hystrix-client 工程，通过 Spring Cloud 提供的负载均衡器 hystrix 实现服务的熔断降级。 新建 sofa-hystrix-client本工程继续使用《SpringCloud-Eureka 服务注册》中的父工程来构建。 右击 sofa-eureka-parent 父工程 -&gt; New -&gt; Module，这里选择 Maven 工程； artifactId：sofa-hystrix-client 修改pom文件pom文件中加入 hysterix 的依赖 123456789101112131415161718192021&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 配置文件123456789eureka: client: service-url: defaultZone: http://localhost:8761/eureka/spring: application: name: hystrix-clientserver: port: 8787 没有什么特殊的配置，还是作为一个 eureka-client 存在。 启动类启动类上增加开启断路器的注解@EnableCircuitBreaker 12345678910111213@SpringBootApplication@EnableCircuitBreakerpublic class SofaHystrixApplication &#123; @Bean @LoadBalanced public RestTemplate restTemplate()&#123; return new RestTemplate(); &#125; public static void main(String[] args) &#123; SpringApplication.run(SofaHystrixApplication.class, args); &#125;&#125; 资源类 NormalService 中通过@HystrixCommand标准一个受保护的资源方法 getByServiceId()。getByServiceId 中通过restTemplate 来调用远程服务。@HystrixCommand注解的 fallbackMethod 属性指定当服务不可用时需要执行的 fallback 方法。 1234567891011121314@Servicepublic class NormalService &#123; @Autowired private RestTemplate restTemplate; @HystrixCommand(fallbackMethod = "fallBack") public String getByServiceId()&#123; return restTemplate.getForObject("http://HELLOSOFABOOTSERVICE/hello",String.class); &#125; private String fallBack()&#123; return "Filed to get data"; &#125;&#125; HystrixRibbonController：通过instanceService调用上面的NormalService资源类 123456789@RestControllerpublic class HystrixRibbonController &#123; @Autowired public NormalService instanceService; @RequestMapping("/hystrix") public String test()&#123; return instanceService.getByServiceId(); &#125;&#125; 启动&amp;验证先后启动sofa-eureka-server-center 、sofa-eureka-provider、sofa-hystrix-client 三个工程。浏览器中输入： http://localhost:8787/hystrix ，结果如下： 1Hello SOFA! Now Port is 8086 And hostname is HelloSOFABootService 关闭 sofa-eureka-provider ，刷新浏览器： 1Filed to get data 执行了 NormalService 中的 fallback 方法了。 资源隔离hystrix 中提供了两中隔离策略，一种是基于线程池的隔离、另外一种是基于信号量的隔离。本篇只演示案例，具体原理请参看 hystrix 原理分析 相关文章。 基于线程池的隔离实现新建一个 SofaThreadPoolHystrixCommand 类，继承 HystrixCommand。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class SofaThreadPoolHystrixCommand extends HystrixCommand &#123; private RestTemplate restTemplate; public SofaThreadPoolHystrixCommand(RestTemplate restTemplate) &#123; super(initailize()); this.restTemplate = restTemplate; &#125; public static HystrixCommand.Setter initailize()&#123; // 线程池配置 HystrixThreadPoolProperties.Setter hystrixThreadPoolProperties = HystrixThreadPoolProperties.Setter() .withCoreSize(5) .withKeepAliveTimeMinutes(5) // 线程等待队列最大长度,默认值:-1 表示不等待直接拒绝,测试表明线程池使用直接决绝策略+ 合适大小的非回缩线程池效率最高.所以不建议修改此值。 .withMaxQueueSize(10) .withQueueSizeRejectionThreshold(100); // 命令属性配置,这里指定隔离策略是 THREAD HystrixCommandProperties.Setter hystrixCommand = HystrixCommandProperties.Setter() .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.THREAD) //意味着线程最多允许执行fallback的并发数为10,超过10 报fallback execution rejected .withFallbackIsolationSemaphoreMaxConcurrentRequests(10); HystrixCommand.Setter setter = HystrixCommand.Setter .withGroupKey(HystrixCommandGroupKey.Factory.asKey("SofaThreadPoolHystrixCommand")) .andCommandKey(HystrixCommandKey.Factory.asKey("sofaBootService")) .andCommandPropertiesDefaults(hystrixCommand) .andThreadPoolPropertiesDefaults(hystrixThreadPoolProperties) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey("sofa-hystrix-thread")); return setter; &#125; /** * 受保护的资源 * @return * @throws Exception */ @Override protected Object run() throws Exception &#123; return restTemplate.getForObject("http://HELLOSOFABOOTSERVICE/hello",String.class); &#125; /** * 失败执行的保护方法 * @return */ @Override protected Object getFallback() &#123; return "this is fail back policy"; &#125;&#125; 相关参数说明： HystrixCommandGroupKey：配置全局唯一标识服务分组的名称，比如账户系统就是一个服务分组，监控时，相同分组的服务会聚合在一起，必填选项。 HystrixCommandKey：配置全局唯一标识服务的名称，比如账户系统有一个获取账号名的服务，那么就可以为这个服务起一个名字来唯一识别该服务，如果不配置，则默认是简单类名。 HystrixThreadPoolKey：配置全局唯一标识线程池的名称，相同线程池名称的线程池是同一个，如果不配置，则默认是分组名，此名字也是线程池中线程名字的前缀。 HystrixThreadPoolProperties：配置线程池参数 HystrixCommandProperties：配置该命令的一些参数，如 executionIsolationStrategy 配置执行隔离策略，默认是使用线程隔离。配置为 THREAD，线程池隔离；配置为 SEMAPHORE ，信号量隔离 这里为了模拟并发，使用 CountDownLatch 类来控制，在 HystrixRibbonController 中添加 testThread 资源方法： 123456789@RequestMapping("/testThread")public String testThread()&#123; CountDownLatch countDownLatch = new CountDownLatch(1); for (int i = 0; i &lt; THREAD_NUM; i ++) &#123; new Thread(new ConsumerThread(countDownLatch)).start(); &#125; countDownLatch.countDown(); return "data";&#125; 内部定义一个内部类，模拟调用： 12345678910111213141516171819class ConsumerThread implements Runnable &#123; private final CountDownLatch startLatch; public ConsumerThread(CountDownLatch startLatch) &#123; this.startLatch = startLatch; &#125; @Override public void run() &#123; try &#123; // 线程等待 startLatch.await(); // 执行操作 SofaThreadPoolHystrixCommand sofaThreadPoolHystrixCommand = new SofaThreadPoolHystrixCommand(restTemplate); System.out.println(sofaThreadPoolHystrixCommand.execute().toString()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; 重启当前工程，浏览器执行 http://localhost:8787/testThread 12345678this is fail back policythis is fail back policythis is fail back policythis is fail back policythis is fail back policy// ... 省略Hello SOFA! Now Port is 8086 And hostname is HelloSOFABootService// ... 省略 基于信号量隔离新建一个 SofaSemaphoreHystrixCommand 类，继承 HystrixCommand。代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class SofaSemaphoreHystrixCommand extends HystrixCommand &#123; private RestTemplate restTemplate; public SofaSemaphoreHystrixCommand(RestTemplate restTemplate) &#123; super(initailize()); this.restTemplate = restTemplate; &#125; public static HystrixCommand.Setter initailize()&#123; // 命令属性配置,这里指定隔离策略是 THREAD HystrixCommandProperties.Setter hystrixCommand = HystrixCommandProperties.Setter() .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.SEMAPHORE) //至少有10个请求，熔断器才进行错误率的计算 .withCircuitBreakerRequestVolumeThreshold(0) //熔断器中断请求5秒后会进入半打开状态,放部分流量过去重试 .withCircuitBreakerSleepWindowInMilliseconds(5000) //错误率达到50开启熔断保护 .withCircuitBreakerErrorThresholdPercentage(50) //最大并发请求量 .withExecutionIsolationSemaphoreMaxConcurrentRequests(10) //意味着信号量最多允许执行fallback的并发数为10,超过10 报fallback execution rejected .withFallbackIsolationSemaphoreMaxConcurrentRequests(10); HystrixCommand.Setter setter = HystrixCommand.Setter. withGroupKey(HystrixCommandGroupKey.Factory.asKey("SofaSemaphoreHystrixCommand")) .andCommandKey(HystrixCommandKey.Factory.asKey("sofaBootService")) .andCommandPropertiesDefaults(hystrixCommand) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey("sofa-hystrix-thread")); return setter; &#125; /** * 受保护的资源 * @return * @throws Exception */ @Override protected Object run() throws Exception &#123; return restTemplate.getForObject("http://HELLOSOFABOOTSERVICE/hello",String.class); &#125; /** * 失败执行的保护方法 * @return */ @Override protected Object getFallback() &#123; return "this is fail back policy"; &#125;&#125; 同样使用 CountDownLatch 来模拟并发。在 HystrixRibbonController 中添加 testSemaphore 资源方法： 123456789@RequestMapping("/testSemaphore") public String testSemaphore()&#123; CountDownLatch countDownLatch = new CountDownLatch(1); for (int i = 0; i &lt; THREAD_NUM; i ++) &#123; new Thread(new ConsumerSemaphore(countDownLatch)).start(); &#125; countDownLatch.countDown(); return "data"; &#125; 内部定义一个内部类 ConsumerSemaphore ，模拟调用： 12345678910111213141516171819class ConsumerSemaphore implements Runnable &#123; private final CountDownLatch startLatch; public ConsumerSemaphore(CountDownLatch startLatch) &#123; this.startLatch = startLatch; &#125; @Override public void run() &#123; try &#123; // 线程等待 startLatch.await(); // 执行操作 SofaSemaphoreHystrixCommand sofaThreadPoolHystrixCommand = new SofaSemaphoreHystrixCommand(restTemplate); System.out.println(sofaThreadPoolHystrixCommand.execute().toString()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; 结果和线程隔离的差不多。不贴结果了。]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
        <tag>Hystrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud-配置中心 Apollo]]></title>
    <url>%2F2019%2F01%2F09%2Fspringboot%2Fspringcloud-config-apollo%2F</url>
    <content type="text"><![CDATA[简介Apollo（阿波罗）是携程框架部门研发的开源配置管理中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性。 本篇将搭建一套 Apollo 配置中心环境，并通过一个 demo 案例来演示如何在 SpringCloud 体系中使用 Apollo。 环境准备 类别 值 JDK 1.8.0_162 SOFABoot/SpringBoot 3.0.0/2.0.x.RELEASE SpringCloud Finchley.RC1 IDE IDEA Mysql 5.7.24 CentOS 7 Apollo 自身需要依赖 Mysql，在部署 Apollo 时需要提前安装 Mysql 数据库。关于 Mysql 的安装可以参考：Linux 下安装Mysql数据库。 根据官方文档，Apollo 服务端需运行在 jdk 1.8 以上，客户端需运行在1.7 以上，Mysql 版本需在 5.6.5 版本以上。具体信息可参考：分布式部署指南。 部署 Apollo部署步骤共三步： 创建数据库 Apollo 服务端依赖于 MySQL 数据库，所以需要事先创建并完成初始化 获取安装包 Apollo 服务端安装包共有3个：apollo-configservice, apollo-adminservice, apollo-portal 可以直接下载事先打好的安装包，也可以自己通过源码构建 部署 Apollo 服务端 获取安装包后就可以部署到公司的测试和生产环境了 创建数据库Apollo 服务端共需要两个数据库：ApolloPortalDB和ApolloConfigDB，我们把数据库、表的创建和样例数据都分别准备了 sql 文件，只需要导入数据库即可。 需要注意的是 ApolloPortalDB 只需要在生产环境部署一个即可，而 ApolloConfigDB 需要在每个环境部署一套，如 fat、uat 和 pro 分别部署 3 套 ApolloConfigDB。 注意：如果本地已经创建过 Apollo 数据库，请注意备份数据；sql 文件会清空 Apollo 相关的表。 两份 SQL 文件： apolloportaldb.sql apolloconfigdb.sql 下载下来之后可通过 Mysql 图形界面工具(如 Navicat )等进行导入。导入完成之后，可以进行如下验证。 portalDB 验证执行 SQL： 1select `Id`, `Key`, `Value`, `Comment` from `ApolloPortalDB`.`ServerConfig` limit 1; 查询结果： Id Key Value Comment 1 apollo.portal.envs dev 可支持的环境列表 configDB 验证执行 SQL: 1select `Id`, `Key`, `Value`, `Comment` from `ApolloConfigDB`.`ServerConfig` limit 1; 执行结果： Id Key Value Comment 1 eureka.service.url http://127.0.0.1:8080/eureka/ Eureka服务Url 本过程只针对新建工程，如果涉及到数据迁移，请参考 Apollo 官方文档 数据库部分完成之后，接下来就是部署 Apollo 的三个工程。 工程配置修改Apollo 配置中心 使用需要启动三个工程：apollo-configservice、apollo-adminservice、apollo-portal。 在自己的服务器上新建一个目录 /thirdserver/apollo/ 将官方提供的安装包直接下载到这个目录下，然后解压得到如下列表： apollo-configservice 部署Apollo 服务端需要知道如何连接到你前面创建的数据库，数据库连接串信息位于上一步下载的压缩包中的apollo-configservice-1.2.0-github/config/application-github.properties中，这里把里面默认的数据库连接地址和账密信息替换成我们自己的就可以。这里使用的是 ApolloConfigDB 库。 1234# DataSourcespring.datasource.url = jdbc:mysql://$&#123;serverhost&#125;:3306/ApolloConfigDB?characterEncoding=utf8spring.datasource.username = $&#123;yourusername&#125;spring.datasource.password = $&#123;yourpassword&#125; apollo-adminservice 配置文件修改这里同样是修改 config/application-github.properties 下面的数据库连接信息。这里也使用的是 ApolloConfigDB 库。配置信息和上面一样。 apollo-portal 配置文件修改 portal 使用的是 ApolloPortalDB，修改数据库配置信息： 1234# DataSourcespring.datasource.url = jdbc:mysql://$&#123;serverhost&#125;:3306/ApolloPortalDB?characterEncoding=utf8spring.datasource.username = $&#123;yourusername&#125;spring.datasource.password = $&#123;yourpassword&#125; 修改 meta service 信息，Apollo Portal 需要在不同的环境访问不同的 meta service(apollo-configservice) 地址，所以我们需要在配置中提供这些信息。默认情况下，meta service 和 config service 是部署在同一个 JVM进程，所以 meta service 的地址就是 config service 的地址。配置文件 /config/apollo-env.properties 1234567# $&#123;serverhost&#125; 是你当前机器的主机地址local.meta=http://localhost:8080dev.meta=http://$&#123;serverhost&#125;:8080fat.meta=http://$&#123;serverhost&#125;:8080uat.meta=http://$&#123;serverhost&#125;:8080lpt.meta=http://$&#123;serverhost&#125;:8080pro.meta=http://$&#123;serverhost&#125;:8080 这里是把所有环境配置成一样的了，如果没有不需要这些环境，可以删除掉。 工程部署在每一个工程的解压包中，都有一个 scripts 文件夹，这里面是 Apollo 工程的启动脚本。三个工程分别先后启动：apollo-configservice、apollo-adminservice、apollo-portal，就是分别执行这三个工程下面的 /scripts/startup.sh 脚本即可，关闭执行的是 /scripts/shutdown.sh 脚本。 访问：http://${serverhost}:8070/ 可以看到配置中心管控端的界面。 新建工程 点击 创建项目，填写一些基本信息，然后提交 新增一个配置项，填写基本信息，然后提交 当前工程界面 发布配置 SpringCloud 工程案例新建 sofa-config-apollo 工程。 依赖引入 apollo 客户端依赖及其他相关依赖： 12345678910111213141516171819&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-context&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.ctrip.framework.apollo&lt;/groupId&gt; &lt;artifactId&gt;apollo-client&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置文件123456# 需要与前面 Apollo 中创建项目的appId保持一致app.id=sofa-config-apollo# 设置 apollo meta service 的地址，因为前面meta和config是部署在一起的，所以就是configService的地址apollo.meta=http://$&#123;serverhost&#125;:8080# 配置项sofa.alipay.glmapper.name=glmapper 资源类&amp;启动类启动类启动类上需要开启对 apollo 的支持，使用 @EnableApolloConfig 注解标注 1234567@SpringBootApplication@EnableApolloConfigpublic class SofaConfigApolloApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SofaConfigApolloApplication.class, args); &#125;&#125; 资源类12345678910111213@RestControllerpublic class ApolloConfigController &#123; @ApolloConfig private Config config; @Value("$&#123;sofa.alipay.glmapper.name&#125;") private String name; @RequestMapping("/apollo") public String getConfig()&#123; return name; &#125;&#125; 注意，我在配置文件中指定的是 sofa.alipay.glmapper.name 值是 glmapper，而在配置中心配置的值是glmapper@leishu。同时这里也把 apollo 自己的这个 Config 配置类也注入进来，稍后看下这的对象的信息。 运行程序&amp;验证启动当前工程之前需要确保 Apollo 的相关服务已经起来了，然后运行当前应用。在浏览器中输入： http://localhost:8080/config ，结果如下： 1glmapper@leishu 可以看到这里拿到的是配置中心的配置值，覆盖了我们本地配置文件中的配置。断点看到 config 的信息： Config 对象就是当前集群环境下，指定 appId 的所有配置信息的集合。 更改配置&amp;及时刷新 更改配置 发布配置 刷新 http://localhost:8080/apollo 地址 1glmapper@leishu-update 这里没有重启服务，配置动态更新了 @ApolloConfigChangeListener 来监听配置变更资源类 ApolloConfigController 中增加一个监听方法 1234@ApolloConfigChangeListenerprivate void onChange(ConfigChangeEvent changeEvent) &#123; System.out.println("发生变更了...");&#125; 然后重新在配置中心的界面上修改配置值：glmapper@leishu-update -&gt; glmapper@leishu-update-event，然后发布。然后可以在控制台看到日志： 12发生变更了...2019-01-07 14:36:08.939 INFO 39072 --- [Apollo-Config-1] c.f.a.s.p.AutoUpdateConfigChangeListener : Auto update apollo changed value successfully, new value: glmapper@leishu-update-event, key: sofa.alipay.glmapper.name, beanName: apolloConfigController, field: com.alipay.sofa.cloud.controller.ApolloConfigController.name]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>Config</tag>
        <tag>spring cloud</tag>
        <tag>Apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud-Config 配置中心原理]]></title>
    <url>%2F2019%2F01%2F05%2Fspringboot%2Fspringcloud-config-analysis%2F</url>
    <content type="text"><![CDATA[本篇可以配合《SpringCloud-配置中心 Config》来看，《SpringCloud-配置中心 Config》中是基于SOFABoot 来集成 Spring Cloud Config 的一个 demo 案例。 在demo中，涉及到三个角色： 配置中心服务端：为配置客户端提供对应的配置信息，配置信息的来源是配置仓库。应用启动时，会从配置仓库拉取配置信息缓存到本地仓库中。 配置中心客户端：应用启动时从配置服务端拉取配置信息。 配置仓库：为配置中心服务端提供配置信息存储，Spring Cloud Config 默认是使用git作为仓库的。 整体过程： 环境部署之前，将所需的配置信息推送到配置仓库 启动配置中心服务端，将配置仓库的配置信息拉取到服务端，配置服务端对外提供REST接口 启动配置客户端，客户端根据 spring.cloud.config 配置的信息去服务器拉取相应的配置 服务端实现配置中心服务端主要做了几件事情：连接配置仓库、拉取远程配置&amp;本地缓存、对外提供API接口服务。 @EnableConfigServer 及配置类注解 EnableConfigServer 可以开启应用服务对配置中心的支持。当开启之后，配置服务器就会在启动时进行自动配置。具体对应的配置类是 ConfigServerAutoConfiguration，然后又在 ConfigServerAutoConfiguration 这个配置类中引入了其他很多配置类。如下： 12345678@Configuration@ConditionalOnBean(&#123;Marker.class&#125;)@EnableConfigurationProperties(&#123;ConfigServerProperties.class&#125;)@Import(&#123;EnvironmentRepositoryConfiguration.class, CompositeConfiguration.class, ResourceRepositoryConfiguration.class, ConfigServerEncryptionConfiguration.class, ConfigServerMvcConfiguration.class&#125;)public class ConfigServerAutoConfiguration &#123; public ConfigServerAutoConfiguration() &#123; &#125;&#125; EnvironmentRepositoryConfiguration： 环境变量存储相关的配置类 CompositeConfiguration：组合方式的环境仓库配置类 ResourceRepositoryConfiguration：资源仓库相关的配置类 ConfigServerEncryptionConfiguration：加密断点相关的配置类 ConfigServerMvcConfiguration：对外暴露的MVC端点控制器的配置类 无论是 Spring Cloud 自身提供的默认实现 git ，还是 zk，或者 apollo ；基本思路都是在程序启动时将远端配置拉取到本地作为环境变量来使用，但这些是针对客户端角度来说的。Spring Cloud Config Server 因为其本身是以服务端存在，所以 Config Server 本身的实现思路也值得后面开发借鉴。 对于服务端来说，其基本职责就是能够将具体存储中的配置信息先拿到，然后提供出 API 供客户端来调用。下面从ConfigServerAutoConfiguration 中 import的这些配置类来具体看下实现。 EnvironmentRepositoryConfigurationEnvironmentRepositoryConfiguration 是环境变量存储相关的配置类，它本身也提供了很多实现： 上图中可以看到，环境配置仓库支持的有JDBC、SVN、本地文件系统、Git等等。这些对不同环境仓库的支持，在实现上基本都差不多，下面以默认提供的方式git来分析。 1234@Configuration@Profile("git")class GitRepositoryConfiguration extends DefaultRepositoryConfiguration &#123;&#125; GitRepositoryConfiguration 集成了 DefaultRepositoryConfiguration，这也说明了 Spring Cloud Config 默认使用的是Git。不同的配置类实现都会被标注一个@Profile，可以通过这个来激活相应的配置类；具体做法是在配置服务端的 application.properties(application.yml) 中来指定： 1spring.profile.active=git 没有设置就是默认使用 GIt。 12345678910111213141516171819@Configuration@ConditionalOnMissingBean(value = EnvironmentRepository.class, search = SearchStrategy.CURRENT)class DefaultRepositoryConfiguration &#123; @Autowired private ConfigurableEnvironment environment; @Autowired private ConfigServerProperties server; @Autowired(required = false) private TransportConfigCallback customTransportConfigCallback; @Bean public MultipleJGitEnvironmentRepository defaultEnvironmentRepository( MultipleJGitEnvironmentRepositoryFactory gitEnvironmentRepositoryFactory, MultipleJGitEnvironmentProperties environmentProperties) throws Exception &#123; return gitEnvironmentRepositoryFactory.build(environmentProperties); &#125;&#125; DefaultRepositoryConfiguration 的 ConditionalOnMissingBean 可以知道，如果上下文中没有 EnvironmentRepository，那么就使用 DefaultRepositoryConfiguration。 MultipleJGitEnvironmentRepositoryMultipleJGitEnvironmentRepository 是 Git 存储的具体实现类，下面是类图结构： MultipleJGitEnvironmentRepository 的顶层接口是 EnvironmentRepository ，当然其他的实现也都是实现了这个接口的。另外一个需要关注的是 SearchPathLocator。 EnvironmentRepository：定义了获取指定应用服务环境信息的方法，返回一个Enviroment 123public interface EnvironmentRepository &#123; Environment findOne(String application, String profile, String label);&#125; 三个参数，application、profile、label；《SpringCloud-配置中心 Config》 中客户端部分有对这三个的参数的说明及使用方式，通过这三个参数可以具体定位到配置信息。 SearchPathLocator ： 根据传入客户端应用信息，获取对应的配置环境文件的位置。代码见：SearchPathLocator)。 SearchPathLocator 中有一个内部类 Locations ，Locdations中定义了应用服务配置存储信息。 除了这两个之外，还有一个 AbstractScmAccessor，这个抽象类里面定义了一些列与git存储相关的属性和方法。包括远程仓库的地址、账户、密码、ssh 私钥、本地仓库的地址等等。 SCM : 软件配置管理 AbstractScmEnvironmentRepositoryAbstractScmEnvironmentRepository 实现了 AbstractScmAccessor 和 EnvironmentRepository ，主要就是EnvironmentRepository 中 findOne 的实现： 123456789101112131415@Override public synchronized Environment findOne(String application, String profile, String label) &#123; //新建了一个本地仓库作为代理仓库来使用 NativeEnvironmentRepository delegate = new NativeEnvironmentRepository(getEnvironment(), new NativeEnvironmentProperties()); //获取本地仓库中指定应用的位置 Locations locations = getLocations(application, profile, label); delegate.setSearchLocations(locations.getLocations()); //根据这个路径搜索应用服务的配置信息 Environment result = delegate.findOne(application, profile, ""); result.setVersion(locations.getVersion()); result.setLabel(label); return this.cleaner.clean(result, getWorkingDirectory().toURI().toString(), getUri()); &#125; getLocations 是一个模板方法，Config Server中提供了三种实现： 分别是单 Git 仓库，多 Git 仓库和 Svn 仓库实现。 123456789101112@Override public synchronized Locations getLocations(String application, String profile, String label) &#123; if (label == null) &#123; label = this.defaultLabel; &#125; // 获取最新的版本号 String version = refresh(label); // 根据最新的版本号返回 Locations 定位到资源的搜索路径 return new Locations(application, profile, label, version, getSearchLocations(getWorkingDirectory(), application, profile, label)); &#125; refresh 方法做的作用就是刷新本地仓库的配置状态，这样就能保证每次都能拉取到最新的配置信息。下面来分析这个方法。 JGitEnvironmentRepository#refresh12345678910111213141516171819202122232425262728293031public String refresh(String label) &#123; Git git = null; try &#123; // 创建一个git客户端 git = createGitClient(); // 是否需要执行 git pull if (shouldPull(git)) &#123; FetchResult fetchStatus = fetch(git, label); if (deleteUntrackedBranches &amp;&amp; fetchStatus != null) &#123; deleteUntrackedLocalBranches(fetchStatus.getTrackingRefUpdates(), git); &#125; // 获取后checkout，这样我们就可以获得任何新的分支、tag等。 checkout(git, label); tryMerge(git, label); &#125; else &#123; // 没有什么要更新，所以只是checkout和merge。 // 合并是因为远程分支以前可能已经更新过 checkout(git, label); tryMerge(git, label); &#125; // 返回当前的版本 return git.getRepository().findRef("HEAD").getObjectId().getName(); &#125; catch (Exception e) &#123; // 异常处理 &#125; finally &#123; // 关闭git &#125; &#125; 这个里面基本就是通过git客户端的一些操作。先是检查远程仓库的状态，然后判断本地仓库是否要执行刷新操作。如果有状态更新，比如新的提交时，Git客户端就会执行fetch，然后再进行merge，更新到本地仓库。 MultipleJGitEnvironmentRepository 多仓库的支持，实际上就是遍历了所有的仓库。其他仓库和单仓库是一样的。 客户端实现Spring Cloud Config Client 没有像其他组件一样提供@EnableConfigClient注解，这里没有必要去标注是一个配置客户端，只要引入了spring-cloud-config-client 依赖即可。 思路也很清楚，就是在启动时从服务端把配置信息拉取到本地，然后设置到 Enviroment 中。Spring Cloud Config中有两种形式，一种是指定 url，另外一种是通过服务发现，默认是通过指定URI的方式。这里还是先从客户端的自动配置来分析。 12345678910111213141516171819202122232425262728293031323334353637383940414243@Configuration@EnableConfigurationPropertiespublic class ConfigServiceBootstrapConfiguration &#123; @Autowired private ConfigurableEnvironment environment; // 客户端配置属性 @Bean public ConfigClientProperties configClientProperties() &#123; ConfigClientProperties client = new ConfigClientProperties(this.environment); return client; &#125; // 从远程服务器上请求对应的配置信息 @Bean @ConditionalOnMissingBean(ConfigServicePropertySourceLocator.class) @ConditionalOnProperty(value = "spring.cloud.config.enabled", matchIfMissing = true) public ConfigServicePropertySourceLocator configServicePropertySource(ConfigClientProperties properties) &#123; ConfigServicePropertySourceLocator locator = new ConfigServicePropertySourceLocator( properties); return locator; &#125; // 重试机制 @ConditionalOnProperty(value = "spring.cloud.config.fail-fast") @ConditionalOnClass(&#123; Retryable.class, Aspect.class, AopAutoConfiguration.class &#125;) @Configuration @EnableRetry(proxyTargetClass = true) @Import(AopAutoConfiguration.class) @EnableConfigurationProperties(RetryProperties.class) protected static class RetryConfiguration &#123; @Bean @ConditionalOnMissingBean(name = "configServerRetryInterceptor") public RetryOperationsInterceptor configServerRetryInterceptor( RetryProperties properties) &#123; return RetryInterceptorBuilder .stateless() .backOffOptions(properties.getInitialInterval(), properties.getMultiplier(), properties.getMaxInterval()) .maxAttempts(properties.getMaxAttempts()).build(); &#125; &#125;&#125; 这个配置类中初始化了两个bean: ConfigClientProperties : 对客户端的属性进行配置。 ConfigServicePropertySourceLocator：从远程服务器上请求对应的配置信息，然后注册到容器的Enviroment 对象中去。 ConfigClientProperties 中就是客户端的一些属性，如：profile、应用名、标签、远端服务地址等。没有什么特殊的逻辑。主要来看下 ConfigServicePropertySourceLocator 。 ConfigServicePropertySourceLocatorConfigServicePropertySourceLocator 实现了 PropertySourceLocator 接口，PropertySourceLocator 接口的作用就是用来定位 PropertySource 的。直接看locate方法的实现(删除了无关代码)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Override@Retryable(interceptor = "configServerRetryInterceptor")public PropertySource&lt;?&gt; locate(Environment environment) &#123; ConfigClientProperties properties = this.defaultProperties.override(environment); CompositePropertySource composite = new CompositePropertySource("configService"); // 实例化一个 restTemplate，用来调用服务端的 API RestTemplate restTemplate = this.restTemplate == null ? getSecureRestTemplate(properties) : this.restTemplate; // ... try &#123; // labels ，对对应于profile 如，dev,pre,test这些 String[] labels = new String[] &#123; "" &#125;; if (StringUtils.hasText(properties.getLabel())) &#123; labels = StringUtils.commaDelimitedListToStringArray(properties.getLabel()); &#125; String state = ConfigClientStateHolder.getState(); // 遍历所有的标签，循环调用获取远程配置信息 for (String label : labels) &#123; // h获取远端环境配置信息 Environment result = getRemoteEnvironment(restTemplate, properties, label.trim(), state); if (result != null) &#123; log(result); // result.getPropertySources() can be null if using xml //使用 xml，可能会为 null if (result.getPropertySources() != null) &#123; for (PropertySource source : result.getPropertySources()) &#123; @SuppressWarnings("unchecked") Map&lt;String, Object&gt; map = (Map&lt;String, Object&gt;) source .getSource(); composite.addPropertySource( new MapPropertySource(source.getName(), map)); &#125; &#125; // 设置客户端状态和版本号信息 if (StringUtils.hasText(result.getState()) || StringUtils.hasText(result.getVersion())) &#123; HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;(); putValue(map, "config.client.state", result.getState()); putValue(map, "config.client.version", result.getVersion()); composite.addFirstPropertySource( new MapPropertySource("configClient", map)); &#125; return composite; &#125; &#125; &#125; catch (Exception e) &#123; // ... &#125; // 如果设置了fial fast ，失败时抛出异常 if (properties.isFailFast()) &#123; // ... &#125; // ... return null;&#125; 上面代码片段中实际从远端获取配置信息是在 getRemoteEnvironment 这个方法中，以Http 请求的方式获取。获取到配置信息之后是放在 CompositePropertySource 对象中，代码较长，逻辑也比较简单，建议直接阅读源码。源于这部分 自定义属性源也有说明。 注入到 Enviroment 中这部分操作是在 Spring Cloud Context 中的入口来完成的。具体参考 bootstrapServiceContext 创建&amp;启动 。这里会通过 Spring Cloud Context 中的 PropertySourceBootstrapConfiguration 配置类将PropertySourceLocator 自定义属性值添加到引导上下文的环境当中。 基于服务发现的方式获取配置前面两个小节均是基于指定 http url 的方式获取配置文件的。Spring Cloud Config 中还有一种方式就是基于服务发现的方式。其实这种方式说到底还是基于指定 http url的方式调用，只是通过服务发现找到服务端地址；当然既然有服务的发现与注册，也就会涉及到客户端与服务端之间的会话保证，及时更新可用服务列表这些功能。 获取服务地址 123456789101112@Retryable(interceptor = "configServerRetryInterceptor") public List&lt;ServiceInstance&gt; getConfigServerInstances(String serviceId) &#123; logger.debug("Locating configserver (" + serviceId + ") via discovery"); List&lt;ServiceInstance&gt; instances = this.client.getInstances(serviceId); if (instances.isEmpty()) &#123; throw new IllegalStateException( "No instances found of configserver (" + serviceId + ")"); &#125; logger.debug("Located configserver (" + serviceId + ") via discovery. No of instances found: " + instances.size()); return instances; &#125; 通过 DiscoveryClient 客户端，以指定serviceId的方式拿到服务地址。 DiscoveryClientConfigServiceBootstrapConfiguration 这个自动配置类实现了 ApplicationListener，用于监听上下文刷新事件；DiscoveryClient 在具体的实现中会将上下文刷新事件进行广播，然后执行刷新操作。心跳里面也是执行的刷新操作。对应的方法是DiscoveryClientConfigServiceBootstrapConfiguration#refresh。也就是 refresh方法会根据上下文环境和心跳事件，刷新服务实例。 以 ZK 作为配置中心《SpringCloud-配置中心 spring-cloud-zk》demo 中介绍了如何使用 zk 作为配置中心。以zk作为配置中心也就是配置信息将从zk中来获取；具体实现也就是实现 PropertySourceLocator 接口，在locate方法中通过zk客户端从zk服务端拉取配置信息。具体实现在ZookeeperPropertySourceLocator#locate中 12345678910@Override public PropertySource&lt;?&gt; locate(Environment environment) &#123; if (environment instanceof ConfigurableEnvironment) &#123; //省略 ... // 获取外部配置源 PropertySource propertySource = create(propertySourceContext); //省略 ... &#125; // .. &#125; 其他代码片段都省略了，获取 PropertySource 是在 create 方法中，create 方法返回一个 ZookeeperPropertySource 实例对象。在构造函数中，有通过zk客户端去拉取配置信息，具体逻辑在findProperties 方法中： 12345678910111213141516171819202122private void findProperties(String path, List&lt;String&gt; children) &#123; try &#123; // 省略 ... for (String child : children) &#123; String childPath = path + "/" + child; List&lt;String&gt; childPathChildren = getChildren(childPath); // 获取节点信息 byte[] bytes = getPropertyBytes(childPath); if (bytes == null || bytes.length == 0) &#123; if (childPathChildren == null || childPathChildren.isEmpty()) &#123; registerKeyValue(childPath, ""); &#125; &#125; else &#123; registerKeyValue(childPath, new String(bytes, Charset.forName("UTF-8"))); &#125; // 检查子节点，即使我们已经找到当前znode的值 findProperties(childPath, childPathChildren); &#125; &#125; catch (Exception exception) &#123; // 省略 ... &#125; &#125; 自动刷新机制当修改配置信息之后，通过zk自身的监听机制，通知客户端。这个机制是在ZookeeperConfigAutoConfiguration自动配置类中提供。 12345678910@Configuration @ConditionalOnClass(RefreshEndpoint.class) protected static class ZkRefreshConfiguration &#123; @Bean @ConditionalOnProperty(name = "spring.cloud.zookeeper.config.watcher.enabled", matchIfMissing = true) public ConfigWatcher configWatcher(ZookeeperPropertySourceLocator locator, CuratorFramework curator) &#123; return new ConfigWatcher(locator.getContexts(), curator); &#125; &#125; ConfigWatcher 实现了 Closeable、TreeCacheListener 和 ApplicationEventPublisherAware 三个接口。Tree Cache 用于观察所有节点的所有数据状态，ApplicationEventPublisherAware用户提供一个publiser，用来发布RefreshEvent 事件。Closeable 用于实现优雅关闭。 所有当我们改变zk数据节点时，就是触发例如 NODE_ADDED 、NODE_REMOVED、NODE_UPDATED 等事件类型，然后publiser就会发布一个 RefreshEvent 事件，通知客户端进行配置更新操作。从而实现配置的自动刷新。]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>Config</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud-配置中心 spring-cloud-config-zookeeper]]></title>
    <url>%2F2019%2F01%2F05%2Fspringboot%2Fspringcloud-config-zk-project%2F</url>
    <content type="text"><![CDATA[SpringCloud 除了config自己的client/server 这套配置中心之外，还可以集成使用 zookeeper 。本篇将演示如何使用 spring-cloud-confg-zookeeper。 环境准备 类别 值 JDK 1.8.0_162 SOFABoot/SpringBoot 3.0.0/2.0.x.RELEASE SpringCloud Finchley.RC1 IDE IDEA zk &amp; zkui这里我是把 zk 和 zkui 部署在一台 linux 服务器上的。 zk从 ZooKeeper官网 下载 zookeeper-3.4.13.tar.gz。 解压 1sudo tar -zxvf zookeeper-3.4.13.tar.gz 目录重命名(可选) 1sudo mv zookeeper-3.4.13 zookeeper 在 zookeeper 下加一个data目录 12&gt; cd zookeeper&gt; mkdir data 修改 zoo.cfg 1vim zoo.cfg 修改 dataDir 地址： 1dataDir=/$&#123;your path&#125;/zookeeper/data 其他随意，启动 zk 1zkServer.sh start zkui下载zkui代码，然后本地安装： 123$ git clone https://gitee.com/ilanni/zkui.git$ cd zkui/ $ mvn clean install # 进行maven打包，执行成功后会生成target文件夹，其中有jar文件。 执行结束后在zkui文件夹下生成一个target文件夹。 将config.cfg文件复制到target文件夹下 1cp config.cfg target/ target文件夹中有两个jar包，我们只需要启动zkui-2.0-SNAPSHOT-jar-with-dependencies.jar就可以了。 修改 config.cfg文件 12&gt; cd target&gt; vim config.cfg 按需修改serverPort、zkServer、userSet 等。 启动 12java -jar zkui-2.0-SNAPSHOT-jar-with-dependencies.jar# nohup java -jar zkui-2.0-SNAPSHOT-jar-with-dependencies.jar &amp; #退出窗口不退出进程 配置文件既然是以 zk 作为配置中，那么就需要将测试用的配置数据先在zk上进行初始化。有两种方式（均基于zkui）： zkui 界面通过 import 进行导入，这里新建一个 config.txt ，内容如下： 1/config/sofa/sofa-config-zk,dev=server.port=8085 设置当前应用启动的端口，这里的 root 为 /config/sofa，应用名是 sofa-config-zk，dev是环境 ，server.port=8085 是具体的配置kv。 手动 add node 这里为了方便，采用import的方式，结果如下： 新建 sofa-config-zookeeper新建一个 SOFABoot 工程，项目为 sofa-config-zookeeper。 依赖引入1234567891011121314&lt;!-- spring-cloud-config zk 的依赖，必选--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zookeeper-config&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- spring-cloud-config zk 的依赖，为了自动刷新监听等--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件1234567891011121314spring: application: name: sofa-config-zk profiles: active: dev cloud: zookeeper: enabled: true # true:开启zookeeper外部化配置, false:读取本地配置; connect-string: sofa.cloud.alipay.net:2181 config: root: /config/sofa #指定zookeeper中属性的根目录 enabled: true watcher: enabled: true #默认值是true, 监控配置变更后是否自动更新，需配合Spring Boot Actuators 使用 启动类123456@SpringBootApplicationpublic class SofaConfigZookeeperApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SofaConfigZookeeperApplication.class,args); &#125;&#125; 没有任何特殊，不需要加额外的注解。 资源类其实这里可以完全不用通过rest来打印这个属性值，但是为了方便看，还是写一下： 1234567891011@RestControllerpublic class ZookeeperConfigController &#123; @Value("$&#123;server.port&#125;") private String serverPort; @RequestMapping("/config") public String getConfig()&#123; return serverPort; &#125;&#125; 启动&amp;验证启动应用，如果成功的话，会有如下的日志：State change: CONNECTED tomcat 启动端口: 因为在上面配置文件部分是没有指定 server.port 的，通常情况下默认是 8080 ，所以可以确定，已经拿到了配置中心的数据了。 动态刷新这里还是需要依赖 actuator 的 /refresh 。上面依赖中已经加入了 actuator的相关依赖，所以只需要在资源类上加一个 @RefreshScope 注解即可。 在 ZookeeperConfigController 类上加 @RefreshScope 注解，然后重启应用 通过 zkui 修改 server.port 为 8086 访问 http://localhost:8085/config ，返回8086 需要注意，这里因为我们启动时应用时拿到的配置是8085，所以当前服务对外提供服务暴露的端口就是8085 ，当我们修改了zk上的值之后，他会改变 当前运行环境中 Enviroment 的值，但是不会使得服务的端口发生变化，除非重启。]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>Config</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 下安装 Mysql 数据库]]></title>
    <url>%2F2019%2F01%2F05%2Fmysql-on-linux%2F</url>
    <content type="text"><![CDATA[最近在搞 Apollo ，熟悉的同学应该知道，Apollo 需要依赖 Mysql。本以为很容易搞定，但是却踩了一路坑，眼高手低，也参考了网上很多博客，果然是残缺就是美！本篇就简单记录一下这个过程，以便后面参考。 环境 linux：centOS 7 jdk：8 Mysql：5.7.24 准备安装前，我们可以检测系统是否自带安装 MySQL: 1rpm -qa | grep mysql 如果你系统有安装，那可以选择进行卸载，有两种模式： 普通删除模式 1rpm -e mysql 强力删除模式 1rpm -e --nodeps mysql 如果使用上面命令删除时，提示有依赖的其它文件，则用该命令可以对其进行强力删除 安装 下载mysql安装包 1234567891011121314151617wget https://dev.mysql.com/get/mysql57-community-release-el7-9.noarch.rpm-2018-06-06 16:41:46-- https://dev.mysql.com/get/mysql57-community-release-el7-9.noarch.rpm正在解析主机 dev.mysql.com (dev.mysql.com)... xxxx正在连接 dev.mysql.com (dev.mysql.com)|xxxxx|:443... 已连接。已发出 HTTP 请求，正在等待回应... 302 Found位置：https://repo.mysql.com//mysql57-community-release-el7-9.noarch.rpm [跟随至新的 URL]--2018-06-06 16:41:48-- https://repo.mysql.com//mysql57-community-release-el7-9.noarch.rpm正在解析主机 repo.mysql.com (repo.mysql.com)... xxxxx正在连接 repo.mysql.com (repo.mysql.com)|xxxx|:443... 已连接。已发出 HTTP 请求，正在等待回应... 200 OK长度：9224 (9.0K) [application/x-redhat-package-manager]正在保存至: “mysql57-community-release-el7-9.noarch.rpm”100%[==========================================================&gt;] 9,224 --.-K/s 用时 0s 2018-06-06 16:41:48 (169 MB/s) - 已保存 “mysql57-community-release-el7-9.noarch.rpm” [9224/9224]) 安装 12345rpm -ivh mysql57-community-release-el7-9.noarch.rpm准备中... ################################# [100%]正在升级/安装... 1:mysql57-community-release-el7-9 ################################# [100%] 下载安装依赖包 123456789101112131415yum install mysql-server已加载插件：fastestmirror, langpacksmysql-connectors-community | 2.5 kB 00:00:00 mysql-tools-community | 2.5 kB 00:00:00 mysql57-community | 2.5 kB 00:00:00 (1/3): mysql-connectors-community/x86_64/primary_db | 20 kB 00:00:00 (2/3): mysql-tools-community/x86_64/primary_db | 41 kB 00:00:00 (3/3): mysql57-community/x86_64/primary_db | 144 kB 00:00:00 Loading mirror speeds from cached hostfile * base: mirrors.cn99.com * extras: mirrors.cn99.com * updates: mirrors.cn99.com正在解决依赖关系 .... 第一次下载这里会比较慢 验证是否安装成功 mysqladmin –version 1mysqladmin Ver 8.42 Distrib 5.7.22, for Linux on x86_64 mysql -V 1mysql Ver 14.14 Distrib 5.7.24, for Linux (x86_64) using EditLine wrapper 启动MySQL 1sudo systemctl start mysqld 查看 MySQL 运行状态 1sudo systemctl status mysqld 停止 MySQL 1sudo systemctl stop mysqld 重启 MySQL 1sudo systemctl restart mysqld 关于密码Mysql 5.7 默认安装之后 root 是有密码的，获取 MySQL 的临时密码 为了加强安全性，MySQL 5.7 为 root 用户随机生成了一个密码，在 error log 中，关于 error log 的位置，如果安装的是 RPM 包，则默认是 /var/log/mysqld.log 。 只有启动过一次 mysql 才可以查看临时密码。 在利用 YUM 安装 mysql 数据库过程中，系统会自动生成一个临时密码，获取方式为： 1grep &apos;temporary password&apos; /var/log/mysqld.log 没有密码 如果以前安装过 mysql，这时的 mysqld.log 中就不会有 temporary password这时就需要删除 mysql 残留的数据： 1rm -rf /var/lib/mysql 执行完毕后需要重新启动MySQL服务: 1sudo systemctl restart mysqld 这时就可以通过上面的命令去查找数据库生成的临时密码了 mysql 1130 错误可能是你的帐号不允许从远程登陆，只能在 localhost。这个时候只要在 localhost 的那台电脑，登入 mysql 后，更改 “mysql” 数据库里的 “user” 表里的 “host” 项，从”localhost”改称”%” 123mysql -u root -pvmwaremysql&gt;use mysql;mysql&gt; update user set host = '%' where user = 'root';mysql&gt; select host, user from user; 使得我们当前的账户和密码能够应用的所有的远程主机连接： 12&gt; GRANT ALL PRIVILEGES ON . TO 'myuser'@'%' IDENTIFIED BY 'mypassword' WITH GRANT OPTION;&gt; FLUSH PRIVILEGES;]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud-Spring Cloud Context]]></title>
    <url>%2F2018%2F12%2F31%2Fspringboot%2Fspringcoud-context-analysis%2F</url>
    <content type="text"><![CDATA[引导程序应用上下文 A Spring Cloud application operates by creating a “bootstrap” context, which is a parent context for the main application. It is responsible for loading configuration properties from the external sources and for decrypting properties in the local external configuration files. The two contexts share an Environment, which is the source of external properties for any Spring application. By default, bootstrap properties (not bootstrap.properties but properties that are loaded during the bootstrap phase) are added with high precedence, so they cannot be overridden by local configuration. The bootstrap context uses a different convention for locating external configuration than the main application context. Instead of application.yml (or .properties), you can use bootstrap.yml, keeping the external configuration for bootstrap and main context nicely separate. 释文：Spring Cloud 应用程序通过创建“引导程序”上下文来运行，该上下文是主应用程序的父上下文共享一个 Environment**，它是任何Spring应用程序的外部属性的来源。 默认情况下，引导属性（不是bootstrap.properties，而是在引导阶段加载的属性）以高优先级添加，因此本地配置无法覆盖它们。 引导上下文使用与主应用程序上下文不同的外部配置约定。 因此使用 bootstrap.yml application.yml（或.properties）代替引导和主上下文的外部配置，保持引导程序和主上下文的外部配置很好地分开。 上面是 SpringCloud 关于引导上下文的一个解释，详见 这里。 spring cloud 有自己的一套配置初始化机制，所以它实际上是自己启动了一个Spring 上下文，也就是我们说的引导上文。在上面的描述中有提到，引导上下文会以应用上下文的父类存在；在Spring中，如果上下文存在父子关系，也就意味着子上下文会集成父上下文的属性源和配置文件。在SpringBoot的启动过程中，prepareContext 这个操作会进行父子上下文的关系设置，调用栈如下: setParent 方法代码片段如下：123456789public void setParent(@Nullable ApplicationContext parent) &#123; this.parent = parent; if (parent != null) &#123; Environment parentEnvironment = parent.getEnvironment(); if (parentEnvironment instanceof ConfigurableEnvironment) &#123; this.getEnvironment().merge((ConfigurableEnvironment)parentEnvironment); &#125; &#125; &#125; 这个可以看到，子上下文会合并掉父上下文的 Environment 。关于父子上下文是怎么关联起来的，下面来看。12345678public void initialize(ConfigurableApplicationContext context) &#123; while(context.getParent() != null &amp;&amp; context.getParent() != context) &#123; context = (ConfigurableApplicationContext)context.getParent(); &#125; this.reorderSources(context.getEnvironment()); (new ParentContextApplicationContextInitializer(this.parent)).initialize(context);&#125; BootstrapApplicationListener上面的代码片段定位在 org.springframework.cloud.bootstrap.BootstrapApplicationListener 这个类；这个监听器监听的事件是 ApplicationEnvironmentPreparedEvent ，对应在SpringBoot启动过程，就是在执行 prepareEnvironment 时触发事件调用。 BootstrapApplicationListener 的 onApplicationEvent 回调方法中实际上就是用够构建和启动 Spring Cloud context 的。spring cloud context 算是一个特殊的 spring boot context， 在分析代码的过程中（bootstrapServiceContext方法中）发现，它只扫描 BootstrapConfiguration 这个注解标注的组件。 这里就着重分析下 SpringCloud Context 的启动过程。 SpringCloud Context 启动过程 通过 spring.cloud.bootstrap.enabled 配置来禁用引导上下文123if (!environment.getProperty("spring.cloud.bootstrap.enabled", Boolean.class,true)) &#123; return;&#125; 回调函数的开始就会对 spring.cloud.bootstrap.enabled 这个配置值进行校验，来决定是否需要禁止引导。这个在官方文档里面也有明确提到。 获取 configName12String configName = environment .resolvePlaceholders("$&#123;spring.cloud.bootstrap.name:bootstrap&#125;"); 可以使用 spring.cloud.bootstrap.name（默认“bootstrap”）或spring.cloud.bootstrap.location（默认为空）指定bootstrap.yml（或.properties）位置，例如在系统属性中。 bootstrapServiceContext 创建&amp;启动bootstrapServiceContext 是完成此过程的核心方法。 加载 BootstrapConfiguration 自动配置类 123456# Bootstrap componentsorg.springframework.cloud.bootstrap.BootstrapConfiguration=\org.springframework.cloud.bootstrap.config.PropertySourceBootstrapConfiguration,\org.springframework.cloud.bootstrap.encrypt.EncryptionBootstrapConfiguration,\org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration,\org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration PropertySourceBootstrapConfiguration 将会把 PropertySourceLocator 自定义属性值添加到引导上下文的环境当中，包括如何从远端仓库拉取配置等过程。 构建 SpringApplicationBuilder 类 123456SpringApplicationBuilder builder = new SpringApplicationBuilder() .profiles(environment.getActiveProfiles()).bannerMode(Mode.OFF) .environment(bootstrapEnvironment) // Don't use the default properties in this builder .registerShutdownHook(false).logStartupInfo(false) .web(WebApplicationType.NONE); 构建 引导上下文并 run 1final ConfigurableApplicationContext context = builder.run(); 这个 build.run 实际执行的就是 SpringApplication.run 方法。 为关联父子上下文准备1addAncestorInitializer(application, context); 这里会把 ParentContextApplicationContextInitializer 加到应用的 spring context 里，来把自己设置为应用的context 的 parent，具体是在SpringBoot启动过程的 prepareContext 中完成 。 重载远程属性通过Bootstrap 上下文添加到应用程序的属性源通常是远程的，比如说来自配置中心的，一般情况下本地的配置文件不能覆盖这些远程属性源。 那么如果想覆盖远程属性源怎么办呢？可以通过启动命令行参数方式设定（启动命令行参数的优先级高于远程配置的优先级）。 如果想使用应用程序的系统属性或者配置文件覆盖远程属性，那么远程属性源必须设置为 spring.cloud.config.allowOverride = true，这个配置在本地设置时不会生效的。在远程属性源中设定上述配置后，就可以通过更为细粒度的设置来控制远程属性是否能被重载，具体配置如下： 12345spring: cloud: config: overrideNone: true overrideSystemProperties: false overrideNone true，本地属性覆盖所有的远程属性 overrideSystemProperties ，仅覆盖远程属性源中的系统属性和环境变量 自定义 Bootstrap 属性源默认情况下，Bootstrap 的外部配置属性源是 spring cloud config server ，也就是使用配置中心加载外部属性，但是Spring中也允许用户通过将 ProoertySourceLocator 类型的Bean实例添加到 Bootstrap 上下文，也就是在 spring.factories 中添加相应的配置类，来添加额外的属性源来源。这里可以通过SpringCloud里面提供的测试用例来看下： 123456789101112131415protected static class PropertySourceConfiguration implements PropertySourceLocator &#123; // 省略其他代码 @Override public PropertySource&lt;?&gt; locate(Environment environment) &#123; if (this.name != null) &#123; assertEquals(this.name, environment.getProperty("spring.application.name")); &#125; if (this.fail) &#123; throw new RuntimeException("Planned"); &#125; return new MapPropertySource("testBootstrap", MAP); &#125; // 省略其他代码 &#125; 上面代码段中传入的Envirement 参数用于创建应用上下文，它具有 SpringBoot 提供的属性源，可以使用它们来加载指定的属性源。 最后将这个自定义的 PropertySourceLocator 配置到 spring.factories 中，这样应用程序就可以使用这个 PropertySourceConfiguration 作为其属性源了。 12org.springframework.cloud.bootstrap.BootstrapConfiguration=\xx.xx.x.x.PropertySourceConfiguration 关于Enviroment 的变化配置中心客户端（Spring Cloud Config Client） 应用会监听 EnviromentChangeEvent 事件，当监听到这个事件时，它将持有一个被改变的键值对列表，然后客户端应用会使用这些值来做一些事情： 重新绑定所有的@ConfigurationProperties的Bean@ConfigurationProperties 实例，更新本地的配置属性。 设置日志等级（logging.level.* 相关配置） Spring Cloud 中，配置中心服务端使用 Spring Cloud Bus 将EnviromentChangeEvent 事件广播到所有的客户端中，通过这种方式来通过它们 Enviroment 发生变化。 RefreshScope注解RefreshScope 注解的作用是，当被这个注解标记的Bean实例在配置发生变化时可以重新进行初始化，可参考 动态刷新配置 这个demo。这个注解很好的解决了状态Bean实例只能在初始化的时候才能进行属性注入的问题。 类org.springframework.cloud.context.scope.refresh.RefreshScope 是上下文中的一个Bean实例，在它的 refreshAll 这个方法中，可以通过清除目标缓存来刷新作用域中的所有Bean实例。RefreshScope中也提供了一个 refresh方法，可以按照名字来刷新单个Bean。]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
        <tag>commons</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud-配置中心 Config]]></title>
    <url>%2F2018%2F12%2F31%2Fspringboot%2Fspringcoud-config-project%2F</url>
    <content type="text"><![CDATA[简介在分布式系统中，每一个功能模块都能拆分成一个独立的服务，一次请求的完成，可能会调用很多个服务协调来完成，为了方便服务配置文件统一管理，更易于部署、维护，所以就需要一个地方来管理这些配置信息。 在 spring cloud Config 就提供了这样的能力，通过集中化管理的方式，支持配置文件放在在配置服务的内存中远程 Git 仓库以及Subversion。 本篇将通过一个简单的 demo ，使用 spring cloud Config 原生提供的基于 Git 的方式来实现微服务体系下的配置管理功能。 环境准备 类别 值 JDK 1.8.0_162 SOFABoot/SpringBoot 3.0.0/2.0.x.RELEASE SpringCloud Finchley.RC1 IDE IDEA 工程背景本节将通过 SOFABoot 来集成 Spring Cloud Config ，以 git 作为存储，来实现分布式环境下的配置管理。本工程的父工程仍然是《SpringCloud-Eureka 服务注册》中构建的父工程。 由于我们是以 git 来存储配置文件的，因此我们需要在 github 上新建一个存储配置文件的空间，为了更方面的模拟，这里创建了两个配置文件： glmapper-dev.properties 123name=leishu@glmapper_devblog=http://www.glmapper.comversion=dev glmapper-pre.properties 123name=leishu@glmapper_preblog=http://www.glmapper.comversion=pre github 地址：https://github.com/glmapper/glmapper-config-repository 新建 sofa-config-server右击 sofa-eureka-parent 父工程 -&gt; New -&gt; Module，这里选择 Maven 工程； artifactId：sofa-config-server 修改 pom 文件这里直接引入 config 的依赖即可 12345678910111213&lt;parent&gt; &lt;artifactId&gt;sofa-eureka-parent&lt;/artifactId&gt; &lt;groupId&gt;com.alipay.sofa&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/parent&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;artifactId&gt;sofa-config-server&lt;/artifactId&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置文件1234567891011121314151617181920212223#服务端口server: port: 8091 #服务名称spring: application: name: sofa-config-server #服务的git仓库地址 cloud: config: server: git: uri: https://github.com/glmapper/glmapper-config-repository search-paths: /** username: glmapper_2018@163.com password: ****** #指定分支 label: master#服务注册中心eureka: client: service-url: defaultZone: http://localhost:8761/eureka/ 如果你的 github 仓库是公开的话，就不需要输入账户和密码就可以访问。 启动类在启动类上加 @EnableConfigServer 注解，激活对配置中心的支持 1234567@SpringBootApplication@EnableConfigServerpublic class SofaConfigServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SofaConfigServerApplication.class,args); &#125;&#125; 启动 &amp; 验证因为配置中心作为一个独立的服务，所以不需要依赖其他服务的先启动，直接运行当前程序即可。这里我们首先需要验证下 server 端是否已经成功拉取到了 github 上面的配置信息： 访问：http://localhost:8091/glmapper/pre/master 12345678910111213141516171819&#123; "name": "glmapper", "profiles": [ "pre" ], "label": "master", "version": "f9e8c1f2825d23031cb13d40e396a23c0f975d2d", "state": null, "propertySources": [ &#123; "name": "https://github.com/glmapper/glmapper-config-repository/glmapper-pre.properties", "source": &#123; "name": "leishu@glmapper-pre", "blog": "http://www.glmapper.com", "version": "pre" &#125; &#125; ]&#125; OK ，说明服务端已经成功拉取到了github上的配置文件了。 关于地址的说明：http://localhost:8091/glmapper/pre/master 。前半部分是ip和端口，没什么好说的。glmapper/pre/master，因为我在github上新建的配置文件名是 glmapper-dev.properties 和 glmapper-pre .properties ;所以这里地址的规则就是 /glmapper/pre ，后面的 master 可带可不带，区别在于返回的 JSON 数据 label 是 null 还是 master，label 指向分支。 客户端本节构建一个简单的客户端工程 sofa-config-client ，用于从 sofa-config-server 上获取配置文件并展示。sofa-config-client 同样基于《SpringCloud-Eureka 服务注册》中构建的父工程。 修改 pom 文件12345678910111213141516171819&lt;parent&gt; &lt;artifactId&gt;sofa-eureka-parent&lt;/artifactId&gt; &lt;groupId&gt;com.alipay.sofa&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/parent&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;artifactId&gt;sofa-config-client&lt;/artifactId&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 引入 spring-cloud-starter-config 和 spring-boot-starter-web 依赖。 配置文件配置文件包括两个，一个是 application.yml ，另一个是 bootstrap.yml application.yml 12345spring: application: name: sofa-config-clientserver: port: 8099 bootstrap.yml 1234567spring: cloud: config: name: glmapper profile: pre uri: http://localhost:8091/ #指向配置中心的地址 label: master 启动类启动类不需要做什么修改，也不需要额外加什么注解123456@SpringBootApplicationpublic class SofaConfigClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SofaConfigClientApplication.class, args); &#125;&#125; 资源类新建一个资源类，用于输出展示拉取到的配置信息123456789101112131415161718@RestControllerpublic class ConfigClientController &#123; @Value("$&#123;name&#125;") private String name; @Value("$&#123;blog&#125;") private String blog; @Value("$&#123;version&#125;") private String version; @RequestMapping("/config") public String config()&#123; return "name:"+name+" ,blog:"+blog+" ,version:"+version; &#125;&#125; 启动和运行先后启动配置中心服务端和客户端程序。在浏览器中输入：http://localhost:8099/config ，返回如下：1name:leishu@glmapper-pre ,blog:http://www.glmapper.com ,version:pre 我们尝试下将 github 中 glmapper-pre.properties 这个配置文件进行修改，看下是否在这能获取到最新的依赖，修改之后，如下：123name=leishu@glmapper_pre_updateblog=http://www.glmapper.comversion=pre 重新刷新浏览器地址，返回如下：1name:leishu@glmapper-pre ,blog:http://www.glmapper.com ,version:pre 这里并没有发生任何变换，因为 SpringBoot 项目只会在项目启动时才会获取一次配置文件信息，当我们修改了 github 上的配置文件之后，当前的配置中心客户端并没有主动去获取配置值，所以不会有新的值，我们获取到的还是旧的值。那么下面通过修改和增加一些组件和配置来实现不停服动态更新配置。 配置动态更新要实现配置的动态更新，需要借助于 springboot 的 actuator 监控模块。所有需要在客户端pom文件中引入 actuator 的依赖信息。1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件 application.yml 中增加配置，将/actuator/refresh 断点暴露出来，注意不要配置在 boostrap.yml 中。12345678910spring: application: name: sofa-config-clientserver: port: 8099management: endpoints: web: exposure: include: refresh 资源类中开启更新机制，在 ConfigClientController 类中增加 @RefreshScope 注解，然后重启客户端。 首先执行：http://localhost:8099/config ，得到结果如下：1name:leishu@glmapper-pre ,blog:http://www.glmapper.com ,version:pre&lt;br /&gt; 更新 github 上配置文件的值，将 name 改为 name:leishu@glmapper-pre，通过 curl 或者 postman 执行下 刷新： 再次刷新执行 http://localhost:8099/config ，结果如下：1name:leishu@glmapper-pre-update ,blog:http://www.glmapper.com ,version:pre]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>Config</tag>
        <tag>spring cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud-负载均衡器 Ribbon]]></title>
    <url>%2F2018%2F12%2F31%2Fspringboot%2Fspringcoud-ribbon-project%2F</url>
    <content type="text"><![CDATA[上一篇 SpringCloud-声明式服务调用 Feign 中介绍了如何使用 Feign 来完成服务调用。因为 Feign 本身已经集成了 Ribbon ，所以也具有负载均衡的能力。那么本篇将使用 RestTemplate + Ribbon 来实现服务调用和负载均衡策略。 Ribbon 简介Ribbon 是管理HTTP和TCP服务客户端的负载均衡器。Ribbon 具有一些列带有名称的客户端，也就是带有名称的Ribbon 客户端。每个客户端由可配置的组件构成，负责一类服务的调用请求。Spring Cloud 通过RibbonClientConfiguration 为每个Ribbon 客户端创建一个ApplicationContext 上下文来进行组件装配。Ribbon 作为 Spring Cloud的负载均衡机制的实现，可以与OpenFeign 和 RestTemplate 进行无缝集成，让二者也具有负载均衡的能力。 负载均衡策略 策略类 命名 备注 RoundRobinRule 轮训策略 按顺序循环选择 Server RandomRule 随机策略 随机选择 Server RetryRule 重试策略 在一个配置时问段内当选择 Server 不成功，则一直尝试选择一个可用的 Server BestAvailableRule 最低并发策略 逐个考察 Server，如果 Server 断路器打开，则忽略，再选择其中并发连接最低的 Server AvailabilityFilteringRule 可用过滤策略 过滤掉一直连接失败并被标记为 circuit tripped 的 Server，过滤掉那些高并发连接的 Server（active connections 超过配置的网值） ResponseTimeWeightedRule 响应时间加权策略 根据 Server 的响应时间分配权重。响应时间越长，权重越低，被选择到的概率就越低；响应时间越短，权重越高，被选择到的概率就越高。这个策略很贴切，综合了各种因素，如：网络、磁盘、IO等，这些因素直接影响着响应时间 ZoneAvoidanceRule 区域权衡策略 综合判断 Server 所在区域的性能和 Server 的可用性轮询选择 Server，并且判定一个 AWS Zone 的运行性能是否可用，剔除不可用的 Zone 中的所有 Server 环境准备 类别 值 JDK 1.8.0_162 SOFABoot/SpringBoot 3.0.0/2.0.x.RELEASE SpringCloud Finchley.RC1 IDE IDEA 工程背景本节将会创建一个 sofa-eureka-consumer-Ribbon 工程，通过 Spring Cloud 提供的负载均衡器 Ribbon 实现服务的负载均衡，并对 Ribbon 中的负载均衡策略进行验证。 新建 sofa-eureka-consumer-ribbon本工程继续使用《SpringCloud-Eureka 服务注册》中的父工程来构建。 右击 sofa-eureka-parent 父工程 -&gt; New -&gt; Module，这里选择 Maven 工程； artifactId：sofa-eureka-consumer-ribbon 前面我们已经对feign进行的实际操作，因此本节使用 Ribbon + RestTemplate 组合实现具体的负载均衡实验。 修改 pom 文件1234567891011121314151617181920212223242526&lt;parent&gt; &lt;artifactId&gt;sofa-eureka-parent&lt;/artifactId&gt; &lt;groupId&gt;com.alipay.sofa&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/parent&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;artifactId&gt;sofa-eureka-consumer-ribbon&lt;/artifactId&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置文件123456789server: port: 8889eureka: client: service-url: defaultZone: http://localhost:8761/eureka/spring: application: name: eureka-consumer-ribbon 启动类这里需要引入 @EnableEurekaClient 注解，表示当前是一个客户端。 1234567891011121314@SpringBootApplication@EnableEurekaClientpublic class SofaEurekaConsumerRibbonApplication &#123; @Bean @LoadBalanced public RestTemplate restTemplate()&#123; return new RestTemplate(); &#125; public static void main(String[] args) &#123; SpringApplication.run(SofaEurekaConsumerRibbonApplication.class, args); &#125;&#125; @LoadBalanced ： Spring Cloud 为客户端负载均衡创建了特定的注解，被该注解修饰的 RestTemplate Bean实例，Spring Cloud 就会让 RestTemplate 使用相关的负载均衡策略，默认情况下使用的就是 Ribbon。 资源类这里我们通过 restTemplate 去访问 Provider 提供的服务，需要注意，这里为了演示作用，直接将资源 Url 固定写成：http://HELLOSOFASERVICE/hello ，HELLOSOFASERVICE 为 Provider 提供的服务的实例名称，也就是 Eureka 服务端界面上对应的 Application。 1234567891011@RestControllerpublic class RibbonController &#123; @Autowired private RestTemplate restTemplate; @RequestMapping("/hello") public String hello()&#123; return restTemplate.getForObject("http://HELLOSOFASERVICE/hello",String.class); &#125;&#125; 启动服务这里正常先后启动 服务注册中心 sofa-eureka-server-center ；服务提供方 sofa-eureka-provider ，服务提供方为了方便演示，这里启动4个实例，对应的端口分别为：8081，8082，8083，8084，如下： 然后启动当前 sofa-eurek-consumer-ribbon 工程。默认情况下，不指定任何负载均衡策略，使用的是轮询策略。 浏览器输入 http://localhost:8889/hello ，调用10次：123456789101112Hello SOFA! Now Port is 8081 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8082 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8083 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8084 And hostname is HelloSOFAService Hello SOFA! Now Port is 8081 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8082 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8083 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8084 And hostname is HelloSOFAService Hello SOFA! Now Port is 8081 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8082 And hostname is HelloSOFAService 从结果来看，默认策略应该是轮询（不用情况下，调用顺序不一定是1-2-3-4，但是以每4组为一组来看，存在周期性）。 负载均衡策略设置全局设置全局设置就是自己定义一个配置类，然后在配置类中指定具体的负载均衡策略。在com.alipay.sofa.cloud.configuration 包下面新建一个配置类，这里使用的策略是随机策略： 1234567@Configurationpublic class RibbonGlobalLoadBalancingConfiguration &#123; @Bean public IRule ribbonRule() &#123; return new RandomRule(); &#125;&#125; 浏览器输入 http://localhost:8889/hello ，调用10次： 123456789101112Hello SOFA! Now Port is 8083 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8084 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8084 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8083 And hostname is HelloSOFAService Hello SOFA! Now Port is 8082 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8083 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8082 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8084 And hostname is HelloSOFAService Hello SOFA! Now Port is 8081 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8081 And hostname is HelloSOFAService 从结果来看，具有随机属性。 针对单个服务的 Ribbon 负载均衡策略新建一个 RibbonRandomLBConfiguration 配置类，这里有个前提是需要删除 全局配置类 。 123456@Configurationpublic class RibbonRandomLBConfiguration &#123; @Bean public IRule ribbonRule() &#123; return new RandomRule(); &#125; 修改启动类，增加 @RibbonClient 注解，并且通过 configuration 指定负载均衡策略。 123456789101112131415@SpringBootApplication@EnableEurekaClient@RibbonClient(name="HELLOSOFASERVICE",configuration = RibbonRandomLBConfiguration.class)public class SofaEurekaConsumerRibbonApplication &#123; @Bean @LoadBalanced public RestTemplate restTemplate()&#123; return new RestTemplate(); &#125; public static void main(String[] args) &#123; SpringApplication.run(SofaEurekaConsumerRibbonApplication.class, args); &#125;&#125; 浏览器输入 http://localhost:8889/hello ，调用10次： 123456789101112Hello SOFA! Now Port is 8083 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8081 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8083 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8083 And hostname is HelloSOFAService Hello SOFA! Now Port is 8084 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8082 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8082 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8081 And hostname is HelloSOFAService Hello SOFA! Now Port is 8081 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8083 And hostname is HelloSOFAService 从结果来看，具有随机属性。 @RibbonClient 注解属性中，name 并非是一个数组，也就是说只能指定一个服务实例。那么基于上述情况，如果还存在另外一个服务，比如 SOFABOOTHELLOSERVICE ，那么对于此服务的调用会是什么情况呢？ 先向注册中心注册两个服务：HELLOSOFABOOTSERVICE 和 HELLOSOFASERVICE** 修改 RibbonController ，增加一个 /helloBoot 资源地址： 12345678910111213141516@RestControllerpublic class RibbonController &#123; @Autowired private RestTemplate restTemplate; @RequestMapping("/hello") public String hello()&#123; return restTemplate.getForObject("http://HELLOSOFASERVICE/hello",String.class); &#125; @RequestMapping("/helloBoot") public String helloBoot()&#123; return restTemplate.getForObject("http://HELLOSOFABOOTSERVICE/hello",String.class); &#125;&#125; 重启启动当前服务。 浏览器中输入：http://localhost:8889/hello ，验证结果满足随机调用。 浏览器中输入：http://localhost:8889/helloBoot ，验证结果满足轮询调用。 基于配置文件的负载均衡策略设置个人感觉基于配置文件配置方式更加直观，而且对于多个服务对应不同的负载策略设置也更加清晰，下面对HELLOSOFASERVICE 和 HELLOSOFABOOTSERVICE 均使用随机策略。 1234567HELLOSOFASERVICE: ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRuleHELLOSOFABOOTSERVICE: ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule 启动类中删除以下注解配置： 1@RibbonClient(name = "HELLOSOFASERVICE", configuration = RibbonRandomLBConfiguration.class) 重启启动当前服务。 浏览器中输入：http://localhost:8889/hello ，验证结果满足随机调用。浏览器中输入：http://localhost:8889/helloBoot ，验证结果满足随机调用。]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
        <tag>Ribbon</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud-声明式服务调用 Feign]]></title>
    <url>%2F2018%2F12%2F31%2Fspringboot%2Fspringcoud-feign-project%2F</url>
    <content type="text"><![CDATA[简介Fegin 是一个声明式的 web 服务客户端，它使得编写 web 服务客户端变得更加容易。使用 Fegin 创建一个接口并对它进行注解。它具有可插拔的注解支持包括 Feign 注解与 JAX-RS 注解，Feign 还支持可插拔的编码器与解码器，Spring Cloud 增加了对 Spring MVC 的注解，Spring Web 默认使用了 HttpMessageConverters。Fegin 还可以集成 Ribbon 和 Hystrix 来提供负载均衡和网络断路器的功能。 本篇将使用 Fegin + eureka client 来完成服务发现和调用。 环境准备 类别 值 JDK 1.8.0_162 SOFABoot/SpringBoot 3.0.0/2.0.x.RELEASE SpringCloud Finchley.RC1 IDE IDEA 工程背景本节将会创建一个 sofa-eureka-consumer-feign 工程，使用 Feign 提供的 web 客户端来访问 sofa-eureka-provider 发布的服务。同时也基于此工程验证基于 Feign 实现的负载均衡。 新建 sofa-eureka-consumer-feign本工程继续使用《SpringCloud-Eureka 服务注册》中的父工程来构建。 右击 sofa-eureka-parent 父工程 -&gt; New -&gt; Module，这里选择 Maven 工程； artifactId：sofa-eureka-consumer-feign 修改pom文件123456789101112131415161718192021222324&lt;parent&gt; &lt;artifactId&gt;sofa-eureka-parent&lt;/artifactId&gt; &lt;groupId&gt;com.alipay.sofa&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/parent&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;artifactId&gt;sofa-eureka-server-center&lt;/artifactId&gt;&lt;dependencys&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencys&gt; 修改配置文件123server.port=8888eureka.client.service-url.defaultZone=http://localhost:8761/eureka/spring.application.name=eureka-consumer-feign 启动类这里需要添加 @EnableEurekaClient 和 @EnableFeignClients 两个注解。 12345678@SpringBootApplication@EnableEurekaClient@EnableFeignClientspublic class SofaEurekaConsumerFeignApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SofaEurekaConsumerFeignApplication.class, args); &#125;&#125; 资源类 com.alipay.sofa.cloud.service 包下新建 HelloSOFAService 12345@FeignClient("helloSOFAService")public interface HelloSOFAService &#123; @RequestMapping(value = "/hello", method = RequestMethod.GET) String hello();&#125; com.alipay.sofa.cloud.controller 包下新建 FeignController 12345678910@RestControllerpublic class FeignController &#123; @Autowired private HelloSOFAService helloSOFAService; @RequestMapping("/hello") public String hello()&#123; return helloSOFAService.hello(); &#125;&#125; 启动 &amp; 验证启动当前工程，在此之前请以此启动 注册中心 sofa-eureka-server-center 和 sofa-eureka-provider 两个工程。 注：这里我启动了两个 provider 工程 浏览器输入：http:localhost:8888/hello，观察到浏览器中依次展示：1234Hello SOFA! Now Port is 8081 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8082 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8081 And hostname is HelloSOFAServiceHello SOFA! Now Port is 8082 And hostname is HelloSOFAService 这里可以看待 通过 feign 提供的客户端能力已经访问到了远程服务，由于 feign 集成了 ribbon 因此也就默认实现了负载均衡的能力。从结果来看，默认的负载均衡策略是轮询。]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
        <tag>Feign</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-Eureka Client 原理解析]]></title>
    <url>%2F2018%2F12%2F31%2Fspringboot%2Fspringcoud-eureka-client-analysis%2F</url>
    <content type="text"><![CDATA[前面一些 demo 中已经介绍了如何使用 SOFABoot 来集成 Spring Cloud Netflix Eureka 组件。本篇将来先解析下 Eureka Client 的工作原理。 Netflix 和 SpringCloudspring-cloud-commons 模块是 spring 在分布式领域上(服务发现，服务注册，断路器，负载均衡)的规范定义。spring-cloud-netflix 是基于此规范的具体实现，Netflix OSS 里的各种组件也都实现了这个 commons 规范。关系如下： Spring Cloud Netflix Eureka 服务发现实现原理基于上图，这里以 Eureka 中的服务发现为例，来具体讲下是如何实现的。Spring Cloud common 中提供了用于服务发现的两个关键类：DiscoveryClient 接口 和 EnableDiscoveryClient 注解。 DiscoveryClient 接口下面这张图描述的是在服务发现这个功能上，SpringCloud 是如何与 Netflix 整合的。在 spring-cloud-netflix-eureka-client 中对 Spring Cloud Common 中的 DiscoveryClient 接口进行了实现，实现类是 EurekaDiscoveryClient 。 DiscoveryClient 的接口定义与方法： 123456789101112131415161718192021222324/** * DiscoveryClient表示服务发现常用的读取操作，例如Netflix Eureka或consul.io * @author Spencer Gibb */public interface DiscoveryClient &#123; /** * 实现描述 * @return the description */ String description(); /** * 获取与特定serviceId关联的所有ServiceInstances * @param serviceId the serviceId to query * @return a List of ServiceInstance */ List&lt;ServiceInstance&gt; getInstances(String serviceId); /** * 返回所有已知的服务ID */ List&lt;String&gt; getServices();&#125; EurekaDiscoveryClient 中实现了这几个方法，但是 EurekaDiscoveryClient 自身没有实现如何与服务端交互的逻辑，而是通过 com.netflix.DiscoveryClient 类来完成。所以 spring-cloud-netflix-eureka-client 干的事情就是实现了 Spring Cloud Common 规范，然后在实现上包装了 netflix 。 @EnableDiscoveryClient 注解123456789@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@Import(EnableDiscoveryClientImportSelector.class)public @interface EnableDiscoveryClient &#123; //是否自动注册，默认是true。 boolean autoRegister() default true;&#125; EnableDiscoveryClientImportSelector 将会从 META-INF/spring.factories 里找出 key 为org.springframework.cloud.client.discovery.EnableDiscoveryClient 的类。 对于 autoRegister ： 如果自动注册属性为true，会在找出的这些类里再加上一个类：AutoServiceRegistrationConfiguration， AutoServiceRegistrationConfiguration 内部会使用@EnableConfigurationProperties(AutoServiceRegistrationProperties.class) 触发构造AutoServiceRegistrationProperties 这个 bean。像eureka，nacos，它们的自动化配置类里都使用了@ConditionalOnBean(AutoServiceRegistrationProperties.class) 来确保存在AutoServiceRegistrationProperties 这个 bean 存在的时候才会构造 AutoServiceRegistration 进行注册。 如果自动注册属性为 false，在Environment 里加一个 PropertySource，内部的配置项是spring.cloud.service-registry.auto-registration.enabled，值是false(代表不构造AutoServiceRegistrationProperties.class)。这样 eureka 就不会注册。 对应上面这段逻辑的代码如下： spring-cloud-netflix-eureka-client 自己也提供了一个注解 EnableEurekaClient，其作用于这个注解一样 Eureka 架构图 consumer : 服务消费方，eureka client 角色，可以从 eureka server 上拉取到其他已注册服务的信息，从而根据这些信息找到自己所需的服务，然后发起远程调用。 provider : 服务提供方，eureka client 角色，可以向 eureka server 上注册和更新自己的信息，当然作为 eureka client ，它也可以从server 上获取到其他服务的信息。 Eureka server : 服务注册中心，提供服务注册和服务发现功能； 同步复制 ： eureka server 之间进行注册服务信息的同步，这样可以保证集群中每个server 都能提供完整的服务信息。 关于 AWS 上 Regin 和 Availability Zone 的概念，请自行查阅相关资料 源码解析配置信息读取Eureka Client的自动配置类是 org.springframework.cloud.netflix.eureka.EurekaClientAutoConfiguration ，这里面主要就负责了一些配置信息的服务诸如 DiscoveryClient 、EurekaServiceRegistry等主要bean的初始化工作。 另外还有一个 EurekaDiscoveryClientConfiguration 类，负责配置自动注册和应用的健康检查器初始化。 读取 eureka.client.*12345678910@Bean@ConditionalOnMissingBean(value = EurekaClientConfig.class, search = SearchStrategy.CURRENT)public EurekaClientConfigBean eurekaClientConfigBean(ConfigurableEnvironment env) &#123; EurekaClientConfigBean client = new EurekaClientConfigBean(); if ("bootstrap".equals(this.env.getProperty("spring.config.name"))) &#123; // 默认情况下，我们不会在引导过程中注册，但是以后会有另一个机会。 client.setRegisterWithEureka(false); &#125; return client;&#125; EurekaClientConfigBean 封装的是 eureka client 和 eureka server 交互所需要的配置信息，比如前面demo工程中的 eureka.client.service-url.defaultZone 的配置。 读取 eureka.instance.*123456@Bean@ConditionalOnMissingBean(value = EurekaInstanceConfig.class, search = SearchStrategy.CURRENT)public EurekaInstanceConfigBean eurekaInstanceConfigBean(InetUtils inetUtils, ManagementMetadataProvider managementMetadataProvider) &#123; // 代码较长，此处省略&#125; EurekaInstanceConfigBean 封装的是 eureka client 自身实例的配置信息，提供服务注册的基本元数据信息。 核心组件 bean 初始化这里也实例化了一些核心的组件bean。 ApplicationInfoManager EurekaClientConfiguration#eurekaApplicationInfoManager 1234567@Bean@ConditionalOnMissingBean(value = ApplicationInfoManager.class, search = SearchStrategy.CURRENT)public ApplicationInfoManager eurekaApplicationInfoManager(EurekaInstanceConfig config) &#123; InstanceInfo instanceInfo = new InstanceInfoFactory().create(config); return new ApplicationInfoManager(config, instanceInfo);&#125; RefreshableEurekaClientConfiguration#eurekaApplicationInfoManager12345678@Bean@ConditionalOnMissingBean(value = ApplicationInfoManager.class, search = SearchStrategy.CURRENT)@org.springframework.cloud.context.config.annotation.RefreshScope@Lazypublic ApplicationInfoManager eurekaApplicationInfoManager(EurekaInstanceConfig config) &#123; InstanceInfo instanceInfo = new InstanceInfoFactory().create(config); return new ApplicationInfoManager(config, instanceInfo);&#125; RefreshScope ，被此注解标注的情况下，将会被动态刷新。包括属性信息等，注意，对于动态刷新，被RefreshScope标记的类不能是final的。 ApplicationInfoManager 是应用信息管理器，用于管理服务实例的信息类 InstanceInfo 和服务实例的配置信息类 EurekaInstanceConfig 。 DiscoveryClient1234@Beanpublic DiscoveryClient discoveryClient(EurekaInstanceConfig config, EurekaClient client) &#123; return new EurekaDiscoveryClient(config, client);&#125; DiscoveryClient ，前面说到，这个类是Spring Cloud 中用于服务发现使用的客户端接口。注意这里是SpringCloud提供的接口，不是netflix中的类。 EurekaServiceRegistry1234@Beanpublic EurekaServiceRegistry eurekaServiceRegistry() &#123; return new EurekaServiceRegistry();&#125; EurekaServiceRegistry 是 ServiceRegistry 的实现类。ServiceRegistry 是 SpringCloud 提供了注册和注销等方法，这些方法允许用户提供自定义注册服务。 EurekaRegistration12345678910@Bean @ConditionalOnBean(AutoServiceRegistrationProperties.class) @ConditionalOnProperty(value = "spring.cloud.service-registry.auto-registration.enabled", matchIfMissing = true) public EurekaRegistration eurekaRegistration(EurekaClient eurekaClient, CloudEurekaInstanceConfig instanceConfig, ApplicationInfoManager applicationInfoManager, ObjectProvider&lt;HealthCheckHandler&gt; healthCheckHandler) &#123; return EurekaRegistration.builder(instanceConfig) .with(applicationInfoManager) .with(eurekaClient) .with(healthCheckHandler) .build(); &#125; 每个 ServiceRegistry 实现都有自己的 Registry 实现。 ZookeeperRegistration -&gt; ZookeeperServiceRegistry ZookeeperRegistration -&gt; EurekaServiceRegistry ConsulRegistration -&gt; ConsulServiceRegistry 如果你需要自定义实现 ServiceRegistry ，则也不要提供一个 Registration 的实现。 服务发现服务发现的基本情况在上面已经提到了，但是由于 SpingCloud 中并没有提供具体的交互操作而是由 com.netflix.discovery.DiscoveryClient 来完成具体工作。所以关于服务服务发现这里就直接围绕这个类来展开。 LookopService123456789101112131415public interface LookupService&lt;T&gt; &#123; // 根据服务实例注册的appName 来获取 Application Application getApplication(String appName); // 返回当前注册表中所有的服务实例信息 Applications getApplications(); // 根据服务实例Id获取服务实例信息 List&lt;InstanceInfo&gt; getInstancesById(String id); /** * 获取下一个可能的服务器，以处理来自从eureka接收到的注册表信息的请求。 * @virtualHostname 与服务器关联的虚拟主机名。 * @secure 指示是HTTP还是HTTPS请求 * */ InstanceInfo getNextServerFromEureka(String virtualHostname, boolean secure);&#125; LookupService 接口的作用就是用于查找活动服务实例；总共提供了四个方法，很好理解。每个方法的作用见注释。 EurekaClientEurekaClient 也是一个接口，集成并且扩展了 LookupService。 This interface does NOT try to clean up the current client interface for eureka 1.x. Rather it triesto provide an easier transition path from eureka 1.x to eureka 2.x.从这来看，EurekaClient 的存在是为了给 Eureka1.x 向 Eureka 2.x 升级提供容错能力。 EurekaClient 在 LookupService 基础上扩展了很多方法，如下：1234567891011121314151617public interface EurekaClient extends LookupService &#123; // 省去@Deprecated方法和获取服务实例信息的接口方法 // 注册健康检查处理器 public void registerHealthCheck(HealthCheckHandler healthCheckHandler); // 监听client服务信息的更新 public void registerEventListener(EurekaEventListener eventListener); // 取消监听 public boolean unregisterEventListener(EurekaEventListener eventListener); // 获取当前健康检查处理器 public HealthCheckHandler getHealthCheckHandler(); // 关闭 eureka 客户端。还向eureka服务器发送撤销注册请求。 public void shutdown(); // EurekaClientConfig public EurekaClientConfig getEurekaClientConfig(); // ApplicationInfoManager public ApplicationInfoManager getApplicationInfoManager();&#125; HealthCheckHandler 这个是用于检查当前客户端状态的，这个在后面心跳机制里面会说道。 DiscoveryClientcom.netflix.discovery.DiscoveryClient，这个类会在构造函数中完成一系列重要的操作，如：拉取注册表信息，服务注册，初始化心跳机制，缓存刷新，按需注册定时任务等等。123456DiscoveryClient(ApplicationInfoManager applicationInfoManager, EurekaClientConfig config, AbstractDiscoveryClientOptionalArgs args, Provider&lt;BackupRegistry&gt; backupRegistryProvider) &#123;// ... &#125; 几个参数的释义如下： applicationInfoManager ：应用信息管理器 config ：client 与 server 交互的配置信息 args ：客户端提供的过滤器类型(支持jersey1和jersey2)，后面用来构建 EurekaTransport backupRegistryProvider ： 备份注册中心 服务发现下面代码片段也是在 DiscoveryClient 的构造函数里面的，这里就是拉取注册服务信息的逻辑： 123if (clientConfig.shouldFetchRegistry() &amp;&amp; !fetchRegistry(false)) &#123; fetchRegistryFromBackup();&#125; clientConfig.shouldFetchRegistry() 这个方法拿到的就是配置文件中 eureka.client.fetch-registry 的值，默认为true，表示从 eureka server 拉取注册表信息。 fetchRegistry(boolean)是从 eureka server 拉取注册信息的方法，参数用于表示是否是强制拉取全量的注册信息；此方法除非在协调eureka服务器和客户端注册表信息方面存在问题，否则此方法只尝试在第一次进行全量获取，后面均是增量获取。 fetchRegistryFromBackup() 如果 eureka server 服务不可用，则采用的备用方案。 底层通信实现 EurekaTransportEurekaTransport 是 DiscoveryClient 的内部类，EurekaTransport 封装了具体的基于 jersey 的底层通信实现。 FetchRegistry上图为拉取注册信息的整个过程。对于黄色贴条上的条件，如果满足其中一个，则都会进行全量拉取；否则进行增量拉取。计算 hash 值是为了后面可以与server端应用信息的进行对比，用于感知是否需要重新进行拉取操作。 服务注册服务注册逻辑也是在 DiscoveryClient 的构造函数中完成，代码片段如下： 12345678910if (clientConfig.shouldRegisterWithEureka() &amp;&amp; clientConfig.shouldEnforceRegistrationAtInit()) &#123; try &#123; if (!register() ) &#123; throw new IllegalStateException("Registration error at startup. Invalid server response."); &#125; &#125; catch (Throwable th) &#123; logger.error("Registration error at startup: &#123;&#125;", th.getMessage()); throw new IllegalStateException(th); &#125;&#125; 向server端注册需要满足的两个条件是：1、允许向server端注册 2、是否在客户端初始化期间强制注册1234567891011121314boolean register() throws Throwable &#123; logger.info(PREFIX + "&#123;&#125;: registering service...", appPathIdentifier); EurekaHttpResponse&lt;Void&gt; httpResponse; try &#123; httpResponse = eurekaTransport.registrationClient.register(instanceInfo); &#125; catch (Exception e) &#123; logger.warn(PREFIX + "&#123;&#125; - registration failed &#123;&#125;", appPathIdentifier, e.getMessage(), e); throw e; &#125; if (logger.isInfoEnabled()) &#123; logger.info(PREFIX + "&#123;&#125; - registration status: &#123;&#125;", appPathIdentifier, httpResponse.getStatusCode()); &#125; return httpResponse.getStatusCode() == 204;&#125; 通过 eurekaTransport 对象，基于 REST 调用向 eureka server 进行服务注册。 心跳机制心跳机制的初始化工作也是在 DiscoveryClient 构造函数中完成。在DiscoveryClient构造函数的最后，有一个初始化调度任务的方法，在这个方法里就包括心跳的初始化。 heartbeatExecutor 心跳线程池： 1234567heartbeatExecutor = new ThreadPoolExecutor( 1, clientConfig.getHeartbeatExecutorThreadPoolSize(), 0, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;(), new ThreadFactoryBuilder() .setNameFormat(&quot;DiscoveryClient-HeartbeatExecutor-%d&quot;) .setDaemon(true) .build() scheduler 提交周期执行：123456789101112// Heartbeat timerscheduler.schedule( new TimedSupervisorTask( "heartbeat", scheduler, heartbeatExecutor, renewalIntervalInSecs, TimeUnit.SECONDS, expBackOffBound, new HeartbeatThread() ), renewalIntervalInSecs, TimeUnit.SECONDS); TimedSupervisorTask 是 eureka 中自动调节间隔的周期性任务类。HeartbeatThread 是具体执行任何的线程，run方法中执行的就是 renew() 续期。 1234567891011121314151617181920212223242526boolean renew() &#123; EurekaHttpResponse&lt;InstanceInfo&gt; httpResponse; try &#123; // 通过 eurekaTransport 来与 server 通信续期 httpResponse = eurekaTransport.registrationClient.sendHeartBeat(instanceInfo.getAppName(), instanceInfo.getId(), instanceInfo, null); logger.debug(PREFIX + "&#123;&#125; - Heartbeat status: &#123;&#125;", appPathIdentifier, httpResponse.getStatusCode()); // 404 标识当前服务实例不存在 if (httpResponse.getStatusCode() == 404) &#123; // 记录心跳次数 REREGISTER_COUNTER.increment(); logger.info(PREFIX + "&#123;&#125; - Re-registering apps/&#123;&#125;", appPathIdentifier, instanceInfo.getAppName()); long timestamp = instanceInfo.setIsDirtyWithTime(); // 重新注册 boolean success = register(); if (success) &#123; instanceInfo.unsetIsDirty(timestamp); &#125; return success; &#125; // 200 状态正常 return httpResponse.getStatusCode() == 200; &#125; catch (Throwable e) &#123; logger.error(PREFIX + "&#123;&#125; - was unable to send heartbeat!", appPathIdentifier, e); return false; &#125;&#125; 服务下线关闭 eureka client，还向 eureka server 发送撤销注册请求。该方法在DiscoveryClient#shutdown 方法中。 123456789101112131415161718192021222324252627282930@PreDestroy @Override public synchronized void shutdown() &#123; // 保证原子操作 if (isShutdown.compareAndSet(false, true)) &#123; logger.info("Shutting down DiscoveryClient ..."); if (statusChangeListener != null &amp;&amp; applicationInfoManager != null) &#123; // 应用管理器取消状态监听 applicationInfoManager.unregisterStatusChangeListener(statusChangeListener.getId()); &#125; // 清理任务调度执行 cancelScheduledTasks(); // If APPINFO was registered if (applicationInfoManager != null &amp;&amp; clientConfig.shouldRegisterWithEureka() &amp;&amp; clientConfig.shouldUnregisterOnShutdown()) &#123; //设置服务实例状态为 DOWN applicationInfoManager.setInstanceStatus(InstanceStatus.DOWN); //注销注册 unregister(); &#125; // 关闭 jersey 客户端 if (eurekaTransport != null) &#123; eurekaTransport.shutdown(); &#125; heartbeatStalenessMonitor.shutdown(); registryStalenessMonitor.shutdown(); logger.info("Completed shut down of DiscoveryClient"); &#125; &#125;]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
        <tag>Eureka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud-Eureka 服务发现]]></title>
    <url>%2F2018%2F12%2F31%2Fspringboot%2Fspringcoud-eureka-discovery%2F</url>
    <content type="text"><![CDATA[本篇将继续接着上一篇 SpringCloud-服务注册 ，通过使用 DiscoveryClient 来实现服务发现，并且消费。 DiscoveryClient 源自于 spring-cloud-client-discovery ，是 spring cloud 中被定义用来服务发现的公共接口，在 spring cloud 的各类服务发现组件中，都有对应的实现，如 eureka、consul、zookeeper 。它提供从服务注册中心获取服务实例信息的能力。如果我们想自己实现一个服务发现组件，集成到spring cloud 中，就完全可以通过实现此接口来完成。 环境准备 类别 值 JDK 1.8.0_162 SOFABoot/SpringBoot 3.0.0/2.0.x.RELEASE SpringCloud Finchley.RC1 IDE IDEA 工程背景本案例使用 SOFABoot 3.0.x 版本集成 SringCloud F版。工程如下： sofa-eureka-consumer-discovery 服务消费方 本工程的父工程继续使用《SpringCloud-Eureka 服务注册》文中新建的父工程。 新建 sofa-eureka-consumer-discovery这里我们通过 sofa-eureka-consumer-discovery 这个工程来手动发现服务。 右击 sofa-eureka-parent 父工程 -&gt; New -&gt; Module，这里选择 Maven 工程； artifactId：sofa-eureka-consumer-discovery。 修改 pom 文件引入 spring-cloud-starter-netflix-eureka-client 依赖。1234567891011121314151617181920 &lt;parent&gt; &lt;artifactId&gt;sofa-eureka-parent&lt;/artifactId&gt; &lt;groupId&gt;com.alipay.sofa&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/parent&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;artifactId&gt;sofa-eureka-consumer-discovery&lt;/artifactId&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置文件123server.port=8088spring.application.name=sofa-eureka-discoveryeureka.client.serviceUrl.defaultZone=http://localhost:8761/eureka/ 启动类1234567@SpringBootApplication@EnableEurekaClientpublic class SofaEurekaConsumerDiscoveryApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SofaEurekaConsumerDiscoveryApplication.class, args); &#125;&#125; 服务获取这里通过 DiscoveryClient 对像手动获取到 HELLOSOFASERVICE 服务对应的所有实例。 123456789101112131415161718192021@RestControllerpublic class DiscoveryController &#123; @Autowired private DiscoveryClient discoveryClient; @RequestMapping("/instance") public String getInstance()&#123; List&lt;ServiceInstance&gt; list = discoveryClient.getInstances("HELLOSOFASERVICE"); System.out.println("current service size = " + discoveryClient.getServices().size()); StringBuilder stringBuilder = new StringBuilder(); for( String s : discoveryClient.getServices())&#123; stringBuilder.append("services=" + s).append("\n"); List&lt;ServiceInstance&gt; serviceInstances = discoveryClient.getInstances(s); for(ServiceInstance si : serviceInstances)&#123; stringBuilder.append("url=").append(si.getUri()).append("\n"); &#125; &#125; stringBuilder.append("instance num").append("=").append(list.size()); return stringBuilder.toString(); &#125;&#125; 启动 &amp; 验证启动当前工程，在此之前确保 注册中心和服务提供工程均已正常启动。然后在浏览器中输入：http:localhost:8088/instance 可以看到获取到的实例信息与注册中心上的实例信息是匹配的。]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
        <tag>Eureka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud-Eureka 服务注册]]></title>
    <url>%2F2018%2F12%2F31%2Fspringboot%2Fspringcoud-eureka-register%2F</url>
    <content type="text"><![CDATA[简介Spring Cloud Netflix Eureka 是 Spring Cloud 提供的用于服务注册和发现的基础组件，在 Spring Cloud 微服务体系中承担着相当重要的角色。Eureka 作为一个开箱即用的基础组件，其屏蔽了底层 Client 和 Server 交互的细节，使得开发者能够快速入手，将更多的精力投入到业务逻辑上去。 Eureka 是基于 Rest 实现的，及底层客户端和服务端之间的交互是通过 Rest 服务进行交互的。Eureka 包括两个部分，即服务端可客户端。 服务端：Eureka Server ,提供服务注册和发现的功能 客户端：Eureka Client ,将自己的信息注册到 Eureka Server ，并从 Eureka Server 中发现其他服务。 Eureka 本篇将先来搭建一个服务端，以作为后续篇幅的注册中心来使用。 环境准备 类别 值 JDK 1.8.0_162 SOFABoot/SpringBoot 3.0.0/2.0.x.RELEASE SpringCloud Finchley.RC1 IDE IDEA 工程背景本案例使用 SOFABoot 3.0.x 版本集成 SringCloud F版。工程如下： sofa-eureka-server 服务注册中心 sofa-eureka-provider 服务提供方 sofa-eureka-comsumer 服务消费方 本工程都是在同一个父工程下面的，因此工程构建开始会新建一个 SOFABoot 工程作为父工程。 新建父工程这里父工程直接新建一个SpringBoot 工程。可以使用 IDEA 的生成，也可以通过 SOFABoot 快速开始 新建一个 SpringBoot 工程，删除 src 目录，然后修改 pom.xml 文件。 gourpId : com.alipay.sofa artifactId : sofa-eureka-parent parent 依赖修改123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; 替换为： 12345&lt;parent&gt; &lt;groupId&gt;com.alipay.sofa&lt;/groupId&gt; &lt;artifactId&gt;sofaboot-dependencies&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt;&lt;/parent&gt; 管控 SpringCloud 依赖在主 pom 里面加入 SpringCloud 的依赖管控。版本为 Finchley.RC1123456789101112131415&lt;properties&gt; &lt;spring-cloud.version&gt;Finchley.RC1&lt;/spring-cloud.version&gt;&lt;/properties&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 配置 SpringCloud 仓库在主pom.xml 中添加如下配置1234567&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; OK，到这里，父工程创建完毕。 新建 sofa-eureka-server-centersofa-eureka-server-center 作为注册中心的服务端。 右击 sofa-eureka-parent 父工程 -&gt; New -&gt; Module，这里选择 Maven 工程； artifactId：sofa-eureka-server-center。 pom 文件修改引入 spring-cloud-starter-netflix-eureka-server 依赖，如下：123456789101112131415 &lt;parent&gt; &lt;artifactId&gt;sofa-eureka-parent&lt;/artifactId&gt; &lt;groupId&gt;com.alipay.sofa&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/parent&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;artifactId&gt;sofa-eureka-server-center&lt;/artifactId&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 新建资源文件 application.yml在 /src/main/resources 目录下新建 application.yml 或者 application.properties。这里以.yml文件为例：12345678910111213server: port: 8761 #指定服务端口eureka: instance: hostname: localhost client: register-with-eureka: false fetch-registry: false service-url: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/spring: application: name: sofa-eureka-server 配置文件后面统一说明 启动类在 /src/main/resources 目录下新建 com.alipay.sofa.cloud 包目录，并且在当前包路劲下新建 SofaEurekaServerApplication 类，并且类上加上 @EnableEurekaServer 注解。1234567@SpringBootApplication@EnableEurekaServerpublic class SofaEurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SofaEurekaServerApplication.class, args); &#125;&#125; 启动程序 &amp; 验证启动当前应用。并且浏览器中输入：http://localhost:8761/ 服务正常运行，界面如上图所示；此时还没有服务注册进来，因此红色框内显示 ：No instances available 新建 sofa-eureka-providersofa-eureka-provider 作为服务提供方，将会向注册中心 sofa-eureka-server-center 上注册服务。 右击 sofa-eureka-parent 父工程 -&gt; New -&gt; Module，这里选择 Maven 工程； artifactId：sofa-eureka-provider。 pom 文件修改引入 spring-cloud-starter-netflix-eureka-client 和 spring-boot-starter-web 依赖，如下： 1234567891011121314151617181920&lt;parent&gt; &lt;artifactId&gt;sofa-eureka-parent&lt;/artifactId&gt; &lt;groupId&gt;com.alipay.sofa&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/parent&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;sofa-eureka-provider&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 新建资源文件在 /src/main/resources 目录下新建 application.yml 或者 application.properties。这里以.yml文件为例：123456789server: port: 8082eureka: client: service-url: defaultZone: http://localhost:8761/eureka/ #指定注册中心地址spring: application: name: HelloSOFAService #服务名称 启动类在 /src/main/resources 目录下新建 com.alipay.sofa.cloud 包目录，并且在当前包路劲下新建 SofaEurekaProviderApplication 类，并且类上加上 @EnableEurekaClient 注解。1234567@SpringBootApplication@EnableEurekaClientpublic class SofaEurekaProviderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SofaEurekaProviderApplication.class, args); &#125;&#125; 服务提供类这里在 com.alipay.sofa.cloud.controller 包下新建 SofaController 类。123456789101112@RestControllerpublic class SofaController &#123; @Value("$&#123;server.port&#125;") private String port; @Value("$&#123;spring.application.name&#125;") private String hostname; @RequestMapping("/hello") public String hello() &#123; return "Hello SOFA! Now Port is "+port +" And hostname is " +hostname; &#125;&#125; 这里在接口中返回 hostname 和 port ，方便后面验证负载均衡测试使用。 启动程序 &amp; 验证在启动 sofa-eureka-provider 之前，需要先启动 sofa-wureka-server-center 。两个都启动成功之后，浏览器输入：http://localhost:8761/ 此时我将 sofa-eureka-provider 中的配置文件的端口修改为 8081，再注册一个。 可以看到 服务为 HELLOSOFASERVICE 的有两个服务提供方。]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
        <tag>Eureka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud 总览]]></title>
    <url>%2F2018%2F12%2F31%2Fspringboot%2Fspringcloud-overview%2F</url>
    <content type="text"><![CDATA[本系列基于Spring Cloud Finchley SR2 &amp; SpringBoot 2.0.7.RELEASE Spring Cloud 为开发人员提供了快速构建分布式系统中一些常见模式的工具（例如配置管理、服务发现、断路器、智能路由、微代理、控制总线、一次性令牌、全局锁、leader选举、分布式session、集群状态）。分布式系统的协调导致了样板模式, 使用 Spring Cloud 开发人员可以快速地支持实现这些模式的服务和应用程序。它们可以在任何分布式环境中很好地工作，包括开发人员自己的笔记本电脑，裸机数据中心，以及Cloud Foundry等托管平台。 Features Spring Cloud专注于为典型用例提供良好的开箱即用体验，并为其他用户提供可扩展性机制。 Distributed/versioned configuration 分布式/版本化配置 Service registration and discovery 服务注册和发现 Routing 智能路由 Service-to-service calls service-to-service调用 Load balancing 负载均衡 Circuit Breakers 断路器 Global locks 全局锁 Leadership election and cluster state leader选举和集群状态管理 Distributed messaging 分布式消息 主要项目 项目名称 项目职能 Spring Cloud Config Spring Cloud 提供的分布式配置中心，为外部配置提供了客户端和服务端的支持。 Spring Cloud Netflix 与各种Netflix OSS组件集成（Eureka，Hystrix，Zuul，Archaius等）。 Spring Cloud Bus 用于将服务和服务实例与分布式消息传递连接在一起的事件总线。用于跨群集传播状态更改（例如，配置更改事件）。 Spring Cloud Cloudfoundry 提供应用程序与 Pivotal Cloud Foundry 集成。提供服务发现实现，还可以轻松实现受SSO和OAuth2保护的资源。 Spring Cloud Open Service Broker 为构建实现 Open service broker API 的服务代理提供了一个起点。 Spring Cloud Cluster 提供Leadership选举，如：Zookeeper, Redis, Hazelcast, Consul等常见状态模式的抽象和实现。 Spring Cloud Consul 封装了Consul操作，consul 是一个服务发现与配置工具，与Docker容器可以无缝集成。 Spring Cloud Security 基于spring security的安全工具包，为你的应用程序添加安全控制。在Zuul代理中为负载平衡的OAuth2 rest客户端和身份验证头中继提供支持。 Spring Cloud Sleuth Spring Cloud 提供的分布式链路跟踪组件，兼容zipkin、HTracer和基于日志的跟踪（ELK） Spring Cloud Data Flow 大数据操作工具，作为Spring XD的替代产品，它是一个混合计算模型，结合了流数据与批量数据的处理方式。 Spring Cloud Stream 数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。 Spring Cloud CLI 基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。 Spring Cloud OpenFeign 一个http client客户端，致力于减少http client客户端构建的复杂性。 Spring Cloud Gateway Spring Cloud 提供的网关服务组件 Spring Cloud Stream App Starters Spring Cloud Stream App Starters是基于Spring Boot的Spring 集成应用程序，可提供与外部系统的集成。 Spring Cloud Task 提供云端计划任务管理、任务调度。 Spring Cloud Task App Starters Spring Cloud任务应用程序启动器是SpringBoot应用程序，它可以是任何进程，包括不会永远运行的Spring批处理作业，并且在有限的数据处理周期后结束/停止。 Spring Cloud Zookeeper 操作Zookeeper的工具包，用于使用zookeeper方式的服务发现和配置管理。 Spring Cloud AWS 提供与托管的AWS集成 Spring Cloud Connectors 便于云端应用程序在各种PaaS平台连接到后端，如：数据库和消息代理服务。 Spring Cloud Starters Spring Boot式的启动项目，为Spring Cloud提供开箱即用的依赖管理。 Spring Cloud Contract Spring Cloud Contract是一个总体项目，其中包含帮助用户成功实施消费者驱动合同方法的解决方案。 Spring Cloud Pipelines Spring Cloud Pipelines提供了一个固定意见的部署管道，其中包含确保您的应用程序可以零停机方式部署并轻松回滚出错的步骤。 Spring Cloud Function Spring Cloud Function通过函数促进业务逻辑的实现。 它支持Serverless 提供商之间的统一编程模型，以及独立运行（本地或PaaS）的能力。 SpringCloud 与 SpringBoot 版本兼容关系 Release Train Boot Version Greenwich 2.1.x Finchley 2.0.x Edgware 1.5.x Dalston 1.5.x SpringCloud 与子工程版本关系 Component Edgware.SR5 Finchley.SR2 Finchley.BUILD-SNAPSHOT spring-cloud-aws 1.2.3.RELEASE 2.0.1.RELEASE 2.0.1.BUILD-SNAPSHOT spring-cloud-bus 1.3.3.RELEASE 2.0.0.RELEASE 2.0.1.BUILD-SNAPSHOT spring-cloud-cli 1.4.1.RELEASE 2.0.0.RELEASE 2.0.1.BUILD-SNAPSHOT spring-cloud-commons 1.3.5.RELEASE 2.0.2.RELEASE 2.0.2.BUILD-SNAPSHOT spring-cloud-contract 1.2.6.RELEASE 2.0.2.RELEASE 2.0.2.BUILD-SNAPSHOT spring-cloud-config 1.4.5.RELEASE 2.0.2.RELEASE 2.0.2.BUILD-SNAPSHOT spring-cloud-netflix 1.4.6.RELEASE 2.0.2.RELEASE 2.0.2.BUILD-SNAPSHOT spring-cloud-security 1.2.3.RELEASE 2.0.1.RELEASE 2.0.1.BUILD-SNAPSHOT spring-cloud-cloudfoundry 1.1.2.RELEASE 2.0.1.RELEASE 2.0.1.BUILD-SNAPSHOT spring-cloud-consul 1.3.5.RELEASE 2.0.1.RELEASE 2.0.2.BUILD-SNAPSHOT spring-cloud-sleuth 1.3.5.RELEASE 2.0.2.RELEASE 2.0.2.BUILD-SNAPSHOT spring-cloud-stream Ditmars.SR4 Elmhurst.SR1 Elmhurst.BUILD-SNAPSHOT spring-cloud-zookeeper 1.2.2.RELEASE 2.0.0.RELEASE 2.0.1.BUILD-SNAPSHOT spring-boot 1.5.16.RELEASE 2.0.6.RELEASE 2.0.7.BUILD-SNAPSHOT spring-cloud-task 1.2.3.RELEASE 2.0.0.RELEASE 2.0.1.BUILD-SNAPSHOT spring-cloud-vault 1.1.2.RELEASE 2.0.2.RELEASE 2.0.2.BUILD-SNAPSHOT spring-cloud-gateway 1.0.2.RELEASE 2.0.2.RELEASE 2.0.2.BUILD-SNAPSHOT spring-cloud-openfeign 2.0.2.RELEASE 2.0.2.BUILD-SNAPSHOT spring-cloud-function 1.0.1.RELEASE 1.0.0.RELEASE 1.0.1.BUILD-SNAPSHOT Finchley 构建并使用Spring Boot 2.0.x，与 Spring Boot 1.5.x 不兼容。 Dalston 和 Edgware 基于 Spring Boot 1.5.x 构建，不兼容 SpringBoot 2.0.x Camden 版本迭代正式结束，Dalston 将于2018年12月结束使用，Edgware 将遵循 Spring Boot 1.5.x 的生命周期结束。 Camden 基于SpringBoot 1.4.x 构建，但是也会支持 1.5.x 版本 Brixton 和 Angel 迭代结束时间是2017年7月，Brixton 基于SpringBoot 1.3.x ，同时也支持 1.4.x 版本 Angel 基于 SpringBoot 1.2.x ,在某些方式不兼容 SpringBoot 1.3.x 。 Brixton 构建在SpringBoot 1.3.x ，不兼容 SpringBoot 1.2.x 。一些基于Angel的库和大多数应用程序可以在Brixton上正常运行，但如果OAuth2具备spring-cloud-security 1.0的特性，则需要在任何地方进行更改。x被使用(它们大多在1.3.0中被移到Spring Boot中)。]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringSession系列-sessionId解析和Cookie读写策略]]></title>
    <url>%2F2018%2F12%2F23%2Fspring%2Fspring-session-idresolver%2F</url>
    <content type="text"><![CDATA[首先需求在这里说明下，SpringSession的版本迭代的过程中肯定会伴随着一些类的移除和一些类的加入，目前本系列使用的版本是github上对象的master的代码流版本。如果有同学对其他版本中的一些类或者处理有疑惑，欢迎交流。 本篇将来介绍下SpringSession中两种sessionId解析的策略，这个在之前的文章中其实是有提到过的，这里再拿出来和SpringSession中Cookie相关策略一起学习下。 sessionId 解析策略SpringSession中对于sessionId的解析相关的策略是通过HttpSessionIdResolver这个接口来体现的。HttpSessionIdResolver有两个实现类： 这两个类就分别对应SpringSession解析sessionId的两种不同的实现策略。再深入了解不同策略的实现细节之前，先来看下HttpSessionIdResolver接口定义的一些行为有哪些。 HttpSessionIdResolverHttpSessionIdResolver定义了sessionId解析策略的契约（Contract）。允许通过请求解析sessionId，并通过响应发送sessionId或终止会话。接口定义如下：12345public interface HttpSessionIdResolver &#123; List&lt;String&gt; resolveSessionIds(HttpServletRequest request); void setSessionId(HttpServletRequest request, HttpServletResponse response,String sessionId); void expireSession(HttpServletRequest request, HttpServletResponse response);&#125; HttpSessionIdResolver中有三个方法： resolveSessionIds：解析与当前请求相关联的sessionId。sessionId可能来自Cookie或请求头。 setSessionId：将给定的sessionId发送给客户端。这个方法是在创建一个新session时被调用，并告知客户端新sessionId是什么。 expireSession：指示客户端结束当前session。当session无效时调用此方法，并应通知客户端sessionId不再有效。比如，它可能删除一个包含sessionId的Cookie，或者设置一个HTTP响应头，其值为空就表示客户端不再提交sessionId。 下面就针对上面提到的两种策略来进行详细的分析。 基于Cookie解析sessionId这种策略对应的实现类是CookieHttpSessionIdResolver，通过从Cookie中获取session；具体来说，这个实现将允许使用CookieHttpSessionIdResolver#setCookieSerializer(CookieSerializer)指定Cookie序列化策略。默认的Cookie名称是“SESSION”。创建一个session时，HTTP响应中将会携带一个指定 Cookie name且value是sessionId的Cookie。Cookie 将被标记为一个 session cookie，Cookie 的 domain path 使用 context path，且被标记为HttpOnly，如果HttpServletRequest#isSecure()返回true，那么Cookie将标记为安全的。如下： 关于Cookie，可以参考：聊一聊session和cookie。 12HTTP/1.1 200 OKSet-Cookie: SESSION=f81d4fae-7dec-11d0-a765-00a0c91e6bf6; Path=/context-root; Secure; HttpOnly 这个时候，客户端应该通过在每个请求中指定相同的Cookie来包含session信息。例如：123GET /messages/ HTTP/1.1Host: example.comCookie: SESSION=f81d4fae-7dec-11d0-a765-00a0c91e6bf6 当会话无效时，服务器将发送过期的HTTP响应Cookie，例如: 12HTTP/1.1 200 OKSet-Cookie: SESSION=f81d4fae-7dec-11d0-a765-00a0c91e6bf6; Expires=Thur, 1 Jan 1970 00:00:00 GMT; Secure; HttpOnly CookieHttpSessionIdResolver 类的实现如下：1234567891011121314151617181920212223242526272829303132333435363738public final class CookieHttpSessionIdResolver implements HttpSessionIdResolver &#123; private static final String WRITTEN_SESSION_ID_ATTR = CookieHttpSessionIdResolver.class .getName().concat(".WRITTEN_SESSION_ID_ATTR"); // Cookie序列化策略，默认是 DefaultCookieSerializer private CookieSerializer cookieSerializer = new DefaultCookieSerializer(); @Override public List&lt;String&gt; resolveSessionIds(HttpServletRequest request) &#123; // 根据提供的cookieSerializer从请求中获取sessionId return this.cookieSerializer.readCookieValues(request); &#125; @Override public void setSessionId(HttpServletRequest request, HttpServletResponse response, String sessionId) &#123; if (sessionId.equals(request.getAttribute(WRITTEN_SESSION_ID_ATTR))) &#123; return; &#125; request.setAttribute(WRITTEN_SESSION_ID_ATTR, sessionId); // 根据提供的cookieSerializer将sessionId回写到cookie中 this.cookieSerializer .writeCookieValue(new CookieValue(request, response, sessionId)); &#125; @Override public void expireSession(HttpServletRequest request, HttpServletResponse response) &#123; // 这里因为是过期，所以回写的sessionId的值是“”，当请求下次进来时，就会取不到sessionId，也就意味着当前会话失效了 this.cookieSerializer.writeCookieValue(new CookieValue(request, response, "")); &#125; // 指定Cookie序列化的方式 public void setCookieSerializer(CookieSerializer cookieSerializer) &#123; if (cookieSerializer == null) &#123; throw new IllegalArgumentException("cookieSerializer cannot be null"); &#125; this.cookieSerializer = cookieSerializer; &#125;&#125; 这里可以看到CookieHttpSessionIdResolver 中的读取操作都是围绕CookieSerializer来完成的。CookieSerializer 是SpringSession中对于Cookie操作提供的一种机制。下面细说。 基于请求头解析sessionId这种策略对应的实现类是HeaderHttpSessionIdResolver，通过从请求头header中解析出sessionId。具体地说，这个实现将允许使用HeaderHttpSessionIdResolver(String)来指定头名称。还可以使用便利的工厂方法来创建使用公共头名称(例如“X-Auth-Token”和“authenticing-info”)的实例。创建会话时，HTTP响应将具有指定名称和sessionId值的响应头。 12345678// 使用X-Auth-Token作为headerNamepublic static HeaderHttpSessionIdResolver xAuthToken() &#123; return new HeaderHttpSessionIdResolver(HEADER_X_AUTH_TOKEN);&#125;// 使用Authentication-Info作为headerNamepublic static HeaderHttpSessionIdResolver authenticationInfo() &#123; return new HeaderHttpSessionIdResolver(HEADER_AUTHENTICATION_INFO);&#125; HeaderHttpSessionIdResolver在处理sessionId上相比较于CookieHttpSessionIdResolver来说简单很多。就是围绕request.getHeader(String)和request.setHeader(String,String)两个方法来玩的。 HeaderHttpSessionIdResolver这种策略通常会在无线端来使用，以弥补对于无Cookie场景的支持。 Cookie 序列化策略基于Cookie解析sessionId的实现类CookieHttpSessionIdResolver 中实际对于Cookie的读写操作都是通过CookieSerializer来完成的。SpringSession 提供了CookieSerializer接口的默认实现DefaultCookieSerializer，当然在实际应用中，我们也可以自己实现这个接口，然后通过CookieHttpSessionIdResolver#setCookieSerializer(CookieSerializer)方法来指定我们自己的实现方式。 PS：不得不说，强大的用户扩展能力真的是Spring家族的优良家风。 篇幅有限，这里就只看下两个点： CookieValue 存在的意义是什么 DefaultCookieSerializer回写Cookie的的具体实现，读Cookie在 SpringSession系列-请求与响应重写 这篇文章中有介绍过，这里不再赘述。 jvm_router的处理 CookieValueCookieValue是CookieSerializer中的内部类，封装了向HttpServletResponse写入所需的所有信息。其实CookieValue的存在并没有什么特殊的意义，个人觉得作者一开始只是想通过CookieValue的封装来简化回写cookie链路中的参数传递的问题，但是实际上貌似并没有什么减少多少工作量。 Cookie 回写Cookie 回写我觉得对于分布式session的实现来说是必不可少的；基于标准servlet实现的HttpSession，我们在使用时实际上是不用关心回写cookie这个事情的，因为servlet容器都已经做了。但是对于分布式session来说，由于重写了response，所以需要在返回response时需要将当前session信息通过cookie的方式塞到response中返回给客户端-这就是Cookie回写。下面是DefaultCookieSerializer中回写Cookie的逻辑，细节在代码中通过注释标注出来。123456789101112131415161718192021222324252627282930313233343536373839404142@Overridepublic void writeCookieValue(CookieValue cookieValue) &#123; HttpServletRequest request = cookieValue.getRequest(); HttpServletResponse response = cookieValue.getResponse(); StringBuilder sb = new StringBuilder(); sb.append(this.cookieName).append('='); String value = getValue(cookieValue); if (value != null &amp;&amp; value.length() &gt; 0) &#123; validateValue(value); sb.append(value); &#125; int maxAge = getMaxAge(cookieValue); if (maxAge &gt; -1) &#123; sb.append("; Max-Age=").append(cookieValue.getCookieMaxAge()); OffsetDateTime expires = (maxAge != 0) ? OffsetDateTime.now().plusSeconds(maxAge) : Instant.EPOCH.atOffset(ZoneOffset.UTC); sb.append("; Expires=") .append(expires.format(DateTimeFormatter.RFC_1123_DATE_TIME)); &#125; String domain = getDomainName(request); if (domain != null &amp;&amp; domain.length() &gt; 0) &#123; validateDomain(domain); sb.append("; Domain=").append(domain); &#125; String path = getCookiePath(request); if (path != null &amp;&amp; path.length() &gt; 0) &#123; validatePath(path); sb.append("; Path=").append(path); &#125; if (isSecureCookie(request)) &#123; sb.append("; Secure"); &#125; if (this.useHttpOnlyCookie) &#123; sb.append("; HttpOnly"); &#125; if (this.sameSite != null) &#123; sb.append("; SameSite=").append(this.sameSite); &#125; response.addHeader("Set-Cookie", sb.toString());&#125; 这上面就是拼凑字符串，然后塞到Header里面去，最终再浏览器中显示大体如下： 1Set-Cookie: SESSION=f81d4fae-7dec-11d0-a765-00a0c91e6bf6; Path=/context-root; Secure; HttpOnly jvm_router的处理在Cookie的读写代码中都涉及到对于jvmRoute这个属性的判断及对应的处理逻辑。 1、读取Cookie中的代码片段 1234if (this.jvmRoute != null &amp;&amp; sessionId.endsWith(this.jvmRoute)) &#123; sessionId = sessionId.substring(0, sessionId.length() - this.jvmRoute.length());&#125; 2、回写Cookie中的代码片段 123if (this.jvmRoute != null) &#123; actualCookieValue = requestedCookieValue + this.jvmRoute;&#125; jvm_route是Nginx中的一个模块，其作用是通过session cookie的方式来获取session粘性。如果在cookie和url中并没有session，则这只是个简单的 round-robin 负载均衡。其具体过程分为以下几步： 1.第一个请求过来，没有带session信息，jvm_route就根据round robin策略发到一台tomcat上面。 2.tomcat添加上 session 信息，并返回给客户。 3.用户再次请求，jvm_route看到session中有后端服务器的名称，它就把请求转到对应的服务器上。 从本质上来说，jvm_route也是解决session共享的一种解决方式。这种和 SpringSession系列-分布式Session实现方案 中提到的基于IP-HASH的方式有点类似。那么同样，这里存在的问题是无法解决宕机后session数据转移的问题，既宕机就丢失。 DefaultCookieSerializer 中除了Cookie的读写之后，还有一些细节也值得关注下，比如对Cookie中值的验证、remember-me的实现等。 参考 SpringSession官方文档 jvm_router原理 SpringSession中文注释持续更新代码分支]]></content>
      <categories>
        <category>spring</category>
        <category>session</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20181216-书法练习]]></title>
    <url>%2F2018%2F12%2F16%2Fshufa%2Fshufa-20181216%2F</url>
    <content type="text"><![CDATA[春江花月夜 -张若虚春江潮水连海平，海上明月共潮生。滟滟随波千万里，何处春江无月明！江流宛转绕芳甸，月照花林皆似霰；空里流霜不觉飞，汀上白沙看不见。江天一色无纤尘，皎皎空中孤月轮。江畔何人初见月？江月何年初照人？人生代代无穷已，江月年年只相似。不知江月待何人，但见长江送流水。白云一片去悠悠，青枫浦上不胜愁。谁家今夜扁舟子？何处相思明月楼？可怜楼上月徘徊，应照离人妆镜台。玉户帘中卷不去，捣衣砧上拂还来。此时相望不相闻，愿逐月华流照君。鸿雁长飞光不度，鱼龙潜跃水成文。昨夜闲潭梦落花，可怜春半不还家。江水流春去欲尽，江潭落月复西斜。斜月沉沉藏海雾，碣石潇湘无限路。不知乘月几人归，落月摇情满江树。 闻王昌龄左迁龙标遥有此寄 -李白杨花落尽子规啼，闻道龙标过五溪。我寄愁心与明月，随风直到夜郎西。]]></content>
      <categories>
        <category>书法</category>
      </categories>
      <tags>
        <tag>书法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringSession系列-存储机制之Redis&Map]]></title>
    <url>%2F2018%2F12%2F16%2Fspring%2Fspring-session-redis-map%2F</url>
    <content type="text"><![CDATA[在之前的文章中已经对SpringSession的功能结构，请求/响应重写等做了介绍。本文将继续来介绍下SpringSession中存储部分的设计。存储是分布式session中算是最核心的部分，通过引入三方的存储容器来实现session的存储，从而有效的解决session共享的问题。 1、SpringSession存储的顶级抽象接口SpringSession存储的顶级抽象接口是org.springframework.session包下的SessionRepository这个接口。SessionRepository的类图结构如下： 这里先来看下SessionRepository这个顶层接口中定义了哪些方法： 12345678910public interface SessionRepository&lt;S extends Session&gt; &#123; //创建一个session S createSession(); //保存session void save(S session); //通过ID查找session S findById(String id); //通过ID删除一个session void deleteById(String id);&#125; 从代码来看还是很简单的，就是增删查。下面看具体实现。在2.0版本开始SpringSession中也提供了一个和SessionRepository具体相同能力的ReactiveSessionRepository，用于支持响应式编程模式。 2、MapSessionRepository基于HashMap实现的基于内存存储的存储器实现，这里就主要看下对于接口中几个方法的实现。 12345public class MapSessionRepository implements SessionRepository&lt;MapSession&gt; &#123; private Integer defaultMaxInactiveInterval; private final Map&lt;String, Session&gt; sessions; //...&#125; 可以看到就是一个Map，那后面关于增删查其实就是操作这个Map了。 createSession123456789@Overridepublic MapSession createSession() &#123; MapSession result = new MapSession(); if (this.defaultMaxInactiveInterval != null) &#123; result.setMaxInactiveInterval( Duration.ofSeconds(this.defaultMaxInactiveInterval)); &#125; return result;&#125; 这里很直接，就是new了一个MapSession，然后设置了session的有效期。 save1234567@Overridepublic void save(MapSession session) &#123; if (!session.getId().equals(session.getOriginalId())) &#123; this.sessions.remove(session.getOriginalId()); &#125; this.sessions.put(session.getId(), new MapSession(session));&#125; 这里面先判断了session中的两个ID，一个originalId，一个当前id。originalId是第一次生成session对象时创建的，后面都不会在变化。通过源码来看，对于originalId，只提供了get方法。对于id呢，其实是可以通过changeSessionId来改变的。 这里的这个操作实际上是一种优化行为，及时的清除掉老的session数据来释放内存空间。 findById123456789101112@Overridepublic MapSession findById(String id) &#123; Session saved = this.sessions.get(id); if (saved == null) &#123; return null; &#125; if (saved.isExpired()) &#123; deleteById(saved.getId()); return null; &#125; return new MapSession(saved);&#125; 这个逻辑也很简单，先从Map中根据id取出session数据，如果没有就返回null，如果有则再判断下是否过期了，如果过期了就删除掉，然后返回null。如果查到了，并且没有过期的话，则构建一个MapSession返回。 OK，基于内存存储的实现系列就是这些了，下面继续来看其他存储的实现。 3、FindByIndexNameSessionRepositoryFindByIndexNameSessionRepository继承了SessionRepository接口，用于扩展对第三方存储的实现。 123456789101112public interface FindByIndexNameSessionRepository&lt;S extends Session&gt; extends SessionRepository&lt;S&gt; &#123; String PRINCIPAL_NAME_INDEX_NAME = FindByIndexNameSessionRepository.class.getName() .concat(".PRINCIPAL_NAME_INDEX_NAME"); Map&lt;String, S&gt; findByIndexNameAndIndexValue(String indexName, String indexValue); default Map&lt;String, S&gt; findByPrincipalName(String principalName) &#123; return findByIndexNameAndIndexValue(PRINCIPAL_NAME_INDEX_NAME, principalName); &#125;&#125; FindByIndexNameSessionRepository添加一个单独的方法为指定用户查询所有会话。这是通过设置名为FindByIndexNameSessionRepository.PRINCIPAL_NAME_INDEX_NAME的Session的属性值为指定用户的username来完成的。开发人员有责任确保属性被赋值，因为SpringSession不会在意被使用的认证机制。官方文档中给出的例子如下： 123String username = "username";this.session.setAttribute( FindByIndexNameSessionRepository.PRINCIPAL_NAME_INDEX_NAME, username); FindByIndexNameSessionRepository的一些实现会提供一些钩子自动的索引其他的session属性。比如，很多实现都会自动的确保当前的Spring Security用户名称可通过索引名称FindByIndexNameSessionRepository.PRINCIPAL_NAME_INDEX_NAME进行索引。一旦会话被索引，就可以通过下面的代码检索： 1234String username = "username";Map&lt;String, Session&gt; sessionIdToSession = this.sessionRepository.findByIndexNameAndIndexValue( FindByIndexNameSessionRepository.PRINCIPAL_NAME_INDEX_NAME,username); 下图是FindByIndexNameSessionRepository接口的三个实现类： 下面来分别分析下这三个存储的实现细节。 3.1 RedisOperationsSessionRepositoryRedisOperationsSessionRepository的类图结构如下，MessageListener是redis消息订阅的监听接口。 代码有点长，就不在这里面贴了，一些注释可以在这个 SpringSession中文分支 来看。这里还是主要来看下对于那几个方法的实现。 3.1.1 createSession这里和MapSessionRepository的实现基本一样的，那区别就在于Session的封装模型不一样，这里是RedisSession，实际上RedisSession的实现是对MapSession又包了一层。下面会分析RedisSession这个类。12345678910@Overridepublic RedisSession createSession() &#123; // RedisSession,这里和MapSession区别开 RedisSession redisSession = new RedisSession(); if (this.defaultMaxInactiveInterval != null) &#123; redisSession.setMaxInactiveInterval( Duration.ofSeconds(this.defaultMaxInactiveInterval)); &#125; return redisSession;&#125; 在看其他两个方法之前，先来看下RedisSession这个类。 3.1.2 RedisSession这个在模型上是对MapSession的扩展，增加了delta这个东西。123456789101112final class RedisSession implements Session &#123; // MapSession 实例对象，主要存数据的地方 private final MapSession cached; // 原始最后访问时间 private Instant originalLastAccessTime; private Map&lt;String, Object&gt; delta = new HashMap&lt;&gt;(); // 是否是新的session对象 private boolean isNew; // 原始主名称 private String originalPrincipalName; // 原始sessionId private String originalSessionId; delta是一个Map结构，那么这里面到底是放什么的呢？具体细节见 saveDelta 这个方法。saveDelta 这个方法会在两个地方被调用，一个是下面要说道的save方法，另外一个是 flushImmediateIfNecessary 这个方法： 12345private void flushImmediateIfNecessary() &#123; if (RedisOperationsSessionRepository.this.redisFlushMode == RedisFlushMode.IMMEDIATE) &#123; saveDelta(); &#125;&#125; RedisFlushMode提供了两种推送模式： ON_SAVE：只有在调用save方法时执行，在web环境中这样做通常是尽快提交HTTP响应 IMMEDIATE：只要有变更就会直接写到redis中，不会像ON_SAVE一样，在最后commit时一次性写入 追踪flushImmediateIfNecessary 方法调用链如下：那么到这里基本就清楚了，首先save这个方法，当主动调用save时就是将数据推到redis中去的，也就是ON_SAVE这种情况。那么对于IMMEDIATE这种情况，只有调用了上面的四个方法，SpringSession 才会将数据推送到redis。 所以delta里面存的是当前一些变更的 key-val 键值对象，而这些变更是由setAttribute、removeAttribute、setMaxInactiveIntervalInSeconds、setLastAccessedTime这四个方法触发的；比如setAttribute(k,v)，那么这个k-&gt;v就会被保存到delta里面。 3.1.3 save在理解了saveDelta方法之后再来看save方法就简单多了。save 对应的就是RedisFlushMode.ON_SAVE。 123456789101112@Overridepublic void save(RedisSession session) &#123; // 直接调用 saveDelta推数据到redis session.saveDelta(); if (session.isNew()) &#123; // sessionCreatedKey-&gt;channl String sessionCreatedKey = getSessionCreatedChannel(session.getId()); // 发布一个消息事件，新增 session，以供 MessageListener 回调处理。 this.sessionRedisOperations.convertAndSend(sessionCreatedKey, session.delta); session.setNew(false); &#125;&#125; 3.1.4 findById查询这部分和基于Map的差别比较大，因为这里并不是直接操作Map，而是与Redis 进行一次交互。1234@Overridepublic RedisSession findById(String id) &#123; return getSession(id, false);&#125; 调用getSession方法： 12345678910111213141516private RedisSession getSession(String id, boolean allowExpired) &#123; // 根据ID从redis中取出数据 Map&lt;Object, Object&gt; entries = getSessionBoundHashOperations(id).entries(); if (entries.isEmpty()) &#123; return null; &#125; //转换成MapSession MapSession loaded = loadSession(id, entries); if (!allowExpired &amp;&amp; loaded.isExpired()) &#123; return null; &#125; //转换成RedisSession RedisSession result = new RedisSession(loaded); result.originalLastAccessTime = loaded.getLastAccessedTime(); return result;&#125; loadSession中构建MapSession： 1234567891011121314151617181920212223242526private MapSession loadSession(String id, Map&lt;Object, Object&gt; entries) &#123; // 生成MapSession实例 MapSession loaded = new MapSession(id); //遍历数据 for (Map.Entry&lt;Object, Object&gt; entry : entries.entrySet()) &#123; String key = (String) entry.getKey(); if (CREATION_TIME_ATTR.equals(key)) &#123; // 设置创建时间 loaded.setCreationTime(Instant.ofEpochMilli((long) entry.getValue())); &#125; else if (MAX_INACTIVE_ATTR.equals(key)) &#123; // 设置最大有效时间 loaded.setMaxInactiveInterval(Duration.ofSeconds((int) entry.getValue())); &#125; else if (LAST_ACCESSED_ATTR.equals(key)) &#123; // 设置最后访问时间 loaded.setLastAccessedTime(Instant.ofEpochMilli((long) entry.getValue())); &#125; else if (key.startsWith(SESSION_ATTR_PREFIX)) &#123; // 设置属性 loaded.setAttribute(key.substring(SESSION_ATTR_PREFIX.length()), entry.getValue()); &#125; &#125; return loaded;&#125; 3.1.5 deleteById根据sessionId删除session数据。具体过程看代码注释。123456789101112131415161718@Overridepublic void deleteById(String sessionId) &#123; // 获取 RedisSession RedisSession session = getSession(sessionId, true); if (session == null) &#123; return; &#125; // 清楚当前session数据的索引 cleanupPrincipalIndex(session); //执行删除操作 this.expirationPolicy.onDelete(session); String expireKey = getExpiredKey(session.getId()); //删除expireKey this.sessionRedisOperations.delete(expireKey); //session有效期设置为0 session.setMaxInactiveInterval(Duration.ZERO); save(session);&#125; 3.1.6 onMessage最后来看下这个订阅回调处理。这里看下核心的一段逻辑： 123456789101112131415boolean isDeleted = channel.equals(this.sessionDeletedChannel);// Deleted 还是 Expired ？if (isDeleted || channel.equals(this.sessionExpiredChannel)) &#123; // 此处省略无关代码 // Deleted if (isDeleted) &#123; // 发布一个 SessionDeletedEvent 事件 handleDeleted(session); &#125; // Expired else &#123; // 发布一个 SessionExpiredEvent 事件 handleExpired(session); &#125;&#125; 3.2 Redis 存储的一些思考首先按照我们自己常规的思路来设计的话，我们会怎么来考虑这个事情。这里首先要声明下，我对 Redis 这个东西不是很熟，没有做过深入的研究；那如果是我来做，可能也就仅仅限于存储。 findByIndexNameAndIndexValue的设计，这个的作用是通过indexName和indexValue来返回当前用户的所有会话。但是这里需要考虑的一个事情是，通常情况下，一个用户只会关联到一个会话上面去，那这种设计很显然，我的理解是为了支持单用户多会话的场景。 indexName：FindByIndexNameSessionRepository.PRINCIPAL_NAME_INDEX_NAME indexValue：username 实现 MessageListener 接口，增加事件通知能力。通过监听这些事件，可以做一些session操作管控。但是实际上 SpringSession 中并没有做任何事情，从代码来看，publishEvent方法是空实现。等待回复中 #issue 1287 12345678private ApplicationEventPublisher eventPublisher = new ApplicationEventPublisher() &#123; @Override public void publishEvent(ApplicationEvent event) &#123; &#125; @Override public void publishEvent(Object event) &#123; &#125;&#125;; RedisFlushMode ，SpringSession中提供了两种模式的推送，一种是ON_SAVE，另外一种是IMMEDIATE。默认是ON_SAVE，也就是常规的在请求处理结束时进行一次sessionCommit操作。RedisFlushMode 的设计感觉是为session数据持久化的时机提供了另外一种思路。 小结存储机制设计部分就一基于内存和基于Redis两种来分析；另外基于jdbc和hazelcast有兴趣的同学可以自己查看源码。 最后也欢迎访问我的个人博客：www.glmapper.com 参考 https://blog.csdn.net/zyhlwzy/article/details/78062646 https://docs.spring.io/spring-session/docs/2.0.0.M4/reference/html5/#api]]></content>
      <categories>
        <category>spring</category>
        <category>session</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SOFATracer 插件埋点机制详解]]></title>
    <url>%2F2018%2F12%2F07%2Fsofa-tracer-mvc-plugin%2F</url>
    <content type="text"><![CDATA[SOFATracer 是一个用于分布式系统调用跟踪的组件，通过统一的 traceId 将调用链路中的各种网络调用情况以日志的方式记录下来，以达到透视化网络调用的目的。这些日志可用于故障的快速发现，服务治理等。 从 RoadMap 和 PR 来看，目前 SOFATracer 已经支持了丰富的组件插件埋点。 目前还未支持的主要是 Dubbo、MQ 以及 Redis 等。本文将从 SOFATracer 已提供的一个插件源码来分析下 SOFATracer 插件的埋点实现。 1 SOFATracer 插件埋点机制SOFATracer 插件的作用实际上就是对于不同组件进行埋点，以便于收集这些组件的链路数据。SOFATracer 埋点方式一般是通过 Filter、Interceptor 机制实现的。 另一个是，SOFATracer 的埋点方式并不是基于 OT-api 进行埋点的，而是基于 SOFATracer 自己的 api 进行埋点的，详见 issue#126。 1.1 Filter or Interceptor目前已实现的插件中，像 MVC 插件是基于 Filter 进行埋点的，httpclient、resttemplate 等是基于Interceptor进行埋点的。在实现插件时，要根据不同插件的特性来选择具体的埋点方式。 当然除了这两种方式之外还可以通过静态代理的方式来实现埋点。比如 sofa-tracer-datasource-plugin 插件就是将不同的数据源进行统一代理给 SmartDatasource，从而实现埋点的。 1.2 AbstractTracer APISOFATracer 中所有的插件均需要实现自己的 Tracer 实例，如 Mvc 的 SpringMvcTracer 、HttpClient的 HttpClientTracer 等，这一点与基于 Opentracing-api 接口埋点的实现有所区别。 1、基于 SOFATracer api 埋点方式插件扩展 AbstractTracer 是 SOFATracer 用于插件扩展使用的一个抽象类，根据插件类型不同，又可以分为 clientTracer 和 serverTracer，分别对应于：AbstractClientTracer 和 AbstractServerTracer，再通过 AbstractClientTracer 和 AbstractServerTracer 衍生出具体的组件 Tracer 实现。这种方式的好处在于，所有的插件实现均由 SOFATracer 本身来管控，对于不同的组件可以轻松的实现差异化和定制化。缺点也源于此，每增加一个组件都需要做一些重复工作。 2、基于 OpenTracing-api 埋点方式插件扩展 这种埋点方式不基于 SOFATracer 自身提供的 API，而是基于 OpenTracing-api 接口。因为均遵循 OpenTracing-api 规范，所以组件和 Tracer 实现可以独立分开来维护。这样就可以对接开源的一些基于 OpenTracing-api 规范实现的组件。例如：OpenTracing API Contributions。 SOFATracer 在后面将会在 4.0 版本中支持基于 OT-api 的埋点方式，对外部组件接入扩展提供支持。 1.3 AbstractTracer这里先来看下 AbstractTracer 这个抽象类中具体提供了哪些抽象方法，也就是对于 AbstractClientTracer 和 AbstractServerTracer 需要分别扩展哪些能力。 1234567891011121314151617181920// 获取client端 摘要日志日志名protected abstract String getClientDigestReporterLogName();// 获取client端 摘要日志滚动策略keyprotected abstract String getClientDigestReporterRollingKey();// 获取client端 摘要日志日志名keyprotected abstract String getClientDigestReporterLogNameKey();// 获取client端 摘要日志编码器protected abstract SpanEncoder&lt;SofaTracerSpan&gt; getClientDigestEncoder();// 创建client端 统计日志Reporter类protected abstract AbstractSofaTracerStatisticReporter generateClientStatReporter();// 获取server端 摘要日志日志名protected abstract String getServerDigestReporterLogName();// 获取server端 摘要日志滚动策略keyprotected abstract String getServerDigestReporterRollingKey();// 获取server端 摘要日志日志名keyprotected abstract String getServerDigestReporterLogNameKey();// 获取server端 摘要日志编码器protected abstract SpanEncoder&lt;SofaTracerSpan&gt; getServerDigestEncoder();// 创建server端 统计日志Reporter类protected abstract AbstractSofaTracerStatisticReporter generateServerStatReporter(); 从 AbstractTracer 类提供的抽象方法来看，不管是 client 还是 server，在具体的 Tracer 组件实现中，都必须提供以下实现： DigestReporterLogName :当前组件摘要日志的日志名称 DigestReporterRollingKey : 当前组件摘要日志的滚动策略 SpanEncoder：对摘要日志进行编码的编码器实现 AbstractSofaTracerStatisticReporter : 统计日志 reporter 类的实现类。 2 SpringMVC 插件埋点分析这里我们以 SpringMVC 插件为例，来分析下如何实现一个埋点插件的。这里是官方给出的案例工程：基于 Spring MVC 示例落地日志 。 2.1 实现 Tracer 实例SpringMvcTracer 继承了 AbstractServerTracer 类，是对 serverTracer 的扩展。 PS：如何确定一个组件是client端还是server端呢？就是看当前组件是请求的发起方还是请求的接受方，如果是请求发起方则一般是client端，如果是请求接收方则是 server 端。那么对于 MVC 来说，是请求接受方，因此这里实现了 AbstractServerTracer 类。 1public class SpringMvcTracer extends AbstractServerTracer 2.1.1 构造函数与单例对象在构造函数中，需要传入当前 Tracer 的 traceType，SpringMvcTracer 的 traceType 为 “springmvc”。这里也可以看到，tracer 实例是一个单例对象，对于其他插件也是一样的。 123456789101112131415161718private volatile static SpringMvcTracer springMvcTracer = null;/*** * Spring MVC Tracer Singleton * @return singleton */public static SpringMvcTracer getSpringMvcTracerSingleton() &#123; if (springMvcTracer == null) &#123; synchronized (SpringMvcTracer.class) &#123; if (springMvcTracer == null) &#123; springMvcTracer = new SpringMvcTracer(); &#125; &#125; &#125; return springMvcTracer;&#125;private SpringMvcTracer() &#123; super("springmvc");&#125; 2.1.2 AbstractServerTracer 抽象类在看 SpringMvcTracer 实现之前，先来看下 AbstractServerTracer。 12345678910111213141516171819202122public abstract class AbstractServerTracer extends AbstractTracer &#123; // 构造函数，子类必须提供一个构造函数 public AbstractServerTracer(String tracerType) &#123; super(tracerType, false, true); &#125; // 因为是server端，所以Client先关的提供了默认实现，返回null protected String getClientDigestReporterLogName() &#123; return null; &#125; protected String getClientDigestReporterRollingKey() &#123; return null; &#125; protected String getClientDigestReporterLogNameKey() &#123; return null; &#125; protected SpanEncoder&lt;SofaTracerSpan&gt; getClientDigestEncoder() &#123; return null; &#125; protected AbstractSofaTracerStatisticReporter generateClientStatReporter() &#123; return null; &#125;&#125; 结合上面 AbstractTracer 小节中抽象方法分析，这里在 AbstractServerTracer 中将 client 对应的抽象方法提供了默认实现，也就是说如果要继承 AbstractServerTracer 类，那么就必须实现 server 对应的所有抽象方法。 2.1.3 SpringMVCTracer 实现下面是 SpringMvcTracer 部分对 server 部分抽象方法的实现。 1234567891011121314151617181920212223242526272829@Overrideprotected String getServerDigestReporterLogName() &#123; return SpringMvcLogEnum.SPRING_MVC_DIGEST.getDefaultLogName();&#125;@Overrideprotected String getServerDigestReporterRollingKey() &#123; return SpringMvcLogEnum.SPRING_MVC_DIGEST.getRollingKey();&#125;@Overrideprotected String getServerDigestReporterLogNameKey() &#123; return SpringMvcLogEnum.SPRING_MVC_DIGEST.getLogNameKey();&#125;@Overrideprotected SpanEncoder&lt;SofaTracerSpan&gt; getServerDigestEncoder() &#123; if (Boolean.TRUE.toString().equalsIgnoreCase( SofaTracerConfiguration.getProperty(SPRING_MVC_JSON_FORMAT_OUTPUT))) &#123; return new SpringMvcDigestJsonEncoder(); &#125; else &#123; return new SpringMvcDigestEncoder(); &#125;&#125;@Overrideprotected AbstractSofaTracerStatisticReporter generateServerStatReporter() &#123; return generateSofaMvcStatReporter();&#125; 目前 SOFATracer 日志名、滚动策略key等都是通过枚举类来定义的，也就是一个组件会对应这样一个枚举类，在枚举类里面定义这些常量。 2.2 SpringMvcLogEnum 类实现SpringMVC 插件中的枚举类是 SpringMvcLogEnum。 123456789101112public enum SpringMvcLogEnum &#123; // 摘要日志相关 SPRING_MVC_DIGEST("spring_mvc_digest_log_name", "spring-mvc-digest.log", "spring_mvc_digest_rolling"), // 统计日志相关 SPRING_MVC_STAT("spring_mvc_stat_log_name", "spring-mvc-stat.log", "spring_mvc_stat_rolling"); // 省略部分代码....&#125; 在 XXXLogEnum 枚举类中定义了当前组件对应的摘要日志和统计日志的日志名和滚动策略，因为 SOFATracer 目前还没有服务端的能力，链路数据不是直接上报给 server 的，因此 SOFATracer 提供了落到磁盘的能力。不同插件的链路日志也会通过 XXXLogEnum 指定的名称将链路日志输出到各个组件对应的日志目录下。 2.3 统计日志 Reportor 实现SOFATracer 中统计日志打印的实现需要各个组件自己来完成，具体就是需要实现一个AbstractSofaTracerStatisticReporter 的子类，然后实现 doReportStat 这个方法。当然对于目前的实现来说，我们也会重写 print 方法。 2.3.1 doReportStat123456789101112131415161718192021222324252627@Overridepublic void doReportStat(SofaTracerSpan sofaTracerSpan) &#123; Map&lt;String, String&gt; tagsWithStr = sofaTracerSpan.getTagsWithStr(); // 构建StatMapKey对象 StatMapKey statKey = new StatMapKey(); // 增加 key:当前应用名 statKey.addKey(CommonSpanTags.LOCAL_APP, tagsWithStr.get(CommonSpanTags.LOCAL_APP)); // 增加 key:请求 url statKey.addKey(CommonSpanTags.REQUEST_URL, tagsWithStr.get(CommonSpanTags.REQUEST_URL)); // 增加 key:请求方法 statKey.addKey(CommonSpanTags.METHOD, tagsWithStr.get(CommonSpanTags.METHOD)); // 压测标志 statKey.setLoadTest(TracerUtils.isLoadTest(sofaTracerSpan)); // 请求响应码 String resultCode = tagsWithStr.get(CommonSpanTags.RESULT_CODE); // 请求成功标识 boolean success = (resultCode != null &amp;&amp; resultCode.length() &gt; 0 &amp;&amp; this .isHttpOrMvcSuccess(resultCode)); statKey.setResult(success ? "true" : "false"); //end statKey.setEnd(TracerUtils.getLoadTestMark(sofaTracerSpan)); //value the count and duration long duration = sofaTracerSpan.getEndTime() - sofaTracerSpan.getStartTime(); long values[] = new long[] &#123; 1, duration &#125;; // reserve this.addStat(statKey, values);&#125; 这里就是就是将统计日志添加到日志槽里，等待被消费(输出到日志)。具体可以参考：SofaTracerStatisticReporterManager.StatReporterPrinter。 2.3.2 printprint 方法是实际将数据写入到磁盘的方法。 12345678910111213141516171819202122232425262728293031323334@Overridepublic void print(StatKey statKey, long[] values) &#123; if (this.isClosePrint.get()) &#123; //关闭统计日志输出 return; &#125; if (!(statKey instanceof StatMapKey)) &#123; return; &#125; StatMapKey statMapKey = (StatMapKey) statKey; try &#123; // 构建需要打印的数据串 jsonBuffer.reset(); jsonBuffer.appendBegin(); jsonBuffer.append("time", Timestamp.currentTime()); jsonBuffer.append("stat.key", this.statKeySplit(statMapKey)); jsonBuffer.append("count", values[0]); jsonBuffer.append("total.cost.milliseconds", values[1]); jsonBuffer.append("success", statMapKey.getResult()); //压测 jsonBuffer.appendEnd("load.test", statMapKey.getEnd()); if (appender instanceof LoadTestAwareAppender) &#123; ((LoadTestAwareAppender) appender).append(jsonBuffer.toString(), statMapKey.isLoadTest()); &#125; else &#123; appender.append(jsonBuffer.toString()); &#125; // 这里强制刷一次 appender.flush(); &#125; catch (Throwable t) &#123; SelfLog.error("统计日志&lt;" + statTracerName + "&gt;输出异常", t); &#125;&#125; print 这个方法里面就是将 statMapKey 中，也就是 doReportStat 中塞进来的数据转换成 json 格式，然后刷到磁盘。需要注意的是这里是强制 flush 了一次。如果没有重写 print 这个方法的话，则是在SofaTracerStatisticReporterManager.StatReporterPrinter 里面调用 print 方法刷到磁盘。 2.4 数据传播格式实现SOFATracer 支持使用 OpenTracing 的内建格式进行上下文传播。 1234567891011121314151617public class SpringMvcHeadersCarrier implements TextMap &#123; private HashMap&lt;String, String&gt; headers; public SpringMvcHeadersCarrier(HashMap&lt;String, String&gt; headers) &#123; this.headers = headers; &#125; @Override public void put(String key, String value) &#123; headers.put(key, value); &#125; @Override public Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator() &#123; return headers.entrySet().iterator(); &#125;&#125; 2.5 自定义编码格式实现这个决定了摘要日志打印的格式，和在统计日志里面的实现要有所区分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class SpringMvcDigestJsonEncoder extends AbstractDigestSpanEncoder &#123; // 重写encode,对span进行编码处理 @Override public String encode(SofaTracerSpan span) throws IOException &#123; JsonStringBuilder jsonStringBuilder = new JsonStringBuilder(); //日志打印时间 jsonStringBuilder.appendBegin("time", Timestamp.format(span.getEndTime())); appendSlot(jsonStringBuilder, span); return jsonStringBuilder.toString(); &#125; // 具体字段处理 private void appendSlot(JsonStringBuilder jsonStringBuilder, SofaTracerSpan sofaTracerSpan) &#123; SofaTracerSpanContext context = sofaTracerSpan.getSofaTracerSpanContext(); Map&lt;String, String&gt; tagWithStr = sofaTracerSpan.getTagsWithStr(); Map&lt;String, Number&gt; tagWithNumber = sofaTracerSpan.getTagsWithNumber(); //当前应用名 jsonStringBuilder .append(CommonSpanTags.LOCAL_APP, tagWithStr.get(CommonSpanTags.LOCAL_APP)); //TraceId jsonStringBuilder.append("traceId", context.getTraceId()); //RpcId jsonStringBuilder.append("spanId", context.getSpanId()); //请求 URL jsonStringBuilder.append(CommonSpanTags.REQUEST_URL, tagWithStr.get(CommonSpanTags.REQUEST_URL)); //请求方法 jsonStringBuilder.append(CommonSpanTags.METHOD, tagWithStr.get(CommonSpanTags.METHOD)); //Http 状态码 jsonStringBuilder.append(CommonSpanTags.RESULT_CODE, tagWithStr.get(CommonSpanTags.RESULT_CODE)); Number requestSize = tagWithNumber.get(CommonSpanTags.REQ_SIZE); //Request Body 大小 单位为byte jsonStringBuilder.append(CommonSpanTags.REQ_SIZE, (requestSize == null ? 0L : requestSize.longValue())); Number responseSize = tagWithNumber.get(CommonSpanTags.RESP_SIZE); //Response Body 大小，单位为byte jsonStringBuilder.append(CommonSpanTags.RESP_SIZE, (responseSize == null ? 0L : responseSize.longValue())); //请求耗时（MS） jsonStringBuilder.append("time.cost.milliseconds", (sofaTracerSpan.getEndTime() - sofaTracerSpan.getStartTime())); jsonStringBuilder.append(CommonSpanTags.CURRENT_THREAD_NAME, tagWithStr.get(CommonSpanTags.CURRENT_THREAD_NAME)); //穿透数据放在最后 jsonStringBuilder.appendEnd("baggage", baggageSerialized(context)); &#125;&#125; 从这里其实也可以看出，统计日志和摘要日志的不同点。统计日志里面核心的数据是 span 里面的 tags 数据，但是其主要作用是统计当前组件的次数。摘要日志里面除了 tags 里面的数据之外还会包括例如 traceId 和 spanId 等信息。 统计日志 1&#123;"time":"2018-11-28 14:42:25.127","stat.key":&#123;"method":"GET","local.app":"SOFATracerSpringMVC","request.url":"http://localhost:8080/springmvc"&#125;,"count":3,"total.cost.milliseconds":86,"success":"true","load.test":"F"&#125; 摘要日志 1&#123;"time":"2018-11-28 14:46:08.216","local.app":"SOFATracerSpringMVC","traceId":"0a0fe91b1543387568214100259231","spanId":"0.1","request.url":"http://localhost:8080/springmvc","method":"GET","result.code":"200","req.size.bytes":-1,"resp.size.bytes":0,"time.cost.milliseconds":2,"current.thread.name":"http-nio-8080-exec-2","baggage":""&#125; 2.6 请求拦截埋点对于基于标准 servlet 实现的组件，要实现对请求的拦截过滤，通常就是 Filter 了。sofa-tracer-springmvc-plugin 插件埋点的实现就是基于 Filter 机制完成的。 SpringMvcSofaTracerFilter 实现了 javax.servlet.Filter 接口，因此遵循标准的 servlet 规范的容器也可以通过此插件进行埋点。参考文档：对于标准 servlet 容器的支持（ tomcat/jetty 等）。 1public class SpringMvcSofaTracerFilter implements Filter 2.6.1 基本埋点思路对于一个组件来说，一次处理过程一般是产生一个 span。这个span的生命周期是从接收到请求到返回响应这段过程。 但是这里需要考虑的问题是如何与上下游链路关联起来呢？在 Opentracing 规范中，可以在 Tracer 中 extract 出一个跨进程传递的 SpanContext 。然后通过这个 SpanContext 所携带的信息将当前节点关联到整个 tracer 链路中去。当然有提取(extract)就会有对应的注入(inject)。 链路的构建一般是 client-server-client-server 这种模式的，那这里就很清楚了，就是会在 client 端进行注入(inject)，然后再 server 端进行提取(extract)，反复进行，然后一直传递下去。 在拿到 SpanContext 之后，此时当前的 span 就可以关联到这条链路中了，那么剩余的事情就是收集当前组件的一些数据。 整个过程大概分为以下几个阶段： 从请求中提取 spanContext 构建 span，并将当前 span 存入当前 tracer上下文中（SofaTraceContext.push(span)） 。 设置一些信息到span中 返回响应 span结束&amp;上报 下面逐一分析下这几个过程。 2.6.2 从请求中提取 spanContext这里的提取用到了上面我们提到的#数据传播格式实现#SpringMvcHeadersCarrier 这个类。上面分析到，因为mvc 做作为 server 端存在的，所以在 server 端就是从请求中 extract 出 SpanContext。 1234567891011121314151617public SofaTracerSpanContext getSpanContextFromRequest(HttpServletRequest request) &#123; HashMap&lt;String, String&gt; headers = new HashMap&lt;String, String&gt;(); // 获取请求头信息 Enumeration headerNames = request.getHeaderNames(); while (headerNames.hasMoreElements()) &#123; String key = (String) headerNames.nextElement(); String value = request.getHeader(key); headers.put(key, value); &#125; // 拿到 SofaTracer 实例对象 SofaTracer tracer = springMvcTracer.getSofaTracer(); // 解析出 SofaTracerSpanContext（SpanContext的实现类） SofaTracerSpanContext spanContext = (SofaTracerSpanContext) tracer.extract( ExtendFormat.Builtin.B3_HTTP_HEADERS, new SpringMvcHeadersCarrier(headers)); spanContext.setSpanId(spanContext.nextChildContextId()); return spanContext;&#125; 2.6.3 获取 span &amp; 数据获取serverReceive 这个方法是在 AbstractTracer 类中提供了实现，子类不需要关注这个。在 SOFATracer 中将请求大致分为以下几个过程： 客户端发送请求 clientSend cs 服务端接受请求 serverReceive sr 服务端返回结果 serverSend ss 客户端接受结果 clientReceive cr 无论是哪个插件，在请求处理周期内都可以从上述几个阶段中找到对应的处理方法。因此，SOFATracer 对这几个阶段处理进行了封装。这四个阶段实际上会产生两个 span，第一个 span 的起点是 cs，到 cr 结束；第二个 span是从 sr 开始，到 ss 结束。也就是说当执行 clientSend 和 serverReceive 时会返回一个 span 对象。来看下MVC中的实现： 红色框内对应的服务端接受请求，也就是 sr 阶段，产生了一个 span 。红色框下面的这段代码是为当前这个 span 设置一些基本的信息，包括当前应用的应用名、当前请求的url、当前请求的请求方法以及请求大小。 2.6.4 返回响应与结束 span在 filter 链执行结束之后，在 finally 块中又补充了当前请求响应结果的一些信息到 span 中去。然后调用serverSend 结束当前 span。这里关于 serverSend 里面的逻辑就不展开说了，不过能够想到的是这里肯定是调用span.finish 这个方法( opentracing 规范中，span.finish 的执行标志着一个 span 的结束)，当前也会包括对于数据上报的一些逻辑处理等。 3 思路总结与插件编写流程在第2节中以 SpringMVC 插件为例，分析了下 SOFATracer 插件埋点实现的一些细节。那么本节则从整体思路上来总结下如何编写一个 SOFATracer 的插件。 1、确定所要实现的插件，然后确定以哪种方式来埋点 2、实现当前插件的 Tracer 实例，这里需要明确当前插件是以 client 存在还是以 server 存在。 3、实现一个枚举类，用来描述当前组件的日志名称和滚动策略 key 值等 4、实现插件摘要日志的 encoder ，实现当前组件的定制化输出 5、实现插件的统计日志 Reporter 实现类，通过继承 AbstractSofaTracerStatisticReporter 类并重写doReportStat。 6、定义当前插件的传播格式 当然最重要的还是对于要实现插件的理解，要明确我们需要收集哪些数据。 小结本文先介绍了SOFATracer的埋点方式与标准OT-api 埋点方式的区别，然后对 SOFATracer 中 SpringMVC 插件的埋点实现进行了分析。希望通过本文能够让更多的同学理解埋点实现这样一个过程以及需要关注的一些点。如果有兴趣或者有什么实际的需求，欢迎来讨论。]]></content>
      <categories>
        <category>SOFA</category>
      </categories>
      <tags>
        <tag>SOFATracer</tag>
        <tag>Disruptor</tag>
        <tag>OpenTracing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringSession系列-分布式Session实现方案]]></title>
    <url>%2F2018%2F11%2F24%2Fspring%2Fspring-session-solutions%2F</url>
    <content type="text"><![CDATA[上一篇文章 SpringSession：集成SpringBoot 中介绍了如何在SpringBoot中来集成 SpringSession，整个过程非常简单，同时也简单分析了下SpringSession的作用原理。继上一篇实践之后，本文主要来分析 SpringSession 的原理。 1、从 session 的一致性方案说起关于 session 和cookie 的一些知识，大家可以参考下我之前写的一篇文章：聊一聊session和cookie。 Session作为服务器端使用的一种记录客户端状态的机制，其对客户端是透明的；但是Session 的正常运作仍然需要客户端浏览器的支持。我们都知道，HTTP协议是无状态的，Session不能依据HTTP连接来判断是否为同一客户，因此服务器需要向客户端浏览器发送一个识别标志（sessionId）,这个识别标志通过是通过Cookie机制来完成。 1.1、session 一致性问题的由来当用户首次访问我们的Servlet时，应用服务器端会给用户创建一个独立的Session，并且存储在内存中。这种情况在单应用服务器场景下是可以满足的（这里不讨论其一个弊端，就是内存占用给服务器带来的压力的问题）。在集群场景下，这种机制就会到来问题： 1.1.1、单机场景 因为是一台应用服务器，用户的每次请求都是由这台机器来处理，所以不会有session共享问题。 1.1.2、集群场景 假设现在集群中有三台机器，（从上到下：A-&gt;B-&gt;C）。当前用户首次发起访问时，请求被分配到 A 机器处理，Session数据被写入 A 机器的内存中；当再次发起访问 时，请求被分配的 B 处理，但此时 B 内存中并没有当前用户的任何数据，这样就出现了session不一致的情况了。 1.2、Session 一致性问题的方案对于当前服务化、单元化应用盛行的时代，简单的内存型的 Session 已经不能够满足我们的要求了。那么我们就需要寻求一种方案来替换目前单机内存存储实现的方案。 1.2.1 基于 IP-HASH 的实现机制在 1.1.2 中因为我们无法知道请求会被分配到哪台机器来处理，所以会导致session不一致的问题出现。如果我们可以解决让每个用户的请求能够固定的打到某一台机器上，那么上面提到的问题其实也就不存在了。IP-HASH 就是这样一种方案。通过对请求的客户端 IP 进行 HASH 计算，并将计算结果映射到具体一台机器，这样就可以将请求固定分配到某一台机器上，从而有效的避免session一致性问题的出现。 这种方案的好处在于: 不需要修改任何应用代码，0 侵入。 安全性高，不依赖其他三方缓存框架带来的风险 成本低 但是问题也很明显，这种方式实际上是规避了session一致性问题的出现，并非是针对session一致性问题给出的解决方案。主要问题： 基于应用内存，会给应用服务器带来一定的压力 服务重启会导致session数据丢失 不利于水平扩展，水平扩展也可能丢失session 存在单点负载高的情况，就是多数请求经过HASH计算之后打到同一台机器，而其他机器处于空闲状态。 1.2.2 session 复制这种方式的实现原理是应用服务器创建session之后通过组播的方式将session发送到组播地址内的其他应用服务器上。这种方式相较于IP-HASH 的方式要靠谱一点： 同样不需要更改任何业务代码 能够适应多种负载策略 机器重启或者宕机之后不怕丢失，因为有冗余备份 但是这种方式也有比较大的问题： 首先就是服务器之间同步session会占用一定的网络资源，同时session在不同的机器之间进行同步存在延迟。 还是基于内存存储，局限于机器内存容量影响，水平扩展能力差 服务器内存因为需要存储其他机器上的session数据，对内存的消耗会随着集群的规模变大而变大，可能会导致机器频繁触发GC。 1.2.3 借助三方缓存框架实现 session 集中管理上面两种方式都是有服务器自己来管理session的，主要问题还是在于对于性能和内存的影响。而这种方式的原理是将session托管给三方软件（如redis）来统一管理。这种方式可以有效的解决性能、内存占用以及水平扩展等问题。但是因为引入了三方软件，在实现复杂度、运维成本等方面会有所增加。 目前所接触到的分布式session的实现方案，大多都是基于这种方式来实现的；SpringSession 也不例外。 2、SpringSession 功能结构分析前面对分布式场景下的 Session一致性问题进行了说明，并对解决Session一致性的问题的几种策略进行的分析（有点糙，网上这些知识有很多）。在了解这些背景之后，我们来看下 SpringSession 的实现原理。 2.1 简介Spring Session 提供了用于管理用户会话信息的API和实现，在不依赖特定于应用程序容器的解决方案的情况下，使得支持群集会话变得更加简单。它还提供了透明的集成： 允许以应用程序容器（Tomcat等）中立的方式替换 HttpSesseion，支持在 headers中提供 session IDs来使用 RESTful API。 提供在接收 WebSocket 消息时保持HTTP 会话存活的能力 允许以应用程序容器中立的方式替换 Spring WebFlux 的 WebSession。 以上来自官网文档翻译 Spring Session 2.2 模块Spring Session 主要包括 4 个模块： spring-session-core ：提供了 Spring Session 核心功能和API spring-session-data-redis：以 redis 作为存储机制的 SessionRepository 实现 spring-session-hazelcast：以 Hazelcast 作为存储机制的 SessionRepository 实现 spring-session-jdbc：以关系型数据库作为存储机制的 SessionRepository 实现 总体来说就是 核心API+存储实现；工程模块截图如下： 2.3 功能结构SpringSession整体上可以分为三块： 对于Web层的处理，这里包括对于请求的重写，自定义的filter加入到filter chain，cookie处理，http header处理等 公共基础封装，比如存储类的顶层抽象接口定义，自定配置，事件处理等。 存储部分，这部分实际上是对公共基础封装接口的实现，提供了丰富的存储实现，包括redis，内存存储，jdbc等。 2.4 多 session 支持对于常用的分布式session，在实现上一般会依赖于 cookie。但是在 springsession 中提供了基于header来传递jessionID的策略实现。同时在 2.0.4 版本之前，对于同一个浏览器同一个网站，springsession 支持多个session问题，但是在此版本之后抛弃了对于对 session 的支持。关于更多关于多session支持可以查看 SpringSession 的官方文档。 小结本文对分布式 session 的几种实现策略进行了简单的介绍。对于分布式 session 而言，如何解决一致性问题是关键，目前我见过的绝大多数方案均是以 【借助三方缓存框架实现 session 集中管理】 这种来实现的，包括本系列文章中所要介绍的 SpringSession。 除分布式session一致性方式解决方案的介绍之外，作为SpringSession 的第二篇文章，在这里简单分析了下Springsession的功能模块，以便后续展开对源码的分析。]]></content>
      <categories>
        <category>spring</category>
        <category>session</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringSession系列-请求与响应重写]]></title>
    <url>%2F2018%2F11%2F24%2Fspring%2Fspring-session-req-resp%2F</url>
    <content type="text"><![CDATA[我们知道，HttpServletRequset和HttpServletResponse是Servlet标准所指定的Java语言与Web容器进行交互的接口。接口本身只规定java语言对web容器进行访问的行为方式，而具体的实现是由不同的web容器在其内部实现的。 那么在运行期，当我们需要对HttpServletRequset和HttpServletResponse的默认实例进行扩展时，我们就可以继承HttpServletRequestWrapper和HttpServletResponseWrapper来实现。 在 SpringSession中因为我们要实现不依赖容器本身的getSession 实现，因此需要扩展 HttpServletRequset，通过重写getSession来实现分布式session的能力。下面就来看下SpringSession中对于HttpServletRequset的扩展。 1、请求重写SpringSession 中对于请求重写，在能力上主要体现在存储方面，也就是getSession方法上。在 SessionRepositoryFilter 这个类中，是通过内部类的方式实现了对HttpServletRequset和HttpServletResponse的扩展。 1.1 HttpServletRequset 扩展实现12345678910111213141516171819private final class SessionRepositoryRequestWrapper extends HttpServletRequestWrapper &#123; // HttpServletResponse 实例 private final HttpServletResponse response; // ServletContext 实例 private final ServletContext servletContext; // requestedSession session对象 private S requestedSession; // 是否缓存 session private boolean requestedSessionCached; // sessionId private String requestedSessionId; // sessionId 是否有效 private Boolean requestedSessionIdValid; // sessionId 是否失效 private boolean requestedSessionInvalidated; // 省略方法&#125; 1.2 构造方法123456private SessionRepositoryRequestWrapper(HttpServletRequest request, HttpServletResponse response, ServletContext servletContext) &#123; super(request); this.response = response; this.servletContext = servletContext;&#125; 构造方法里面将 HttpServletRequest、HttpServletResponse 以及 ServletContext 实例传递进来，以便于后续扩展使用。 1.3 getSession 方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Overridepublic HttpSessionWrapper getSession(boolean create) &#123; // 从当前请求线程中获取 session HttpSessionWrapper currentSession = getCurrentSession(); // 如果有直接返回 if (currentSession != null) &#123; return currentSession; &#125; // 从请求中获取 session，这里面会涉及到从缓存中拿session的过程 S requestedSession = getRequestedSession(); if (requestedSession != null) &#123; // 无效的会话id(不支持的会话存储库)请求属性名称。 // 这里看下当前的sessionId是否有效 if (getAttribute(INVALID_SESSION_ID_ATTR) == null) &#123; // 设置当前session的最后访问时间，用于延迟session的有效期 requestedSession.setLastAccessedTime(Instant.now()); // 将requestedSessionIdValid置为true this.requestedSessionIdValid = true; // 包装session currentSession = new HttpSessionWrapper(requestedSession, getServletContext()); // 不是新的session，如果是新的session则需要改变sessionId currentSession.setNew(false); // 将session设置到当前请求上下文 setCurrentSession(currentSession); // 返回session return currentSession; &#125; &#125; else &#123; // 这里处理的是无效的sessionId的情况，但是当前请求线程 session有效 if (SESSION_LOGGER.isDebugEnabled()) &#123; SESSION_LOGGER.debug( "No session found by id: Caching result for getSession(false) for this HttpServletRequest."); &#125; // 将invalidSessionId置为true setAttribute(INVALID_SESSION_ID_ATTR, "true"); &#125; // 是否需要创建新的session if (!create) &#123; return null; &#125; if (SESSION_LOGGER.isDebugEnabled()) &#123; SESSION_LOGGER.debug( "A new session was created. To help you troubleshoot where the session was created we provided a StackTrace (this is not an error). You can prevent this from appearing by disabling DEBUG logging for " + SESSION_LOGGER_NAME, new RuntimeException( "For debugging purposes only (not an error)")); &#125; // 创建新的session S session = SessionRepositoryFilter.this.sessionRepository.createSession(); // 设置最后访问时间，也就是指定了当前session的有效期限 session.setLastAccessedTime(Instant.now()); // 包装下当前session currentSession = new HttpSessionWrapper(session, getServletContext()); //设置到当前请求线程 setCurrentSession(currentSession); return currentSession;&#125; 上面这段代码有几个点，这里单独来解释下。 getCurrentSession 这是为了在同一个请求过程中不需要重复的去从存储中获取session，在一个新的进来时，将当前的 session 设置到当前请求中，在后续处理过程如果需要getSession就不需要再去存储介质中再拿一次。 getRequestedSession 这个是根据请求信息去取session，这里面就包括了sessionId解析，从存储获取session对象等过程。 是否创建新的session对象 在当前请求中和存储中都没有获取到session信息的情况下，这里会根据create参数来判断是否创建新的session。这里一般用户首次登录时或者session失效时会走到。 1.4 getRequestedSession根据请求信息来获取session对象 1234567891011121314151617181920212223private S getRequestedSession() &#123; // 缓存的请求session是否存在 if (!this.requestedSessionCached) &#123; // 获取 sessionId List&lt;String&gt; sessionIds = SessionRepositoryFilter.this.httpSessionIdResolver .resolveSessionIds(this); // 通过sessionId来从存储中获取session for (String sessionId : sessionIds) &#123; if (this.requestedSessionId == null) &#123; this.requestedSessionId = sessionId; &#125; S session = SessionRepositoryFilter.this.sessionRepository .findById(sessionId); if (session != null) &#123; this.requestedSession = session; this.requestedSessionId = sessionId; break; &#125; &#125; this.requestedSessionCached = true; &#125; return this.requestedSession;&#125; 这段代码还是很有意思的，这里获取sessionId返回的是个列表。当然这里是SpringSession的实现策略，因为支持session，所以这里以列表的形式返回的。OK，继续来看如何解析sessionId的： 这里可以看到SpringSession对于sessionId获取的两种策略，一种是基于cookie，一种是基于header；分别来看下具体实现。 1.4.1 CookieHttpSessionIdResolver 获取 sessionIdCookieHttpSessionIdResolver 中获取sessionId的核心代码如下：其实这里没啥好说的，就是读cookie。从request将cookie信息拿出来，然后遍历找当前sessionId对应的cookie,这里的判断也很简单， 如果是以SESSION开头，则表示是 SessionId，毕竟cookie是共享的，不只有sessionId，还有可能存储其他内容。 另外这里面有个 jvmRoute，这个东西实际上很少能够用到，因为大多数情况下这个值都是null。这个我们在分析CookieSerializer时再来解释。 1.4.2 HeaderHttpSessionIdResolver 获取 sessionId这个获取更直接粗暴，就是根据 headerName 从 header 中取值。 回到getRequestedSession，剩下的代码中核心的都是和sessionRepository这个有关系，这部分就会涉及到存储部分。不在本篇的分析范围之内，会在存储实现部分来分析。 1.5 HttpSessionWrapper 上面的代码中当我们拿到session实例是通常会包装下，那么用到的就是这个HttpSessionWrapper。 HttpSessionWrapper 继承了 HttpSessionAdapter，这个HttpSessionAdapter就是将SpringSession 转换成一个标准HttpSession的适配类。HttpSessionAdapter 实现了标准servlet规范的HttpSession接口。 1.5.1 HttpSessionWrapperHttpSessionWrapper 重写了 invalidate方法。从代码来看，调用该方法产生的影响是： requestedSessionInvalidated 置为true，标识当前 session 失效。 将当前请求中的session设置为null，那么在请求的后续调用中通过getCurrentSession将拿不到session信息。 当前缓存的 session 清楚，包括sessionId，session实例等。 删除存储介质中的session对象。 1.5.2 HttpSessionAdapterSpringSession和标准HttpSession的配置器类。这个怎么理解呢，来看下一段代码： 12345@Overridepublic Object getAttribute(String name) &#123; checkState(); return this.session.getAttribute(name);&#125; 对于基于容器本身实现的HttpSession来说，getAttribute的实现也是有容器本身决定。但是这里做了转换之后，getAttribute将会通过SpringSession中实现的方案来获取。其他的API适配也是基于此实现。 SessionCommittingRequestDispatcher实现了 RequestDispatcher 接口。关于RequestDispatcher可以参考这篇文章【Servlet】关于RequestDispatcher的原理。SessionCommittingRequestDispatcher对forward的行为并没有改变。对于include则是在include之前提交session。为什么这么做呢？ 因为include方法使原先的Servlet和转发到的Servlet都可以输出响应信息，即原先的Servlet还可以继续输出响应信息；即请求转发后，原先的Servlet还可以继续输出响应信息，转发到的Servlet对请求做出的响应将并入原先Servlet的响应对象中。 所以这个在include调用之前调用commit，这样可以确保被包含的Servlet程序不能改变响应消息的状态码和响应头。 2 响应重写响应重写的目的是确保在请求提交时能够把session保存起来。来看下SessionRepositoryResponseWrapper类的实现： 这里面实现还就是重写onResponseCommitted，也就是上面说的，在请求提交时能够通过这个回调函数将session保存到存储容器中。 2.1 session 提交最后来看下 commitSession 这个过程不会再去存储容器中拿session信息，而是直接从当前请求中拿。如果拿不到，则在回写cookie时会将当前session对应的cookie值设置为空，这样下次请求过来时携带的sessionCookie就是空，这样就会重新触发登陆。 如果拿到，则清空当前请求中的session信息，然后将session保存到存储容器中，并且将sessionId回写到cookie中。 小结本篇主要对SpringSession中重写Request和Response进行了分析。通过重写Request请求来将session的存储与存储容器关联起来，通过重写Response来处理session提交，将session保存到存储容器中。 后面我们会继续来分析SpringSession的源码。最近也在学习链路跟踪相关的技术，也准备写一写，有兴趣的同学可以一起讨论。 附 SOFA 开源社区 SOFATracer]]></content>
      <categories>
        <category>spring</category>
        <category>session</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SOFABoot 健康检查能力分析]]></title>
    <url>%2F2018%2F11%2F16%2Fsofa-boot-health-analizy%2F</url>
    <content type="text"><![CDATA[Liveness Check &amp; Readiness CheckSpring Boot 提供了一个基础的健康检查的能力，中间件和应用都可以扩展来实现自己的健康检查逻辑。但是 Spring Boot 的健康检查只有 Liveness Check 的能力，缺少 Readiness Check 的能力，这样会有比较致命的问题。当一个微服务应用启动的时候，必须要先保证启动后应用是健康的，才可以将上游的流量放进来（来自于 RPC，网关，定时任务等等流量），否则就可能会导致一定时间内大量的错误发生。 针对 Spring Boot 缺少 Readiness Check 能力的情况，SOFABoot 增加了 Spring Boot 现有的健康检查的能力，提供了 Readiness Check 的能力。利用 Readiness Check 的能力，SOFA 中间件中的各个组件只有在 Readiness Check 通过之后，才将流量引入到应用的实例中，比如 RPC，只有在 Readiness Check 通过之后，才会向服务注册中心注册，后面来自上游应用的流量才会进入。 除了中间件可以利用 Readiness Check 的事件来控制流量的进入之外，PAAS 系统也可以通过访问 http://localhost:8080/actuator/readiness 来获取应用的 Readiness Check 的状况，用来控制例如负载均衡设备等等流量的进入。 使用方式SOFABoot 的健康检查能力需要引入： 1234&lt;dependency&gt; &lt;groupId&gt;com.alipay.sofa&lt;/groupId&gt; &lt;artifactId&gt;healthcheck-sofa-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt; 区别于SpringBoot的： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 详细工程科参考：sofa-boot 健康检查启动日志 代码分析既然是个Starter，那么就先从 spring.factories 文件来看： 12345org.springframework.context.ApplicationContextInitializer=\com.alipay.sofa.healthcheck.initializer.SofaBootHealthCheckInitializerorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\com.alipay.sofa.healthcheck.configuration.SofaBootHealthCheckAutoConfiguration SofaBootHealthCheckInitializerSofaBootHealthCheckInitializer 实现了 ApplicationContextInitializer 接口。 ApplicationContextInitializer 是 Spring 框架原有的概念，这个类的主要目的就是在 ConfigurableApplicationContext 类型（或者子类型）的 ApplicationContext 做 refresh 之前，允许我们 对 ConfigurableApplicationContext 的实例做进一步的设置或者处理。 123456789101112131415public class SofaBootHealthCheckInitializer implements ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt; &#123; @Override public void initialize(ConfigurableApplicationContext applicationContext) &#123; Environment environment = applicationContext.getEnvironment(); if (SOFABootEnvUtils.isSpringCloudBootstrapEnvironment(environment)) &#123; return; &#125; // init logging.level.com.alipay.sofa.runtime argument String healthCheckLogLevelKey = Constants.LOG_LEVEL_PREFIX + HealthCheckConstants.SOFABOOT_HEALTH_LOG_SPACE; SofaBootLogSpaceIsolationInit.initSofaBootLogger(environment, healthCheckLogLevelKey); SofaBootHealthCheckLoggerFactory.getLogger(SofaBootHealthCheckInitializer.class).info( "SOFABoot HealthCheck Starting!"); &#125;&#125; SofaBootHealthCheckInitializer 在 initialize 方法中主要做了两件事： 验证当前 environment 是否是 SpringCloud 的（3.0.0 开始支持 springCloud，之前版本无此 check） 初始化 logging.level 这两件事和健康检查没有什么关系，但是既然放在这个模块里面还是来看下。 1、springCloud 环境验证首先就是为什么会有这个验证。SOFABoot 在支持 SpringcLoud 时遇到一个问题，就是当在 classpath 中添加spring-cloud-context 依赖关系时,org.springframework.context.ApplicationContextInitializer会被调用两次。具体背景可参考 # issue1151 &amp;&amp; # issue 232 1234567891011121314private final static String SPRING_CLOUD_MARK_NAME = "org.springframework.cloud.bootstrap.BootstrapConfiguration";public static boolean isSpringCloudBootstrapEnvironment(Environment environment) &#123; if (environment instanceof ConfigurableEnvironment) &#123; return !((ConfigurableEnvironment) environment).getPropertySources().contains( SofaBootInfraConstants.SOFA_BOOTSTRAP) &amp;&amp; isSpringCloud(); &#125; return false;&#125;public static boolean isSpringCloud() &#123; return ClassUtils.isPresent(SPRING_CLOUD_MARK_NAME, null);&#125; 上面这段代码是 SOFABoot 提供的一个用于区分 引导上下文 和 应用上下文 的方法： 检验是否有&quot;org.springframework.cloud.bootstrap.BootstrapConfiguration&quot;这个类来判断当前是否引入了spingCloud的引导配置类 从environment 中获取 MutablePropertySources 实例，验证 MutablePropertySources 中是否包括 sofaBootstrap （ 如果当前环境是 SOFA bootstrap environment，则包含 sofaBootstrap；这个是在 SofaBootstrapRunListener 回调方法中设置进行的 ） 2、初始化 logging.level这里是处理 SOFABoot 日志空间隔离的。 1234567891011121314151617181920public static void initSofaBootLogger(Environment environment, String runtimeLogLevelKey) &#123; // 初始化 logging.path 参数 String loggingPath = environment.getProperty(Constants.LOG_PATH); if (!StringUtils.isEmpty(loggingPath)) &#123; System.setProperty(Constants.LOG_PATH, environment.getProperty(Constants.LOG_PATH)); ReportUtil.report("Actual " + Constants.LOG_PATH + " is [ " + loggingPath + " ]"); &#125; //for example : init logging.level.com.alipay.sofa.runtime argument String runtimeLogLevelValue = environment.getProperty(runtimeLogLevelKey); if (runtimeLogLevelValue != null) &#123; System.setProperty(runtimeLogLevelKey, runtimeLogLevelValue); &#125; // init file.encoding String fileEncoding = environment.getProperty(Constants.LOG_ENCODING_PROP_KEY); if (!StringUtils.isEmpty(fileEncoding)) &#123; System.setProperty(Constants.LOG_ENCODING_PROP_KEY, fileEncoding); &#125;&#125; SofaBootHealthCheckAutoConfiguration这个类是 SOFABoot 健康检查机制的自动化配置实现。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Configurationpublic class SofaBootHealthCheckAutoConfiguration &#123; /** ReadinessCheckListener: 容器刷新之后回调 */ @Bean public ReadinessCheckListener readinessCheckListener() &#123; return new ReadinessCheckListener(); &#125; /** HealthCheckerProcessor: HealthChecker处理器 */ @Bean public HealthCheckerProcessor healthCheckerProcessor() &#123; return new HealthCheckerProcessor(); &#125; /** HealthCheckerProcessor: HealthIndicator处理器 */ @Bean public HealthIndicatorProcessor healthIndicatorProcessor() &#123; return new HealthIndicatorProcessor(); &#125; /** AfterReadinessCheckCallbackProcessor: ReadinessCheck之后的回调处理器 */ @Bean public AfterReadinessCheckCallbackProcessor afterReadinessCheckCallbackProcessor() &#123; return new AfterReadinessCheckCallbackProcessor(); &#125; /** 返回 SofaBoot健康检查指标类 实例*/ @Bean public SofaBootHealthIndicator sofaBootHealthIndicator() &#123; return new SofaBootHealthIndicator(); &#125; @ConditionalOnClass(Endpoint.class) public static class ConditionReadinessEndpointConfiguration &#123; @Bean @ConditionalOnEnabledEndpoint public SofaBootReadinessCheckEndpoint sofaBootReadinessCheckEndpoint() &#123; return new SofaBootReadinessCheckEndpoint(); &#125; &#125; @ConditionalOnClass(Endpoint.class) public static class ReadinessCheckExtensionConfiguration &#123; @Bean @ConditionalOnMissingBean @ConditionalOnEnabledEndpoint public ReadinessEndpointWebExtension readinessEndpointWebExtension() &#123; return new ReadinessEndpointWebExtension(); &#125; &#125;&#125; ReadinessCheckListener12public class ReadinessCheckListener implements PriorityOrdered, ApplicationListener&lt;ContextRefreshedEvent&gt; 从代码来看，ReadinessCheckListener 实现了 ApplicationListener 监听器接口，其所监听的事件对象是ContextRefreshedEvent，即当容器上下文刷新完成之后回调。 SOFABoot 中通过这个监听器来完成 readniess check 的处理。 onApplicationEvent 回调方法： 12345678910public void onApplicationEvent(ContextRefreshedEvent event) &#123; // healthCheckerProcessor init healthCheckerProcessor.init(); // healthIndicatorProcessor init healthIndicatorProcessor.init(); // afterReadinessCheckCallbackProcessor init afterReadinessCheckCallbackProcessor.init(); // readiness health check execute readinessHealthCheck();&#125; 初始化 healthCheckerProcessor，这个里面就是将当前所有的HealthChecker类型的bean找出来，然后放在一个map中，等待后面的 readiness check。 12345678910111213141516171819public void init() &#123; // 是否已经初始化了 if (isInitiated.compareAndSet(false, true)) &#123; // applicationContext 应用上下文不能为null Assert.notNull(applicationContext, () -&gt; "Application must not be null"); // 获取所有类型是 HealthChecker 的bean Map&lt;String, HealthChecker&gt; beansOfType = applicationContext .getBeansOfType(HealthChecker.class); // 排序 healthCheckers = HealthCheckUtils.sortMapAccordingToValue(beansOfType, applicationContext.getAutowireCapableBeanFactory()); // 构建日志信息，对应在健康检查日志里面打印出来的是： // ./logs/health-check/common-default.log:Found 0 HealthChecker implementation StringBuilder healthCheckInfo = new StringBuilder(512).append("Found ") .append(healthCheckers.size()).append(" HealthChecker implementation:") .append(String.join(",", healthCheckers.keySet())); logger.info(healthCheckInfo.toString()); &#125;&#125; 初始化 healthIndicatorProcessor，将所有的healthIndicator 类型的bean 找出来，然后放在一个map中等待readiness check。如果想要在 SOFABoot 的 Readiness Check 里面增加一个检查项，那么可以直接扩展 Spring Boot 的HealthIndicator这个接口。 12345678910111213141516171819202122232425public void init() &#123; // 是否已经初始化 if (isInitiated.compareAndSet(false, true)) &#123; // applicationContext 验证 Assert.notNull(applicationContext, () -&gt; "Application must not be null"); // 获取所有HealthIndicator类型的bean Map&lt;String, HealthIndicator&gt; beansOfType = applicationContext .getBeansOfType(HealthIndicator.class); // 支持 Reactive 方式 if (ClassUtils.isPresent(REACTOR_CLASS, null)) &#123; applicationContext.getBeansOfType(ReactiveHealthIndicator.class).forEach( (name, indicator) -&gt; beansOfType.put(name, () -&gt; indicator.health().block())); &#125; // 排序 healthIndicators = HealthCheckUtils.sortMapAccordingToValue(beansOfType, applicationContext.getAutowireCapableBeanFactory()); // 构建日志信息 // Found 2 HealthIndicator implementation: // sofaBootHealthIndicator, diskSpaceHealthIndicator StringBuilder healthIndicatorInfo = new StringBuilder(512).append("Found ") .append(healthIndicators.size()).append(" HealthIndicator implementation:") .append(String.join(",", healthIndicators.keySet())); logger.info(healthIndicatorInfo.toString()); &#125;&#125; 初始化 afterReadinessCheckCallbackProcessor。如果想要在 Readiness Check 之后做一些事情，那么可以扩展 SOFABoot 的这个接口 12345678910111213141516171819public void init() &#123; // 是否已经初始化 if (isInitiated.compareAndSet(false, true)) &#123; // applicationContext 验证 Assert.notNull(applicationContext, () -&gt; "Application must not be null"); // 找到所有 ReadinessCheckCallback 类型的 bean Map&lt;String, ReadinessCheckCallback&gt; beansOfType = applicationContext .getBeansOfType(ReadinessCheckCallback.class); // 排序 readinessCheckCallbacks = HealthCheckUtils.sortMapAccordingToValue(beansOfType, applicationContext.getAutowireCapableBeanFactory()); // 构建日志 StringBuilder applicationCallbackInfo = new StringBuilder(512).append("Found ") .append(readinessCheckCallbacks.size()) .append(" ReadinessCheckCallback implementation: ") .append(String.join(",", beansOfType.keySet())); logger.info(applicationCallbackInfo.toString()); &#125;&#125; readinessHealthCheck，前面的几个init方法中均是为readinessHealthCheck做准备的，到这里SOFABoot已经拿到了当前多有的HealthChecker、HealthIndicator 和 ReadinessCheckCallback 类型的 bean 信息。 12345678910111213141516171819202122232425262728293031323334// readiness health checkpublic void readinessHealthCheck() &#123; // 是否跳过所有check,可以通过 com.alipay.sofa.healthcheck.skip.all 配置项配置决定 if (skipAllCheck()) &#123; logger.warn("Skip all readiness health check."); &#125; else &#123; // 是否跳过所有 HealthChecker 类型bean的 readinessHealthCheck, // 可以通过com.alipay.sofa.healthcheck.skip.component配置项配置 if (skipComponent()) &#123; logger.warn("Skip HealthChecker health check."); &#125; else &#123; //HealthChecker 的 readiness check healthCheckerStatus = healthCheckerProcessor .readinessHealthCheck(healthCheckerDetails); &#125; // 是否跳过所有HealthIndicator 类型bean的readinessHealthCheck // 可以通过 com.alipay.sofa.healthcheck.skip.indicator配置项配置 if (skipIndicator()) &#123; logger.warn("Skip HealthIndicator health check."); &#125; else &#123; //HealthIndicator 的 readiness check healthIndicatorStatus = healthIndicatorProcessor .readinessHealthCheck(healthIndicatorDetails); &#125; &#125; // ReadinessCheck 之后的回调函数，做一些后置处理 healthCallbackStatus = afterReadinessCheckCallbackProcessor .afterReadinessCheckCallback(healthCallbackDetails); if (healthCheckerStatus &amp;&amp; healthIndicatorStatus &amp;&amp; healthCallbackStatus) &#123; logger.info("Readiness check result: success"); &#125; else &#123; logger.error("Readiness check result: fail"); &#125;&#125; Readiness Check 做了什么前面是 SOFABoot 健康检查组件处理健康检查逻辑的一个大体流程，了解到了 Readiness 包括检查 HealthChecker 类型的bean和HealthIndicator 类型的 bean。其中HealthIndicator是SpringBoot自己的接口 ，而 HealthChecker 是 SOFABoot 提供的接口。下面继续通过 XXXProcess 来看下 Readiness Check 到底做了什么？ HealthCheckerProcessorHealthChecker 的健康检查处理器，readinessHealthCheck 方法 12345678910111213public boolean readinessHealthCheck(Map&lt;String, Health&gt; healthMap) &#123; Assert.notNull(healthCheckers, "HealthCheckers must not be null."); logger.info("Begin SOFABoot HealthChecker readiness check."); boolean result = healthCheckers.entrySet().stream() .map(entry -&gt; doHealthCheck(entry.getKey(), entry.getValue(), true, healthMap, true)) .reduce(true, BinaryOperators.andBoolean()); if (result) &#123; logger.info("SOFABoot HealthChecker readiness check result: success."); &#125; else &#123; logger.error("SOFABoot HealthChecker readiness check result: failed."); &#125; return result;&#125; 这里每个HealthChecker又委托给doHealthCheck来检查 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152private boolean doHealthCheck(String beanId, HealthChecker healthChecker, boolean isRetry, Map&lt;String, Health&gt; healthMap, boolean isReadiness) &#123; Assert.notNull(healthMap, "HealthMap must not be null"); Health health; boolean result; int retryCount = 0; // check 类型 readiness ？ liveness String checkType = isReadiness ? "readiness" : "liveness"; do &#123; // 获取 Health 对象 health = healthChecker.isHealthy(); // 获取 健康检查状态结果 result = health.getStatus().equals(Status.UP); if (result) &#123; logger.info("HealthChecker[&#123;&#125;] &#123;&#125; check success with &#123;&#125; retry.", beanId, checkType,retryCount); break; &#125; else &#123; logger.info("HealthChecker[&#123;&#125;] &#123;&#125; check fail with &#123;&#125; retry.", beanId, checkType,retryCount); &#125; // 重试 &amp;&amp; 等待 if (isRetry &amp;&amp; retryCount &lt; healthChecker.getRetryCount()) &#123; try &#123; retryCount += 1; TimeUnit.MILLISECONDS.sleep(healthChecker.getRetryTimeInterval()); &#125; catch (InterruptedException e) &#123; logger .error( String .format( "Exception occurred while sleeping of %d retry HealthChecker[%s] %s check.", retryCount, beanId, checkType), e); &#125; &#125; &#125; while (isRetry &amp;&amp; retryCount &lt; healthChecker.getRetryCount()); // 将当前 实例 bean 的健康检查结果存到结果集healthMap中 healthMap.put(beanId, health); try &#123; if (!result) &#123; logger .error( "HealthChecker[&#123;&#125;] &#123;&#125; check fail with &#123;&#125; retry; fail details:&#123;&#125;; strict mode:&#123;&#125;", beanId, checkType, retryCount, objectMapper.writeValueAsString(health.getDetails()), healthChecker.isStrictCheck()); &#125; &#125; catch (JsonProcessingException ex) &#123; logger.error( String.format("Error occurred while doing HealthChecker %s check.", checkType), ex); &#125; // 返回健康检查结果 return !healthChecker.isStrictCheck() || result;&#125; 这里的 doHealthCheck 结果需要依赖具体 HealthChecker 实现类的处理。通过这样一种方式可以SOFABoot可以很友好的实现对所以 HealthChecker 的健康检查。HealthIndicatorProcessor 的 readinessHealthCheck 和HealthChecker的基本差不多；有兴趣的可以自行阅读源码 Alipay-SOFABoot。 AfterReadinessCheckCallbackProcessor这个接口是 SOFABoot 提供的一个扩展接口， 用于在 Readiness Check 之后做一些事情。其实现思路和前面的XXXXProcessor 是一样的，对之前初始化时得到的所有的ReadinessCheckCallbacks实例bean逐一进行回调处理。 123456789101112131415public boolean afterReadinessCheckCallback(Map&lt;String, Health&gt; healthMap) &#123; logger.info("Begin ReadinessCheckCallback readiness check"); Assert.notNull(readinessCheckCallbacks, "ReadinessCheckCallbacks must not be null."); boolean result = readinessCheckCallbacks.entrySet().stream() .map(entry -&gt; doHealthCheckCallback(entry.getKey(), entry.getValue(), healthMap)) .reduce(true, BinaryOperators.andBoolean()); if (result) &#123; logger.info("ReadinessCheckCallback readiness check result: success."); &#125; else &#123; logger.error("ReadinessCheckCallback readiness check result: failed."); &#125; return result;&#125; 同样也是委托给了doHealthCheckCallback来处理 123456789101112131415161718private boolean doHealthCheckCallback(String beanId, ReadinessCheckCallback readinessCheckCallback, Map&lt;String, Health&gt; healthMap) &#123; Assert.notNull(healthMap, () -&gt; "HealthMap must not be null"); boolean result = false; Health health = null; try &#123; health = readinessCheckCallback.onHealthy(applicationContext); result = health.getStatus().equals(Status.UP); // print log 省略 &#125; catch (Throwable t) &#123; // 异常处理 &#125; finally &#123; // 存入 healthMap healthMap.put(beanId, health); &#125; return result;&#125; 扩展 Readiness Check 能力按照上面的分析，我们可以自己来实现下这几个扩展。 实现 HealthChecker 接口1234567891011121314151617181920212223242526272829303132333435363738@Componentpublic class GlmapperHealthChecker implements HealthChecker &#123; @Override public Health isHealthy() &#123; // 可以检测数据库连接是否成功 // 可以检测zookeeper是否启动成功 // 可以检测redis客户端是否启动成功 // everything you want ... if(OK)&#123; return Health.up().build(); &#125; return Health.down().build(); &#125; @Override public String getComponentName() &#123; // 组件名 return "GlmapperComponent"; &#125; @Override public int getRetryCount() &#123; // 重试次数 return 1; &#125; @Override public long getRetryTimeInterval() &#123; // 重试间隔 return 0; &#125; @Override public boolean isStrictCheck() &#123; return false; &#125;&#125; 实现 ReadinessCheckCallback 接口123456789101112@Componentpublic class GlmapperReadinessCheckCallback implements ReadinessCheckCallback &#123; @Override public Health onHealthy(ApplicationContext applicationContext) &#123; Object glmapperHealthChecker = applicationContext.getBean("glmapperHealthChecker"); if (glmapperHealthChecker instanceof GlmapperHealthChecker)&#123; return Health.up().build(); &#125; return Health.down().build(); &#125;&#125; 再来看下健康检查日志： 可以看到我们自己定义的检查类型ready了。 从日志看到有一个 sofaBootHealthIndicator，实现了HealthIndicator 接口。 1234567891011121314151617public class SofaBootHealthIndicator implements HealthIndicator &#123; private static final String CHECK_RESULT_PREFIX = "Middleware"; @Autowired private HealthCheckerProcessor healthCheckerProcessor; @Override public Health health() &#123; Map&lt;String, Health&gt; healths = new HashMap&lt;&gt;(); // 调用了 healthCheckerProcessor 的 livenessHealthCheck boolean checkSuccessful = healthCheckerProcessor.livenessHealthCheck(healths); if (checkSuccessful) &#123; return Health.up().withDetail(CHECK_RESULT_PREFIX, healths).build(); &#125; else &#123; return Health.down().withDetail(CHECK_RESULT_PREFIX, healths).build(); &#125; &#125;&#125; livenessHealthCheck 和 readinessHealthCheck 两个方法都是交给 doHealthCheck 来处理的，没有看出来有什么区别。 小结本文基于 SOFABoot 3.0.0 版本，与之前版本有一些区别。详细变更见：SOFABoot upgrade_3_x。本篇文章简单介绍了 SOFABoot 对 SpringBoot 健康检查能力扩展的具体实现细节。 最后再来补充下 liveness 和 readiness，从字面意思来理解，liveness就是是否是活的，readiness 就是意思是否可访问的。 readiness：应用即便已经正在运行了，它仍然需要一定时间才能 提供 服务，这段时间可能用来加载数据，可能用来构建缓存，可能用来注册服务，可能用来选举 Leader等等。总之 Readiness 检查通过前是不会有流量发给应用的。目前 SOFARPC 就是在 readiness check 之后才会将所有的服务注册到注册中心去。 liveness：检测应用程序是否正在运行]]></content>
      <categories>
        <category>SOFA</category>
      </categories>
      <tags>
        <tag>SOFABoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20181111-书法练习]]></title>
    <url>%2F2018%2F11%2F10%2Fshufa%2Fshufa-20181111%2F</url>
    <content type="text"><![CDATA[黄鹤楼送孟浩然之广陵 -李白故人西辞黄鹤楼，烟花三月下扬州孤帆远影碧空尽，唯见长江天际流 将进酒 -李白君不见，黄河之水天上来，奔流到海不复回。君不见，高堂明镜悲白发，朝如青丝暮成雪。人生得意须尽欢，莫使金樽空对月。天生我材必有用，千金散尽还复来。烹羊宰牛且为乐，会须一饮三百杯。岑夫子，丹丘生，将进酒，杯莫停。与君歌一曲，请君为我倾耳听。(倾耳听 一作：侧耳听)钟鼓馔玉不足贵，但愿长醉不复醒。(不足贵 一作：何足贵；不复醒 一作：不愿醒/不用醒)古来圣贤皆寂寞，惟有饮者留其名。(古来 一作：自古；惟 通：唯)陈王昔时宴平乐，斗酒十千恣欢谑。主人何为言少钱，径须沽取对君酌。五花马，千金裘，呼儿将出换美酒，与尔同销万古愁。 青玉案·元夕 -辛弃疾东风夜放花千树。更吹落、星如雨。宝马雕车香满路。凤箫声动，玉壶光转，一夜鱼龙舞。蛾儿雪柳黄金缕。笑语盈盈暗香去。众里寻他千百度。蓦然回首，那人却在，灯火阑珊处。]]></content>
      <categories>
        <category>书法</category>
      </categories>
      <tags>
        <tag>书法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[菜鸟成长系列-策略模式]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-design-model-strategy%2F</url>
    <content type="text"><![CDATA[上次在模板方法模式中有提及到，模板方法模式通常不会单独来试用，在一些实际的应用中会搭配其他的模式来使用，比如说今天要学习的策略模式。 一直我都很喜欢策略这个词，有种莫名的高大上，对三国有了解的小伙伴肯定会知道，有的谋士是比较直接的，献计就是献计，有话当面说；但是也有的谋士就是比较喜欢搞一种神秘感，弄个小布袋子里面塞个小布条（简称：锦囊）；对于一件很棘手的事情，在交代下去的时候就会有这样的嘱咐：“此事关系重大，还望XXX（昵称）务必处理妥帖；这里有三个锦囊，如果XXXX，你就拆开第X个锦囊，然后XXXX”；有时候我就很不解，假如真在遇到事情的时候来看，那路上丢了怎么办？一摸口袋就懵逼了有木有？ 扯远了，不过意思就是这个意思，一个锦囊其实就是一种策略；然后它有一个总的背景（我们称之为上下文环境），这个大背景下，每个不同的场景都会有一中策略来对应处理； 我们先以上面的列子为背景来撸一个小的例子，然后再去看一个spring中比较典型的策略模式使用，最后再来探讨下策略模式的类图，并以此来说明策略模式中的一些基本角色及其职责。 锦囊妙计 兵马未动，粮草先行；但是这个运输粮草到底是走水路还是走陆地呢？那这得看往哪运… 12345678910111213141516171819202122232425package com.glmapper.designmode.policy;/** * @description: 大背景，运输粮草 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/5/5 */public class TransportFood &#123; //持有一个运输策略的对象 private TransportFoodStrategy strategy; /** * 构造函数，传入一个具体策略对象 * @param strategy 具体策略对象 */ public TransportFood(TransportFoodStrategy strategy)&#123; this.strategy = strategy; &#125; /** * 策略方法 */ public void trasportFood()&#123; strategy.trasport(); &#125;&#125; 这个是我们的总体背景，就是运输粮草；但是这个只是说要运输粮草，但是并没有说是怎么运？这就得TransportFoodStrategy这个运输策略有具体的运输方案。 运输方案1：如果粮草是从武汉到南京，OK，那就走水运吧。 123456789101112/** * @description: 运输粮草的策略之水运运输 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/5/5 */public class WaterTransportStrategy implements TransportFoodStrategy &#123; @Override public void trasport() &#123; System.out.println("用船，走水运"); &#125;&#125; 运输方案2：如果从内蒙到北京；那就走陆运吧。 123456789101112/** * @description: 运输粮草的策略之陆地运输 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/5/5 */public class LandTransportStrategy implements TransportFoodStrategy &#123; @Override public void trasport() &#123; System.out.println("用马车，走陆运"); &#125;&#125; 好了，来看下妙计使用： 1234567891011121314151617181920212223242526272829303132333435/** * @description: 决策制定-客户端 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/5/5 */public class Client &#123; public static void main(String[] args) &#123; TransportFoodStrategy strategy = getTransportFoodStrategy("内蒙到北京"); TransportFood transportFood = new TransportFood(strategy); transportFood.trasportFood(); &#125; /** * 获取运输方案 * @param lineType 运输路线 * @return */ private static TransportFoodStrategy getTransportFoodStrategy( String lineType)&#123; if (lineType.equals("内蒙到北京"))&#123; return new LandTransportStrategy(); &#125; if (lineType.equals("武汉到南京"))&#123; return new WaterTransportStrategy(); &#125; return null; &#125;&#125;-&gt; 用马车，走陆运 粮草运完了，真正的表演开始了… Spring中典型的策略模式使用我们知道spring加载资源文件是通过ResourceLoader来搞定的。在ResourceLoader中提供了一个： 1Resource getResource(String location); 这个方法的注解中说道 12//允许多个资源调用。allowing for multiple &#123;@link Resource#getInputStream()&#125; calls. 这里就很赤裸裸了，他告诉了你要获取资源，但是如果获取资源呢？这就得看有哪些具体的获取策略了。 上图就是Resource的具体子类实现，也就是一些具体的策略。我们比较常见的应该算是UrlResource（加载URL指定的资源）和ClasspathResource（加载类路径中的资源）这两个。再来看下这个getResource这个方法的实现： getResource方法是在DefaultResourceLoader中具体实现的；DefaultResourceLoader是ResourceLoader的默认实现。 12345678910111213141516171819202122232425262728293031323334@Overridepublic Resource getResource(String location) &#123; Assert.notNull(location, "Location must not be null"); //首先使用ProtocolResolver来通过location参数创建Resource对象 // spring4.3.x开始才有的 for (ProtocolResolver protocolResolver : this.protocolResolvers) &#123; Resource resource = protocolResolver.resolve(location,this); if (resource != null) &#123; return resource; &#125; &#125; //指定路径的 if (location.startsWith("/")) &#123; return getResourceByPath(location); &#125; //以classpath开头的 else if (location.startsWith(CLASSPATH_URL_PREFIX)) &#123; return new ClassPathResource(location.substring( CLASSPATH_URL_PREFIX.length()), getClassLoader()); &#125; //这里是先尝试解析是否是带有网络协议的资源， //如果解析异常，则是在异常处理中使用了一种默认的机制。 else &#123; try &#123; // Try to parse the location as a URL... URL url = new URL(location); return new UrlResource(url); &#125; catch (MalformedURLException ex) &#123; // No URL -&gt; resolve as resource path. return getResourceByPath(location); &#125; &#125;&#125; 关于ProtocolResolver 其实我们可以发现，这里的location其实和我们上面那个例子中的lineType的作用是一样的，根据这个来确定具体使用哪个策略方法。 策略1：使用ProtocolResolver来通过location参数创建Resource对象，在ProtocolResolver中关于ProtocolResolver的解释是：A resolution strategy for protocol-specific resource handles-协议专用资源句柄的解析策略。 策略2：返回给定路径上资源的资源句柄。 策略3：以classpath:为前缀的，这种location参数直接返回一个ClassPathResource对象，表示加载classes路径下的资源； 策略4：使用网络协议作为前缀的，比如http、ftp等，这种直接返回一个UrlResource对象； 策略5：无前缀的，在默认实现中和第三种一样是加载classes路径下的资源，不同的是此处当作是ClassPathContextResource来处理的。 Spring中Resource的策(tao)略(lu)说完了，再回过头来看下策略模式的一些具体理论知识。 策略模式 定义：策略模式属于对象的行为模式。其用意是针对一组算法，将每一个算法封装到具有共同接口的独立的类中，从而使得它们可以相互替换。策略模式使得算法可以在不影响到客户端的情况下发生变化。 结合前面的例子分析和这段定义，可以知道，其实策略模式真的意图不是如何实现策略算法，它更在意的是如何组织这些算法。 这也是策略模式的使用可以让程序结构更灵活，具有更好的维护性和扩展性的重要因素。 类图：这个类图画的确实是有点丑，但是为了亲手绘制一下，所以还请多多见谅！ 类图中的一些角色： context：策略背景，也就是需要使用策略的主体；它持有一个strategy类的引用 strategy：抽象策略，这个角色给出了所有具体策略类所需的接口。所以通常是一个抽象类或者接口。 strategyPolicy：具体策略，它的作用就是包装具体的算法或者行为 那么在实际的应用中，策略模式到底给我们带来的好处是什么，它能够帮助我们解决什么样的问题呢？这个需要从模式本身的优缺点来看： 优点 策略模式提供了管理相关的算法族的办法。策略类的等级结构定义了一个算法或行为族。恰当使用继承可以把公共的代码移到父类里面，从而避免代码重复。 策略模式可以避免使用多重条件(if-else)语句。通常对于一个背景主体，一般只会有一种策略算法可供使用，使用多重条件句的话不易维护；因为它把采取哪一种算法或采取哪一种行为的逻辑与算法或行为的逻辑混合在一起了。 缺点 客户端必须知道所有的策略类，并自行决定使用哪一个策略类。这就意味着客户端必须理解这些算法的区别，以便适时选择恰当的算法类。换言之，策略模式只适用于客户端知道算法或行为的情况。 由于策略模式把每个具体的策略实现都单独封装成为类，如果备选的策略很多的话，那么对象的数目就会很可观。–如果策略很多，通常会采用一些混合策略来避免策略类的不断膨胀。 在了解其优缺点的情况下，我们就可以合理的将其放在一些适当的场景中来；如以下场景： 如果在一个系统里面有许多类，它们之间的区别仅在于它们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为。 一个系统需要动态地在几种算法中选择一种。 如果一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重的条件选择语句来实现。 参考 《JAVA与模式》 《JAVA与模式》之策略模式 策略模式]]></content>
      <categories>
        <category>架构之路</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[菜鸟成长系列-模板方法模式]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-design-model-modulemethod%2F</url>
    <content type="text"><![CDATA[模板方法模式在sring中有大量的应用，一般我们会使用模板方法来将当前的实现委托给子类来实现，增强代码的可扩展性和复用性。因为涉及到父子类关系，所以模板方法模式是基于“继承”来实现的；模板方法模式属于行为型模式。 简单地说就是，通过父类来定义一系列的算法骨架，并且约定这些方法及其调用顺序，而具体的某些特定方法由子类实现。 先来看一个小demo；我们以写博客来举例子，一般我们写博客的步骤如下： 打开目标网站 打开编辑器 写文章 发布文章 实例代码首先是定义一个父类，并且提供一个模板方法。 123456789101112131415161718192021222324252627282930313233343536373839404142package com.glmapper.designmode.mudolmethod;/** * @description: 抽象模板父类 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/4/30 */public abstract class AbstractTemplateMethod &#123; /** * 流程方法1：抽象方法，供子类实现 */ public abstract void openTargetWebSite(); /** * 流程方法2：子类可选择重写 */ public void openMarkDown()&#123; System.out.println("打开编辑器"); &#125; /** * 流程方法3：抽象方法，供子类实现 */ public abstract void writeBlog(); /** * 流程方法4：子类可选择重写 */ protected void publisher()&#123; System.out.println("发布文章"); &#125; /** * 模板方法，此处申明为final，是不希望子类覆盖这个方法，防止更改流程的执行顺序 */ public final void templateWriteBlog()&#123; openTargetWebSite(); openMarkDown(); writeBlog(); publisher(); &#125;&#125; 上面代码中我们提供了一个templateWriteBlog方法，这里方法中包括了写博客的一些流程。在这些流程方法中有些方法父类提供了默认实现，而一些具有差异性的方法则让子类来实现。 123456789101112131415161718package com.glmapper.designmode.mudolmethod;/** * @description: 子类 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/4/30 */public class JueJinTemplateMethodPolicy extends AbstractTemplateMethod &#123; @Override public void openTargetWebSite() &#123; System.out.println("打开掘金网站"); &#125; @Override public void writeBlog() &#123; System.out.println("写一篇Spring相关的文章"); &#125;&#125; 子类1：JueJinTemplateMethodPolicy，这个子类中实现了父类中的部分方法，包括：openTargetWebSite和writeBlog。（一般情况下不会去重写父类默认已经实现的方法，仅实现父类中预留的抽象方法来实现）。 1234567891011121314151617181920package com.glmapper.designmode.mudolmethod;/** * @description: 子类 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/4/30 */public class CSDNTemplateMethodPolicy extends AbstractTemplateMethod&#123; @Override public void openTargetWebSite() &#123; System.out.println("打开CSDN网站"); &#125; @Override public void writeBlog() &#123; System.out.println("写一篇设计模式文章"); &#125;&#125; 子类2：CSDNTemplateMethodPolicy,这个子类的作用其实和子类1是一样的，只不过是提供了另外的一种实现策略；（很多情况下，模板方法模式都是和策略模式来联合使用的，通过一套模板机制，对于模板中的部分流程通过不同的策略来实现不同的功能） 1234567891011121314151617181920212223242526package com.glmapper.designmode.mudolmethod;/** * @description: 子类 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/4/30 */public class MainTest &#123; public static void main(String[] args) &#123; AbstractTemplateMethod csdnTemplate = new CSDNTemplateMethodPolicy(); csdnTemplate.templateWriteBlog(); AbstractTemplateMethod juejinTemplate = new JueJinTemplateMethodPolicy(); juejinTemplate.templateWriteBlog(); &#125;&#125;打开CSDN网站打开编辑器写一篇设计模式文章发布文章打开掘金网站打开编辑器写一篇Spring相关的文章发布文章 上面是客户端代码及输出结果。通过输出我们可以明显的看出，模板中的一些方法将延迟到子类中去实现，模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。因此对于模板方法这个模式来说，父类是始终控制着整个流程主动权的，而子类只是辅助父类实现某些可定制的步骤。 模式解析先看下模板方法模式的类图： 从类图中可以看出，模板方法模式中的角色也是很简单的，主要包括两个角色： 抽象模板（AbstractTemplate）： 定义一个或者多个抽象操作，以便于让子类实现。这些抽象操作就是流程中的基本操作（对应的是模板方法中的某个具体的操作方法）；这些基本操作是一个顶级逻辑的组成步骤 定义并且实现了一个模板方法。这个模板方法一般是一个具体方法，它给出了一个顶级逻辑的骨架，而逻辑的组成步骤在相应的抽象操作中，推迟到子类中取实现，当然，在这个顶级逻辑中，部分方法也可以由父类来提供默认实现的。 具体类（SubTemplateImpl)： 实现父类所定义的一个或者多个抽象方法 每一个抽象模板角色都可以有任意多个具体模板角色与之对应，而每一个具体模板角色都可以给出这些抽象方法的不同实现。 模板方法中的这个方法的概念拆开来说包括两种，一种是模板方法，还有一种是模板方法里面的基本方法。模板方法定义游戏规则，基本方法实现规则中的每个部分。 模板方法带来的优势是显而易见的，它可以帮助我们有效的帮助我们搞定下面的这些场景问题： 封装不变部分，扩展可变部分。 提取公共代码，便于维护。 行为由父类控制，子类实现。 但是缺点也很明显，因为对于每一个不同的实现都需要一个子类来实现，导致类的个数增加，使得系统更加庞大。 典型的模板方法模式的应用最先想到的就是servlet，servlet的生命周期(以前经常遇到的面试点，现在已经没人问了吧) 初始化 init 处理 service 销毁 destroy 其实这里我觉得也是模板方法的一种体现，虽然在servlet中没有定义顶层的模板方法来控制这个流程(我的想法是这个流程是由容器来控制的，也可能是一种默认的约定)。 在其子类GenericServlet中对init和destroy有了默认的实现，而service方法则是交由子类来实现的，也就是说任何servlet类均必须实现service方法。 这里的service方法就是一个模板方法。service方法中调用了7个do方法中的一个或者几个，完成对客户端的响应，这些do方法需要由HttpServlet的具体子类提供。 HttpServlet中的实现： 1234567891011121314151617181920212223242526272829303132333435363738394041protected void service(HttpServletRequest req, HttpServletResponseresp) throws ServletException, IOException &#123; String method = req.getMethod(); long lastModified; if (method.equals("GET")) &#123; lastModified = this.getLastModified(req); if (lastModified == -1L) &#123; this.doGet(req, resp); &#125; else &#123; long ifModifiedSince = req.getDateHeader("If-Modified-Since"); if (ifModifiedSince &lt; lastModified) &#123; this.maybeSetLastModified(resp, lastModified); this.doGet(req, resp); &#125; else &#123; resp.setStatus(304); &#125; &#125; &#125; else if (method.equals("HEAD")) &#123; lastModified = this.getLastModified(req); this.maybeSetLastModified(resp, lastModified); this.doHead(req, resp); &#125; else if (method.equals("POST")) &#123; this.doPost(req, resp); &#125; else if (method.equals("PUT")) &#123; this.doPut(req, resp); &#125; else if (method.equals("DELETE")) &#123; this.doDelete(req, resp); &#125; else if (method.equals("OPTIONS")) &#123; this.doOptions(req, resp); &#125; else if (method.equals("TRACE")) &#123; this.doTrace(req, resp); &#125; else &#123; String errMsg = lStrings.getString("http.method_not_implemented"); Object[] errArgs = new Object[]&#123;method&#125;; errMsg = MessageFormat.format(errMsg, errArgs); resp.sendError(501, errMsg); &#125; &#125; FrameworkServlet中的实现(FrameworkServlet是SpringMVC核心控制器DispatchServlet的父类)： 1234567891011121314151617/** * Override the parent class implementation in order to intercept PATCH requests. */@Overrideprotected void service(HttpServletRequest request,HttpServletResponse response) throws ServletException, IOException &#123; HttpMethod httpMethod = HttpMethod.resolve(request.getMethod()); if (HttpMethod.PATCH == httpMethod || httpMethod == null) &#123; processRequest(request, response); &#125; else &#123; super.service(request, response); &#125;&#125; 关于模板方法模式的学习就到这里了。]]></content>
      <categories>
        <category>架构之路</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[菜鸟成长系列-观察者模式]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-design-model-obs%2F</url>
    <content type="text"><![CDATA[最近想深入研究下响应式编程，作为基础很有必要来把观察者模式撸一遍；一开始我是觉得很easy,然后就直接开撸了，撸着撸着发现撸不动了。因为我突然不太明白这个模式了，说好的观察者，到底发布-订阅的两者执行者谁才是观察者？又或者说还有其他角色？但是根据《JAVA与模式》一书中的结构，并没有额外的角色出现。 思考中….，好吧想不出来….，跑步去… 跑步时我给自己罗列了几个问题： 这里先抛出定义：GOF给观察者模式如下定义：定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。 既然是对象状态发生变更，那么到底是谁的状态发生了变更，又导致了谁被通知。 观察者模式既然又可以称之为“发布-订阅模式”，那么对应起来，观察者到底承当了“发布”的角色还是“订阅”的角色。就是说观察者到底是主动的还是被动的？ 被观察者又干了什么事？它是主动的还是被动的角色？ 这里由于一些定式思维，总会觉得既然是“被观察者”，那么这个“被”字就是不是就表明“被观察者”是被动接受变更的一方，也就是接受通知的一方呢？ 之前我也是走到这个胡同里了，程序写完总觉得哪里不对；回过头看，还是自己太年轻，没有get到哪些大佬们的点。 先来看程序；这里用掘金来打个比方，我的博客glmmaper作为被观察者，也就是发布者。掘金小伙伴们作为观察者，也就是订阅者。 具体逻辑：小伙伴们（订阅者）关注（订阅）了我的博客（发布者），如果我发布了一篇文章（状态变更），就会通知（推送消息）所有关注我的小伙伴。 123456789101112131415161718192021222324252627package com.glmapper.designmode.observor;/** * @description: 抽象主题接口 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/4/22 */public interface Subject &#123; /** * 新增关注者 * @param observer 关注的小伙伴 */ void addFocusObserver(Observer observer); /** * 取消关注 * @param observer 取消关注的小伙伴 */ void removeFocusObserver(Observer observer); /** * 通知机制，通知机制由相关事件来触发，比如说发布文章 * @param blogName 博客名 * @param articleName 文章名 */ void notifyObservers(String blogName,String articleName);&#125; 三个方法，一个是博客主页增加了一个关注者；一个是博客主页有小伙伴取消的关注（对于博客来说就是移除一个关注者，这里不知道是否也会觉得别扭？明明你取消的关注，为啥说成是我移除你，也就是不让你关注了，还能这么玩?这里肯定是需要在引入其他的一些辅助机制，比如说你在客户端发起了一个取消关注的请求，后端处理的时候掘金的工程师们就是在我的关注列表中将你移除的，嗯，这么一想确实是我不让你关注了。😄….）；最后一个方法是发起一个通知。下面是一个具体的博客，比如说是glmapper； 1234567891011121314151617181920212223242526272829303132333435363738394041package com.glmapper.designmode.observor;import java.util.ArrayList;import java.util.List;/** * @description: 这个是具体发布者，这里比喻成我的博客glmapper * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/4/22 */public class ConcreteSubject implements Subject &#123; /** 我的当前关注列表 */ List&lt;Observer&gt; Observers = new ArrayList&lt;&gt;(); /** 我的博客名 ：求关注 */ private static final String blogName = "glmapper"; @Override public void addFocusObserver(Observer observer) &#123; Observers.add(observer); &#125; @Override public void removeFocusObserver(Observer observer) &#123; Observers.remove(observer); &#125; @Override public void notifyObservers(String blogName,String articleName) &#123; for (Observer observer:Observers) &#123; observer.update(blogName,articleName); &#125; &#125; /** * 这里是发布文章，触发通知事件 */ public void publishArticle(String articleName)&#123; notifyObservers(blogName,articleName); &#125;&#125; 前面提到，通知事件肯定是由于某些状态发生变更了，才会进行通知，这里就可以比方为我发布了一篇博客，然后通知你（这里只能假如你关注了）。再来看观察者： 12345678910111213141516package com.glmapper.designmode.observor;/** * @description: 订阅者抽象接口 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/4/22 */public interface Observer &#123; /** * 调用此方法会更新状态，做出相应的动作 * @param blogName * @param articleName */ void update(String blogName,String articleName);&#125; 抽象订阅者，有一个update方法，通知你去做出相应的动作，具体动作每个观察者都可能不同。 123456789101112131415161718192021package com.glmapper.designmode.observor;/** * @description: 这个是具体订阅者,这里可以比喻成博客关注者， * 收到变更信息之后需要做出相应的动作 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/4/22 */public class ConcreteObserver implements Observer &#123; @Override public void update(String blogName,String articleName) &#123; System.out.println(blogName+"发布了新的文章，文章名为："+articleName); read(articleName); &#125; private void read(String articleName)&#123; System.out.println("即将阅读 "+articleName+" 这篇文章"); &#125;&#125; 上面是一个具体的关注者，加入说就是你。博客更新之后发了一个通知给你(掘金app推送的消息)，然后你点了一下，这个也是一种动作。例子中举的是read,就是关注者做出阅读的动作。 看下最后的运行结果： 1234567891011121314151617181920212223242526package com.glmapper.designmode.observor;/** * @description: [描述文本] * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: 磊叔 * @date: 18/4/22 */public class MyMainIndex&#123; public static void main(String[] args) &#123; //博客主体 ConcreteSubject subject = new ConcreteSubject(); //关注者：handSome是帅气的意思 Observer handSome = new ConcreteObserver(); //增加一个关注者 subject.addFocusObserver(handSome); //发一篇文章 subject.publishArticle("设计模式-观察者模式"); &#125;&#125;glmapper发布了新的文章，文章名为：设计模式-观察者模式即将阅读 设计模式-观察者模式 这篇文章 酒桶说：啊，欢乐时光总是短暂的 所以作为积累，还是需要将一些基本的概念来罗列一下的。 主要角色： 抽象主题角色（Subject：主题角色将所有对观察者对象的引用保存在一个集合中，每个主题可以有任意多个观察者。抽象主题提供了增加和删除等观察者对象的接口。 抽象观察者角色（Observer）：为所有的具体观察者定义一个接口，在观察的主题发生改变时更新自己。 具体主题角色（ConcreteSubject）(1个)：存储相关状态到具体观察者对象，当具体主题的内部状态改变时，给所有登记过的观察者发出通知。具体主题角色通常用一个具体子类实现。 具体观察者角色（ConcretedObserver）(多个)：存储一个具体主题对象，存储相关状态，实现抽象观察者角色所要求的更新接口，以使得其自身状态和主题的状态保持一致。 具体关系： 抽象主题（Subject）(接口)–&gt;被具体主题（ConcreteSubject）角色(1个)实现 抽象观察者（Observer）(接口)–&gt;被具体观察者（ConcretedObserver）角色(N个)实现 观察者对象载入主题方法,并在主题方法中调用观察者对象实现的接口方法update来让自己发生变更响应。 一些场景： 当对一个对象的的改动会引发其他对象的变动时，而且你无法预测有多少个对象需要被改动。 当一个对象需要有能力通知其他对象，且不需要了解这些对象是什么类型时。 基于发布订阅的具体实现例子还是很多的，比较典型的就是这种订阅一个博客，然后博客更新推送；还有微信公众号，服务号这些。 到这里我们再回过头来看一开始留下的几个问题： 被观察者的状态发生变更，然后“主动通知”观察者，并不是说，观察者主动去获取通知。 被观察者是消息发布者，观察者是消息订阅者；观察者是被动接受者。 被观察者的作用就是存储当前的观察者列表，然后提供一些通知机制来告诉观察者自己发生了状态变更，是主动者。 OK，观察者模式就撸到这里，也欢迎小伙伴们提出自己珍贵的意见；有写的不当之处烦请及时提出。]]></content>
      <categories>
        <category>架构之路</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码系列：九大组件小记]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-base-webmvc5%2F</url>
    <content type="text"><![CDATA[前面几篇文章都是针对于SpringMVC中的具体组件进行源码分析的；本文主要用于补充记录一下关于SpringMVC中九大组件的学习。这个会牵扯出除之前的几篇HandlerMapping之外的其他一些基础组件。 之前简单的有介绍过DispatcherServlet这个类的体系结构，此处就不再赘述了。在DispatcherServlet类中，其在mvc子容器进行初始化时就会完成对九大组件的初始化工作，具体哪九大组件后面会慢慢说到。先来看下在DispatcherServlet中是通过哪些方法来完成初始化工作的,先贴一段代码： 123456789101112131415protected void onRefresh(ApplicationContext context) &#123; this.initStrategies(context);&#125;protected void initStrategies(ApplicationContext context) &#123; this.initMultipartResolver(context); this.initLocaleResolver(context); this.initThemeResolver(context); this.initHandlerMappings(context); this.initHandlerAdapters(context); this.initHandlerExceptionResolvers(context); this.initRequestToViewNameTranslator(context); this.initViewResolvers(context); this.initFlashMapManager(context);&#125; 上面代码中的onRefresh方法就是DispatcherServlet的入口方法。在onRefresh中又通过调用initStrategies方法来将各个组件的初始化逻辑进行整合，个人理解其实就是策略套策略，在一个就是职责也明确。 在initStrategies方法中又通过调用组件各自的初始化方法来完成具体的初始化工作。从这个地方其实就可以清楚的看出SpringMVC中的9个组件名称了。下面就来捋一捋这九大组件的基本职责。 HandlerMapping关于handlermapping在下面几篇文章中做过一些基本介绍，但是还不是很全，对于handlermapping的子类还没有分析完，这个会后期更新的。 SpringMVC源码系列：HandlerMapping SpringMVC源码系列：AbstractHandlerMapping SpringMVC源码系列：AbstractUrlHandlerMapping 对于HandlerMapping来说，其作用就是根据request找到相应的处理器Handler和Intecepter拦截器。具体细节参数上面第一篇文章。 HandlerAdapter如果说HandlerMapping是一支笔，那么HandlerAdapter就是用笔的人。也就是说HandlerAdapter就是使用处理器干活的人。为什么呢？来看下代码： 12345public interface HandlerAdapter &#123; boolean supports(Object var1); ModelAndView handle(HttpServletRequest var1, HttpServletResponse var2, Object var3) throws Exception; long getLastModified(HttpServletRequest var1, Object var2);&#125; 是不是一目了然了，在HandlerAdapter接口中提供了handle这样一个方法，参数中Object handler第三个参数其实就是一个处理器，那我们就知道了，handle方法就是使用handler来处理逻辑的。处理之后返回一个ModelAndView。 HandlerExceptionResolver这个是SpringMVC中的异常处理组件，HandlerExceptionResolver这个组件的作用就是根据异常设置ModelAndView，然后再将处理结果交给render方法进行渲染。当然render也仅仅只是负责将ModelAndView渲染成页面，ModelAndView的具体来源它不关心。 这里需要说明一下，加入在渲染过程中发生异常怎么办？从上面的分析我们可以清楚的知道，HandlerExceptionResolver这个组件对异常的处理结果是ModelAndView，然后再由render方法进行渲染，也就是说HandlerExceptionResolver是在渲染之前工作的，因此渲染过程中发生异常，HandlerExceptionResolver是不会处理的。 123public interface HandlerExceptionResolver &#123; ModelAndView resolveException(HttpServletRequest var1, HttpServletResponse var2, Object var3, Exception var4);&#125; 在HandlerExceptionResolver中也只有一个方法，这个方法就是从异常中解析出ModelAndView。 ViewResolverViewResolver的作用是将String类型的逻辑视图根据local解析为View视图的。下面是ViewResolver的源码接口定义： 123public interface ViewResolver &#123; View resolveViewName(String viewName, Locale local) throws Exception;&#125; 从代码中可以看到，在ViewResolver中也是只有一个方法，从resolveViewName方法的参数和返回结果就很好的解释了其作用。 viewName String类型的视图名 local 区域，可以用来做国际化。 View实际上是用来渲染页面的，也就是说将程序返回的结果填入到具体的模板里面，生成具体的视图文件，比如：jsp，ftl，html等。 但是这里又会牵扯出两个问题： 用什么模板？ 参数怎么填入？ 当然，这两个问题也就是本小节说的ViewResolver需要解决的问题。大体分为两种： 针对单一视图类型的解析器 InternalResourceViewResolver FreeMarkerViewResolver 上面两种是用的最多的两种，InternalResourceViewResolver用来解析jsp，而FreeMarkerViewResolver则是针对FreeMarker。 针对同时解析多种类型视图的解析器 BeanNameViewResolver 需要同时使用视图名和对应的local来解析视图。它需要将每一个视图名和对应的视图类型配置到相应的properties文件中。（后面讲组件实现细节时给出列子） XmlViewResolver XmlViewResolver和BeanNameViewResolver有点差不多，BeanNameViewResolver使用的是xml格式的配置文件。 ResourceBundleViewResolver 这个其实就是根据viewName从Spring容器中查找bean，再根据这个bean来找到对应的视图。 LocalResolver在上面的ViewResolver中提到，解析视图需要两个参数，一个是String类型的逻辑视图名，另外一个是local。LocalResolver的作用就是从request中解析出local的。 12345public interface LocaleResolver &#123; Locale resolveLocale(HttpServletRequest request); void setLocale(HttpServletRequest request, HttpServletResponse response, Locale local);&#125; 第一个方法是从request中解析出local，第二个方法是将local设置到request中。 关于local大多数情况下都是用来做国际化处理的。 ThemeResolver解析主题的。这个我平时除了SpringMVC自己提供的功能外，很少自己去扩展使用，即使是换主题也没有做过。不过既然存在肯定是有存在的原因的。对于我们常见的网页界面活着手机界面来说，一套主题无非就是换一套图片，活着css样式文件等等。我们通过ThemeResolver这个就可以实现这样的功能。具体使用其实也就是配一套properties文件供系统在不同的时候读取切换；当然使用这个也是可以实现国际化的。 12345public interface ThemeResolver &#123; String resolveThemeName(HttpServletRequest request); void setThemeName(HttpServletRequest request, HttpServletResponse response, String themeName);&#125; RequestToViewNameTranslator这个其实还是挺有意思的，就是将request请求转换为视图名称。 123public interface RequestToViewNameTranslator &#123; String getViewName(HttpServletRequest request) throws Exception;&#125; RequestToViewNameTranslator只有一个默认的实现类DefaultRequestToViewNameTranslator。 在DefaultRequestToViewNameTranslator具体实现了getViewName(HttpServletRequest request)方法： 1234public String getViewName(HttpServletRequest request) &#123; String lookupPath = this.urlPathHelper.getLookupPathForRequest(request); return this.prefix + this.transformPath(lookupPath) + this.suffix;&#125; 主要是委派给urlPathHelper帮助类得到请求的后缀名称，比如通过 请求路径比如/glmapper/login.do转换得到/login.do ；具体怎么转换成视图也会在后面的组件介绍中给出具体的例子。 MultipartResolver这个相应小伙伴们也不陌生，做网站多多少少会涉及到文件上传。MultipartResolver就是用来处理上传请求的。其处理方式就是将request包装成MultipartHttpServletRequest。然后我们就可以用MultipartHttpServletRequest这个直接调用getFile获取的文件了。 FlashMapManager这个在redirect是进行参数传递需要用到。 12345public interface FlashMapManager &#123; FlashMap retrieveAndUpdate(HttpServletRequest request, HttpServletResponse response); void saveOutputFlashMap(FlashMap flashMap, HttpServletRequest request, HttpServletResponse response);&#125; retrieveAndUpdate这个方法是用来恢复参数的，对于恢复过的和超时的参数将都会被删除掉。 saveOutputFlashMap这个方法是用来保存参数的。 FlashMapManager的默认实现机制中参数的存储是放在session中的。我之前在一个项目中就有遇到过这种情况，对于一些我们不想暴露在url中的参数，在进行请求转发时，可以使用@RedirectAttributes将参数保存，然后在下一个处理器中获取到。 小结本篇主要是来对九大组件做一个笼统的介绍，细节实现及案例均不涉及；在后续的SpringMVC源码系列中对各个组件的实现细节分析时再一探究竟吧。]]></content>
      <categories>
        <category>spring mvc</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>web</tag>
        <tag>mvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码系列：AbstractUrlHandlerMapping]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-base-webmvc4%2F</url>
    <content type="text"><![CDATA[AbstractUrlHandlerMapping是通过url来进行匹配的，也就是说通过url与对应的Handler包存到一个Map中，然后在getHandlerInternal方法中使用url作为key从Map中获取我们的handler。 AbstractUrlHandlerMapping实现了从url获取handler的过程，具体的映射关系，也就是handlerMap则是交给具体子类来去完成的。AbstractUrlHandlerMapping中定义了handlerMap用来维护映射关系，如下：12private final Map&lt;String, Object&gt; handlerMap = new LinkedHashMap&lt;String, Object&gt;(); 除此之外，还有一个rootHandler,这个用于处理“/”请求。 在前面三篇文章中提到过，handler的获取是通过getHandlerInternal方法完成的，下面看下具体的源码，分析下handler的获取和handlerMap的构建。 12345678910111213141516171819202122232425262728293031323334353637383940414243//查找给定请求的URL路径的Handler。protected Object getHandlerInternal(HttpServletRequest request) throwsException &#123; String lookupPath = getUrlPathHelper().getLookupPathForRequest(request); //使用lookupPath从Map中查找handler Object handler = lookupHandler(lookupPath, request); if (handler == null) &#123; //临时变量，保存原始的handler Object rawHandler = null; //是否是‘/’根路径 if ("/".equals(lookupPath)) &#123; //获取rootHandler rawHandler = getRootHandler(); &#125; //如果rawHandler是null if (rawHandler == null) &#123; //获取默认的handler rawHandler = getDefaultHandler(); &#125; //如果rawHandler不是null if (rawHandler != null) &#123; // 如果是string类型，则到容器中查找具体的bean if (rawHandler instanceof String) &#123; String handlerName = (String) rawHandler; //容器中获取 rawHandler = getApplicationContext().getBean(handlerName); &#125; //校验handler和request是否匹配 validateHandler(rawHandler, request); //注册拦截器 handler = buildPathExposingHandler(rawHandler, lookupPath, lookupPath, null); &#125; &#125; //日志debug if (handler != null &amp;&amp; logger.isDebugEnabled()) &#123; logger.debug("Mapping [" + lookupPath + "] to " + handler); &#125; else if (handler == null &amp;&amp; logger.isTraceEnabled()) &#123; logger.trace("No handler mapping found for [" + lookupPath + "]"); &#125; //返回handler return handler;&#125; 在getHandlerInternal方法中有几个方法调用，像getLookupPathForRequest、getRootHandler、getDefaultHandler、lookupHandler、buildPathExposingHandler等。其中getLookupPathForRequest、getRootHandler、getDefaultHandler这几个没啥好说的；比较核心的就是lookupHandler、buildPathExposingHandler这两个方法。 lookupHandler lookupHandler使用getUrlPathHelper().getLookupPathForRequest(request)获取到的lookupPath从Map中查找需要的Handler,通常情况下是直接get不到的。为什么呢？原因在于很多的handler都是使用了Pattern的匹配模式，比如说“/user/*”,星号表示匹配任意内容，并非是指定url串中的字符。如果Pattern中包含了PathVariable,也不能直接从Map中获取到。 除此之外，一个url还可能和多个Pattern相匹配，那么这个时候咱们肯定就需要选择最优的，所以说查找过程其实并不是直接从map中获取那么简单。那么就来看下在lookupHandler中都干了哪些事情： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374protected Object lookupHandler(String urlPath, HttpServletRequest request) throws Exception &#123; // 直接匹配，直接从Map中获取 Object handler = this.handlerMap.get(urlPath); //取到了 if (handler != null) &#123; // 如果是string类型，则从容器中获取Bean if (handler instanceof String) &#123; String handlerName = (String) handler; handler = getApplicationContext().getBean(handlerName); &#125; //验证是否匹配 validateHandler(handler, request); //注册拦截器 return buildPathExposingHandler(handler, urlPath, urlPath, null); &#125; // Pattern 匹配，带*号的模式与url进行匹配 List&lt;String&gt; matchingPatterns = new ArrayList&lt;String&gt;(); for (String registeredPattern : this.handlerMap.keySet()) &#123; if (getPathMatcher().match(registeredPattern, urlPath)) &#123; matchingPatterns.add(registeredPattern); &#125; else if (useTrailingSlashMatch()) &#123; if (!registeredPattern.endsWith("/") &amp;&amp; getPathMatcher().match(registeredPattern + "/", urlPath)) &#123; matchingPatterns.add(registeredPattern +"/"); &#125; &#125; &#125; //获取最佳匹配 String bestPatternMatch = null; Comparator&lt;String&gt; patternComparator = getPathMatcher().getPatternComparator(urlPath); if (!matchingPatterns.isEmpty()) &#123; Collections.sort(matchingPatterns, patternComparator); if (logger.isDebugEnabled()) &#123; logger.debug("Matching patterns for request [" + urlPath + "] are " + matchingPatterns); &#125; bestPatternMatch = matchingPatterns.get(0); &#125; //最佳匹配不为null if (bestPatternMatch != null) &#123; //从Map中看看是否有对应的Handler handler = this.handlerMap.get(bestPatternMatch); //如果Map中没有 if (handler == null) &#123; //是否以/结尾 Assert.isTrue(bestPatternMatch.endsWith("/")); //去除/之后再获取一次 handler = this.handlerMap.get(bestPatternMatch.substring(0, bestPatternMatch.length() - 1)); &#125; // 如果是String类型，则从容器中获取Bean? if (handler instanceof String) &#123; String handlerName = (String) handler; handler = getApplicationContext().getBean(handlerName); &#125; //验证是否匹配 validateHandler(handler, request); String pathWithinMapping = getPathMatcher().extractPathWithinPattern(bestPatternMatch, urlPath); // 可能有多种最佳模式，让我们确保我们有正确的URI模板变量（译） Map&lt;String, String&gt; uriTemplateVariables = new LinkedHashMap&lt;String, String&gt;(); for (String matchingPattern : matchingPatterns) &#123; if (patternComparator.compare(bestPatternMatch, matchingPattern) == 0) &#123; Map&lt;String, String&gt; vars = getPathMatcher().extractUriTemplateVariables(matchingPattern, urlPath); Map&lt;String, String&gt; decodedVars = getUrlPathHelper().decodePathVariables(request, vars); uriTemplateVariables.putAll(decodedVars); &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug("URI Template variables for request [" + urlPath + "] are " + uriTemplateVariables); &#125; return buildPathExposingHandler(handler, bestPatternMatch, pathWithinMapping, uriTemplateVariables); &#125; // No handler found... return null;&#125; 上面代码中，关于译注的部分需要说一下；代码如下： 123456789Map&lt;String, String&gt; uriTemplateVariables = new LinkedHashMap&lt;String, String&gt;();for (String matchingPattern : matchingPatterns) &#123; if (patternComparator.compare(bestPatternMatch, matchingPattern) == 0) &#123; Map&lt;String, String&gt; vars = getPathMatcher().extractUriTemplateVariables(matchingPattern, urlPath); Map&lt;String, String&gt; decodedVars = getUrlPathHelper().decodePathVariables(request, vars); uriTemplateVariables.putAll(decodedVars); &#125;&#125; 之前是通过sort方法进行排序的，然后将第一个作为bestPatternMatch，但是如果多个pattern的顺序相同，也就是说sort返回的是0,存在多种最佳匹配，那就需要确保我们有正确的URI模板变量。上面代码就是处理这种情况的。 buildPathExposingHandler 这个方法在上面的两段代码中都频繁出现，那么这个方法到底有什么作用呢？代码中我注释的是注册拦截器，那么注册的又是什么拦截器？带着这两个问题，我们来看下代码。 123456789101112131415161718//buildPathExposingHandler为给定的rawHandler构建一个Handler对象，并在执//行处理程序之前暴露实际的处理程序PATH_WITHIN_HANDLER_MAPPING_ATTRIBUT//E以及URI_TEMPLATE_VARIABLES_ATTRIBUTE。//默认实现用一个特殊的拦截器构建一个HandlerExecutionChain，该拦截器暴露//path属性和uri模板变量。protected Object buildPathExposingHandler(Object rawHandler, String bestMatchingPattern, String pathWithinMapping, Map&lt;String, String&gt; uriTemplateVariables) &#123; HandlerExecutionChain chain = new HandlerExecutionChain(rawHandler); chain.addInterceptor(new PathExposingHandlerInterceptor(bestMatchingPattern, pathWithinMapping)); if (!CollectionUtils.isEmpty(uriTemplateVariables)) &#123; chain.addInterceptor(new UriTemplateVariablesHandlerInterceptor(uriTemplateVariables)); &#125; return chain;&#125; 四个参数： rawHandler 原始处理程序 bestMatchingPattern 最佳匹配模式 pathWithinMapping 在执行Handler之前公开的路径 uriTemplateVariables 如果没有找到变量，URI模板变量可以是{null} 从代码注释翻译及代码内容可以了解到，buildPathExposingHandler的作用就是给已经查找到的handler注册两个拦截器 ExposingHandlerInterceptor UriTemplateVariablesHandlerInterceptor 这两个类均是AbstractUrlHandlerMapping的内部类，也就是两个内部拦截器。这两个拦截器的主要作用就是将与当前url实际匹配的pattern、匹配条件以及url模板参数等设置到request的属性里面去，这样在后面的处理过程中就可以直接从request属性中获取。看下两个内部类的定义： 123456789101112131415161718192021222324252627282930313233private class PathExposingHandlerInterceptor extends HandlerInterceptorAdapter &#123; private final String bestMatchingPattern; private final String pathWithinMapping; public PathExposingHandlerInterceptor(String bestMatchingPattern, String pathWithinMapping) &#123; this.bestMatchingPattern = bestMatchingPattern; this.pathWithinMapping = pathWithinMapping; &#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; exposePathWithinMapping(this.bestMatchingPattern, this.pathWithinMapping, request); //设置request属性 request.setAttribute(HandlerMapping.INTROSPECT_TYPE_LEVEL_MAPPING, supportsTypeLevelMappings()); return true; &#125;&#125;private class UriTemplateVariablesHandlerInterceptor extends HandlerInterceptorAdapter &#123; private final Map&lt;String, String&gt; uriTemplateVariables; public UriTemplateVariablesHandlerInterceptor(Map&lt;String, String&gt; uriTemplateVariables) &#123; this.uriTemplateVariables = uriTemplateVariables; &#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; //这exposeUriTemplateVariables种设置request属性 exposeUriTemplateVariables(this.uriTemplateVariables, request); return true; &#125;&#125; 从内部类的代码可以看出，这两个内部类是通过在preHandle方法中调用exposePathWithinMapping和exposeUriTemplateVariables完成属性设置到request中的。 对于查找handler的关键其实就是维护url和handler的映射关系，也就是handlerMap的构建。在AbstractUrlHandlerMapping中是通过registerHandler这个方法来构建handlerMap的。AbstractUrlHandlerMapping提供了两个registerHandler方法，下面就通过代码来看下具体的实现。 1234567protected void registerHandler(String[] urlPaths, String beanName) throws BeansException, IllegalStateException &#123; Assert.notNull(urlPaths, "URL path array must not be null"); for (String urlPath : urlPaths) &#123; registerHandler(urlPath, beanName); &#125;&#125; 第一个registerHandler是将多个url注册到一个处理器。beanName其实就是咱们处理器的名称，可以通过beanName到容器中去找到真正的处理器Bean。具体处理就是通过遍历所有的url，然后再通过调用第二个registerHandler将handler注册到handlerMap中。来看第二个： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748protected void registerHandler(String urlPath, Object handler) throwsBeansException, IllegalStateException &#123; Assert.notNull(urlPath, "URL path must not be null"); Assert.notNull(handler, "Handler object must not be null"); Object resolvedHandler = handler; // 如果的handler是string类型，并且不是lazyInitHandlers，则从SpringMV //C容器中获取handler if (!this.lazyInitHandlers &amp;&amp; handler instanceof String) &#123; String handlerName = (String) handler; if (getApplicationContext().isSingleton(handlerName)) &#123; resolvedHandler = getApplicationContext().getBean(handlerName); &#125; &#125; Object mappedHandler = this.handlerMap.get(urlPath); if (mappedHandler != null) &#123; if (mappedHandler != resolvedHandler) &#123; //异常处理 &#125; &#125; else &#123; //是否是跟路径 if (urlPath.equals("/")) &#123; if (logger.isInfoEnabled()) &#123; logger.info("Root mapping to " + getHandlerDescription(handler)); &#125; setRootHandler(resolvedHandler); &#125; //是否是*模式 else if (urlPath.equals("/*")) &#123; if (logger.isInfoEnabled()) &#123; logger.info("Default mapping to " + getHandlerDescription(handler)); &#125; setDefaultHandler(resolvedHandler); &#125; //加入到handlerMap中 else &#123; this.handlerMap.put(urlPath, resolvedHandler); if (logger.isInfoEnabled()) &#123; logger.info("Mapped URL path [" + urlPath + "] onto " + getHandlerDescription(handler)); &#125; &#125; &#125;&#125; 这个里面首先是看Map中是否有原来传入的url，如果没有就加入，如果有就看下原来保存的和当前注册的handler是否是同一个，如果不是同一个就抛出异常。（同一个url不可能存在两个不同的handler）。 在put之前，也做了一些“/”和“/*”的验证处理，如果是这两种路径的话就不保存到handlerMap中了。 “/”：setRootHandler(resolvedHandler); “/*”：setDefaultHandler(resolvedHandler); OK，到这AbstractUrlHandlerMapping这个类就分析完了，其实AbstractUrlHandlerMapping做的事情就是定义了一个框子，子类只要完成对Map的初始化就可以了。关于AbstractUrlHandlerMapping的子类后续再谈。]]></content>
      <categories>
        <category>spring mvc</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>web</tag>
        <tag>mvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：注解说明]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-annotation%2F</url>
    <content type="text"><![CDATA[因为要看Spring中注解的具体定义，所以在说之前，先来简单说下JAVA中注解的一些基本知识。 元注解什么是元注解呢，就是注解的注解。java中提供了以下几种： @Target 注解的作用域描述 123456789101112131415161718public enum ElementType &#123; /** 类, 接口 或者枚举 */ TYPE, /** 字段 */ FIELD, /** 方法 */ METHOD, /** 参数 */ PARAMETER, /** 构造方法 */ CONSTRUCTOR, /** 局部变量 */ LOCAL_VARIABLE, /** 注解类型 */ ANNOTATION_TYPE, /** 包 */ PACKAGE&#125; @Retention 生命周期描述 1234567891011121314public enum RetentionPolicy &#123; /** * 在原文件中有效，被编译器丢弃。 */ SOURCE, /** * 在class文件有效，可能会被虚拟机忽略。 */ CLASS, /** * 在运行时有效。 */ RUNTIME&#125; @Inherited 标识性的元注解，它允许子注解继承它。 @Documented 用于标准生成javadoc时会包含的注解。 JAVA中注解的定义方式1public @interface 注解名 &#123;定义体&#125; 上面试一些基本概念点，关注注解其他的一些特性和用法就不细说了。直接看Spring中的注解吧。 1、@Component12345678910111213@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Component &#123; /** * The value may indicate a suggestion for a logical component name, * to be turned into a Spring bean in case of an autodetected component. * @return the suggested component name, if any */ String value() default "";&#125; 指示注释类是“组件”。 当使用基于注释的配置和类路径扫描时，这些类被认为是自动检测的候选对象。 2、@Controller1234567@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface Controller &#123; String value() default "";&#125; 使用过Spring mvc的小伙伴对于这个注解肯定不陌生。@Controller表示注释的类是“控制器”（例如Web控制器）。这个注解作为@Component的一个特定方式存在，允许通过类路径扫描来自动检测实现类。通常情况下会结合RequestMapping注解使用。从它的定义层面来看，这个注解只能用于接口或者类上面，不能用于方法或者属性字段上面。 3、@Service1234567@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface Service &#123; String value() default "";&#125; 表示注释类是一个“服务”，最初由Domain-Driven Design （Evans，2003）定义为“作为模型中独立的接口提供的操作，没有封装状态”。 在一般情况下，我们把他用在标准我们的service服务接口的实现类上面，实际上这相当于缩小它们的语义和适当的使用。 @Service这个注释作为 @Component的一个特例，允许通过类路径扫描来自动检测实现类。 4、@Repository123456789101112@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface Repository &#123; /** * The value may indicate a suggestion for a logical component name, * to be turned into a Spring bean in case of an autodetected component. * @return the suggested component name, if any */ String value() default "";&#125; 用于标注数据访问组件，即DAO组件 5、@RequestMapping123456789101112131415161718192021222324@Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Mappingpublic @interface RequestMapping &#123; String name() default ""; @AliasFor("path") String[] value() default &#123;&#125;; @AliasFor("value") String[] path() default &#123;&#125;; RequestMethod[] method() default &#123;&#125;; String[] params() default &#123;&#125;; String[] headers() default &#123;&#125;; String[] consumes() default &#123;&#125;; String[] produces() default &#123;&#125;;&#125; @RequestMapping是一个用来处理地址映射请求的注解，从定义可以看出，可作用于方法或者类上。 用于类上，大多数是为了进行区分controller 用于方法上则是对方法进行注解以产生访问的路径。 它包括了几个属性： value 用于设置方法或者类的映射路径，可以直接写路径。我们通常都是直接写，例如：@RequestMapping(“/XXX”); method 用于指定请求的方法，可以设置单个或多个，如果请求方法不满足条件则会请求失败。 params 指定request中必须包含某些参数值是，才让该方法处理。 name 此映射指定一个名称 path 仅在Servlet环境中：路径映射URI（例如“/myPath.do”）。也支持Ant风格的路径模式（例如“/myPath/*.do”）。在方法级别，在类型级别表示的主映射内支持相对路径（例如“edit.do”）。 路径映射URI可能包含占位符（例如“/ $ {connect}”） consumes 指定处理请求的提交内容类型（Content-Type），例如application/json, text/html; produces 指定返回的内容类型，仅当request请求头中的(Accept)类型中包含该指定类型才返回； headers 指定request中必须包含某些指定的header值，才能让该方法处理请求。 其他的几个没怎么用过，确实不了解，有知道的小伙伴，欢迎留言。 6、@ResponseBody@ResponseBody这个我一般是用在做异步请求调用的方法上来使用的。因为在使用@RequestMapping后，返回值通常解析为跳转路径。加上@responsebody后，返回结果直接写入HTTP response body中，不会被解析为跳转路径。 对于异步请求，我们不希望返回解析视图，二是希望响应的结果是json数据，那么加上@responsebody后，就会直接返回json数据。 7、@AutowiredAutowired就是自动装配的意思，其作用是为了消除代码Java代码里面的getter/setter与bean属性中的property。当然，getter看个人需求，如果私有属性需要对外提供的话，就应该保留。 @Autowired默认按类型匹配的方式，在容器查找匹配的Bean，当有且仅有一个匹配的Bean时，Spring将其注入@Autowired标注的变量中。 但是当接口存在两个实现类的时候必须使用@Qualifier指定注入哪个实现类，否则可以省略，只写@Autowired。 8、@Qualifier@Qualifier用于指定注入Bean的名称，就是上面说到的，如果容器中有一个以上匹配的Bean，则可以通过@Qualifier注解限定Bean的名称。 9、@Resource这个注解不是Spring的，放在这里是为了和@Autowired做一个区别。@Resource默认按名称装配，当找不到与名称匹配的bean才会按类型装配。 10、@PathVariable当使用@RequestMapping URI template 样式映射时， 即 someUrl/{paramId}, 这时的paramId可通过 @Pathvariable注解绑定它传过来的值到方法的参数上。 123456@RequestMapping("/user/&#123;userId&#125;")public ModelAndView userCenter(HttpServletRequest request, HttpServletResponse response, @PathVariable String userId)&#123; //do something &#125; 如果方法参数名称和需要绑定的uri template中变量名称不一致，需要在@PathVariable(“name”)指定uri template中的名称。 11、@RequestParam@RequestParam注解有两个属性： value、required； value用来指定要传入值的id名称 required用来指示参数是否必须绑定； 举个例子： 123456789@RequestMapping("/t_rparam1") public String t_rparam1(@RequestParam Long userId) &#123; //do something &#125; @RequestMapping("/t_rparam2") public String t_rparam2(Long userId) &#123; //do something &#125; t_rparam1 必须带有参数,也就是说你直接输入localhost:8080/t_rparam1 会报错只能输入localhost:8080/t_rparam1?userId=? 才能执行相应的方法 t_rparam2 可带参数也可不带参数;也就是说输入localhost:8080/t_rparam2和输入 localhost:8080/t_rparam2?userId=?都可以正常运行 当然我们也可以设置 @RequestParam 里面的required为false(默认为true 代表必须带参数) 这样t_rparam1就跟t_rparam2是一样的了。 12、@RequestHeader利用@RequestHeader 注解可以把Request请求header部分的值绑定到方法的参数上。 1234567@RequestMapping("/t_heander") public void getRequestHeaderTest(HttpServletRequest request, HttpServletResponse response, @RequestHeader("Accept-Encoding")String encoding) &#123; //do something &#125; 13、@CookieValue@CookieValue就是把Request header中cookie的值绑定到方法的参数上。比如说我们的cookie如下： 1Cookie:JSESSIONID=ka8A5L5t7WTUPXbaLupBieqOdmc0ZpD5MyKvea6oQr7JJSIZzM;userId=001;sysFlag=glmapper 获取如下：1234@RequestMapping("/t_cookie") public void getCookieValueTest(@CookieValue("JSESSIONID") String cookie) &#123; //do something &#125; 14、@RequestBody@RequestBody这个注解常用来处理Content-Type不是application/x-www-form-urlencoded编码的内容，比如说：application/json, application/xml等等；这个和ResonseBody可以反过来理解。 15、@ModelAttribute 方法上 通常用来在处理@RequestMapping之前，为请求绑定需要从后台查询的model； 参数上 用来通过名称对应，把相应名称的值绑定到注解的参数bean上； 参考 《Spring技术内幕》 https://www.cnblogs.com/FrankLei/p/6579843.html]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC·ThreadPoolExecutor 线程池]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-up-juc2%2F</url>
    <content type="text"><![CDATA[ThreadPoolExecutor算是JUC中最常用的类之一了。ThreadPoolExecutor，顾名思义，thread-pool-executor,硬翻译就是“线程-池-执行者”；java中，通过ThreadPoolExecutor可以很容易的创建一个线程池。但是我们为什么要使用线程池？呢？它能够带来什么样的优势呢？它又是怎么实现的呢？OK，带着这几个问题，我们来学习一下JAVA中的线程池技术。 为什么要使用线程池？关于这个问题其实有点鸡肋，我觉得再问这个问题之前更应该问为什么要有线程池。那为什么呢? this is a 例子： 快递行业最近两年发展的灰常火热，听说工资也非常的高，搞得我一天天的都没有心思去好好写代码了... 之前的小快递公司都是没有固定的快递员的，就是说，每次去送一件快递，站点负责人就需要去找一个人来帮忙送，送完之后就没有然后了(当然，钱还是要给的)。 但是后来随着货越来越多，找人给钱成本太大，而且农忙时还需要花很长时间去找人，所以就雇用了5个人，签了合同，长期为站点配送。 以前都是随时用随时找，现在不是，现在是成立了一个物流公司，开了一个配送部，配送部门规定正式配送员最多只能有五个人。 之前配送的缺点是什么： 每次有货，我都会去临时找一个人，然后签订临时合同，送完之后解除合同。很麻烦。这也是不用线程池的缺点，就是任务来了，我们需要频繁的去创建新的线程，用完之后还需要释放线程资源，对于系统的消耗是很大的。 因为配送的货车只有那么几个，如果临时签订的人多了，车子不够用，其他人只能等着车子送完之后才能用。 成立配送部之后解决的问题 成立配送部之后呢，因为签订的是劳务合同，我们可以重复的让配送员配送不同的货物。达到线程资源的复用。 因为限定了最多招聘的人数，可以很好的避免招过多无用的人。 OK，我们以上述例子来对应理解线程池的基本原理 先来看下，JAVA对ThreadPoolExecutor的类申明： 1public class ThreadPoolExecutor extends AbstractExecutorService 在【初识】-JUC·Executor框架中给出了Executor的继承体系。ThreadPoolExecutor就是具备线程池功能的集成者。 构造方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//构造方法一public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); &#125; //构造方法二 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, defaultHandler);&#125;//构造方法三public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), handler);&#125;//构造方法四public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 从上面的代码可以看出，构造方法（一、二、三）都是通过调用（四）来做具体属性初始化的。那么我们直接来看构造方法四；在构造方法四中总共需要7个参数，先来看下每个参数的具体含义： corePoolSize 核心线程数大小。那么什么是核心线程数呢，我们可以类比于上面例子中的配送部中签订劳动合同的人的个数。 maximumPoolSize 最大线程数。加入说现在是双十一期间，快递异常的多，配送部的5个人完全忙不过来，而且仓库也满了，怎么办呢？这个时候就需要再招聘一些临时配送员，假设maximumPoolSize为10，那么也就是说，临时招聘可以招5个人，配送部签订正式劳动合同的人和签订临时合同的人加一块不能超过配送部规定的最大人数（10人）。所以说，maximumPoolSize就是线程池能够允许的存在的最大线程的数量。 keepAliveTime 存活时间。为什么要有这个呢？想一下，双十一过去了，货物已经配送的差不多了。临时合同写的是如果临时配送员2天没有配送了，那配送部就有权利终止临时合同，现在已经达到2天这个点了，需要开除这些临时配送专员了。对于线程池来说，keepAliveTime就是用来表示，当除核心线程池之外的线程超过keepAliveTime时间之后，就需要被系统回收了。 unit keepAliveTime的时间单位。 workQueue 工作队列。这个就相当于一个仓库，现在配送部5个人都在配送，但是还不断的有新的快递达到，这个时候就需要一个仓库来存放这些快递。对于线程池来说，当核心线程都有自己的任务处理，并且还有任务进来的时候，就会将任务添加到工作队列中去。 threadFactory 线程工厂。就是用来创建线程的。可以类比成招聘组，会给每个线程分配名字或者编号这样。 handler RejectedExecutionHandler 用来描述拒绝策略的。假设现在我的仓库也满足，并且配送部已经达到10个人了。怎么办呢，那么只能采用一些策略来拒绝任务了。 线程池的状态 1234567891011// runState is stored in the high-order bits//RUNNING；该状态的线程池接收新任务，并且处理阻塞队列中的任务private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;//SHUTDOWN；该状态的线程池不接收新任务，但会处理阻塞队列中的任务；private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;//STOP；不接收新任务，也不处理阻塞队列中的任务，并且会中断正在运行的任务；private static final int STOP = 1 &lt;&lt; COUNT_BITS;//所有的任务已终止，ctl记录的”任务数量”为0，线程池会变为TIDYING状态private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;//线程池彻底终止，就变成TERMINATED状态。 private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; 下面是在网上发现的一位大牛的图；感觉可以较为直观的描述状态的变更 工作原理 有几个点需要注意。 1、如何提交一个任务到线程池？12345678910111213141516171819202122public void execute(Runnable command) &#123; //任务为null,直接抛出空指针异常 if (command == null) throw new NullPointerException(); int c = ctl.get(); //如果线程数大于等于基本线程数，将任务加入队列 if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command);&#125; 如果少于corePoolSize线程正在运行，请尝试使用给定命令启动一个新线程作为其第一个任务。 对addWorker的调用会自动检查runState和workerCount，从而防止错误报警，在不应该的时候通过返回false来添加线程。 如果一个任务能够成功排队，那么我们仍然需要再次检查是否应该添加一个线程（因为现有的线程自上次检查以来已经死掉）或者自从进入这个方法以来，池关闭了。所以我们重新检查状态，如果当前command已经stop了，那么就退出工作队列，如果没有的话就开始一个新的线程。 如果队列满了，会想尝试去创建一个新的线程去执行，如果创建不了，那就执行拒绝策略。 2、如何创建一个线程去处理任务？通过实现这个接口去创建一个新的线程 123public interface ThreadFactory &#123; Thread newThread(Runnable r);&#125; 3、如何将任务添加到队列？通过addWorker方法来添加，其实在excute中只是作为一个提交任务的入口，实际的处理逻辑都是在addWorker这个方法里来完成的。addWorker有两个参数： firstTask 当前任务 core 用来标注当前需要创建的线程是否是核心线程，如果core为true，则表明创建的是核心线程，也就是说当前还没有达到最大核心线程数。 先来看下这个方法的前半部分：1234567891011121314151617181920212223242526272829private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: //自旋方式 for (;;) &#123; //获取当前线程池的状态 int c = ctl.get(); int rs = runStateOf(c); //如果状态是STOP，TIDYING,TERMINATED状态的话，则会返回false //如果状态是SHUTDOWN，但是firstTask不为空或者workQueue为空的话，那么直接返回false。 if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; //通过自旋的方式，判断要添加的worker是否为corePool范畴之内的 for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; //如果超过CAPACITY限制了则直接返回false1wc &gt;= CAPACITY //判断当前的workerCount是否大于corePoolsize，否则则判断是否大于maximumPoolSize//具体的比较取决于入参core是true还是false。1wc &gt;= (core ? corePoolSize : maximumPoolSize) 如果上面两个有一个满足了，则直接返回false。 下面是判断WorkerCount通过CAS操作增加1是否成功，成功的话就到此结束12if (compareAndIncrementWorkerCount(c)) break retry; 如果不成功，则再次判断当前线程池的状态，如果现在获取到的状态与进入自旋的状态不一致的话，那么则通过continue retry重新进行状态的判断。123c = ctl.get(); // Re-read ctlif (runStateOf(c) != rs) continue retry; 再来看下这个方法的后面半个部分： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849boolean workerStarted = false;boolean workerAdded = false;Worker w = null;try &#123; final ReentrantLock mainLock = this.mainLock; //创建一个新的Worker对象 w = new Worker(firstTask); final Thread t = w.thread; // if (t != null) &#123; //加锁 mainLock.lock(); try &#123; // 在锁定的情况下重新检查。 // 在一下情况退出：ThreadFactory 创建失败或者在获取锁之前shut down了 int c = ctl.get(); int rs = runStateOf(c); //状态校验 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // 预先检查t是可以启动的 throw new IllegalThreadStateException(); //添加至workers中 workers.add(w); int s = workers.size(); //如果超过了历史最大线程数，则将当前池数量设置为历史最大线程记录数 if (s &gt; largestPoolSize) largestPoolSize = s; //标识添加工作线程成功 workerAdded = true; &#125; &#125; finally &#123; //解锁 mainLock.unlock(); &#125; //如果添加成功则启动当前工作线程 if (workerAdded) &#123; t.start(); //并将当前线程状态设置为已启动 workerStarted = true; &#125; &#125;&#125; finally &#123;//添加失败 if (! workerStarted) addWorkerFailed(w);&#125;return workerStarted;&#125; 拒绝策略有哪些？ 1、AbortPolicy：直接抛出异常，默认策略； 2、CallerRunsPolicy：使用调用者自己的当前线程来执行任务； 3、DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务； 4、DiscardPolicy：直接丢弃任务； 当然我们也可以自定义拒绝策略。 常用工作队列类型1、ArrayBlockingQueue 基于数组的阻塞队列，长度有限 2、LinkedBlockingQuene 基于链表的阻塞队列，长度无限，使用这个可能会导致我们的拒绝策略失效。因为可以无限的创建新的工作线程。 3、PriorityBlockingQueue 具有优先级的无界阻塞队列； 3、SynchronousQuene SynchronousQuene是一个是一个不存储元素的BlockingQueue；每一个put操作必须要等待一个take操作，否则不能继续添加元素。所以这个比较特殊，它不存我们的任务，也就说说它的每个put操作必须等到另一个线程调用take操作，否则put操作一直处于阻塞状态。 Worker这个是ThreadPoolExecutor的一个内部类，表示一个工作线程。重要的是这个内部类实现了AbstractQueuedSynchronizer（AQS:抽象队列同步器）抽象类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475private final class Worker extends AbstractQueuedSynchronizer implements Runnable&#123; /** * This class will never be serialized, but we provide a * serialVersionUID to suppress a javac warning. */ private static final long serialVersionUID = 6138294804551838833L; /** 当前work持有的线程 */ final Thread thread; /** 运行的初始任务。 可能为空。*/ Runnable firstTask; /** 每个线程完成任务的计数器 */ volatile long completedTasks; /** * 构造函数 */ Worker(Runnable firstTask) &#123; // 禁止中断，直到runWorker setState(-1); //想提交的任务交给当前工作线程 this.firstTask = firstTask; //通过线程工厂创建一个新的线程 this.thread = getThreadFactory().newThread(this); &#125; /** 将run方法的执行委托给外部runWorker */ public void run() &#123; runWorker(this); &#125; // 是否锁定 // // 0代表解锁状态。 // 1代表锁定状态。 protected boolean isHeldExclusively() &#123; return getState() != 0; &#125; //尝试获取锁（重写AQS的方法） protected boolean tryAcquire(int unused) &#123; if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125; //尝试释放锁（重写AQS的方法） protected boolean tryRelease(int unused) &#123; setExclusiveOwnerThread(null); setState(0); return true; &#125; //加锁 public void lock() &#123; acquire(1); &#125; //尝试加锁 public boolean tryLock() &#123; return tryAcquire(1); &#125; //解锁 public void unlock() &#123; release(1); &#125; //是否锁定 public boolean isLocked() &#123; return isHeldExclusively(); &#125; //如果启动则中断 void interruptIfStarted() &#123; Thread t; if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; &#125; &#125;&#125; runWorker最后来看下runWorker这个方法（ThreadPoolExecutor中的方法）： 12345678910111213141516171819202122232425262728293031323334353637383940414243final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125; 下面是对注释的蹩脚翻译，欢迎吐槽，但注意尺度，O(∩_∩)O哈哈~ 主要工作循环运行。重复地从队列中获取任务并执行它们，同时处理一些问题: 我们可能会从最初的任务开始，在这种情况下，我们不需要得到第一个任务。否则，只要池正在运行，我们就从getTask获得任务。 如果它返回null，则由于更改池状态或配置参数而导致worker退出。其他退出的结果是在外部代码中抛出的异常，在这种情况下completeAbruptly成立，这通常会导致processWorkerExit来取代这个线程。 在运行任何任务之前，获取锁以防止任务正在执行时发生其他池中断，调用clearInterruptsForTaskRun确保除非池正在停止，则此线程没有设置其中断。 每个任务运行之前都会调用beforeExecute，这可能会引发一个异常，在这种情况下，我们会导致线程死亡（断开循环completeAbruptly为true），而不处理任务。 假设beforeExecute正常完成，我们运行任务，收集任何抛出的异常发送到afterExecute。 我们分别处理RuntimeException，Error（这两个规范保证我们陷阱）和任意的Throwables。 因为我们不能在Runnable.run中重新抛出Throwable，所以我们把它们封装在Errors中（到线程的UncaughtExceptionHandler）。 任何抛出的异常也保守地导致线程死亡。 task.run完成后，我们调用afterExecute，这也可能会抛出一个异常，这也会导致线程死亡。 根据JLS Sec 14.20，即使task.run抛出，这个异常也是有效的。 异常机制的最终效果是afterExecute和线程的UncaughtExceptionHandler拥有关于用户代码遇到的任何问题的准确信息。 总结本文是JUC的第二篇，意在通过查看源码来了解线程池的具体工作原理。文中如果存在不当的描述，希望小伙伴们能够及时提出。灰常感谢！ 欢迎关注微信公众号，干货满满哦~]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>线程</tag>
        <tag>thread</tag>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC·Executor 框架]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-up-juc1%2F</url>
    <content type="text"><![CDATA[前言多线程和并发这两个东西真的是向往已久，总是有一种神秘的感觉，想去探索一波，又担心水平不够无法驾驭。想以读书笔记的方式来写，但是又觉得缺少自己的一些思考；但是在没有足够并发编程经验的情况下又没法去写出很深刻的东西，毕竟没有踩过坑。所以在阅读spring源码的同时，也想抽点时间来看一看JUC的东西，关于这块只能说是记录自己学习JUC的一个过程，尝试用一些具体的代码demo来加深理解。所以就把本系列写成《【 初识】-JUC·XXXX》，用来让自己打开并发编程的大门。 JUCJUC即java.util.concurrent；也就是java提供的并发包。JUC中从包结构上来看主要是： java.util.concurrent 在这个包下面主要是线程池、并发集合以及一些并发工具类。线程池相关是围绕Excetor框架来构建；这也是本文下面部分的重点。 java.util.concurrent.atomic 这个包下面是一些原子操作类，算是并发辅助工具类，基本实现依赖于CAS； java.util.concurrent.locks 这个从名字就可以知道它的作用，就是提供锁。 JUC各个模块的类 整体框架 atomic locks 并发集合 并发工具 forkJoin fork-join在JUC中有下面三个类： 1public class ForkJoinPool extends AbstractExecutorService 1public abstract class ForkJoinTask&lt;V&gt; implements Future&lt;V&gt;, Serializable 1public class ForkJoinWorkerThread extends Thread FutureFuture提供了可以获取异步执行结果的方法，区别于Runnable的run方法，run是不提供返回结果的。1234567891011121314public interface Future&lt;V&gt; &#123; //取消 boolean cancel(boolean mayInterruptIfRunning); //如果任务完成前被取消，则返回true。 boolean isCancelled(); //如果任务执行结束，无论是正常结束或是中途取消还是发生异常，都返回true。 boolean isDone(); //获取异步执行的结果，如果没有结果可用，此方法会阻塞直到异步计算完成。 V get() throws InterruptedException, ExecutionException; //获取异步执行结果，如果没有结果可用，此方法会阻塞，但是会有时间限制， //如果阻塞时间超过设定的timeout时间，该方法将抛出异常。 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; Callable声明了一个名称为call()的方法，同时这个方法可以有返回值V，也可以抛出异常123public interface Callable&lt;V&gt; &#123; V call() throws Exception; &#125; 关于Callable和Future的使用一般情况下都是结合我们的线程池来使用的。 ExecutorExecutor接口是线程池实现的顶级接口，其和spring中的BeanFactory所承担的角色差不多，就是提供顶级的功能约束，具体实现交于不同子类来完成。12345678910111213public interface Executor &#123; /** * Executes the given command at some time in the future. The command * may execute in a new thread, in a pooled thread, or in the calling * thread, at the discretion of the &lt;tt&gt;Executor&lt;/tt&gt; implementation. * * @param command the runnable task * @throws RejectedExecutionException if this task cannot be * accepted for execution. * @throws NullPointerException if command is null */ void execute(Runnable command);&#125; 下面是JUC中Executor框架的整体结构： ExecutorService12345678910111213141516171819202122232425262728293031323334353637383940414243public interface ExecutorService extends Executor &#123; //关闭线程池 void shutdown(); List&lt;Runnable&gt; shutdownNow(); //是否为Shutdown状态 boolean isShutdown(); //是否为Terminated状态 boolean isTerminated(); //超过超时时间时，会监测ExecutorService是否已经关闭 //若关闭则返回true，否则返回false。 //一般情况下会和shutdown方法组合使用。 boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; //返回一个Future对象，参数接收的是一个Callable的实现 //Callable接口中的call()方法有一个返回值，可以返回任务的执行结果 //区别于Runnable接口中的run()方法（void修饰，没有返回值）。 &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); //返回一个Future对象，通过返回的Future对象，我们可以检查提交的任务是否执行完成了。 Future&lt;?&gt; submit(Runnable task); //返回一个Future的List，其中对应着每个Callable任务执行后的Future对象。 &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException; //增加了超时控制 &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException; //接收参数是一个Callable的集合， //返回的是所有Callable集合任务中某一个任务的执行结果 &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException; //增加了超时控制 &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; ExecutorService 再Executor接口的基础上扩展了对线程池状态的控制以及提交任务执行的超时控制。线程池的基本功能还不够完善，不能真正的具备处理具体业务的能力（毕竟是个接口，O(∩_∩)O哈哈~）。 开个篇，慢慢学~]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>线程</tag>
        <tag>thread</tag>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：BeanWrapper]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-beanwrapper%2F</url>
    <content type="text"><![CDATA[BeanWrapper 是Spring提供的一个用来操作javaBean属性的工具，使用它可以直接修改一个对象的属性。 对于bean属性的操作，大家熟知的主要有下面这些工具类： 1.Apache的BeanUtils和PropertyUtils 2.cglib的BeanMap和BeanCopier 3.spring的BeanUtils Spring中BeanWrapper 的主要功能在于： 1.支持设置嵌套属性 2.支持属性值的类型转换（设置ConversionService） 3.提供分析和操作标准JavaBean的操作：获取和设置属性值（单独或批量），获取属性描述符以及查询属性的可读性/可写性的能力。 BeanWrapper本身是一个接口，它提供了一整套处理Bean的方法。源码如下： 1234567891011121314public interface BeanWrapper extends ConfigurablePropertyAccessor &#123; //为数组和集合自动增长指定一个限制。在普通的BeanWrapper上默认是无限的。 void setAutoGrowCollectionLimit(int autoGrowCollectionLimit); //返回数组和集合自动增长的限制。 int getAutoGrowCollectionLimit(); //如果有的话,返回由此对象包装的bean实例 Object getWrappedInstance(); //返回被包装的JavaBean对象的类型。 Class&lt;?&gt; getWrappedClass(); //获取包装对象的PropertyDescriptors（由标准JavaBeans自省确定）。 PropertyDescriptor[] getPropertyDescriptors(); //获取包装对象的特定属性的属性描述符。 PropertyDescriptor getPropertyDescriptor(String propertyName) throws InvalidPropertyException;&#125; 上面的BeanWrapper是基于4.3.6版本的，这个接口在4.1版本之后略有改动。BeanWrapperImpl是BeanWrapper的实现类，BeanWrapperImpl的父类是AbstractNestablePropertyAccessor，通过这个使得BeanWrapper具有处理属性的能力。 下面是一个使用BeanWrapper 包装对象的例子： 1234567891011121314151617181920212223242526272829303132package com.glmapper.spring.test;import org.springframework.beans.BeanWrapper;import org.springframework.beans.PropertyAccessorFactory;import org.springframework.beans.PropertyValue;/** * BeanWrapper 测试类 */public class BeanWrapperTest &#123; public static void main(String[] args) &#123; User user=new User(); //通过PropertyAccessorFactory将user对象封装成BeanWrapper BeanWrapper bw=PropertyAccessorFactory.forBeanPropertyAccess(user); //方式一：直接对属性值进行设置 bw.setPropertyValue("userName", "张三"); //方式二：通过PropertyValue进行设置 PropertyValue pv=new PropertyValue("userName","李四"); bw.setPropertyValue(pv); System.out.println(user.getUserName()); &#125;&#125;//一个User类class User&#123; private String userName; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125;&#125; 在Spring中，有很多Bean属性的操作都是通过BeanWrapper来完成的，比如常见的HttpServletBean的属性设置就是。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：依赖注入（四）-总结]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-inject-summary%2F</url>
    <content type="text"><![CDATA[Spring源码系列：依赖注入（一）getBean Spring源码系列：依赖注入（二）createBean Spring源码系列：依赖注入（三）-属性注入 在上面三篇文章中对依赖注入做了一个大致的梳理；里面都是大量代码的分析，本文在此基础上进行一个总结归纳。 依赖注入调用过程 如前几篇文章所述，依赖注入是由getBean来触发的；然后涉及到bean实例的创建、依赖关系的建立、属性注入等子过程。 getBean 方法触发依赖注入 doGetBean 从容器中查找Bean（BeanFactory链，当前容器-&gt;双亲容器-双亲容器…） 当然，在获取到某个Bean的时候也会通过递归的方式来依赖注入依赖的bean createBeanInstance 生成了Bean所包含的Java对象，Spring中用SimpleInstantiationStrategy类来生成Bean对象的实例，实例化Java对象的方法有两种（CGlib是默认方式）： 通过BeanUtils，它使用了JVM的反射功能来生成Java对象实例 用CGLIB来生成，CGLIB是一种常用的字节码生成器的类库 populateBean 设置Bean对象的依赖关系 resolveValueIfNecessary 注入类型的处理；解析不同类型的属性 setPropertyValues 属性注入 关于lazy-initIoc容器的初始化过程中，主要的工作就是对BeanDefinition的定位、载入、解析和注册；但是就像之前说过的，此时依赖注入还没有发生。在Spring源码系列：依赖注入（一）getBean文中提到，依赖注入发生在应用第一次向容器获取Bean的时候；也就是上面说到的通过getBean来触发。 当然，依赖注入也可以在容器初始化的过程中就完成。这个就是lazy-init属性的存在意义了。就是说我们可以通过设置Bean的lazy-init属性来控制预实例化的过程。 预实例化：在初始化容器时完成Bean的依赖注入 这种做法的好处在于提高了我们第一次获取Bean的的效率，但是它也降低了容器初始化的速度。（这个其实很好理解的，因为第一次获取Bean的时候，依赖注入已经完成了，直接拿过来用就行） 关于lazy-init属性的处理也是在wac.refresh这个方法中完成的，具体是在finishBeanFactoryInitialization方法中。如果继续追溯的话，最终是交给DefaultListableBeanFactory容器中的preInstantiateSingletons方法中完成。 lazy-init这种实例化方式就是通过将依赖注入委托给容器来处理，而不是在用户第一向容器申请的Bean的时候完成依赖注入，不同的阶段，也有不同的优劣。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：依赖注入（三）-属性注入]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-inject-props%2F</url>
    <content type="text"><![CDATA[前面文章中对依赖注入的触发和bean的创建做了学习记录，本文将来记录一下bean的属性注入过程。Bean的属性注入发生在BeanDefinitionValueResolver这个类中，BeanDefinitionValueResolver这类是用于bean工厂实现的Helper类，职责就是将bean定义对象中包含的值解析为应用于目标bean实例的实际值。 BeanDefinitionValueResolver类中的resolveValueIfNecessary()方法包含了对所有注入类型的处理。所以本文主要围绕这个方法展开来说。 resolveValueIfNecessary方法resolveValueIfNecessary():给定一个PropertyValue，返回一个value，解析对工厂中其他bean的引用。 value可能是： RuntimeBeanReference : 在解析到依赖的Bean的时侯，解析器会依据依赖bean的name创建一个RuntimeBeanReference对像，将这个对像放入BeanDefinition的MutablePropertyValues中。 ManagedList：用来保存它所管理的List元素，它可以包含运行时期的bean引用(将被解析为bean对象). ManagedSet ：用来保存它所管理的set值，它可以包含运行时期的bean引用(将被解析为bean对象) ManagedMap ：用来保存它所管理的map值，它可以包含运行时期的bean引用(将被解析为bean对象) 1、方法申明 argName ：为其定义的参数的名称 value ：要解析的值对象1public Object resolveValueIfNecessary(Object argName, Object value) 2、RuntimeBeanReference 当在beanfactory中作为另外一个bean的引用时，作为属性值对象，将在运行时进行解析。 RuntimeBeanReference是在对BeanDefinition进行解析时生成的数据对象。1234if (value instanceof RuntimeBeanReference) &#123; RuntimeBeanReference ref = (RuntimeBeanReference) value; return resolveReference(argName, ref);&#125; 3、RuntimeBeanNameReference 当在beanfactory中作为另外一个bean名称的引用时，作为属性值对象，将在运行时进行解析。12345678else if (value instanceof RuntimeBeanNameReference) &#123; String refName = ((RuntimeBeanNameReference) value).getBeanName(); refName = String.valueOf(doEvaluate(refName)); if (!this.beanFactory.containsBean(refName)) &#123; //异常：Invalid bean name '" + refName + "' in bean reference for " + argName &#125; return refName;&#125; 4、BeanDefinitionHolder 解析BeanDefinitionHolder：包含具有名称和别名的BeanDefinition。BeanDefinitionHolder就是使用名称或者别名来保存BeanDefinition的。1234else if (value instanceof BeanDefinitionHolder) &#123; BeanDefinitionHolder bdHolder = (BeanDefinitionHolder) value; return resolveInnerBean(argName, bdHolder.getBeanName(), bdHolder.getBeanDefinition());&#125; 5、BeanDefinition 解析纯粹的BeanDefinition1234567else if (value instanceof BeanDefinition) &#123; // Resolve plain BeanDefinition, without contained name: use dummy name. BeanDefinition bd = (BeanDefinition) value; String innerBeanName = "(inner bean)" + BeanFactoryUtils.GENERATED_BEAN_NAME_SEPARATOR + ObjectUtils.getIdentityHexString(bd); return resolveInnerBean(argName, innerBeanName, bd); &#125; 6、ManagedArray 包含运行时期的bean引用(将被解析为bean对象)1234567891011121314151617181920212223else if (value instanceof ManagedArray) &#123; // May need to resolve contained runtime references. ManagedArray array = (ManagedArray) value; Class&lt;?&gt; elementType = array.resolvedElementType; if (elementType == null) &#123; String elementTypeName = array.getElementTypeName(); if (StringUtils.hasText(elementTypeName)) &#123; try &#123; elementType = ClassUtils.forName(elementTypeName, this.beanFactory.getBeanClassLoader()); array.resolvedElementType = elementType; &#125; catch (Throwable ex) &#123; // Improve the message by showing the context. //异常：Error resolving array type for " + argName &#125; &#125; else &#123; elementType = Object.class; &#125; &#125; return resolveManagedArray(argName, (List&lt;?&gt;) value, elementType);&#125; 7、ManagedList，ManagedSet，ManagedMap 包含运行时期的bean引用(将被解析为bean对象)123456789101112//对ManagedList进行解析else if (value instanceof ManagedList) &#123; return resolveManagedList(argName, (List&lt;?&gt;) value);&#125;//对ManagedSet进行解析else if (value instanceof ManagedSet) &#123; return resolveManagedSet(argName, (Set&lt;?&gt;) value);&#125;//对ManagedMap进行解析else if (value instanceof ManagedMap) &#123; return resolveManagedMap(argName, (Map&lt;?, ?&gt;) value);&#125; 8、ManagedProperties ManagedProperties表示的是一个spring管理的属性实例，它支持父/子 definition的合并。1234567891011121314151617//对ManagedProperties进行解析else if (value instanceof ManagedProperties) &#123; Properties original = (Properties) value; Properties copy = new Properties(); for (Map.Entry&lt;Object, Object&gt; propEntry : original.entrySet()) &#123; Object propKey = propEntry.getKey(); Object propValue = propEntry.getValue(); if (propKey instanceof TypedStringValue) &#123; propKey = evaluate((TypedStringValue) propKey); &#125; if (propValue instanceof TypedStringValue) &#123; propValue = evaluate((TypedStringValue) propValue); &#125; copy.put(propKey, propValue); &#125; return copy;&#125; 9、TypedStringValue TypedStringValue保存的是一个类型的属性值。1234567891011121314151617181920//对TypedStringValue进行解析else if (value instanceof TypedStringValue) &#123; // Convert value to target type here. TypedStringValue typedStringValue = (TypedStringValue) value; Object valueObject = evaluate(typedStringValue); try &#123; Class&lt;?&gt; resolvedTargetType = resolveTargetType(typedStringValue); if (resolvedTargetType != null) &#123; return this.typeConverter.convertIfNecessary(valueObject, resolvedTargetType); &#125; else &#123; return valueObject; &#125; &#125; catch (Throwable ex) &#123; // Improve the message by showing the context. throw new BeanCreationException( //异常：Error converting typed String value for " + argName &#125;&#125; 10、作为表达式进行评估 将给定的值作为表达式进行评估。123else &#123; return evaluate(value);&#125; 在完成上述解析之后，已经为我们的依赖注入做好了准备。这是真正把Bean对象设置到它所依赖的另一个Bean的属性中去的地方，可以看到，处理的属性也是各式各样的。具体属性的注入是在之前提到的BeanWrapper接口的实现类BeanWrapperImpl的setPropertyValue方法来完成。 setPropertyValue方法a、方法声明这个方法是私有的，是BeanWrapperImpl实际处理的方法，其对外也提供了setPropertyValue的其它重载方法来提供服务。 12private void setPropertyValue(PropertyTokenHolder tokens, PropertyValue pv) throws BeansException b、PropertyTokenHolder是BeanWrapperImpl的内部类12345private static class PropertyTokenHolder &#123; public String canonicalName; public String actualName; public String[] keys;&#125; 在setPropertyValue方法中会根据tokens变量是否为null,有两个不同的分支。其中当tokens为null时，则会对属性名进行递归调用分析处理，返回分析处理后的BeanWrapImpl对象nestedBw。如果nestedBw==this,则会设置pv的resolvedTokens属性值，最后将调用nestedBw对象的设置属性值方法设置属性。来具体看看： c、其中当tokens为null时，即对集合类的域进行注入1234567891011121314// 设置tokens的索引和keysPropertyTokenHolder getterTokens = new PropertyTokenHolder();getterTokens.canonicalName = tokens.canonicalName;getterTokens.actualName = tokens.actualName;getterTokens.keys = new String[tokens.keys.length - 1];System.arraycopy(tokens.keys, 0, getterTokens.keys, 0, tokens.keys.length - 1);Object propValue;//getPropertyValue用来获取Bean中对对象注入的引用；try &#123; propValue = getPropertyValue(getterTokens);&#125;catch (NotReadablePropertyException ex) &#123;//异常：Cannot access indexed value in property referenced &#125; 1、propValue为null propValue为null12345678910111213if (propValue == null) &#123; // 空值映射的情况 if (this.autoGrowNestedPaths) &#123; // TODO: cleanup, this is pretty hacky int lastKeyIndex = tokens.canonicalName.lastIndexOf('['); getterTokens.canonicalName = tokens.canonicalName.substring(0, lastKeyIndex); propValue = setDefaultValue(getterTokens); &#125; else &#123; //异常：Cannot access indexed value in property referenced " + "in indexed property path '" + propertyName + "': returned null" &#125;&#125; 2、对array进行注入 对array进行注入1234567891011121314151617if (propValue.getClass().isArray()) &#123; PropertyDescriptor pd = getCachedIntrospectionResults().getPropertyDescriptor(actualName); Class requiredType = propValue.getClass().getComponentType(); int arrayIndex = Integer.parseInt(key); Object oldValue = null; try &#123; if (isExtractOldValueForEditor() &amp;&amp; arrayIndex &lt; Array.getLength(propValue)) &#123; oldValue = Array.get(propValue, arrayIndex); &#125; Object convertedValue = convertIfNecessary(propertyName, oldValue, pv.getValue(), requiredType, TypeDescriptor.nested(property(pd), tokens.keys.length)); Array.set(propValue, arrayIndex, convertedValue); &#125; catch (IndexOutOfBoundsException ex) &#123; //异常：Invalid array index in property path '" + propertyName &#125;&#125; 2、对list进行注入 对list进行注入123456789101112131415161718192021222324252627282930313233else if (propValue instanceof List) &#123; PropertyDescriptor pd = getCachedIntrospectionResults().getPropertyDescriptor(actualName); Class requiredType = GenericCollectionTypeResolver.getCollectionReturnType( pd.getReadMethod(), tokens.keys.length); List list = (List) propValue; int index = Integer.parseInt(key); Object oldValue = null; if (isExtractOldValueForEditor() &amp;&amp; index &lt; list.size()) &#123; oldValue = list.get(index); &#125; Object convertedValue = convertIfNecessary(propertyName, oldValue, pv.getValue(), requiredType, TypeDescriptor.nested(property(pd), tokens.keys.length)); int size = list.size(); if (index &gt;= size &amp;&amp; index &lt; this.autoGrowCollectionLimit) &#123; for (int i = size; i &lt; index; i++) &#123; try &#123; list.add(null); &#125; catch (NullPointerException ex) &#123; //异常：InvalidPropertyException &#125; &#125; list.add(convertedValue); &#125; else &#123; try &#123; list.set(index, convertedValue); &#125; catch (IndexOutOfBoundsException ex) &#123; //异常：Invalid list index in property path '" + propertyName + "'" &#125; &#125;&#125; 2、对map进行注入 对map进行注入1234567891011121314151617181920else if (propValue instanceof Map) &#123; PropertyDescriptor pd = getCachedIntrospectionResults().getPropertyDescriptor(actualName); Class mapKeyType = GenericCollectionTypeResolver.getMapKeyReturnType( pd.getReadMethod(), tokens.keys.length); Class mapValueType = GenericCollectionTypeResolver.getMapValueReturnType( pd.getReadMethod(), tokens.keys.length); Map map = (Map) propValue; //重要提示：不要在这里传递完整的属性名称 TypeDescriptor typeDescriptor = (mapKeyType != null ? TypeDescriptor.valueOf(mapKeyType) : TypeDescriptor.valueOf(Object.class)); Object convertedMapKey = convertIfNecessary(null, null, key, mapKeyType, typeDescriptor); Object oldValue = null; if (isExtractOldValueForEditor()) &#123; oldValue = map.get(convertedMapKey); &#125; // 在这里传递完整的属性名称和旧值，因为希望对map值有完整的转换能力。 Object convertedMapValue = convertIfNecessary(propertyName, oldValue, pv.getValue(), mapValueType, TypeDescriptor.nested(property(pd), tokens.keys.length)); map.put(convertedMapKey, convertedMapValue);&#125; 其中当tokens不为null时，即对非集合类的域进行注入这里是核心的地方，取得注入属性的set方法，通过反射机制，把对象注入进去。123final Method writeMethod = (pd instanceof GenericTypeAwarePropertyDescriptor ? ((GenericTypeAwarePropertyDescriptor) pd).getWriteMethodForActualAccess() : pd.getWriteMethod()); 总结通过上面的几篇分析我们大概的熟悉了Bean创建和对象依赖注入的一个过程，在这个过程中，spring需要根据Beandefinition中的信息来递归完成依赖注入。并且这些递归的入口都是getBean这个方法。 一个递归是在上下文体系中查找需要的Bean和创建Bean的递归调用； 另一个递归是在依赖注入时通过递归调用容器的getBean方法，得到当前Bean的依赖Bean，同时也触发对依赖Bean的创建和注入。 在对Bean的属性进行依赖注入时解析的过程也是一个递归的过程。这样就可以根据依赖关系，一层一层的完成Bean的创建和注入，直到最后完成当前Bean的创建。 参考 《Spring技术内幕》 https://www.cnblogs.com/davidwang456/p/4213652.html]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：依赖注入（二）createBean]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-createbean%2F</url>
    <content type="text"><![CDATA[在Spring源码系列：依赖注入（一）（AbstractBeanFactory-getBean）最后说道getBean是依赖注入的起点，bean的创建都是通过createBean来完成具体的创建的。createBean的具体实现是在AbstractAutowireCapableBeanFactory中的。本篇就捋一捋这个方法看下bean的创建过程。 这个方法是AbstractAutowireCapableBeanFactory这个类的中心方法，其作用就是创建一个bean实例，填充bean实例，后置处理等。 在createBean中主要做了三件事： 判断需要创建的Bean是否可以实例化，这个类是否可以通过类装载器来载入 是否配置了后置处理器相关处理（如果配置了则返回一个代理） 创建Bean 具体来看方法： 123456789101112131415161718192021222324252627282930313233343536373839404142protected Object createBean(String beanName, RootBeanDefinition mbd, Object[] args) throws BeanCreationException &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Creating instance of bean '" + beanName + "'"); &#125; RootBeanDefinition mbdToUse = mbd; // Make sure bean class is actually resolved at this point, and // clone the bean definition in case of a dynamically resolved Class // which cannot be stored in the shared merged bean definition. //判断需要创建的Bean是否可以实例化，这个类是否可以通过类装载器来载入 Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName); if (resolvedClass != null &amp;&amp; !mbd.hasBeanClass() &amp;&amp; mbd.getBeanClassName() != null) &#123; mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); &#125; // Prepare method overrides. try &#123; mbdToUse.prepareMethodOverrides(); &#125; catch (BeanDefinitionValidationException ex) &#123; //异常：Validation of method overrides failed &#125; try &#123; // Give BeanPostProcessors a chance to return a proxy instead of the target //bean instance. //是否配置了后置处理器相关处理（如果配置了则返回一个代理） Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) &#123; return bean; &#125; &#125; catch (Throwable ex) &#123; //异常:BeanPostProcessor before instantiation of bean failed &#125; //创建Bean Object beanInstance = doCreateBean(beanName, mbdToUse, args); if (logger.isDebugEnabled()) &#123; logger.debug("Finished creating instance of bean '" + beanName + "'"); &#125; return beanInstance;&#125; 从上面的代码中可以看到，创建bean是交给doCreateBean方法来创建的。继续看doCreateBean这个方法：（这里面涉及到一个BeanWrapper这个接口，小伙伴可以移步了解一下《Spring源码系列：BeanWrapper》） 代码 1：123456789101112131415// 用BeanWrapper来持有创建出来的Bean对象BeanWrapper instanceWrapper = null;//如果是单例的话，则先把缓存中的同名bean清除if (mbd.isSingleton()) &#123; instanceWrapper = this.factoryBeanInstanceCache.remove(beanName);&#125;//实际创建的交给createBeanInstance来完成，//bean的生成，这里会使用默认的类生成器，包装成BeanWrapperImpl类，//为了下面的populateBean方法的属性注入做准备 if (instanceWrapper == null) &#123; instanceWrapper = createBeanInstance(beanName, mbd, args);&#125;final Object bean = (instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null);Class&lt;?&gt; beanType = (instanceWrapper != null ? instanceWrapper.getWrappedClass() : null);mbd.resolvedTargetType = beanType; 代码 2： 允许后处理器修改合并的bean定义。1234567891011synchronized (mbd.postProcessingLock) &#123; if (!mbd.postProcessed) &#123; try &#123; applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); &#125; catch (Throwable ex) &#123; //异常：Post-processing of merged bean definition failed &#125; mbd.postProcessed = true; &#125; &#125; 代码 3 ： 即使被BeanFactoryAware等生命周期接口触发，也要尽快地缓存singletons 以便能够解析循环引用。1234567891011121314boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName));if (earlySingletonExposure) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Eagerly caching bean &apos;&quot; + beanName + &quot;&apos; to allow for resolving potential circular references&quot;); &#125; addSingletonFactory(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; return getEarlyBeanReference(beanName, mbd, bean); &#125; &#125;);&#125; 代码 4: 这里是对bean的初始化的地方，一般情况下依赖注入就在这里发生；这个exposedObject变量保存的是在初始化处理完以后返回的作为依赖注入完成之后的bean。123456789101112131415161718// Initialize the bean instance.Object exposedObject = bean;try &#123; populateBean(beanName, mbd, instanceWrapper); if (exposedObject != null) &#123; exposedObject = initializeBean(beanName, exposedObject, mbd); &#125;&#125;catch (Throwable ex) &#123; //抛出 if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123; throw (BeanCreationException) ex; &#125; else &#123; //异常:Initialization of bean failed &#125;&#125; 代码 5: 这里是注册bean12345678try &#123; registerDisposableBeanIfNecessary(beanName, bean, mbd);&#125;catch (BeanDefinitionValidationException ex) &#123; //异常处理&#125;//返回结果return exposedObject; 上面的5个代码段均是doCreateBean中的处理逻辑，有兴趣的小伙伴可以自行查阅源码。从上面的代码中我们依然没有得到具体创建的过程，因为在doCreateBean中又依赖：createBeanInstance和populateBean两个方法。 在createBeanInstance中生成了Bean所包含的java对象。来看是怎么生成的： 12345678910111213141516171819202122232425262728293031323334353637383940414243protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, Object[] args) &#123; // 确保bean类实际上已经解析过了，可以实例化 Class&lt;?&gt; beanClass = resolveBeanClass(mbd, beanName); if (beanClass != null &amp;&amp; !Modifier.isPublic(beanClass.getModifiers()) &amp;&amp; !mbd.isNonPublicAccessAllowed()) &#123; //异常：Bean class isn't public, and non-public access not allowed:beanName &#125; //1. 使用工厂方法来进行bean的实例化 if (mbd.getFactoryMethodName() != null) &#123; return instantiateUsingFactoryMethod(beanName, mbd, args); &#125; // 重新创建相同的bean时快捷方式... boolean resolved = false; boolean autowireNecessary = false; if (args == null) &#123; synchronized (mbd.constructorArgumentLock) &#123; if (mbd.resolvedConstructorOrFactoryMethod != null) &#123; resolved = true; autowireNecessary = mbd.constructorArgumentsResolved; &#125; &#125; &#125; if (resolved) &#123; if (autowireNecessary) &#123; return autowireConstructor(beanName, mbd, null, null); &#125; else &#123; return instantiateBean(beanName, mbd); &#125; &#125; // 2.需要确定构造函数...,使用构造函数进行bean实例化 Constructor&lt;?&gt;[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); if (ctors != null || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) &#123; return autowireConstructor(beanName, mbd, ctors, args); &#125; //3.没有特殊的处理：只需使用无参数构造函数。（默认构造函数） return instantiateBean(beanName, mbd);&#125; 从上面这段代码可以看出，对象的生成有许多不同的方式，有通过工厂的，也有通过容器的autowire特性生成的。当然这些生成方式都是由相关的BeanDefinition来指定的。 Spring中配置Bean的方式我们常用的一种是通过xml文件来配置，还有就是通过注解的方式来配置。 demo1 123&lt;bean id="user" class="com.glmapper.test.User"&gt; &lt;property name="name" value="glmapper"&gt;&lt;/property&gt; &lt;/bean&gt; 这种方式，通过class提供的权限定名，spring就可以利用反射机制创建这个bean。 demo2 123&lt;bean id="user" class="com.glmapper.test.UserFactory" factory-method="getUser"&gt; &lt;constructor-arg value="glmapper"&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; 这种是利用静态工厂方法来创建的，提供的class并非是类的权限定名， 而是静态工厂的全类名；除此之外还需要指定获取bean的方法（此处是getUser）和参数（参数是glmapper）。 demo3 123456789101112131415161718192021222324252627&lt;bean id=&quot;userFactory&quot; class=&quot;com.glmapper.test.UserInstanceFactory&quot;&gt; &lt;!--用一个集合来保存我当前的对象实例--&gt; &lt;property name=&quot;map&quot;&gt; &lt;map&gt; &lt;entry key=&quot;user1&quot;&gt; &lt;bean class=&quot;com.glmapper.test.User&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;glmapper1&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/entry&gt; &lt;entry key=&quot;user2&quot;&gt; &lt;bean class=&quot;com.glmapper.test.User&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;glmapper2&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; //实例1 &lt;bean id=&quot;user1&quot; factory-bean=&quot;userFactory&quot; factory-method=&quot;getUserInstance&quot;&gt; &lt;constructor-arg value=&quot;user1&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt;//实例2 &lt;bean id=&quot;user2&quot; factory-bean=&quot;userFactory&quot; factory-method=&quot;getUserInstance&quot;&gt; &lt;constructor-arg value=&quot;user2&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean 这种方式和静态工厂的区别在于我们需要先实例化一个工厂对象，然后才能使用这个工厂对象来创建我们的bean。getUserInstance通过key值来获取我们已经实例化好的对象（当然方式有很多，此处以map来举个例子）。关于注解的和使用FactoryBean接口的这里就暂时不说，后期再聊 OK，继续来分钟，上面说到的是以工厂方法创建bean，具体的源码有点长，这里就不放了，大概思路就如上面所提到的那几种方式。接下来看下常见的使用instantiateBean方式（使用它的默认构造函数）来构建bean的代码： 1234567891011121314151617181920212223242526protected BeanWrapper instantiateBean(final String beanName, final RootBeanDefinition mbd) &#123; try &#123; Object beanInstance; final BeanFactory parent = this; //获取系统安全接口。 //如果已经为当前应用程序建立了安全管理器，则返回该安全管理器; //否则，返回null。 if (System.getSecurityManager() != null) &#123; beanInstance = AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; @Override public Object run() &#123; return getInstantiationStrategy().instantiate(mbd, beanName, parent); &#125; &#125;, getAccessControlContext()); &#125; else &#123; beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); &#125; BeanWrapper bw = new BeanWrapperImpl(beanInstance); initBeanWrapper(bw); return bw; &#125; catch (Throwable ex) &#123; //异常：Instantiation of bean failed &#125;&#125; 可以看出，上面的创建都是通过： 1getInstantiationStrategy().instantiate(mbd, beanName, parent); 这样一段代码来完成的，是的，这里已经快接近真相了。从语义上来分析，先是获取了一种策略，然后利用当前获取的策略再去执行实例化。OK，我们看下getInstantiationStrategy()拿到的是什么： 123456//返回实例化策略用于创建bean实例。protected InstantiationStrategy getInstantiationStrategy() &#123; return this.instantiationStrategy;&#125;//默认的实例化测试是使用CGLIB代理private InstantiationStrategy instantiationStrategy = new CglibSubclassingInstantiationStrategy(); 看到这里我们清楚了，默认构造函数的情况下，在spring中会使用Cglib来进行bean的实例化（关于cglib此处不再赘述）。我们看下CglibSubclassingInstantiationStrategy这个类的申明： 1public class CglibSubclassingInstantiationStrategy extends SimpleInstantiationStrategy 它继承自SimpleInstantiationStrategy ，这个又是什么鬼呢？ SimpleInstantiationStrategy是Spring用来生成Bean对象的默认类，在这个类中提供了两种实例化java对象的方法，一种是基于java自身反射机制的BeanUtils，还有一种就是基于Cglib。 如何创建的就不说了；到这里createBeanInstance就说完了（Bean已经创建了）；但是仅仅是创建，spring还没有处理它们，比如说bean对象的属性，依赖关系等等。这些就是上面提到的另外一个方法populateBean； 这个方法其实就做了一件事：使用bean定义中的属性值在给定的BeanWrapper中填充bean实例。分段来看：下面这段代码是先将BeanDefinition中设置的property值封装成PropertyValues，然后检测我们的BeanWrapper是否为Null，如果为null则抛出异常或者跳过当前空实例赋值阶段1234567891011//获取到BeanDefinition中设置的property值，封装成PropertyValuesPropertyValues pvs = mbd.getPropertyValues();if (bw == null) &#123; if (!pvs.isEmpty()) &#123; //异常：Cannot apply property values to null instance &#125; else &#123; // Skip property population phase for null instance. return; &#125;&#125; 下面这段代码的意思是给任何InstantiationAwareBeanPostProcessors提供在设置属性之前修改bean状态的机会。12345678910111213141516boolean continueWithPropertyPopulation = true;if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; continueWithPropertyPopulation = false; break; &#125; &#125; &#125;&#125;if (!continueWithPropertyPopulation) &#123; return;&#125; 下面就是对具体注入方式的处理： 1234567891011121314151617//处理autowire的注入；可以根据bean的名称和类型来注入if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // 则根据名称添加基于自动装配的属性值。 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) &#123; autowireByName(beanName, mbd, bw, newPvs); &#125; // 根据类型添加基于自动装配的属性值。 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; autowireByType(beanName, mbd, bw, newPvs); &#125; pvs = newPvs;&#125; 两个判断条件，在满足的情况下做的处理分别是： 在工厂将给定属性值应用到给定的bean后，对其进行后处理。 允许检查所有的依赖关系是否被满足，例如基于bean属性设置器上的“Required”注解。还允许替换要应用的属性值，通常通过创建基于原始PropertyValues的新MutablePropertyValues实例，添加或删除特定值。 执行依赖性检查 123456789101112131415161718192021222324//返回这个工厂是否拥有一个InstantiationAwareBeanPostProcessorboolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors();//返回依赖检查代码。boolean needsDepCheck = (mbd.getDependencyCheck() != RootBeanDefinition.DEPENDENCY_CHECK_NONE);if (hasInstAwareBpps || needsDepCheck) &#123;//从给定的BeanWrapper中提取一组已过滤的PropertyDescriptors，//不包括在被忽略的依赖性接口上定义的被忽略的依赖类型或属性（译注）。 PropertyDescriptor[] filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); if (hasInstAwareBpps) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; pvs = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvs == null) &#123; return; &#125; &#125; &#125; &#125; if (needsDepCheck) &#123; checkDependencies(beanName, mbd, filteredPds, pvs); &#125;&#125; 最后是对属性进行注入： 1applyPropertyValues(beanName, mbd, bw, pvs); 这个方法描述的是对属性进行解析然后注入的过程；先来分析下applyPropertyValues的申明：12protected void applyPropertyValues(String beanName, BeanDefinition mbd, BeanWrapper bw, PropertyValues pvs) beanName bean名称 mbd 合并的bean definition bw 包装目标对象的BeanWrapper pvs 新的属性值 代码分段来看： 参数验证 123if (pvs == null || pvs.isEmpty()) &#123; return;&#125; pvs参数处理 1234567891011121314151617if (pvs instanceof MutablePropertyValues) &#123; mpvs = (MutablePropertyValues) pvs; if (mpvs.isConverted()) &#123; // 使用预先转换后的值。 try &#123; bw.setPropertyValues(mpvs); return; &#125; catch (BeansException ex) &#123; //异常：Error setting property values &#125; &#125; original = mpvs.getPropertyValueList(); &#125; else &#123; original = Arrays.asList(pvs.getPropertyValues()); &#125; valueResolver来解析BeanDefinition 12BeanDefinitionValueResolver valueResolver = new BeanDefinitionValueResolver(this, beanName, mbd, converter); 为解析值创建一个副本，注入到bean中的是副本的数据 12// Create a deep copy, resolving any references for values.List&lt;PropertyValue&gt; deepCopy = new ArrayList&lt;PropertyValue&gt;(original.size()); 遍历处理 123456789101112131415161718192021222324252627282930313233343536boolean resolveNecessary = false;for (PropertyValue pv : original) &#123; //返回此持有者是否已经包含转换后的值（true），还是需要转换值（false）。 if (pv.isConverted()) &#123; deepCopy.add(pv); &#125; else &#123; String propertyName = pv.getName(); Object originalValue = pv.getValue(); //看下面的注释resolveValueIfNecessary Object resolvedValue = valueResolver.resolveValueIfNecessary(pv, originalValue); Object convertedValue = resolvedValue; boolean convertible = bw.isWritableProperty(propertyName) &amp;&amp; !PropertyAccessorUtils.isNestedOrIndexedProperty(propertyName); if (convertible) &#123; convertedValue = convertForProperty(resolvedValue, propertyName, bw, converter); &#125; // 可能将转换的值存储在合并的bean定义中，以避免为每个创建的bean实例重新转换。 if (resolvedValue == originalValue) &#123; if (convertible) &#123; pv.setConvertedValue(convertedValue); &#125; deepCopy.add(pv); &#125; else if (convertible &amp;&amp; originalValue instanceof TypedStringValue &amp;&amp; !((TypedStringValue) originalValue).isDynamic() &amp;&amp; !(convertedValue instanceof Collection || ObjectUtils.isArray(convertedValue))) &#123; pv.setConvertedValue(convertedValue); deepCopy.add(pv); &#125; else &#123; resolveNecessary = true; deepCopy.add(new PropertyValue(pv, convertedValue)); &#125; &#125;&#125; resolveValueIfNecessary 给定一个PropertyValue，返回一个value，必要时解析对工厂中其他bean的引用。value可以是： 一个BeanDefinition，它导致创建一个相应的新的bean实例。 Singleton标志和这样的”inner beans”的名字被忽略：内部beans是匿名原型。 RuntimeBeanReference(必须解析) ManagedList ManagedSet ManagedMap 一个普通的对象或null，在这种情况下，它是孤立的。 下面这段代码时依赖注入发生的地方，其实际上是在BeanWrapperImpl中来完成。123456try &#123; bw.setPropertyValues(new MutablePropertyValues(deepCopy));&#125;catch (BeansException ex) &#123; //异常：Error setting property values&#125; 上面说到spring是通过BeanDefinitionValueResolver来解析BeanDefinition的，然后再注入到property中，关于这个过程在下一篇中来说。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：依赖注入（一）getBean]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-getbean%2F</url>
    <content type="text"><![CDATA[在Spring源码系列：BeanFactory的创建文章中我们谈到了BeanFactory这容器，这个里面提供了注入的实现接口。其具体的实现还需要从AbstractBeanFactory和DefaultListableBeanFactory中来看。今天就先撸一下AbstractBeanFactory这个类中的getBean这个方法。 1、getBean方法 getBean提供了四个重载方法，如下：12345678910111213141516171819//通过name获取Bean@Overridepublic Object getBean(String name) throws BeansException &#123; return doGetBean(name, null, null, false);&#125;//通过name和类型获取Bean@Overridepublic &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType) throws BeansException &#123; return doGetBean(name, requiredType, null, false);&#125;//通过name和对象参数获取Bean@Overridepublic Object getBean(String name, Object... args) throws BeansException &#123; return doGetBean(name, null, args, false);&#125;//通过name、类型和参数获取Beanpublic &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType, Object... args) throws BeansException &#123; return doGetBean(name, requiredType, args, false);&#125; 从这四个重载方法的方法体中可以看出，他们都是通过doGetBean来实现的。所以doGetBean其实才是真正获取Bean的地方，也是触发依赖注入发生的地方。（这个方法比较长，分段来说） 2、doGetBean 先来看下方法的定义： 1234@SuppressWarnings("unchecked") protected &lt;T&gt; T doGetBean( final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException &#123; name 要检索的bean的名称 requiredType 要检索的bean所需的类型 args 使用显式参数创建bean实例时使用的参数（仅在创建新实例时应用，而不是在检索现有实例时应用） typeCheckOnly 是否为类型检查而获得实例，而不是实际使用 12345678//返回bean名称，剥离工厂引用前缀，并将别名解析为规范名称。final String beanName = transformedBeanName(name);//声明当前需要返回的bean对象Object bean;// 先从缓存中获取bean，处理已经被创建的单例模式的bean，//对于此类bean的请求不需要重复的创建(singleton)Object sharedInstance = getSingleton(beanName); 如果当前获取到的sharedInstance不为null并且参数为空，则进行FactoryBean的相关处理，并获取FactoryBean的处理结果。1234567891011121314if (sharedInstance != null &amp;&amp; args == null) &#123; if (logger.isDebugEnabled()) &#123; //返回指定的singleton bean是否正在创建（在整个工厂内）。 if (isSingletonCurrentlyInCreation(beanName)) &#123; logger.debug("Returning eagerly cached instance of singleton bean '" + beanName +"' that is not fully initialized yet - a consequence of a circular reference"); &#125; else &#123; logger.debug("Returning cached instance of singleton bean '" + beanName + "'"); &#125; &#125; //完成FactoryBean的相关处理，并用来获取FactoryBean的处理结果 bean = getObjectForBeanInstance(sharedInstance, name, beanName, null);&#125; 如果当前获取到的sharedInstance为null，我们再来看下做了哪些处理（下面的都在一个大的else里面）： 123else &#123; //分解到下面&#125; 1234//在当前线程中，返回指定的prototype bean是否正在创建。if (isPrototypeCurrentlyInCreation(beanName)) &#123; throw new BeanCurrentlyInCreationException(beanName);&#125; 下面这段的作用是对Ioc容器中的BeanDefinition是否存在进行检测，先是检测当前BeanFactory中是否能够获取到，如果取不到则继续到双亲容器中进行尝试获取，如果双亲还是取不到，则继续向上一级父容器中尝试获取。1234567891011121314// 检查该工厂是否存在bean定义。BeanFactory parentBeanFactory = getParentBeanFactory();if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; // 如果没有，则继续检查父类 String nameToLookup = originalBeanName(name); if (args != null) &#123; // 用明确的参数代表父项。 return (T) parentBeanFactory.getBean(nameToLookup, args); &#125; else &#123; // 如果没有args - &gt;委托给标准的getBean方法。 return parentBeanFactory.getBean(nameToLookup, requiredType); &#125;&#125; 将指定的bean标记为已经创建（或即将创建）；这里允许bean工厂优化其缓存以重复创建指定的bean。123if (!typeCheckOnly) &#123; markBeanAsCreated(beanName);&#125; 先根据beanName来获取BeanDefinition，然后获取当前bean的所有依赖bean，这里是通过递归调用getBean来完成，直到没有任何依赖的bean为止。12345678910111213141516final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName);//检查给定的合并bean定义，可能抛出验证异常。checkMergedBeanDefinition(mbd, beanName, args);// 保证当前bean依赖的bean的初始化。String[] dependsOn = mbd.getDependsOn();if (dependsOn != null) &#123; for (String dep : dependsOn) &#123; if (isDependent(beanName, dep)) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, "Circular depends-on relationship between '" + beanName + "' and '" + dep + "'"); &#125; registerDependentBean(dep, beanName); //递归处理依赖bean getBean(dep); &#125;&#125; 下面这段就是创建一个bean实例；这里通过调用getSingleton方法来创建一个单例bean实例；从代码中可以看到，getSingleton的调用是通过getObject这个回调函数来间接调用createBean完成的。 12345678910111213141516171819if (mbd.isSingleton()) &#123; sharedInstance = getSingleton(beanName, new ObjectFactory&lt;Object&gt;() &#123; //回调函数getObject @Override public Object getObject() throws BeansException &#123; try &#123; //创建bean return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; //发生异常则销毁 destroySingleton(beanName); throw ex; &#125; &#125; &#125;); //获取给定bean实例的对象，无论是bean实例本身，还是FactoryBean创建的对象。 bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd);&#125; 下面是创建prototype bean123456789101112else if (mbd.isPrototype()) &#123; Object prototypeInstance = null; try &#123; beforePrototypeCreation(beanName); //创建prototype bean prototypeInstance = createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd);&#125; 最后是对创建的bean进行类型检查，没有问题就返回已经创建好的bean；此时这个bean是包含依赖关系的bean 1234567891011121314if (requiredType != null &amp;&amp; bean != null &amp;&amp; !requiredType.isAssignableFrom(bean.getClass())) &#123; try &#123; return getTypeConverter().convertIfNecessary(bean, requiredType); &#125; catch (TypeMismatchException ex) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Failed to convert bean &apos;&quot; + name + &quot;&apos; to required type &apos;&quot; + ClassUtils.getQualifiedName(requiredType) + &quot;&apos;&quot;, ex); &#125; throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); &#125;&#125;//返回beanreturn (T) bean; getBean是依赖注入的起点，从上面的分析可以看出，bean的创建都是通过createBean来完成具体的创建的。createBean的具体实现是在AbstractAutowireCapableBeanFactory中的，这里createBean不仅仅负责创建bean，还需要完成对bean的初始化。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：依赖注入-引言]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-inject-intro%2F</url>
    <content type="text"><![CDATA[Spring源码系列：BeanDefinition源码解析 Spring源码系列：BeanDefinition载入(上) Spring源码系列：BeanDefinition载入(中) Spring源码系列：BeanDefinition载入(下) 在上面四篇文章中大概分析了一下Bean的载入，其实这个过程就是在Ioc容器中建立BeanDefinition的数据映射。但是对于Bean的实例化并未涉及，在之前的分析中也提到，bean的实例化是在依赖注入是才具体完成。 关于依赖注入关于Spring，我们最先想到的就两个Ioc和Aop；然后关于Ioc我们又能牵扯出两个：控制反转和依赖注入。控制反转和依赖注入在网上被无数大神亦或菜鸟解读过，这里就不罗列那些概念了，直接看： 不使用Spring1234567public class UserService &#123; //手动new一个 UserDao userDao = new UserDaoImpl(); public int insertUser(String userName) &#123; return userDao.insertUser(userName); &#125;&#125; 使用Spring(以注解方式) 1234567public class UserService &#123; @Autowired private UserDao userDao; public int insertUser(String userName) &#123; return userDao.insertUser(userName); &#125;&#125; 看起来貌似没有啥很大的改变，区别呢？ 我们先来分析下在一个类中这两种申明的区别： UserDao userDao; userDao是UserDao类型的引用名称。仅仅是声明了一个变量引用名称。并没有做实例化，userDao的实例化可以通过set方法进行设置（Spring中之前常用的就是set方法注入）；当我们初始化持有userDao的这个类时我们还不知道userDao到底具体指向哪个堆中的对象地址。 UserDao userDao = new UserDaoImpl(); 而这个，申明一个变量名称，并将userDao直接指向new UserDaoImpl()创建的对象。 我们来看Spring中关于注入之后对象地址以及不使用注入方式对象的地址： 1、直接注入2、注入覆盖了我自己的对象3、自己手动new 通过上面三幅图可以明显的看出，自己手动new的对象没有使用代理的方式，而托管给Spring注入的对象均是通过动态代理来完成的。 关于动态代理：《猪弟拱Java》连载番外篇：Java代理（中） 总结：当某个角色(可能是一个Java实例，调用者)需要另一个角色(另一个Java实例，被调用者)的协助时，在未使用Spring来管理Bean的程序设计过程中，通常由调用者来创建被调用者的实例。但在Spring里，创建被调用者的工作不再由调用者来完成，因此称为控制反转;创建被调用者实例的工作通常由Spring容器来完成，然后注入调用者，因此也称为依赖注入。 三个问题那么现在要考虑问题就是，什么时候会触发我们的依赖注入呢？Bean的实例化是否必须在依赖注入时才能完成呢？在Spring中又是通过哪些类来完成注入工作的呢？ 1、什么时候会触发我们的依赖注入 答：用户第一次向容器获取Bean的时候出发。 2、Bean的实例化是否必须在依赖注入时才能完成 这个其实不是必须的，咱们都知道BeanDefinition中有lazy-init这样一个属性，我们可以通过控制这个属性的设置来让容器完成对Bean的预实例化。预实例化就是说它的依赖注入是在实例化过程中完成的。 第一和第二个问题将会在分析第三个问题的时候慢慢的细化分析。所以第三个问题其实没啥鸟用，但也是最最最核心的，就是为了引出后面关于一些具体类的分析的。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：Spring的版本变更]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-version-change%2F</url>
    <content type="text"><![CDATA[Spring是一个开放源代码的设计层面框架，它解决的是业务逻辑层和其他各层的松耦合问题，因此它将面向接口的编程思想贯穿整个系统应用。Spring是于2003 年兴起的一个轻量级的Java 开发框架，由Rod Johnson创建。简单来说，Spring是一个分层的JavaSE/EEfull-stack(一站式) 轻量级开源框架【来自百度百科】。 目前Spring已经发展到5.0.4 版本了，今天咱们就来聊一聊spring各个版本都提供了哪些新的特性。 spring 2.xspring的2.x包括下面两个： 2.0.0-2.0.8 2.5.0-2.5.6此时的spring还是很年轻的，所包含的模块也不是很多。如下图： 2.0 新特性 XML Schema的新XML配置语法的出现 新的Bean的作用域 以前的Spring版本对IoC容器级支持两个不同的bean作用域（singleton和prototype）。Spring 2.0在这方面进行了改进，不仅根据部署Spring的环境（例如Web环境中的请求和会话作用域Bean）提供了一些额外的作用域，还提供了“挂钩”），使Spring用户可以创建自己的范围。 应该注意的是，虽然singleton和prototype作用域bean的底层（和内部）实现已经改变，但是所述改变对于最终用户是完全透明的…不需要改变现有的配置，并且不存在现有的配置将会中断。 可扩展的XML 支持@AspectJ方面 更加简单XML配置(aop&amp;事务) 异步JMS Spring MVC的表单标签库 Java 5（Tiger）支持 Spring 2.0现在支持用Java以外的语言编写的bean，目前支持的动态语言是JRuby，Groovy和BeanShell 提供了一个关于任务调度的抽象概念 引入了对各种注释的支持，用于配置目的，例如@ Transactional， @Required和@PersistenceContext / @PersistenceUnit。 2.5 新特性 在Spring 2.0在整个框架中对Java 5的深入支持之后，Spring 2.5引入了对Java 6的专门支持。 @Autowired结合对JSR-250注释@Resource，@ PostConstruct和@PreDestroy的支持 。 在类路径中自动检测组件 Spring 2.5引入了支持组件扫描：在类路径中自动检测带注释的组件。典型地，这样的组件类将与定型如进行注释@Component， @Repository，@Service， @Controller。根据应用程序的上下文配置，这些组件类将被自动检测并转换为Spring bean定义，而不需要为每个这样的bean显式配置。 支持bean名称切入点元素 Spring 2.5引入了对bean（…） pointcut元素的支持，根据Spring定义的bean名称匹配特定的命名bean 支持AspectJ加载时织入 Spring 2.5显着扩展了SimpleJdbcTemplate的功能， 并引入了 SimpleJdbcCall和SimpleJdbcInsert 操作对象。 基于注释的控制器。 Spring 2.5为MVC控制器引入了一个基于注释的编程模型，使用@ RequestMapping，@ RequestParam，@ ModelAttribute等注解。这个注解支持可用于Servlet MVC和Portlet MVC。以这种风格实现的控制器不必扩展特定的基类或实现特定的接口。此外，他们通常不直接依赖于Servlet或Portlet API，尽管他们可以很容易地访问Servlet或Portlet设施。 引入了Spring TestContext框架 Spring 3.xspring 3.x包括以下几个系列： 3.0.0-3.0.7 3.1.0-3.1.4 3.2.0-3.2.18 在2.x的模块上页拓展了新的模块 3.0.x 新特性 针对Java 5更新的核心API spring 表达语言 基于Java的bean元数据和在组件中定义bean元数据 通用型转换系统和现场格式化系统 全面的REST支持 声明式模型验证 早期支持Java EE 6 支持嵌入式数据库 3.1.x 新特性 缓存抽象 Bean定义配置文件 环境抽象 PropertySource抽象 Spring的XML名称空间的代码等价物 支持Hibernate 4.x TestContext框架支持@Configuration类和bean定义配置文件 更简洁的构造函数注入的命名空间 支持针对非标准JavaBeans设置器的注入 支持Servlet 3基于代码的Servlet Container配置 支持Servlet 3 MultipartResolver 没有persistence.xml的JPA EntityManagerFactory引导 用于注释的控制器处理的新的基于HandlerMethod的支持类 Flash属性和RedirectAttributes(请求重定向参数的支持) “consumes” and “produces” conditions in @RequestMapping 改进了对通过’Content-Type’标题指定方法消耗的媒体类型以及通过标题指定的可生成类型的支持’Accept’ URI模板变量增强 @Valid on @RequestBody控制器方法参数 控制器方法参数上的@RequestPart注释 UriComponentsBuilder和UriComponents 3.2.x 新特性 支持基于Servlet 3的异步请求处理 Spring MVC测试框架 @ControllerAdvice注解 基于代码的Servlet 3+容器初始化的抽象基类 ResponseEntityExceptionHandler类引入 在RestTemplate和中的 @RequestBody参数支持泛型类型 JackJSON 2和相关的改进 @RequestBody改进 HTTP PATCH方法 使用注释点和bean定义方法的元注释 初步支持JCache 0.5 全球日期和时间格式 整个框架的并发优化 新的基于Gradle的构建和移动到GitHub 精炼的Java SE 7 / OpenJDK 7支持 Spring 4.xspring 4包括以下系列版本： 4.0.0-4.0.9 4.1.0-4.1.9 4.2.0-4.2.9 4.3.0-4.3.13 spring 4.0.x 新特性 删除弃用的软件包和方法 可选的第三方依赖已被提升到2010/2011最低（即Spring 4通常只支持2010年末或之后发布的版本）：特别是，Hibernate 3.6+，EhCache 2.1+，Quartz 1.8+，Groovy 1.8+和Joda-Time 2.0+。作为规则的一个例外，Spring 4需要最近的Hibernate Validator 4.3+，并且对Jackson的支持已经集中在2.0+以上（当前Spring 3.2已经保留了对Jackson 1.8 / 1.9的支持;现在只是弃用了形成）。 Java 8（以及6和7） 可以使用Spring的回调接口使用 lambda表达式和方法引用 Java EE 6和7 Java EE 6或更高版本现在被认为是Spring Framework 4的基准，JPA 2.0和Servlet 3.0规范特别相关。为了与Google App Engine和较早的应用程序服务器保持兼容，可以将Spring 4应用程序部署到Servlet 2.5环境中。不过，强烈建议使用Servlet 3.0+，这是Spring开发环境中测试设置的测试和模拟包中的先决条件。 Groovy Bean定义DSL 核心容器改进 Spring现在将泛型类型作为注入Beans时限定符的形式 。例如，如果您正在使用Spring Data Repository，则现在可以轻松注入一个特定的实现： @Autowired Repository customerRepository。 如果您使用Spring的元注释支持，现在可以开发自定义注释来 显示源注释中的特定属性。 bean现在可以在自动装配到列表和数组中时进行排序。无论是标注和接口的支持。 @OrderOrdered 该@Lazy注释现在可以在注入点使用，以及对@Bean 定义。 该@Description批注已经推出了使用基于Java的配置开发。 已经通过注释添加 了有条件地过滤bean的通用模型@Conditional。这与@Profile支持类似，但允许以编程方式开发用户定义的策略。 基于CGLIB的代理类不再需要默认的构造函数。支持通过提供objenesis 其重新打包库在线，并将其作为Spring框架的一部分。有了这个策略，所有的构造函数都不再被调用代理实例。 整个框架现在都有管理时区的支持，例如LocaleContext。 web 模块支持 可以在Spring MVC应用程序中使用新的@RestController注释，不需要添加@ResponseBody到每个 @RequestMapping方法中。 该AsyncRestTemplate已添加，允许异步非阻塞支持开发REST客户端时。 开发Spring MVC应用程序时提供了全面的时区支持。 测试改进 WebSocket，SockJS和STOMP消息传递 spring 4.1.x 新特性 JMS改进 缓存改进 网络改进 WebSocket消息传递改进 测试改进 spring 4.2.x 新特性 Spring 4 官方文档 核心容器改进 数据访问改进 spring 4.3.x 新特性 支持新的包和服务 相关改进 目前还没有用过spring5，总会有憧憬。当我们使用spring越来越简单时，危机也在一步步逼近；看到表象，进一步，再进一步！]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：BeanDefinition载入(下)]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-bd-loaded-3%2F</url>
    <content type="text"><![CDATA[在Spring源码系列：BeanDefinition载入(上)中已经大概捋了一下解析过程，本篇将记录一下bean的注册过程。bean的注册就是DefaultListableBeanFactory中registerBeanDefinition方法来完成的。那我就来看registerBeanDefinition这个方法的具体逻辑。1、beanDefinition类型判断和验证这里的验证主要是验证不能将静态工厂方法与方法重写相结合(静态工厂方法必须创建实例);if (beanDefinition instanceof AbstractBeanDefinition) { try { ((AbstractBeanDefinition) beanDefinition).validate(); } catch (BeanDefinitionValidationException ex) { throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName,“Validation of bean definition failed”, ex); }}2、尝试从beanDefinitionMap中获取老的bean这里就是先根据beanName从beanDefinitionMap去取BeanDefinition，并将结果给oldBeanDefinition。BeanDefinition oldBeanDefinition;oldBeanDefinition = this.beanDefinitionMap.get(beanName);3、beanDefinitionMap中已经存在名为beanName的Beandefinition如果当前beanDefinitionMap中已经存在名为beanName的Beandefinition了(即检查是否有相同名称的beanDefinition已经在Ioc容器中注册过了)。，如果有，则进行以下具体策略：如果不允许bean被覆盖，则直接抛出不能重新注册，bean已经存在这样的异常信息使用框架生成的Bean来代替用户自定义的bean覆盖原有的Beandefinitionif (oldBeanDefinition != null) { if (!isAllowBeanDefinitionOverriding()) { //省略异常代码 } else if (oldBeanDefinition.getRole() &lt; beanDefinition.getRole()) { //省略异常代码 } else if (!beanDefinition.equals(oldBeanDefinition)) { //提示覆盖log信息 } else { //提示覆盖log信息 } //覆盖原有的Beandefinition this.beanDefinitionMap.put(beanName, beanDefinition);}4、beanDefinitionMap不存在名为beanName的Beandefinition//检查bean的创建阶段是否已经开始，也就是说是否已经创建了if (hasBeanCreationStarted()) { //Cannot modify startup-time collection elements anymore (for stable iteration) // 无法修改启动时间收集元素（用于稳定迭代）（译注） //注册过程需要保证数据的一致性，所有需要加锁同步 synchronized (this.beanDefinitionMap) { //注册到beanDefinitionMap中 this.beanDefinitionMap.put(beanName, beanDefinition); //下面就是将当前beanName存放到beanDefinitionNames中 List&lt;String&gt; updatedDefinitions = new ArrayList&lt;String&gt;( this.beanDefinitionNames.size() + 1); updatedDefinitions.addAll(this.beanDefinitionNames); updatedDefinitions.add(beanName); this.beanDefinitionNames = updatedDefinitions; //如果单例模式的bean名单中有该bean的name，那么移除掉它。 //也就是说着，将一个原本是单例模式的bean重新注册成一个普通的bean if (this.manualSingletonNames.contains(beanName)) { Set&lt;String&gt; updatedSingletons = new LinkedHashSet&lt;String&gt;(this.manualSingletonNames); updatedSingletons.remove(beanName); this.manualSingletonNames = updatedSingletons; } }}// 仍处于启动阶段，bean还没有开始注册else { // Still in startup registration phase this.beanDefinitionMap.put(beanName, beanDefinition); this.beanDefinitionNames.add(beanName); this.manualSingletonNames.remove(beanName);}this.frozenBeanDefinitionNames = null;5、执行缓存清除1：oldBeanDefinition如果存在，且执行到了这里也没有抛出异常，说明该BeanDefinition已经被覆盖，缓存需要更新。2：如果是单例模式的bean对象则Set中包含该beanName，执行到这里说明该BeanDefinition已经从一个单例模式的bean变为了一个普通的bean，所以缓存也需要更新。if (oldBeanDefinition != null || containsSingleton(beanName)) { resetBeanDefinition(beanName);}OK，我们来看下resetBeanDefinition这个方法:这个方法的作用就是重置给定bean的所有bean定义缓存，包括从它派生的bean的缓存。protected void resetBeanDefinition(String beanName) { // 如果已经创建，则删除给定bean的合并bean定义。 clearMergedBeanDefinition(beanName); // 如果有的话，从singleton 高速缓存中删除相应的bean。 //但是这也不是必须的，而只是为了覆盖上下文的默认bean //（就是从manualSingletonNames中移除） destroySingleton(beanName); //递归的方式来 重置具有给定bean作为父项的所有bean定义。 for (String bdName : this.beanDefinitionNames) { if (!beanName.equals(bdName)) { BeanDefinition bd = this.beanDefinitionMap.get(bdName); if (beanName.equals(bd.getParentName())) { resetBeanDefinition(bdName); } } }}Bean的注册就到这里了，下一篇学习的是DefaultListableBeanFactory这个集大成者容器。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：BeanDefinition载入（中）]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-bd-loaded-2%2F</url>
    <content type="text"><![CDATA[上一篇是将Bean的解析注册流程进行了梳理，对于一些细节问题没有进行细究，比如说元素属性值的处理，构造函数的处理等等。本篇就学习记录一下相关点。 首先来看下是在哪个地方具体生成BeanDefinitiond的。下面是方法请求的顺序。 DefaultBeanDefinitionDocumentReader.parseDefaultElement DefaultBeanDefinitionDocumentReader.processBeanDefinition BeanDefinitionParserDelegate.parseBeanDefinitionElement 关于元素的解析绝大多数都是在BeanDefinitionParserDelegate及其子类中完成的。OK，来看下parseBeanDefinitionElement这个方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, BeanDefinition containingBean) &#123; this.parseState.push(new BeanEntry(beanName)); String className = null; //在这里是读取&lt;bean&gt;的class名字，然后载入到BeanDefinition中，并未做实例化 if (ele.hasAttribute(CLASS_ATTRIBUTE)) &#123; className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); &#125; try &#123; String parent = null; if (ele.hasAttribute(PARENT_ATTRIBUTE)) &#123; parent = ele.getAttribute(PARENT_ATTRIBUTE); &#125; //生成BeanDefinition对象 AbstractBeanDefinition bd = createBeanDefinition(className, parent); //解析当前bean的属性 parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); //设置description信息 bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); //对bean的元素信息进行解析 parseMetaElements(ele, bd); parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); //解析bean的构造函数设置 parseConstructorArgElements(ele, bd); //解析property设置 parsePropertyElements(ele, bd); parseQualifierElements(ele, bd); bd.setResource(this.readerContext.getResource()); bd.setSource(extractSource(ele)); return bd; &#125; //异常1：ClassNotFoundException catch (ClassNotFoundException ex) &#123; error("Bean class [" + className + "] not found", ele, ex); &#125; //异常2：NoClassDefFoundError catch (NoClassDefFoundError err) &#123; error("Class that bean class [" + className + "] depends on not found", ele, err); &#125; //其他未知错误 catch (Throwable ex) &#123; error("Unexpected failure during bean definition parsing", ele, ex); &#125; finally &#123; this.parseState.pop(); &#125; return null;&#125; 此处我们以解析property为例，看下具体的处理细节： 12345678910111213//解析给定bean元素的属性子元素。public void parsePropertyElements(Element beanEle, BeanDefinition bd) &#123; //获取子元素节点 NodeList nl = beanEle.getChildNodes(); //遍历 for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); //是否包含property标识 if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, PROPERTY_ELEMENT)) &#123; parsePropertyElement((Element) node, bd); &#125; &#125;&#125; 接着是执行具体property,在parsePropertyElement中完成： 12345678910111213141516171819202122232425262728//解析一个property元素。public void parsePropertyElement(Element ele, BeanDefinition bd) &#123; //首先获取到property的名称 String propertyName = ele.getAttribute(NAME_ATTRIBUTE); //检查是否有name if (!StringUtils.hasLength(propertyName)) &#123; error("Tag 'property' must have a 'name' attribute", ele); return; &#125; this.parseState.push(new PropertyEntry(propertyName)); try &#123; //验证在同一个bean中存在同名的property，存在的话就不解析了，直接返回 if (bd.getPropertyValues().contains(propertyName)) &#123; error("Multiple 'property' definitions for property '" + propertyName + "'", ele); return; &#125; //解析出property的值 Object val = parsePropertyValue(ele, bd, propertyName); //封装成PropertyValue对象 PropertyValue pv = new PropertyValue(propertyName, val); parseMetaElements(ele, pv); pv.setSource(extractSource(ele)); bd.getPropertyValues().addPropertyValue(pv); &#125; finally &#123; this.parseState.pop(); &#125;&#125; 在parsePropertyValue中，是对所有的property子元素进行具体解析的。我们知道property中除了单值之外，还会包括如：list，set，map，prop等集合元素；这些都会被封装成对应的Managerd对象。比如：ManagedList等。不同的集合类型页同样对应一种解析方法，比如解析list的是使用parseListElement。这些解析都是在BeanDefinitionParserDelegate类中完成的。这个后面我会抽一篇来学习BeanDefinitionParserDelegate这个类。 Bean的载入过程就是这样通过层层解析来完成的，但是对于目前的Ioc容器来说，仅仅是完成了对Bean对象管理的一些数据准备工作，也就是初始化工作，目前的BeanDefginiton中包含的就是一些静态的配置信息，Bean的实例化还没有进行，这个实例化的过程是在依赖注入时候完成的。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：BeanDefinition载入(上)]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-bd-loaded-1%2F</url>
    <content type="text"><![CDATA[继上一篇BeanFactory的创建之后，其实就是BeanDefinition载入了。同样也是在AbstractRefreshableApplicationContext类的refreshBeanFactory方法中完成：//创建默认的DefaultListableBeanFactory工厂DefaultListableBeanFactory beanFactory = createBeanFactory();//设置IdbeanFactory.setSerializationId(getId());//这个方法其实就是设置了allowBeanDefinitionOverriding和allowCircularReferences两个属性customizeBeanFactory(beanFactory);//调用子类的加载bean定义方法，这里会调用XmlWebApplicationContext子类的复写方法loadBeanDefinitions(beanFactory);这里的loadBeanDefinitions也是一个抽象方法，AbstractRefreshableApplicationContext类中并没有给出具体的实现，二是通过子类XmlWebApplicationContext的loadBeanDefinitions完成具体实现。protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException { //创建XmlBeanDefinitionReader，并通过回调设置到BeanFactory中 XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); //为XmlBeanDefinitionReader配置Environment beanDefinitionReader.setEnvironment(getEnvironment()); //为XmlBeanDefinitionReader配置ResourceLoader， //因为DefaultResourceLoader是父类，所以this可以直接被使用 beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // 允许子类提供reader的自定义初始化，然后继续实际加载bean定义。 initBeanDefinitionReader(beanDefinitionReader); loadBeanDefinitions(beanDefinitionReader);}initBeanDefinitionReader初始化用于加载此上下文的bean定义的bean定义读取器；默认实现是空的。然后下面通过重载的loadBeanDefinitions来做具体的bean解析（这里用到的是XmlBeanDefinitionReader这个解析器）；使用给定的XmlBeanDefinitionReader加载bean definitions。protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws IOException { String[] configLocations = getConfigLocations(); //遍历xml文件 if (configLocations != null) { for (String configLocation : configLocations) { reader.loadBeanDefinitions(configLocation); } }}此时会将我们的applicationContext.xml读入（当然如何还有其他的spring配置文件，同样会一块拿到路径），如下图所示：然后继续通过loadBeanDefinitions的重载方法继续委托调用。最后交给AbstractBeanDefinitionReader的loadBeanDefinitions来完成；这个代码比较长，拆开一步一步来说，先看下整体的：public int loadBeanDefinitions(String location, Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException { //获取ResourceLoader资源定位器 ResourceLoader resourceLoader = getResourceLoader(); //如果定位器为null，则抛出异常 if (resourceLoader == null) { throw new BeanDefinitionStoreException( “Cannot import bean definitions from location [“ + location + “]: no ResourceLoader available”); } //是否是ResourcePatternResolver类型的定位器 if (resourceLoader instanceof ResourcePatternResolver) { // Resource pattern matching available. try { Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); int loadCount = loadBeanDefinitions(resources); if (actualResources != null) { for (Resource resource : resources) { actualResources.add(resource); } } if (logger.isDebugEnabled()) { logger.debug(“Loaded “ + loadCount + “ bean definitions from location pattern [“ + location + “]”); } return loadCount; } catch (IOException ex) { throw new BeanDefinitionStoreException( “Could not resolve bean definition resource pattern [“ + location + “]”, ex); } } //非ResourcePatternResolver类型的 else { // Can only load single resources by absolute URL. Resource resource = resourceLoader.getResource(location); int loadCount = loadBeanDefinitions(resource); if (actualResources != null) { actualResources.add(resource); } if (logger.isDebugEnabled()) { logger.debug(“Loaded “ + loadCount + “ bean definitions from location [“ + location + “]”); } return loadCount; }}上面的代码中需要说明下为什么要判断当前resourceLoader是否是ResourcePatternResolver类型的，因为ResourceLoader只是提供了对classpath前缀的支持。而ResourcePatternResolver提供了对classpath前缀的支持。也就是说ResourceLoader提供classpath下单资源文件的载入，而ResourcePatternResolver提供多资源文件的载入。先看下假如是ResourcePatternResolver类型的（略去了部分log代码）：try { //先得到我们的resources Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); //解析并返回beanDefinition的数量 int loadCount = loadBeanDefinitions(resources); //加载过程中已经被解析过的实际的Resource的填充集合 if (actualResources != null) { for (Resource resource : resources) { actualResources.add(resource); } } return loadCount;}catch (IOException ex) { throw new BeanDefinitionStoreException( “Could not resolve bean definition resource pattern [“ + location + “]”, ex);}非ResourcePatternResolver类型情况：// Can only load single resources by absolute URL.//只能通过绝对URL加载单个资源Resource resource = resourceLoader.getResource(location);//解析并返回beanDefinition的数量int loadCount = loadBeanDefinitions(resource);if (actualResources != null) { actualResources.add(resource);}return loadCount;然后继续通过重载方法loadBeanDefinitions(Resource… resources)来解析（AbstractBeanDefinitionReader类中）public int loadBeanDefinitions(Resource… resources) throws BeanDefinitionStoreException { Assert.notNull(resources, “Resource array must not be null”); //初始化beanDefiniton个数 int counter = 0; //遍历当前资源数组 for (Resource resource : resources) { //解析具体resource中的bean counter += loadBeanDefinitions(resource); } return counter;}然后交给子类XmlBeanDefinitionReader中的loadBeanDefinitions(Resource resource)方法：public int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException { return loadBeanDefinitions(new EncodedResource(resource));}继续通过重载方法loadBeanDefinitions(EncodedResource encodedResource)执行，这个方法我们只关注最核心的代码：//获取输入流InputStream inputStream = encodedResource.getResource().getInputStream();try { //资源读取inputSource InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) { inputSource.setEncoding(encodedResource.getEncoding()); } //委托给doLoadBeanDefinitions来完成 return doLoadBeanDefinitions(inputSource, encodedResource.getResource());}finally { inputStream.close();}doLoadBeanDefinitions是XmlBeanDefinitionReader中的方法，来看核心代码：//解析成符合w3c标准的DocumentDocument doc = doLoadDocument(inputSource, resource);//继续交给registerBeanDefinitions来处理return registerBeanDefinitions(doc, resource);这个时候已经将loadBeanDefinitions换成registerBeanDefinitions了，也就是载入并注册；registerBeanDefinitions同样也是XmlBeanDefinitionReader中的方法：public int registerBeanDefinitions(Document doc, Resource resource) throwsBeanDefinitionStoreException { //得到documentReader用来读取document文档 BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); //注册之前的bean个数 int countBefore = getRegistry().getBeanDefinitionCount(); //解析并注册bean documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); return getRegistry().getBeanDefinitionCount() - countBefore;}仍然没有处理，继续交给BeanDefinitionDocumentReader的registerBeanDefinitions方法来完成：//这个实现根据“spring-beans”XSD（或DTD）解析bean定义。public void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) { this.readerContext = readerContext; logger.debug(“Loading bean definitions”); Element root = doc.getDocumentElement(); doRegisterBeanDefinitions(root);}还是没处理，又细化一步，交给DefaultBeanDefinitionDocumentReader的doRegisterBeanDefinitions(Element root)方法：protected void doRegisterBeanDefinitions(Element root) { BeanDefinitionParserDelegate parent = this.delegate; this.delegate = createDelegate(getReaderContext(), root, parent); if (this.delegate.isDefaultNamespace(root)) { String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); if (StringUtils.hasText(profileSpec)) { String[] specifiedProfiles = StringUtils.tokenizeToStringArray( profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); if (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) { if (logger.isInfoEnabled()) { logger.info(“Skipped XML bean definition file due to specified profiles [“ + profileSpec + “] not matching: “ + getReaderContext().getResource()); } return; } } } preProcessXml(root); parseBeanDefinitions(root, this.delegate); postProcessXml(root); this.delegate = parent;}任何嵌套的&lt;beans&gt;元素都将导致此方法的递归。 为了正确传播和保存&lt;beans&gt; default- 属性，请跟踪当前（父）委托，该委托可能为null。 为了回退的目的，创建一个引用父对象的新（子）委托，然后最终重置this.delegate回到它的原始（父）引用。这个行为模仿了一堆委托，而实际上并不需要一个委托。（翻译的有点蹩脚，大概意思就是这）所以说DefaultBeanDefinitionDocumentReader自己也没干这事，又给了BeanDefinitionParserDelegate，然后就是preProcessXml()、parseBeanDefinitions()、postProcessXml()方法；其中preProcessXml()和postProcessXml()默认是空方法，自己没有实现。具体解析在parseBeanDefinitions(root, this.delegate)中完成。BeanDefinitionParserDelegate用于将 Document 的内容转成 BeanDefinition实例；BeanDefinitionDocumentReader 本身不具备该功能而是交给了该类来完成。protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) { //查看定义的命名空间是否为默认的命名空间 if (delegate.isDefaultNamespace(root)) { NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) { Node node = nl.item(i); if (node instanceof Element) { Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) { parseDefaultElement(ele, delegate); } else { delegate.parseCustomElement(ele); } } } } else { delegate.parseCustomElement(root); }}这个方法就是解析文档中根目录下的元素：“import”，“alias”，“bean”。private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) { //解析一个“import”元素，并将给定资源的bean定义加载到bean工厂中。 if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) { importBeanDefinitionResource(ele); } //处理给定的别名元素，向注册表注册别名。 else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) { processAliasRegistration(ele); } //处理给定的bean元素，解析bean定义并将其注册到注册表中。 else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) { processBeanDefinition(ele, delegate); } //在给定的根&lt;beans /&gt;元素内注册每个bean定义。 else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) { // recurse doRegisterBeanDefinitions(ele); }}先来看processBeanDefinition这个方法；BeanDefinitionHolder是一个BeanDefinition的持有者，其定义了一下变量，并对以下变量提供get和set操作。这个在后面的说道BeanDefinition体系的时候再聊。protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) { //获取一个BeanDefinitionHolder BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) { //首先根据自定义属性进行装饰。 //基于自定义嵌套元素进行装饰。 bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try { // 注册最终装饰的实例。 BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); } catch (BeanDefinitionStoreException ex) { getReaderContext().error(“Failed to register bean definition with name ‘“ + bdHolder.getBeanName() + “‘“, ele, ex); } // 发送注册事件。 getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); }}接着看registerBeanDefinition这个方法：通过给定的bean工厂注册给定的bean definition 。public static void registerBeanDefinition( BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException { // 在主名称下注册bean定义。 String beanName = definitionHolder.getBeanName(); registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); // 如果有的话，注册bean名称的别名， String[] aliases = definitionHolder.getAliases(); if (aliases != null) { for (String alias : aliases) { registry.registerAlias(beanName, alias); } }}registerBeanDefinition里面又通过调用BeanDefinitionRegistry接口的实现DefaultListableBeanFactory来完成具体的注册过程。关于DefaultListableBeanFactory中registerBeanDefinition方法的解析逻辑将在Spring源码系列：BeanDefinition载入(下)中来说.欢迎关注微信公众号]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：BeanDefinition 源码解析]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-beandefinition%2F</url>
    <content type="text"><![CDATA[Bean的定义主要由BeanDefinition来描述的。作为Spring中用于包装Bean的数据结构，今天就来看看它的面纱下的真容吧。 首先就是BeanDefinition的类定义：1public interface BeanDefinition extends AttributeAccessor, BeanMetadataElement 对，没错，这货是个接口，而不是类，是不是有点莫名奇妙呢？我们都知道在JAVA中，接口是不能用来new出新的对象的，那么在Spring中，到底将XML解析出来的Bean包装成了什么呢？（这个密等下揭开） 先来看看BeanDefinition一个继承结构吧（均是与BeanDefinition有直接关联的类或者接口）！ 从类图中可以看出，BeanDefinition继承了AttributeAccessor和BeanMetadataElement两个接口；一个一个看。 AttributeAccessor AttributeAccessor接口定义了最基本的对任意对象的元数据的修改或者获取，主要方法有：1234567891011//将name定义的属性设置为提供的value值。如果value的值为null，则该属性为&#123;@link #removeAttribute removed&#125;。//通常，用户应该注意通过使用完全限定的名称（可能使用类或包名称作为前缀）来防止与其他元数据属性重叠。void setAttribute(String name, Object value);//获取标识为name的属性的值。Object getAttribute(String name);//删除标识为name的属性，并返回属性值Object removeAttribute(String name);//如果名为name的属性是否存在，存在返回true，否则返回false。boolean hasAttribute(String name);//返回所有属性的名称。String[] attributeNames(); BeanMetadataElement BeanMetadataElement接口提供了一个getResource()方法,用来传输一个可配置的源对象。 12//返回此元数据元素的配置源对象（可能为null）。Object getSource(); BeanDifinition源码分析一个BeanDefinition描述了一个bean的实例，包括属性值，构造方法参数值和继承自它的类的更多信息。BeanDefinition仅仅是一个最简单的接口，主要功能是允许BeanFactoryPostProcessor 例如PropertyPlaceHolderConfigure 能够检索并修改属性值和别的bean的元数据（译注）。 123//标准单例作用域的作用域标识符：“singleton”。//对于扩展的bean工厂可能支持更多的作用域。String SCOPE_SINGLETON = ConfigurableBeanFactory.SCOPE_SINGLETON; 123//标准原型作用域的范围标识符：“prototype”。//对于扩展的bean工厂可能支持更多的作用域。String SCOPE_PROTOTYPE = ConfigurableBeanFactory.SCOPE_PROTOTYPE; 12//表示BeanDefinition是应用程序主要部分的角色提示。 通常对应于用户定义的bean。int ROLE_APPLICATION = 0; ROLE_SUPPORT =1实际上就是说，我这个Bean是用户的，是从配置文件中过来的。1234//表示BeanDefinition是某些大型配置的支持部分的角色提示，通常是一个外部ComponentDefinition。//当查看某个特定的ComponentDefinition时，认为bean非常重要，//以便在查看应用程序的整体配置时能够意识到这一点。int ROLE_SUPPORT = 1; ROLE_INFRASTRUCTURE = 2就是我这Bean是Spring自己的，和你用户没有一毛钱关系。123//角色提示表明一个BeanDefinition是提供一个完全背景的角色，并且与最终用户没有关系。//这个提示用于注册完全是ComponentDefinition内部工作的一部分的beanint ROLE_INFRASTRUCTURE = 2; 上面是BeanDifinition的一些基本属性信息，一个就是标识下当前Bean的作用域，另外就是标识一下这个Bean是内部的还是外部的。下面来看这个接口为其子类都提供了哪些具体的行为方法： 1.当前Bean父类名称get&amp;set方法1234//如果父类存在，设置这个bean定义的父定义的名称。void setParentName(String parentName);//如果父类存在，则返回当前Bean的父类的名称String getParentName(); 2.当前Bean的className get&amp;set方法123456789//指定此bean定义的bean类名称。//类名称可以在bean factory后期处理中修改，通常用它的解析变体替换原来的类名称。void setBeanClassName(String beanClassName);//返回此bean定义的当前bean类名称。//需要注意的是，这不一定是在运行时使用的实际类名，以防子类定义覆盖/继承其父类的类名。//此外，这可能只是调用工厂方法的类，或者它 在调用方法的工厂bean引用的情况下甚至可能是空的。//因此，不要认为这是在运行时定义的bean类型，而只是将其用于在单独的bean定义级别进行解析。String getBeanClassName(); 3.Bean的作用域get&amp;set方法1234//覆盖此bean的目标范围，指定一个新的范围名称。void setScope(String scope);//返回此bean的当前目标作用域的名称，如果没有确定，返回nullString getScope(); 4.懒加载的get&amp;set方法 123456//设置这个bean是否应该被延迟初始化。如果&#123;false&#125;，那么这个bean将在启动时由bean工厂实例化，//这些工厂执行单例的立即初始化。//懒加载 &lt;bean lazy-init="true/false"&gt;void setLazyInit(boolean lazyInit);//返回这个bean是否应该被延迟初始化，即不是在启动时立即实例化。只适用于单例bean。boolean isLazyInit(); 5.依赖关系设置12345//设置这个bean依赖被初始化的bean的名字。 bean工厂将保证这些bean首先被初始化。//&lt;bean depends-on=""&gt;void setDependsOn(String... dependsOn);//返回这个bean依赖的bean名称。String[] getDependsOn(); 6.是否是自动转配设置123456789//设置这个bean是否是获得自动装配到其他bean的候选人。//需要注意是，此标志旨在仅影响基于类型的自动装配。//它不会影响按名称的显式引用，即使指定的bean没有标记为autowire候选，也可以解决这个问题。//因此，如果名称匹配，通过名称的自动装配将注入一个bean。void setAutowireCandidate(boolean autowireCandidate);//返回这个bean是否是自动装配到其他bean的候选者。就是是否在其他类中使用autowired来注入当前Bean的//是否为被自动装配 &lt;bean autowire-candidate="true/false"&gt;boolean isAutowireCandidate(); 7.主候选Bean1234//是否为主候选bean 使用注解：@Primaryvoid setPrimary(boolean primary);//返回这个bean是否是主要的autowire候选者。boolean isPrimary(); 8.定义创建该Bean对象的工厂类1234//指定要使用的工厂bean（如果有的话）。 这是调用指定的工厂方法的bean的名称。void setFactoryBeanName(String factoryBeanName);//返回工厂bean的名字，如果有的话。String getFactoryBeanName(); 9.创建该Bean对象的工厂方法 123456//如果有的话，指定工厂方法。//这个方法先将通过构造函数参数被调用，或者如果参数，将调用该方法的无参数构造。//方法将在指定的工厂bean（如果有的话）上被调用，或者作为本地bean类的静态方法被调用。void setFactoryMethodName(String factoryMethodName);//如果存在，返回工厂方法名String getFactoryMethodName(); 10.返回此bean的构造函数参数值。12//返回此bean的构造函数参数值。ConstructorArgumentValues getConstructorArgumentValues(); 11.获取普通属性集合12//获取普通属性集合MutablePropertyValues getPropertyValues(); 12.当前Bean的基本特性123456//是否是单例的boolean isSingleton();//是否是多例的boolean isPrototype();//是否是抽象类boolean isAbstract(); 13.当前Bean的应用12//获取这个bean的应用int getRole(); 13.可读描述12//返回对bean定义的可读描述。String getDescription(); 12//返回该bean定义来自的资源的描述String getResourceDescription(); 123//返回原始的BeanDefinition;如果没有，则返回null。允许检索装饰的bean定义（如果有的话）。//注意，这个方法返回直接的发起者。 迭代原始链，找到用户定义的原始BeanDefinition。BeanDefinition getOriginatingBeanDefinition(); 从上面的属性和方法分析可以看出，BeanDefinition对于一个Bean的描述做了较为完整的一套约束。这为后续的子类提供的最基本的职责和属性。 欢迎关注glmapper工作室公众号]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC系列源码：DispatcherServlet]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-base-webmvc3%2F</url>
    <content type="text"><![CDATA[前面两篇文章直接对SpringMVC里面的组件进行了源码分析，可能很多小伙伴都会觉得有点摸不着头脑。所以今天再岔回来说一说SpringMVC的核心控制器，以此为轴心来学习整个SpringMVC的知识体系。 SpringMVC在项目中如何使用的？前面在《项目开发框架-SSM》一篇文章中已经详细的介绍过了SSM项目中关于Spring的一些配置文件，对于一个Spring应用，必不可少的是： 12345678910&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;!-- &lt;param-value&gt;classpath*:config/applicationContext.xml&lt;/param-value&gt; --&gt; &lt;param-value&gt;classpath:spring/applicationContext.xml&lt;/param-value&gt;&lt;/context-param&gt;&lt;!-- 配置一个监听器将请求转发给 Spring框架 --&gt;&lt;!-- Spring监听器 --&gt;&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; 通过ContextLoadListener来完成Spring容器的初始化以及Bean的装载《Spring技术内幕学习：Spring的启动过程》。那么如果在我们需要提供WEB功能，则还需要另外一个，那就是SpringMVC,当然我们同样需要一个用来初始化SpringMVC的配置（初始化9大组件的过程：前面两篇《SpringMVC源码系列：HandlerMapping》和《SpringMVC源码系列：AbstractHandlerMapping》是关于HnadlerMapping的，当然不仅仅这两个，还有其他几个重要的子类，后续会持续更新）： 1234567891011121314151617&lt;servlet&gt; &lt;servlet-name&gt;mvc-dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 配置springMVC需要加载的配置文件 spring-dao.xml,spring-service.xml,spring-web.xml Mybatis（如果有） - &gt; spring -&gt; springmvc --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;async-supported&gt;true&lt;/async-supported&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;mvc-dispatcher&lt;/servlet-name&gt; &lt;!-- 默认匹配所有的请求 --&gt; &lt;url-pattern&gt;*.htm&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 当我们在web.xml中配置好上述内容（当然还得保证咱们的Spring的配置以及SpringMVC的配置文件没有问题的情况下），启动web容器（如jetty），就可以通过在浏览器输入诸如：http://localhost:80/myproject/index.do 的方式来访问我们的应用了。 俗话说知其然，之气所以然；那么为什么在配置好相关的配置文件之后，我们就能访问我们的SSM项目了呢？从发送一条那样的请求（http://localhost:80/myproject/index.do）展示出最后的界面，这个过程在，Spring帮我们做了哪些事情呢？（SpringIOC容器的初始化在《[Spring技术内幕-容器刷新：wac.refresh](https://juejin.im/post/5a3f5b43f265da432e5c37ea)》文中已经大概的说了下大家可以参考一下） SpringMVC处理请求的过程先通过下面这张图来整个了解下SpringMVC请求处理的过程；图中从1-13，大体上描述了请求从发送到界面展示的这样一个过程。从上面这张图中，我们可以很明显的看到有一个DispatcherServlet这样一个类，处于各个请求处理过程中的分发站。实际上，在SpringMVC中，整个处理过程的顶层设计都在这里面。通常我们将DispatcherServlet称为SpringMVC的前端控制器，它是SpringMVC中最核心的类。下面我们就来揭开DispatcherServlet的面纱吧！ DispatcherServletOK，我们直接来看DispatcherServlet的类定义：1public class DispatcherServlet extends FrameworkServlet DispatcherServlet继承自FrameworkServlet，就这样？ 下面才是他家的族谱： 首先为什么要有绿色的部门，有的同学可能已经想到了，绿色部分不是Spring的，而是java自己的；Spring通过HttpServletBean这位年轻人成功的拥有了JAVA WEB 血统（本来Spring就是用JAVA写的，哈哈）。关于Servlet这个小伙伴可以看下我之前的文章，有简单的介绍了这个接口。 话说回来，既然DispatcherServlet归根揭底是一个Servlet，那么就肯定具有Servlet功能行为。 敲黑板！！！Servlet的生命周期是啥（init-&gt;service-&gt;destroy ： 加载-&gt;实例化-&gt;服务-&gt;销毁）。 其实这里我想说的就是service这个方法，当然，在DispatcherServlet中并没有service方法，但是它有一个doService方法！（引的好难…） doService是DispatcherServlet的入口，我们来看下这个方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748protected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; if (logger.isDebugEnabled()) &#123; String resumed = WebAsyncUtils.getAsyncManager(request).hasConcurrentResult() ? " resumed" : ""; logger.debug("DispatcherServlet with name '" + getServletName() + "'" + resumed + " processing " + request.getMethod() + " request for [" + getRequestUri(request) + "]"); &#125; // 在include的情况下保留请求属性的快照，以便能够在include之后恢复原始属性。 Map&lt;String, Object&gt; attributesSnapshot = null; //确定给定的请求是否是包含请求，即不是从外部进入的顶级HTTP请求。 //检查是否存在“javax.servlet.include.request_uri”请求属性。 可以检查只包含请求中的任何请求属性。 //(可以看下面关于isIncludeRequest解释) if (WebUtils.isIncludeRequest(request)) &#123; attributesSnapshot = new HashMap&lt;String, Object&gt;(); Enumeration&lt;?&gt; attrNames = request.getAttributeNames(); while (attrNames.hasMoreElements()) &#123; String attrName = (String) attrNames.nextElement(); if (this.cleanupAfterInclude || attrName.startsWith("org.springframework.web.servlet")) &#123; attributesSnapshot.put(attrName, request.getAttribute(attrName)); &#125; &#125; &#125; // 使框架可用于handler和view对象。 request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext()); request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver); request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver); request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource()); //FlashMap用于保存转发请求的参数的 FlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response); if (inputFlashMap != null) &#123; request.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap)); &#125; request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap()); request.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager); try &#123; doDispatch(request, response); &#125; finally &#123; if (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; // Restore the original attribute snapshot, in case of an include. if (attributesSnapshot != null) &#123; restoreAttributesAfterInclude(request, attributesSnapshot); &#125; &#125; &#125;&#125; PS：“javax.servlet.include.request_uri”是INCLUDE_REQUEST_URI_ATTRIBUTE常量的值。isIncludeRequest(request)方法的作用我们可以借助一条JSP的指令来理解： 1&lt;jsp:incluede page="index.jsp"/&gt; 这条指令是指在一个页面中嵌套了另一个页面，那么我们知道JSP在运行期间是会被编译成相应的Servlet类来运行的，所以在Servlet中也会有类似的功能和调用语法，这就是RequestDispatch.include()方法。 那么在一个被别的servlet使用RequestDispatcher的include方法调用过的servlet中，如果它想知道那个调用它的servlet的上下文信息该怎么办呢，那就可以通过request中的attribute中的如下属性获取： 12345javax.servlet.include.request_urijavax.servlet.include.context_pathjavax.servlet.include.servlet_pathjavax.servlet.include.path_infojavax.servlet.include.query_string 在doService中，下面的try块中可以看到： 123try &#123; doDispatch(request, response);&#125; doService并没有直接进行处理，二是将请求交给了doDispatch进行具体的处理。当然在调用doDispatch之前，doService也是做了一些事情的，比如说判断请求是不是inclde请求，设置一些request属性等。 FlashMap支撑的Redirect参数传递问题在doService中除了webApplicationContext、localeResolver、themeResolve和themeSource四个提供给handler和view使用的四个参数外，后面的三个都是和FlashMap有关的，代码如下：1234567//FlashMap用于保存转发请求的参数的FlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response);if (inputFlashMap != null) &#123; request.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap));&#125;request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap());request.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager); 注释中提到，FlashMap主要用于Redirect转发时参数的传递； 就拿表单重复提交这个问题来说,一种方案就是：在处理完post请求之后，然后Redirect到一个get的请求，这样即使用户刷新也不会有重复提交的问题。但是问题在于,前面的post请求时提交订单，提交完后redirect到一个显示订单的页面，显然在显示订单的页面我们需要知道订单的信息，但是redirect本身是没有参数传递功能的，按照普通的模式如果想传递参数，就只能将参数拼接在url中，但是url在get请求下又是有长度限制的；另外，对于一些场景下，我们也不希望自己的参数暴露在url中。 对于上述问题，我们就可以用FlashMap来进行参数传递了；我们需要在redirect之前将需要的参数写入OUTPUT_FLASH_MAP_ATTRIBUTE，例如： 1234ServletRequestAttributes SRAttributes = (ServletRequestAttributes)(RequestContextHolder.getRequestAttributes());HttpServletRequest req = SRAttributes.getRequest();FlashMap flashMap = (FlashMap)(req.getAttribute(DispatcherServlet.OUTPUT_FLASH_MAP_ATTRIBUTE));flashMap.put("myname","glmapper_2018"); 这样在redirect之后的handler中spring就会自动将其设置到model里面。但是如果仅仅是这样，每次redirect时都写上面那样一段代码是不是又显得很鸡肋呢？当然，spring也为我们提供了更加方便的用法，即在我们的handler方法的参数中使用RedirectAttributes类型变量即可（前段时间用到这个，本来是想单独写一篇关于参数传递问题的，借此机会就省略一篇吧，吼吼…），来看一段代码： 123456789101112131415161718@RequestMapping("/detail/&#123;productId&#125;")public ModelAndView detail(HttpServletRequest request,HttpServletResponse response,RedirectAttributes attributes, @PathVariable String productId) &#123; if (StringUtils.isNotBlank(productId)) &#123; logger.info("[产品详情]:detail = &#123;&#125;",JSONObject.toJSONString(map)); mv.addObject("detail",JSONObject.toJSONString(getDetail(productId))); mv.addObject("title", "详情"); mv.setViewName("detail.ftl");&#125;//如果没有获取到productIdelse&#123; attributes.addFlashAttribute("msg", "产品不存在"); attributes.addFlashAttribute("productName", productName); attributes.addFlashAttribute("title", "有点问题！"); mv.setViewName("redirect:"/error/fail.htm");&#125;return mv;&#125; 这段代码时我前段时间做全局错误处理模块时对原有业务逻辑错误返回的一个抽象，因为要将错误统一处理，就不可能在具体的handler中直接返回到错误界面，所以就将所有的错误处理都redirect到error/fail.htm这个handler method中处理。redirect的参数问题上面已经描述过了，这里就不在细说，就是简单的例子和背景，知道怎么去使用RedirectAttributes。 RedirectAttributes这个原理也很简单，就是相当于存在了一个session中，但是这个session在用过一次之后就销毁了，即在fail.htm这个方法中获取之后如果再进行redirect，参数还会丢失，那么就在fail.htm中继续使用RedirectAttributes来存储参数再传递到下一个handler。 doDispatch方法为了偷懒，上面强行插入了对Spring中redirect参数传递问题的解释。回归到咱们的doDispatch方法。 作用：处理实际的调度到handler。handler将通过按顺序应用servlet的HandlerMappings来获得。HandlerAdapter将通过查询servlet已安装的HandlerAdapter来查找支持处理程序类的第一个HandlerAdapter。所有的HTTP方法都由这个方法处理。这取决于HandlerAdapter或处理程序自己决定哪些方法是可以接受的。 其实在doDispatch中最核心的代码就4行，我们来看下： 根据request找到我们的handler 12// Determine handler for the current request. mappedHandler = getHandler(processedRequest); 根据handler找到对应的HandlerAdapter 12// Determine handler adapter for the current request. HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); HandlerAdapter处理handler 12// Actually invoke the handler. mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); 调用processDispatchResult方法处理上述过程中得结果综合，当然也包括找到view并且渲染输出给用户1processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); 我们以上述为轴心，来看下它的整个源码(具体代码含义在代码中标注)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; //当前请求request HttpServletRequest processedRequest = request; //处理器链（handler和拦截器） HandlerExecutionChain mappedHandler = null; //用户标识multipartRequest（文件上传请求） boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; //很熟悉吧，这个就是我们返回给用户的包装视图 ModelAndView mv = null; //处理请求过程中抛出的异常。这个异常是不包括渲染过程中抛出的异常的 Exception dispatchException = null; try &#123; //检查是不是上传请求 processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // 通过当前请求确定相应的handler mappedHandler = getHandler(processedRequest); //如果没有找到：就会报异常，这个异常我们在搭建SpringMVC应用时会经常遇到： //No mapping found for HTTP request with URI XXX in //DispatcherServlet with name XXX if (mappedHandler == null || mappedHandler.getHandler() == null) &#123; noHandlerFound(processedRequest, response); return; &#125; // 根据handler找到HandlerAdapter HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); //处理GET和Head请求的Last-Modified //获取请求方法 String method = request.getMethod(); //这个方法是不是GET方法 boolean isGet = "GET".equals(method); if (isGet || "HEAD".equals(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (logger.isDebugEnabled()) &#123; logger.debug("Last-Modified value for [" + getRequestUri(request) + "] is: " + lastModified); &#125; if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; //这里就是我们SpringMVC拦截器的preHandle方法的处理 if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; // 调用具体的Handler，并且返回我们的mv对象. mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); //如果需要异步处理的话就直接返回 if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; //这个其实就是处理视图（view）为空的情况，会根据request设置默认的view applyDefaultViewName(processedRequest, mv); //这里就是我们SpringMVC拦截器的postHandle方法的处理 mappedHandler.applyPostHandle(processedRequest, response, mv); &#125; catch (Exception ex) &#123; dispatchException = ex; &#125; catch (Throwable err) &#123; // As of 4.3, we're processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException("Handler dispatch failed", err); &#125; //处理返回结果；（异常处理、页面渲染、拦截器的afterCompletion触发等） processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125; catch (Exception ex) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125; catch (Throwable err) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException("Handler processing failed", err)); &#125; finally &#123; //判断是否执行异步请求 if (asyncManager.isConcurrentHandlingStarted()) &#123; // 如果是的话，就替代拦截器的postHandle 和 afterCompletion方法执行 if (mappedHandler != null) &#123; mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); &#125; &#125; else &#123; // 删除上传请求的资源 if (multipartRequestParsed) &#123; cleanupMultipart(processedRequest); &#125; &#125; &#125;&#125; 整体来看，doDispatch做了两件事情： 处理请求 页面渲染 doDispatch处理过程流程图 那上面就是整个DispatcherServlet的一个大概内容了，关于SpringMVC容器的初始化，我们在先把DispatcherServlet中涉及到的九大组件撸完之后再回头来学习。关于九大组件目前已经有过两篇是关于HandlerMapping的了，由于我们打算对于整个SpringMVC体系结构都进行一次梳理，因此，会将九大组件从接口设计以及子类都会通过源码的方式来呈现。 大家如果有什么意见或者建议可以在下方评论区留言，也可以给我们发邮件（glmapper_2018@163.com）!欢迎小伙伴与我们一起交流，一起成长。]]></content>
      <categories>
        <category>spring mvc</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>web</tag>
        <tag>mvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码系列：AbstractHandlerMapping]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-base-webmvc2%2F</url>
    <content type="text"><![CDATA[AbstractHandlerMapping是实现HandlerMapping接口的一个抽象基类。支持排序，默认处理程序，处理程序拦截器，包括由路径模式映射的处理程序拦截器。所有的HandlerMapping都继承自AbstractHandlerMapping。另外，此基类不支持PATH_WITHIN_HANDLER_MAPPING_ATTRIBUTE的暴露，此属性的支持取决于具体的子类，通常基于请求URL映射。 前面说到，HandlerMapping的作用就是通过request查找Handler和Interceptors。具体的获取均是通过子类来实现的。 1.AbstractHandlerMapping 的类定义12public abstract class AbstractHandlerMapping extends WebApplicationObjectSupport implements HandlerMapping, Ordered &#123; AbstractHandlerMapping继承了WebApplicationObjectSupport，初始化时会自动调用模板方法initApplicationContext；AbstractHandlerMapping的创建也就是在这个方法里面完成的。同时实现了HandlerMapping和Ordered接口，这也就是上面提到的支持排序的原因。 2.AbstractHandlerMapping属性分析 排序值 order 默认值为Integer的最大值，后面注释的意思是和没有排序是一样的，因为只有理论上才可能超过Integer.MAX_VALUE。 1private int order = Integer.MAX_VALUE; // default: same as non-Ordered 默认处理器 defaultHandler 1private Object defaultHandler; Spring工具类 urlPathHelper Helper类用于URL路径匹配。提供对RequestDispatcher中URL路径的支持，包括并支持一致的URL解码。 1private UrlPathHelper urlPathHelper = new UrlPathHelper(); spring工具类 PathMatcher(AntPathMatcher) 用于基于字符串的路径匹配的策略接口。 1private PathMatcher pathMatcher = new AntPathMatcher(); 拦截器列表 interceptors 用于配置SpringMVC的拦截器，配置方式由两种： 1.注册HandlerMapping时通过属性设置 2.通过子类的extendInterceptors钩子方法进行设置（extendInterceptors方法是在initApplicationContext中调用的） interceptors并不会直接使用，二是通过initInterceptors方法按照类型分配到mappedInterceptors和adaptedInterceptors中进行使用，interceptors只用于配置。 1private final List&lt;Object&gt; interceptors = new ArrayList&lt;Object&gt;(); adaptedInterceptors 被分配到adaptedInterceptors中的类型的拦截器不需要进行匹配，在getHandler中会全部添加到返回值HandlerExecutionChain里面。他 只能从 interceptors中获取。 1private final List&lt;HandlerInterceptor&gt; adaptedInterceptors = new ArrayList&lt;HandlerInterceptor&gt;(); corsProcessor CorsProcessor作用是接受请求和CorsConfiguration并更新响应的策略。 此组件不关心如何选择CorsConfiguration，而是采取后续操作，例如应用CORS验证检查，并拒绝响应或将CORS头添加到响应中。 1private CorsProcessor corsProcessor = new DefaultCorsProcessor(); corsConfigSource 根据路径模式上映射的CorsConfiguration集合提供每个请求的CorsConfiguration实例。支持精确的路径映射URI（如“/ admin”）以及Ant样式的路径模式（如“/ admin / **”） 12private final UrlBasedCorsConfigurationSource corsConfigSource = new UrlBasedCorsConfigurationSource(); 跨域相关问题 CorsConfiguration 具体封装跨域配置信息的pojoCorsConfigurationSource request与跨域配置信息映射的容器CorsProcessor 具体进行跨域操作的类 3.AbstractHandlerMapping 中的get&amp;set方法3.1 setOrder指定此HandlerMapping bean的排序值。123public final void setOrder(int order) &#123; this.order = order;&#125; 3.2 setDefaultHandler指定此HandlerMapping bean的排序值。设置此处理程序映射的默认处理程序。如果没有找到特定的映射，这个处理程序将被返回。缺省值为null，表示没有默认处理程序。123public void setDefaultHandler(Object defaultHandler) &#123; this.defaultHandler = defaultHandler;&#125; 3.3 getDefaultHandler返回此处理程序映射的默认处理程序，如果没有，则返回null。123public Object getDefaultHandler() &#123; return this.defaultHandler;&#125; 3.4 setAlwaysUseFullPath如果URL查找始终使用当前servlet上下文中的完整路径，请进行设置。 否则，如果适用，则使用当前servlet映射中的路径（即，在web.xml中“… / *”servlet映射的情况下）。默认是“false”。setAlwaysUseFullPath中的实现具体是委托给urlPathHelper和corsConfigSource来完成的。1234public void setAlwaysUseFullPath(boolean alwaysUseFullPath) &#123; this.urlPathHelper.setAlwaysUseFullPath(alwaysUseFullPath); this.corsConfigSource.setAlwaysUseFullPath(alwaysUseFullPath);&#125; 3.5 setUrlDecode如果上下文路径和请求URI应该被URL解码，则设置。两者都是由Servlet API返回“undecoded”，与servlet路径相反。根据Servlet规范（ISO-8859-1）使用请求编码或默认编码。setUrlDecode中的实现具体是委托给urlPathHelper和corsConfigSource来完成的。1234public void setUrlDecode(boolean urlDecode) &#123; this.urlPathHelper.setUrlDecode(urlDecode); this.corsConfigSource.setUrlDecode(urlDecode);&#125; 3.6 setRemoveSemicolonContent如果“;” （分号）内容应该从请求URI中去除,则设置。默认值是true。setRemoveSemicolonContent中的实现具体是委托给urlPathHelper和corsConfigSource来完成的。1234public void setRemoveSemicolonContent(boolean removeSemicolonContent) &#123; this.urlPathHelper.setRemoveSemicolonContent(removeSemicolonContent); this.corsConfigSource.setRemoveSemicolonContent(removeSemicolonContent);&#125; 3.7 setUrlPathHelper设置UrlPathHelper以用于解析查找路径。使用它可以用自定义子类覆盖默认的UrlPathHelper，或者跨多个HandlerMappings和MethodNameResolvers共享通用的UrlPathHelper设置。12345public void setUrlPathHelper(UrlPathHelper urlPathHelper) &#123; Assert.notNull(urlPathHelper, &quot;UrlPathHelper must not be null&quot;); this.urlPathHelper = urlPathHelper; this.corsConfigSource.setUrlPathHelper(urlPathHelper);&#125; 3.8 getUrlPathHelper返回UrlPathHelper实现以用于解析查找路径。123public UrlPathHelper getUrlPathHelper() &#123; return urlPathHelper;&#125; 3.9 setPathMatcher将PathMatcher实现设置为用于匹配注册的URL模式的URL路径。 默认是AntPathMatcher。12345public void setPathMatcher(PathMatcher pathMatcher) &#123; Assert.notNull(pathMatcher, &quot;PathMatcher must not be null&quot;); this.pathMatcher = pathMatcher; this.corsConfigSource.setPathMatcher(pathMatcher);&#125; 3.10 setInterceptors设置拦截器以应用此处理程序映射映射的所有处理程序。支持的拦截器类型是HandlerInterceptor，WebRequestInterceptor和MappedInterceptor。映射拦截器仅适用于请求与其路径模式相匹配的URL。映射的拦截器Bean在初始化期间也会按类型检测到。123ublic void setInterceptors(Object... interceptors) &#123; this.interceptors.addAll(Arrays.asList(interceptors));&#125; 其他几个get&amp;set方法就不列出来了，有兴趣的小伙伴可以自行阅读... 4. AbstractHandlerMapping的创建因为AbstractHandlerMapping继承了WebApplicationObjectSupport类，因此AbstractHandlerMapping的创建就是依托于模板方法initApplicationContext来完成的。123456@Overrideprotected void initApplicationContext() throws BeansException &#123; extendInterceptors(this.interceptors); detectMappedInterceptors(this.adaptedInterceptors); initInterceptors();&#125; 从方法结构可以了解到，initApplicationContext中包括三个子处理方法。 extendInterceptors：这也是一个模板方法，在AbstractHandlerMapping中并没有具体实现（方法体是空的），主要是用于给子类提供一个添加（修改）Interceptors的入口（现有的SpringMVC实现中均未使用）。 detectMappedInterceptors：用于将SpringMVC容器及父容器中的所有MappedInterceptor类型的Bean添加到MappedInterceptors属性中。 检测MappedInterceptor类型的bean，并将它们添加到映射的拦截器列表中。 除了可能通过setInterceptors提供的任何MappedInterceptors之外，还会调用此方法，默认情况下将从当前上下文及其祖先中添加所有MappedInterceptor类型的Bean。子类可以覆盖和优化这个策略。 12345protected void detectMappedInterceptors(List&lt;HandlerInterceptor&gt; mappedInterceptors) &#123; mappedInterceptors.addAll( BeanFactoryUtils.beansOfTypeIncludingAncestors( getApplicationContext(), MappedInterceptor.class, true, false).values());&#125; initInterceptors：初始化指定的拦截器，检查MappedInterceptors并根据需要调整HandlerInterceptors和WebRequestInterceptors。（当前Spring版本时4.3.6） 1234567891011protected void initInterceptors() &#123; if (!this.interceptors.isEmpty()) &#123; for (int i = 0; i &lt; this.interceptors.size(); i++) &#123; Object interceptor = this.interceptors.get(i); if (interceptor == null) &#123; throw new IllegalArgumentException(&quot;Entry number &quot; + i + &quot; in interceptors array is null&quot;); &#125; this.adaptedInterceptors.add(adaptInterceptor(interceptor)); &#125; &#125;&#125; 这个是4.1.5版本的initInterceptors方法： 12345678910111213141516protected void initInterceptors() &#123; if (!this.interceptors.isEmpty()) &#123; for (int i = 0; i &lt; this.interceptors.size(); i++) &#123; Object interceptor = this.interceptors.get(i); if (interceptor == null) &#123; throw new IllegalArgumentException(&quot;Entry number &quot; + i + &quot; in interceptors array is null&quot;); &#125; if (interceptor instanceof MappedInterceptor) &#123; this.mappedInterceptors.add((MappedInterceptor) interceptor); &#125; else &#123; this.adaptedInterceptors.add(adaptInterceptor(interceptor)); &#125; &#125; &#125; &#125; 在4.1.5中版本中，initInterceptors的工作是将interceptors属性里面所包含的对象按照类型添加到adaptedInterceptors或者mappedInterceptors中。在4.1.5版本中mappedInterceptors是AbstractHandlerMapping的属性之一。主要原因是因为，springMVC自4.2开始添加了跨域的支持，也就是上面属性中的后两个。PS：在阅读Spring相关源码时需要关注不同版本的变更及区别，不要只关注某一个版本，另外就是个人觉得阅读源码的关注点应该在编码方式、设计模式使用、设计思想及理念，而不仅仅是知道他是如何实现的】 这里顺便说下mappedInterceptors的作用：mappedInterceptors中的拦截器在使用时需要与请求的url进行匹配，只有匹配成功后才会添加到getHandler的返回值HandlerExecytionChain里。 adaptInterceptor方法: 使给定的拦截器对象适配HandlerInterceptor接口。默认情况下，支持的拦截器类型是HandlerInterceptor和WebRequestInterceptor。每个给定的WebRequestInterceptor将被封装在WebRequestHandlerInterceptorAdapter中。可以在子类中重写。1234567891011protected HandlerInterceptor adaptInterceptor(Object interceptor) &#123; if (interceptor instanceof HandlerInterceptor) &#123; return (HandlerInterceptor) interceptor; &#125; else if (interceptor instanceof WebRequestInterceptor) &#123; return new WebRequestHandlerInterceptorAdapter((WebRequestInterceptor) interceptor); &#125; else &#123; throw new IllegalArgumentException(&quot;Interceptor type not supported: &quot; + interceptor.getClass().getName()); &#125;&#125; 5.Handler和Interceptor的获取HandlerMapping是通过getHandler方法来获取Handler和Interceptor的。因此在抽象基类AbstractHandlerMapping中提供了具体的实现。并且在AbstractHandlerMapping中，getHandler使用final关键字修饰的，也就是说，子类不能再进行对此方法进行覆盖重写了。 getHandler的作用就是查找给定请求的handler，如果找不到特定请求，则返回到默认handler。123456789101112131415161718192021222324252627282930@Overridepublic final HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123; //通过getHandlerInternal方法来获取handler Object handler = getHandlerInternal(request); //如果前一个方法没有获取到，则使用默认的handler if (handler == null) &#123; //默认的Handler就是AbstractHandlerMapping中的handler属性通过set得到的值 handler = getDefaultHandler(); &#125; //如果还是没有找到Hander，则直接返回Null if (handler == null) &#123; return null; &#125; // Bean name or resolved handler? //如果找到的handler是String类型的， if (handler instanceof String) &#123; //则以它为名到spring Mvc的容器中查找相应的Bean String handlerName = (String) handler; handler = getApplicationContext().getBean(handlerName); &#125; //先根据handler和request创建一个HandlerExecutionChain对象， HandlerExecutionChain executionChain = getHandlerExecutionChain(handler, request); if (CorsUtils.isCorsRequest(request)) &#123; CorsConfiguration globalConfig = this.corsConfigSource.getCorsConfiguration(request); CorsConfiguration handlerConfig = getCorsConfiguration(handler, request); CorsConfiguration config = (globalConfig != null ? globalConfig.combine(handlerConfig) : handlerConfig); executionChain = getCorsHandlerExecutionChain(request, executionChain, config); &#125; return executionChain;&#125; getHandlerInternal： 1protected abstract Object getHandlerInternal(HttpServletRequest request) throws Exception; 查找给定请求的handler，如果找不到特定请求，则返回null。 这个方法被getHandler调用; 如果设置了null返回值，将导致默认handler。在CORS pre-flight请求上，这个方法应该返回一个不匹配飞行前请求的匹配项，而是根据URL路径，“Access-Control-Request-Method”头中的HTTP方法和头文件 从“Access-Control-Request-Headers”头部获得，从而允许CORS配置通过getCorsConfigurations获得，注意：这个方法也可以返回一个预先构建的HandlerExecutionChain，将一个处理程序对象与动态确定的拦截器组合在一起。状态指定的拦截器将被合并到这个现有的链中。 getHandlerExecutionChain： getLookupPathForRequest:返回给定请求的映射查找路径，如果适用的话，在当前的servlet映射中，或者在web应用程序中返回。如果在RequestDispatcher中调用include请求，则检测包含请求URL。1234567891011121314151617181920212223protected HandlerExecutionChain getHandlerExecutionChain(Object handler, HttpServletRequest request) &#123; //如果handler是HandlerExecutionChain类型则直接强转为HandlerExecutionChain类型， //如果不是则根据handler创建一个新的HandlerExecutionChain实例对象 HandlerExecutionChain chain = (handler instanceof HandlerExecutionChain ? (HandlerExecutionChain) handler : new HandlerExecutionChain(handler)); //返回给定请求的映射查找路径 String lookupPath = this.urlPathHelper.getLookupPathForRequest(request); //遍历当前adaptedInterceptors链表 for (HandlerInterceptor interceptor : this.adaptedInterceptors) &#123; //如果是MappedInterceptor类型则 if (interceptor instanceof MappedInterceptor) &#123; MappedInterceptor mappedInterceptor = (MappedInterceptor) interceptor; //拦截器是否应用于给定的请求路径，如果是则返回true if (mappedInterceptor.matches(lookupPath, this.pathMatcher)) &#123; chain.addInterceptor(mappedInterceptor.getInterceptor()); &#125; &#125; else &#123; chain.addInterceptor(interceptor); &#125; &#125; return chain;&#125; 为给定的handler构建一个HandlerExecutionChain，包括可用的拦截器。默认实现用给定的handler，handler映射的通用拦截器以及与当前请求URL相匹配的任何MappedInterceptors构建标准的HandlerExecutionChain。拦截器按照他们注册的顺序添加。为了扩展/重新排列拦截器列表，子类可以覆盖它。 需要注意的是，传入的handler对象可能是原始handler或预构建的HandlerExecutionChain。这个方法应该明确地处理这两种情况，建立一个新的HandlerExecutionChain或者扩展现有的链。为了简单地在自定义子类中添加拦截器，可以考虑调用super.getHandlerExecutionChain（handler，request）并在返回的链对象上调用HandlerExecutionChain＃addInterceptor。 getCorsHandlerExecutionChain： 1234567891011121314protected HandlerExecutionChain getCorsHandlerExecutionChain(HttpServletRequest request, HandlerExecutionChain chain, CorsConfiguration config) &#123; //通过请求头的http方法是否options判断是否预请求， if (CorsUtils.isPreFlightRequest(request)) &#123; HandlerInterceptor[] interceptors = chain.getInterceptors(); //如果是使用PreFlightRequest替换处理器 chain = new HandlerExecutionChain(new PreFlightHandler(config), interceptors); &#125; else &#123; //如果是普通请求，添加一个拦截器CorsInterceptor。 chain.addInterceptor(new CorsInterceptor(config)); &#125; return chain;&#125; 更新HandlerExecutionChain进行与CORS（HTTP访问控制：跨域资源共享）相关的处理。 对于pre-flight请求，默认实现用一个简单的HttpRequestHandler来替换选择的handler，该HttpRequestHandler调用已配置的setCorsProcessor。（将处理器替换为内部类PreFlightHandler） 对于普通的请求，默认实现插入一个HandlerInterceptor，它执行与CORS有关的检查并添加CORS头。（添加CorsInterceptor拦截器） AbstractHandlerMapping中的两个内部类这两个内部类就是用来校验request是否cors，并封装对应的Adapter的。 PreFlightRequest是CorsProcessor对于HttpRequestHandler的一个适配器。这样HandlerAdapter直接使用HttpRequestHandlerAdapter处理。 CorsInterceptor 是CorsProcessor对于HandlerInterceptorAdapter的适配器。 具体的类信息如下： PreFlightHandler123456789101112131415161718private class PreFlightHandler implements HttpRequestHandler, CorsConfigurationSource &#123; private final CorsConfiguration config; public PreFlightHandler(CorsConfiguration config) &#123; this.config = config; &#125; @Override public void handleRequest(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; corsProcessor.processRequest(this.config, request, response); &#125; @Override public CorsConfiguration getCorsConfiguration(HttpServletRequest request) &#123; return this.config; &#125;&#125; CorsInterceptor1234567891011121314151617181920private class CorsInterceptor extends HandlerInterceptorAdapter implements CorsConfigurationSource &#123; private final CorsConfiguration config; public CorsInterceptor(CorsConfiguration config) &#123; this.config = config; &#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; return corsProcessor.processRequest(this.config, request, response); &#125; @Override public CorsConfiguration getCorsConfiguration(HttpServletRequest request) &#123; return this.config; &#125;&#125; 至此AbstractHandlerMapping中的一些源码就结束了，AbstractHandlerMapping为HandlerMapping的功能提供的一些具体的模板描述，但是具体的细节实现还需要从其子类中来慢慢分析。关于这部分中涉及到的如HandlerExecutionChain，cors跨域等问题，后面会根据实际情况另开篇幅来学习。 大家如果有什么意见或者建议可以在下方评论区留言，也可以给我们发邮件（glmapper_2018@163.com）!欢迎小伙伴与我们一起交流，一起成长。]]></content>
      <categories>
        <category>spring mvc</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>web</tag>
        <tag>mvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码系列：HandlerMapping]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-base-webmvc1%2F</url>
    <content type="text"><![CDATA[HandlerMapping接口是用来查找Handler的。在SpringMvc中，DispatcherServlet处理分发很多请求，而每个请求都需要一个Handler来处理，具体接受到一个请求后使用哪个Handler来处理呢？这就是Handler要做的事情。因此，HandlerMapping的作用就是根据request找到相应的处理器Handler和Interceptors。 下面是Spring中对HandlerMapping接口的说明： This class can be implemented by application developers, although this is not necessary, as BeanNameUrlHandlerMapping and DefaultAnnotationHandlerMapping are included in the framework. The former is the default if no HandlerMapping bean is registered in the application context.这个类可以由应用程序开发人员实现，尽管这不是必须的，因为BeanNameUrlHandlerMapping和DefaultAnnotationHandlerMapping已经包含在框架中，作为HandlerMapping的默认实现。 如果在应用程序上下文中没有注册HandlerMapping bean，BeanNameUrlHandlerMapping是默认值。 HandlerMapping implementations can support mapped interceptors but do not have to. A handler will always be wrapped in a HandlerExecutionChain instance, optionally accompanied by some HandlerInterceptor instances.The DispatcherServlet will first call each HandlerInterceptor&#39;s preHandle method in the given order, finally invoking the handler itself if all preHandle methods have returned trueHandlerMapping实现可以支持映射的拦截器，但不必如此；handler将始终被封装在HandlerExecutionChain实例中，并可由一些HandlerInterceptor实例执行。在给定的顺序中，DispatcherServlet将首先调用每个HandlerInterceptor的preHandle方法，如果所有的preHandle方法都返回true，那么最后调用handler本身。 The ability to parameterize this mapping is a powerful and unusual capability of this MVC framework. For example, it is possible to write a custom mapping based on session state, cookie state or many other variables. No other MVC framework seems to be equally flexible.参数化这个映射的能力是这个MVC框架的一个强大且不同寻常的能力。 例如，可以根据会话状态，cookie状态或许多其他变量编写自定义映射。 没有其他MVC框架似乎同样灵活。 Note: Implementations can implement the Ordered interface to be able to specify a sorting order and thus a priority for getting applied by DispatcherServlet. Non-Ordered instances get treated as lowest priority.注：实现可以实现Ordered接口，以便能够指定排序顺序，从而指定由DispatcherServlet应用的优先级。 无序实例被视为最低优先级。 1.接口常量1.1、PATH_WITHIN_HANDLER_MAPPING_ATTRIBUTEHttpServletRequest属性的名称，它包含处理程序映射中的路径，比如模式匹配，或者完全相关的URI(通常在DispatcherServlet的映射中)。此属性不需要所有HandlerMapping实现支持。基于url的HandlerMappings通常会支持它，但是处理程序不应该期望这个请求属性在所有场景中都存在。12345678910/** * Name of the &#123;@link HttpServletRequest&#125; attribute that contains the path * within the handler mapping, in case of a pattern match, or the full * relevant URI (typically within the DispatcherServlet&apos;s mapping) else. * &lt;p&gt;Note: This attribute is not required to be supported by all * HandlerMapping implementations. URL-based HandlerMappings will * typically support it, but handlers should not necessarily expect * this request attribute to be present in all scenarios. */String PATH_WITHIN_HANDLER_MAPPING_ATTRIBUTE = HandlerMapping.class.getName() + &quot;.pathWithinHandlerMapping&quot;; 1.2、BEST_MATCHING_PATTERN_ATTRIBUTEHttpServletRequest属性的名称，包括处理程序映射中的最佳匹配模式123456789/** * Name of the &#123;@link HttpServletRequest&#125; attribute that contains the * best matching pattern within the handler mapping. * &lt;p&gt;Note: This attribute is not required to be supported by all * HandlerMapping implementations. URL-based HandlerMappings will * typically support it, but handlers should not necessarily expect * this request attribute to be present in all scenarios. */String BEST_MATCHING_PATTERN_ATTRIBUTE = HandlerMapping.class.getName() + &quot;.bestMatchingPattern&quot;; 1.3、INTROSPECT_TYPE_LEVEL_MAPPINGHttpServletRequest属性的名称，指示是否应该检查类型级别的映射。 1234567/** * Name of the boolean &#123;@link HttpServletRequest&#125; attribute that indicates * whether type-level mappings should be inspected. * &lt;p&gt;Note: This attribute is not required to be supported by all * HandlerMapping implementations. */String INTROSPECT_TYPE_LEVEL_MAPPING = HandlerMapping.class.getName() + &quot;.introspectTypeLevelMapping&quot;; 1.4、URI_TEMPLATE_VARIABLES_ATTRIBUTE包含URI模板映射的HttpServletRequest属性的名称，将变量名称映射到值。123456789/** * Name of the &#123;@link HttpServletRequest&#125; attribute that contains the URI * templates map, mapping variable names to values. * &lt;p&gt;Note: This attribute is not required to be supported by all * HandlerMapping implementations. URL-based HandlerMappings will * typically support it, but handlers should not necessarily expect * this request attribute to be present in all scenarios. */String URI_TEMPLATE_VARIABLES_ATTRIBUTE = HandlerMapping.class.getName() + &quot;.uriTemplateVariables&quot;; 1.5、MATRIX_VARIABLES_ATTRIBUTE包含带有URI矩阵变量的映射的HttpServletRequest属性的名称。此属性不需要所有HandlerMapping实现支持，也可能不存在，这取决于HandlerMapping是否被配置为在请求URI中保留矩阵变量内容。 123456789/** * Name of the &#123;@link HttpServletRequest&#125; attribute that contains a map with * URI matrix variables. * &lt;p&gt;Note: This attribute is not required to be supported by all * HandlerMapping implementations and may also not be present depending on * whether the HandlerMapping is configured to keep matrix variable content * in the request URI. */String MATRIX_VARIABLES_ATTRIBUTE = HandlerMapping.class.getName() + &quot;.matrixVariables&quot;; 1.6、PRODUCIBLE_MEDIA_TYPES_ATTRIBUTEHttpServletRequest属性的名称，该属性包含可用于映射处理程序的可生成的MediaTypes集合。 12345678/** * Name of the &#123;@link HttpServletRequest&#125; attribute that contains the set of * producible MediaTypes applicable to the mapped handler. * &lt;p&gt;Note: This attribute is not required to be supported by all * HandlerMapping implementations. Handlers should not necessarily expect * this request attribute to be present in all scenarios. */String PRODUCIBLE_MEDIA_TYPES_ATTRIBUTE = HandlerMapping.class.getName() + &quot;.producibleMediaTypes&quot;; 2.核心方法HandlerMapping接口中只有一个方法，如下：1HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception; 从方法定义可以看出，getHandler方法就是通过request来获取一个HandlerExecutionChain；该方法在不同的子类中都有实现，具体的实现后面说子类的时候在详细分析。 3.HandlerMapping的子类图中黄色部分表示已经过时的类，时间开发中不建议再使用。 在HandlerMapping的体系中可以看出，HandlerMapping下属子类可分为两个分支； AbstractHandlerMethodMapping AbstractUrlHandlerMapping 上述两个抽象类又均是AbstractHandlerMapping的子类。关于AbstractHandlerMapping我们下篇文章来学习。 大家如果有什么意见或者建议可以在下方评论区留言，也可以给我们发邮件（glmapper_2018@163.com）!欢迎小伙伴与我们一起交流，一起成长。]]></content>
      <categories>
        <category>spring mvc</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>web</tag>
        <tag>mvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[菜鸟成长系列-Builder 建造者模式]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-design-model-builder%2F</url>
    <content type="text"><![CDATA[菜鸟成长系列-面向对象的四大基础特性菜鸟成长系列-多态、接口和抽象类菜鸟成长系列-面向对象的6种设计原则菜鸟成长系列-单例模式菜鸟成长系列-工厂模式 建造者模式也是创建型中的一种，用于负责创建对象。建造者模式可以将一个产品的内部表象与产品的生成过程分割开来，从而可以使一个建造过程生成具有不同的内部表象的产品对象。 什么是产品的内部表象？一个产品常有不用的组成成分作为产品的零件，这些零件有可能是对象，也有可能不是对象，他们通常又叫做产品的内部表象。不同的产品可以有不同的内部表象，也就是可以有不同的零件。使用建造者模式可以使客户端不需要知道所生成的产品对象有哪些零件，每个产品的对应零件彼此有何不同，是怎么建造出来的，以及怎样组成产品。 工厂模式与建造者模式上一篇我们讨论了工厂模式，我们知道工厂模式一般都是创建一个产品，注重的是把这个产品创建出来就行，只要创建出来，不关心这个产品的组成部分。从代码上看，工厂模式就是一个方法，用这个方法就能生产出产品。那么对于建造者模式呢？建造者模式也是创建一个产品，但是不仅要把这个产品创建出来，还要关系这个产品的组成细节， 组成过程。从代码上看（下面给出），建造者模式在建造产品时，这个产品有很多方法，建造者模式会根据这些相同方法但是不同执行顺序建造出不同组成细节的产品。 建造者模式的结构结构组件角色说明： 抽象建造者（Builder）:抽象类， 规范产品的组建，一般是由子类实现具体的组件过程 具体建造者（ConcreteBuilder ）：具体的构建器 导演者（Director） : 统一组装过程(可省略) 产品（Product）:产品的抽象类 Director角色是与客户端打交道的角色。Director将客户端创建产品的请求划分为对各个零件的建造请求，再将这些请求委派给具体的ConcreteBuilder角色。ConcreteBuilder是做具体建造工作的，但是对客户端是透明的。 一般来说，每有一个产品类，就有一个相应的具体建造者类。这些产品应当有一样数目的零件，而每有一个零件就相应的在所有的建造者角色里有一个建造方法。 建造一封邮件通过代码来看下各个角色的职责： Director 123456789101112131415161718192021222324package com.glmapper.model.builder;/** * * 导演者类 * @author glmapper * @time 2017年12月30日上午11:33:26 * @version glmapper_v1.0 * */public class Director &#123; private Builder builder; /** * 产品构造方法，负责调用各个零件建造方法 */ public EmailProduct construct()&#123; builder = new ConcreteBuilder(); builder.buildFromAddress(); builder.buildToAddress(); builder.buildSubject(); builder.buildContent(); builder.buildSupplement(); return builder.returnEmailProduct(); &#125;&#125; Builder 1234567891011121314151617181920212223242526272829303132333435package com.glmapper.model.builder;/** * 抽象建造者 提供不同的构建组件方法 * @author glmapper * @time 2017年12月30日上午11:31:34 * @version glmapper_v1.0 * */public interface Builder &#123; /** * 构建发件人信息 */ public void buildFromAddress(); /** * 构建收件人信息 */ public void buildToAddress(); /** * 构建邮件内容 */ public void buildContent(); /** * 构建邮件附件 */ public void buildSupplement(); /** * 构建邮件主题 */ public void buildSubject(); /** * 返回构建的产品 */ public EmailProduct returnEmailProduct(); &#125; ConcreteBuilder 1234567891011121314151617181920212223242526272829303132333435363738package com.glmapper.model.builder;/** * * 具体产品的建造器 * @author glmapper * @time 2017年12月30日上午11:31:11 * @version glmapper_v1.0 * */public class ConcreteBuilder implements Builder&#123; private EmailProduct emailProduct = new EmailProduct(); public void buildFromAddress() &#123; emailProduct.setFromAddress(&quot;00001111@glmapper.com&quot;); &#125; public void buildToAddress() &#123; emailProduct.setToAddress(&quot;00001112@glmapper.com&quot;); &#125; public void buildContent() &#123; emailProduct.setContent(&quot;我写了一个建造者模式的例子，希望大佬给点意见&quot;); &#125; public void buildSupplement() &#123; emailProduct.setSupplement(&quot;附件：BuilderDemo.rar&quot;); &#125; public void buildSubject() &#123; emailProduct.setSubject(&quot;给大佬的一封建造者模式的Demo&quot;); &#125; public EmailProduct returnEmailProduct() &#123; System.out.println(emailProduct.toString()); return emailProduct; &#125;&#125; Product 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.glmapper.model.builder;/** * 产品类（产品类中包括不同组件：此处使用字段方式模式组件） * @author glmapper * @time 2017年12月30日上午11:30:13 * @version glmapper_v1.0 * */public class EmailProduct &#123; /** * 发件地址 */ private String fromAddress; /** * 收件地址 */ private String toAddress; /** * 邮件主题 */ private String subject; /** * 邮件内容 */ private String content; /** * 邮件附件 */ private String supplement; public String getFromAddress() &#123; return fromAddress; &#125; public void setFromAddress(String fromAddress) &#123; this.fromAddress = fromAddress; &#125; public String getToAddress() &#123; return toAddress; &#125; public void setToAddress(String toAddress) &#123; this.toAddress = toAddress; &#125; public String getSubject() &#123; return subject; &#125; public void setSubject(String subject) &#123; this.subject = subject; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125; public String getSupplement() &#123; return supplement; &#125; public void setSupplement(String supplement) &#123; this.supplement = supplement; &#125; @Override public String toString() &#123; return &quot;EmailProduct [fromAddress=&quot; + fromAddress + &quot;, toAddress=&quot; + toAddress + &quot;, subject=&quot; + subject + &quot;, content=&quot; + content + &quot;, supplement=&quot; + supplement + &quot;]&quot;; &#125;&#125; 客户端 12345678910111213package com.glmapper.model.builder;/** * 客户端 * @author glmapper * @time 2017年12月30日上午11:45:53 * @version glmapper_v1.0 * */public class MainTest &#123; public static void main(String[] args) &#123; new Director().construct(); &#125;&#125; 结果 1234567891011121314建造发件人信息组件...建造收件人信息组件...建造邮件主题信息组件...建造邮件内容信息组件...建造邮件附件信息组件...（为了方便看，这里把结果的显示做了调整）EmailProduct ：[ fromAddress=00001111@glmapper.com, toAddress=00001112@glmapper.com, subject=给大佬的一封建造者模式的Demo, content=我写了一个建造者模式的例子，希望大佬给点意见, supplement=附件：BuilderDemo.rar] 建造者模式的关注点有些情况下，一个对象会有一些重要的性质，在他们没有恰当的值之前，对象不能作为一个完整的产品来使用。就如上面发送一个电子邮件所示，电子邮件有发件人地址、收件人地址、主题、内容、附录等部分，而在收件人地址没有赋值之前，这个电子邮件是不能发送的。在某些情况下，一个对象的一些性质必须按照某个顺序赋值才有意义，在某个性质没有赋值之前，另一个性质则无法赋值。这些情况使得性质本身的建造涉及到复杂的业务逻辑。 而此时，对象相当于一个有待建造的产品，而对象的这些性质相当于产品的零件，建造产品的过程是建造零件的过程。由于建造零件的过程很复杂，因此，这些零件的健在过程往往会被“外部化”到另一个称作为建造者的对象里，建造者对象返回给客户端的是一个全部零件都建造完毕的产品对象。 在实际的应用过程中，建造者模式也有不同的变种，比如说省略抽象建造者角色或者省略导演者角色等等，在某些情况下，建造者模式可以通过省略某些角色来达到过度到模板方法模式。 OK，关于建造者模式的其他变种这里就不讨论了，留一个想象空间！（这段字代表一个微笑的表情(*￣︶￣)） java中建造者模式：JavaMailJavaMail是一组J2SE的扩展API的一个类库，我们可以使用这个API来开发一个功能完备的电子邮件客户端软件。在JavaMail中就主要使用了建造者模式，当然还有我们上一篇中说道的抽象工厂模式。 建造者模式在JavaMail中的使用1234567891011121314151617181920212223242526272829303132333435363738394041package com.glmapper.model.builder;import java.util.Properties;import javax.mail.Message;import javax.mail.Session;import javax.mail.Transport;import javax.mail.internet.InternetAddress;import javax.mail.internet.MimeMessage;/** * 邮件发送-建造者模式 * @author glmapper * @time 2017年12月30日下午2:04:46 * @version glmapper_v1.0 * */public class MailSender &#123; private static MimeMessage message; public static void main(String[] args) &#123; //基本属性 String smptHost = &quot;smpt.xxx.com&quot;; String fromAddress = &quot;00001111@glmapper.com&quot;; String toAddress = &quot;00001112@glmapper.com&quot;; Properties p= new Properties(); p.put(&quot;mail.smtp.host&quot;, smptHost); Session session = Session.getDefaultInstance(p); try &#123; InternetAddress to = new InternetAddress(toAddress); InternetAddress from = new InternetAddress(fromAddress); //创建message对象 message = new MimeMessage(session); //下面就是组装零件的过程 message.setFrom(from); message.setRecipient(Message.RecipientType.TO, to); message.setSubject(&quot;hello builder&quot;); message.setText(&quot;我写了一个建造者模式的例子，希望大佬给点意见&quot;); Transport.send(message); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; OK,建造者模式就到这里了!]]></content>
      <categories>
        <category>架构之路</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列-容器刷新]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-context-refresh%2F</url>
    <content type="text"><![CDATA[Spring对于程序员说来说都不陌生；作为一个强大的开源技术，帮助我们能够更好的进行项目的开发与维护。 上次在Spring的启动过程文章中对Spring的启动过程做了一个较为详细的说明和分析。那么在实际的过程中，Spring的启动实际上就是Spring容器的初始化过程。本文将从源码的角度结合自己断点执行过程中保留的现场来分析一下容器的刷新过程（主要分析前几个方法，后面几个会分开来说）。 Spring的启动是通过ContextLoaderListener来进行的，在ContextLoaderListener中通过委托父类ContextLoader的initWebApplicationContext来完成具体的初始化过程。具体的启动过程可以看下之前的那篇文章。 在initWebApplicationContext方法是用来创建容器的，核心代码如下： 今天主要来看configureAndRefreshWebApplicationContext方法中最后的wac.refresh()到底发生了哪些事; 1、obtainFreshBeanFactory：BeanFactory的刷新和创建refresh()方法主要为IoC容器Bean的生命周期管理提供条件，Spring IoC容器载入Bean定义资源文件从其子类容器的refreshBeanFactory()方法启动，所以整个refresh()中“ConfigurableListableBeanFactory beanFactory =obtainFreshBeanFactory();”这句以后代码的都是注册容器的信息源和生命周期事件，载入过程就是从这句代码启动。AbstractApplicationContext的obtainFreshBeanFactory()方法调用子类容器的refreshBeanFactory()方法，启动容器载入Bean定义资源文件的过程。refresh()方法的作用是：在创建IoC容器前，如果已经有容器存在，则需要把已有的容器销毁和关闭，以保证在refresh之后使用的是新建立起来的IoC容器。refresh的作用类似于对IoC容器的重启，在新建立好的容器中对容器进行初始化，对Bean定义资源进行载入。和refreshBeanFactory方法类似，载入Bean定义的方法loadBeanDefinitions也使用了委派模式，在AbstractRefreshableApplicationContext类中只定义了抽象方法，具体的实现调用子类容器中的方法实现。 //通知子类去刷新内部bean 工厂 再来看refreshBeanFactory 此实现执行该上下文的底层bean工厂的实际刷新，关闭以前的bean工厂（如果有的话），并为上下文生命周期的下一阶段初始化一个新的bean工厂。 customizeBeanFactory(DefaultListableBeanFactory beanFactory) 1234567891011121314151617181920212223/** //通过当前上下文来自定义内部bean工厂&lt;br&gt; * Customize the internal bean factory used by this context. * Called for each &#123;@link #refresh()&#125; attempt. * &lt;p&gt;The default implementation applies this context&apos;s * &#123;@linkplain #setAllowBeanDefinitionOverriding &quot;allowBeanDefinitionOverriding&quot;&#125; * and &#123;@linkplain #setAllowCircularReferences &quot;allowCircularReferences&quot;&#125; settings, * if specified. Can be overridden in subclasses to customize any of * &#123;@link DefaultListableBeanFactory&#125;&apos;s settings. * @param beanFactory the newly created bean factory for this context * @see DefaultListableBeanFactory#setAllowBeanDefinitionOverriding * @see DefaultListableBeanFactory#setAllowCircularReferences * @see DefaultListableBeanFactory#setAllowRawInjectionDespiteWrapping * @see DefaultListableBeanFactory#setAllowEagerClassLoading */ protected void customizeBeanFactory(DefaultListableBeanFactory beanFactory) &#123; if (this.allowBeanDefinitionOverriding != null) &#123; beanFactory.setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); &#125; if (this.allowCircularReferences != null) &#123; beanFactory.setAllowCircularReferences(this.allowCircularReferences); &#125; &#125; XmlWebApplicationContext类中loadBeanDefinitions（beanFactory）12345678910111213141516@Override protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // Create a new XmlBeanDefinitionReader for the given BeanFactory. XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // Configure the bean definition reader with this context&apos;s // resource loading environment. beanDefinitionReader.setEnvironment(getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // Allow a subclass to provide custom initialization of the reader, // then proceed with actually loading the bean definitions. initBeanDefinitionReader(beanDefinitionReader); loadBeanDefinitions(beanDefinitionReader); &#125; AbstractApplicationContext调用loadBeanDefinitions(DefaultListableBeanFactory beanFactory) ，此方法根据首先创建XmlBeanDefinitionReader对象，然后配置该对象的上下文和资源加载环境，同时调用子类实现的initBeanDefinitionReader对XmlBeanDefinitionReader进行个性化配置，最近后入到initBeanDefinitionReader(beanDefinitionReader)的调用： 据给定的BeanFactory创建XmlBeanDefinitionReader 对象 配置beanDefinitionReader的上下文和资源加载环境 用子类实现的initBeanDefinitionReader对XmlBeanDefinitionReader进行个性化配置initBeanDefinitionReader(beanDefinitionReader); 调用载入Bean定义的方法，在当前类中只定义了抽象的loadBeanDefinitions方法，具体的实现调用子类容器 装载bean定义通过XmlBeanDefinitionReader。 123456789101112// Create a new XmlBeanDefinitionReader for the given BeanFactory. 通过给定的bean工厂创建一个新的XmlBeanDefinitionReader1.XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory);2.使用上下文的资源加载环境配置bean定义读取器。 beanDefinitionReader.setEnvironment(getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this));3.允许子类提供reader的自定义初始化，然后继续实际加载bean定义。 //通过制定的XmlBeanDefinitionReader来载入beandefinitionReader initBeanDefinitionReader(beanDefinitionReader) // 通过制定的XmlBeanDefinitionReader来载入bean definitions loadBeanDefinitions(beanDefinitionReader) AbstractApplicationContext调用loadBeanDefinitions(beanDefinitionReader)，这个方法是取得资源或资源路径然后通过传入的reader去加载BeanDefinitions。 2、loadBeanDefinitions目前使用Spring的配置都是基于XML的，因此使用XmlBeanDefinitionReader 中的loadBeanDefinitions方法。 看doLoadBeanDefinitions,这个就是具体的读取文件配置，然后注册成Bean 3、prepareBeanFactory配置工厂的标准上下文特性，如上下文的类装载器和后处理器。 告诉内部bean工厂使用上下文的类装入器等。 上下文回调配置bean工厂。 BeanFactory接口未登记为普通工厂的解析式。MessageSource登记（为自动装配创建）作为一个Bean 如果创建；就去寻找LoadTimeWeaver，然后准备组织 注册默认环境bean。 通过断点来看下当前的beanFactory 继续执行… beanDefinitionMap manualSingletonNames 4、postProcessBeanFactory注册web特性的全局域 1).registerWebApplicationScopes注册具有web特性的域；包括：”request”, “session”, “globalSession”, “application” 看下存储结构：registerScope方法2).registerEnvironmentBeans 注册web特性 环境bean（“contextparameters”、“ContextAttribute”）与给定的WebApplicationContext使用BeanFactory。 1.servletContext2.servletConfig3.registerSingleton 这里是找到了我们默认的配置文件参数：beanName=contextParameters 最后是将contextAttributes放入；contextAttributes中包含的属性值比较多，具体如下面所示： 主要包括：javax.servlet.context.tempdir,org.apache.catalina.resources, org.springframework.web.context.support.ServletContextScope, org.apache.tomcat.util.scan.MergedWebXml,org.apache.tomcat.InstanceManager,org.apache.catalina.jsp_classpath,javax.websocket.server.ServerContainer,org.apache.tomcat.JarScanner 这里是把需要的东西全部载入进来了，有很多。就不贴了(mime-mapping)…. 5、invokeBeanFactoryPostProcessorsBeanDefinitionRegistryPostProcessor实例化：标准BeanFactoryPostProcessor的扩展，BeanFactoryPostProcessor的作用是用来进一步定义注册的BeanDefinition，IoC容器本质就是Bean管理，所以BeanFactoryPostProcessor本身也是Bean，要对BeanFactoryPostProcessor的BeanDefinition进一步定义就通过BeanDefinitionRegistryPostProcessor进行注册，BeanDefinitionRegistryPostProcessor及其子类是Ioc容器最实例化的一类Bean。它们在ConfigurableApplicationContext（ApplicationContext子接口）实现类调用refresh()方法调用invokeBeanFactoryPostProcessors(beanFactory);方法时就被实例化。 OK，今天关于这部分的分析就到此结束了，后面的过程会在下一篇Spring系列文章中继续来讲refresh中的过程。 如果您对系列文章有任何意见，可以给我留言，感谢大家。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[菜鸟成长系列-工厂模式]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-design-model-factory%2F</url>
    <content type="text"><![CDATA[菜鸟成长系列-面向对象的四大基础特性菜鸟成长系列-多态、接口和抽象类菜鸟成长系列-面向对象的6种设计原则菜鸟成长系列-单例模式 上一篇我们已经对创建型模式中的单例模式进行了学习，今天来学习另外一个比较重要并且经常使用的模式-工厂模式；工厂模式专门负责将大量有共同接口的类实例化。其可以动态的决定将哪一个类实例化，不必事先知道每次要实例化哪一个类。 工厂模式具有以下三种形态： 简单工厂模式：又称静态工厂模式 工厂方法模式：又称多态性工厂模式或者虚拟构造子模式 抽象工厂模式：又称工具箱模式 本篇文章将对这三种形态的工厂模式进行一些基本的学习，并通过例子来直观的感受下不同形态的具体实现方式。最后再分析下JAVA以及Spring中是如何使用的。 1、简单工厂模式从上图可以看出，简单工厂模式涉及到工厂角色、抽象产品角色以及具体产品角色等三个角色。各角色职能如下： 工厂类：担任这个角色的是工厂方法模式的核心，含有与应用紧密相关的具体业务逻辑。工厂类在客户端的直接调用下创建产品对象，它往往由一个具体的java类实现 抽象产品：担任这个角色的类是工厂方法模式所创建的对象的父类，或它们共同拥有的接口。抽象产品角色可以用一个java接口或者抽象类来实现 具体产品：工厂方法模式所创建的任何对象都是这个角色的实例，具体产品角色由一个java类实现 来看例子，考虑到今天有小伙伴来我这做客，本demo将以做菜来实现一波。首先工厂就是厨房，抽象类就是笼统的菜，实现类就是具体哪个菜。 抽象产品 1234567891011121314package com.glmapper.design.factory;/** * 抽象类角色：food接口，约束类型 * @author glmapper * @date 2017年12月24日上午10:38:36 * */public interface IFood &#123; /** * 提供一个展示食物细节的方法 * @param foodName 食物名称 */ public void showFood();&#125; 具体产品-鱼 1234567891011121314package com.glmapper.design.factory;/** * 具体产品-食物鱼 * @author glmapper * @date 2017年12月24日上午10:51:29 * */public class FishFood implements IFood&#123; @Override public void showFood() &#123; System.out.println(&quot;一盘鱼&quot;); &#125;&#125; 具体产品-土豆丝 123456789101112131415package com.glmapper.design.factory;/** * * 具体食物：土豆丝 * @author glmapper * @date 2017年12月24日上午10:47:17 * */public class ShreddedPotatoesFood implements IFood&#123; @Override public void showFood() &#123; System.out.println(&quot;一盘土豆丝&quot;); &#125;&#125; 工厂角色 - 食物工厂 12345678910111213141516171819202122232425package com.glmapper.design.factory;/** * 工厂角色-食物工厂 * * @author glmapper * @date 2017年12月24日上午10:41:10 * */public class SimpleFoodFactory &#123; /** * 提供一个静态方法，用于获取食物 * @param foodType 食物类型 * @return 具体食物 */ public static IFood getFood(String foodType)&#123; IFood food = null; if (foodType.equals(&quot;fish&quot;)) &#123; food = new FishFood(); &#125; if (foodType.equals(&quot;potatoes&quot;)) &#123; food = new ShreddedPotatoesFood(); &#125; return food; &#125;&#125; 客户端 12345678910111213141516package com.glmapper.design.factory;/** * 客户端 * @author glmapper * @date 2017年12月24日上午10:45:17 * */public class MainTest &#123; public static void main(String[] args) &#123; IFood fishfood = SimpleFoodFactory.getFood(&quot;fish&quot;); fishfood.showFood(); IFood potatoesfood = SimpleFoodFactory.getFood(&quot;potatoes&quot;); potatoesfood.showFood(); &#125;&#125; 结果 12一盘鱼一盘土豆丝 OK，菜做完了，可以吃了。。。 我们来讨论下简单工厂模式的优缺点： 优点：模式的核心是工厂类，这个类含有必要的判断逻辑，可以决定在什么时候创建哪一个产品类的实例。而客户端则可以免除直接创建产品对象的责任，而仅仅负责消费产品即可。用一句话来说就是：简单工厂模式这个做法实现了对责任的分割。缺点：集中了所有产品的创建逻辑，形成了一个无所不能的全职类，但是之前我们在讨论设计原则的时候说过，我们要尽量避免这种情况的发生，这种就很明显破坏了单一职责这条原则，另外也不满足开闭原则的约束。当我们需要进行品类扩展时，我们需要不断的去修改我们的工厂的业务逻辑，一方面是工厂类会急速的膨胀，另一方面因为囊括了不同的产品对于我们后期的维护造成一定的影响。 2、工厂方法模式这个时候一个同事说他是南方人，另外一个同事说他是北方人，吃不惯今天的菜。 好吧，既然这样，那我就只能点外卖了。。。但是为了防止他们变卦自己的家乡，我需要做一个计划，下面就是计划图： 从上图中我们可以看出，工厂方法模式的角色包括以下几种： 抽象工厂：是工厂方法模式的核心，与应用程序无关。任何在模式中创建的对象的工厂类必须实现这个接口。 具体工厂：这是实现抽象工厂接口的具体工厂类，包含与应用程序密切相关的逻辑，并且受到应用程序调用以创建产品对象。 抽象产品：工厂方法模式所创建的对象的超类型，也就是产品对象的共同父类或共同拥有的接口 具体产品：这个角色实现了抽象产品角色所定义的接口。某具体产品有专门的具体工厂创建，它们之间往往一一对应。 因为我的同事都是来自不同地方的，他们的口味也都不一样，但是呢同事都是第一次来我家吃饭，所以为了招待周全，根据同事不同的口味叫不同口味的鱼。 抽象工厂角色：获取食物 1234567891011package com.glmapper.design.factory;/** * * 角色1：抽象工厂 - 负责获取食物 * @author glmapper * @date 2017年12月24日下午1:59:28 */public interface MethodFoodFactory &#123; //获取食物的方法 public IFishFood getFood();&#125; 具体工厂1：获取南方食物-鱼 123456789101112package com.glmapper.design.factory;/** * 南方口味外卖 - 鱼 * @author glmapper * @date 2017年12月24日下午2:03:36 */public class SouthFishFactory implements MethodFoodFactory&#123; @Override public IFishFood getFood() &#123; return new SouthFishFood(); &#125;&#125; 具体工厂2：获取北方食物-鱼 1234567891011121314package com.glmapper.design.factory;/** * 北方口味外卖 - 鱼 * @author glmapper * @date 2017年12月24日下午2:03:36 */public class NorthFishFactory implements MethodFoodFactory&#123; @Override public IFishFood getFood() &#123; // TODO Auto-generated method stub return new NorthFishFood(); &#125;&#125; 具体产品1：南方食物- 鱼 123456789101112package com.glmapper.design.factory;/** * 南方口味-鱼 * @author glmapper * @date 2017年12月24日下午2:16:17 */public class SouthFishFood implements IFishFood&#123; @Override public void showFood() &#123; System.out.println(&quot;来自南方厨师做的鱼&quot;); &#125;&#125; 具体产品2：北方食物-鱼 123456789101112package com.glmapper.design.factory;/** * 北方口味 - 鱼 * @author glmapper * @date 2017年12月24日下午2:12:55 */public class NorthFishFood implements IFishFood &#123; @Override public void showFood() &#123; System.out.println(&quot;来自北方厨师做的鱼&quot;); &#125;&#125; 客户端 123456789101112131415161718package com.glmapper.design.factory;/** * 客户端 * @author glmapper * @date 2017年12月24日上午10:45:17 */public class MainTest &#123; public static void main(String[] args) &#123; //点一个南方口味外卖 MethodFoodFactory southFoodFactory = new SouthFishFactory(); //点一个北方口味外卖 MethodFoodFactory northFoodFactory = new NorthFishFactory(); //拿到南方口味外卖鱼 southFoodFactory.getFood().showFood(); //拿到北方口味外卖鱼 northFoodFactory.getFood().showFood(); &#125;&#125; 结果：12来自南方厨师做的鱼来自北方厨师做的鱼 OK，这样我们就满足了不同区域同时关于鱼口味的需求了，以后升值加薪就指望他们了。。。 关于工厂方法模式的优缺点： 优点：1、 在工厂方法中，用户只需要知道所要产品的具体工厂，无须关系具体的创建过程，甚至不需要具体产品类的类名。2、 在系统增加新的产品时，我们只需要添加一个具体产品类和对应的实现工厂，无需对原工厂进行任何修改，很好地符合了“开闭原则”缺点：每次增加一个产品时，都需要增加一个具体类和对象实现工厂，是的系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。这并不是什么好事。 3、抽象工厂模式准备吃饭的时候突然又来了几位同事，而且他们有的喜欢吃酸菜鱼，有的喜欢吃红烧鱼，这就很头疼。于是，只能根据他们的需要继续点外卖。（这个就给出一个结构图，并且将每种角色都用具体的场景来说明了，具体的代码可以参考这个图例自己尝试一下。） OK，终于可以吃饭了！ 4、三种形态的工厂模式在java中的使用4.1、简单工厂模式在java中的使用java.text.DateFormat （一个抽象类）这个类相信很多小伙伴都用到过，在java API中，这个类算是一个简单工厂模式的典型应用了（此处还是与上篇一样，不考虑期源码细节，也不介绍基本用法）。来看它的几个方法： public final static DateFormat getDateInstance() public final static DateFormat getDateInstance(int style) public final static DateFormat getDateInstance(int style, Locale aLocale) 作为一个抽象类，却提供了很多的静态工厂方法，就像上面列举的那三个一样。有小伙伴可能会疑惑，为啥子一个抽象类阔以有自己的实例，并通过几个方法提供自己的实例。我们知道，抽象类是不可以有自己的实例对象的，但是需要注意的是，DateFormat的工厂方法是静态的，并不是普通的方法，也就是说，不需要通过创建实例对象的方式去调用。 1234public final static DateFormat getDateInstance()&#123; return get(0, DEFAULT, 2, Locale.getDefault(Locale.Category.FORMAT));&#125; getDateInstance方法并没有通过调用DateFormat的构造方法来创建对象。 4.2、工厂方法模式在java中的应用java.net.URL类，类图如下，URL对象通过一个工厂方法openConnection()返回一个URLConnection类型的对象。URLConnection是一个抽象类，因此所返还的不可能是这个抽象类的实例，而必然是其具体子类的实例。 4.3、抽象工厂模式在java中的应用根据java与模式一书的介绍，在java中使用抽象工厂模式的是 JAVA awt的peer架构，通过抽象工厂模式来构建分属于不同操作系统的peer构件。这个我也没用过，了解即可。 关于Spring中工厂模式的使用会在后续Spring源码分析系列中给大家详细分析，这里就不重复了。 今天的学习就到此结束了，祝大家周末愉快。话说今天平安夜，大家都是在家写代码吗？ 如果您对系列文章有任何意见，可以给我留言，感谢大家。]]></content>
      <categories>
        <category>架构之路</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[菜鸟成长系列-单例模式]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-design-model-singleton%2F</url>
    <content type="text"><![CDATA[菜鸟成长系列-面向对象的四大基础特性菜鸟成长系列-多态、接口和抽象类菜鸟成长系列-面向对象的6种设计原则 前面已经将设计模式中的基本内容撸了一下，今天开始正式开始设计模式系列的内容，因为网上也有很多关于设计模式的技术博客，从不同的角度对设计模式都做了很详细的解读；本系列的模式除了基本的概念和模型之外，还会结合java自身使用的和Spring中使用的一些案例来进行学习分析。水平有限，如果存在不当之处，希望大家多提意见，灰常感谢！设计模式中总体分为三类:一、创建型(5)： 工厂方法[Factory Method] 抽象工厂[Abstract Factory] 原型[Prototype] 建造者[Builder] 单例[Singleton] 还有一个简单工厂[Simple Factory]，目前有两种，有的把单例模式作为这5种之一，有的是将简单工厂作为这5种之一。这里不做讨论，原则上两个都是，只是划分规则不同。 二、结构型(7) 适配器[Adapter] 桥接[Bridge] 组合[Composite] 装饰[Decorator] 外观[Facade] 享元[Flyweight] 代理[Proxy] 三、行为型(11) 策略[Strategy] 模板方法[Template method] 职责链[Chain of Responsibility] 迭代器[Iterator] 状态[State] 访问者[Visitor] 命令[Command] 备忘录[Memento] 观察者[Observer] 中介者[Mediator] 解释器[Interpreter] 单例模式首先它是一种创建型模式，与其他模式区别在于：单例模式确保被创建的类只有一个实例对象，而且自行实例化并向整个系统提供这个实例。一般情况下我们称当前这个类为单例类。 从上面这段话中我们可以了解到，单例模式具备以下三个要点： 某个类只能有一个实例 必须自行创建这个实例[具体的对象创建由类本身负责，其他类不负责当前类的创建] 必须向整个系统提供这个实例[也就是说，当前类需要对外提供一个获取当前实例的一个方法，且该方法不能是私有的] OK，来看单例模式的几种实现方式。 方式一：饿汉式123456789101112131415161718192021222324252627package com.glmapper.design.singleton;/** * 单例模式-饿汉式 * @author glmapper * @date 2017年12月17日下午10:30:38 */public class EagerSingleton &#123; /** * 内部直接提供一个eagerSingletonInstance； * 我们知道，一般情况下，如果一个变量被static final修饰了，那么该变量将会被视为常量。 * 满足要点：自行创建 */ private static final EagerSingleton eagerSingletonInstance = new EagerSingleton(); /** * 提供一个私有的构造函数，这样其他类就无法通过new * EagerSingleton()来获取对象了，同样也保证了当前类不可以被继承 * 满足要点：某个类只能有一个实例 */ private EagerSingleton()&#123;&#125; /** * 对外提供一个获取实例的方法 * 满足要点：向整个系统提供这个实例 */ public static EagerSingleton getInstance()&#123; return eagerSingletonInstance; &#125;&#125; 方式二：懒汉式 1234567891011121314151617181920212223package com.glmapper.design.singleton;/** * 单例模式-懒汉式 * @author glmapper * @date 2017年12月17日下午10:45:54 */public class LazySingleton &#123; //提供一个私有静态变量，注意区别与饿汉式中的static final。 private static LazySingleton lazySingletonInstance = null ; //同样需要提供一个私有的构造方法，其作用与饿汉式中的作用一样 private LazySingleton()&#123;&#125; /** * 1.使用synchronized来保证线程同步 * 2.实例的具体创建被延迟到第一次调用getInstance方法时来进行 * 3.如果当前实例已经存在，不再重复创建 */ public synchronized static LazySingleton getInstance()&#123; if (lazySingletonInstance == null) &#123; lazySingletonInstance = new LazySingleton(); &#125; return lazySingletonInstance; &#125;&#125; 饿汉式单例类在自己被加载时就自己实例化了，即便加载器是静态的，在饿汉式单例类被加载时仍会将自己实例化。从资源利用角度来说，这个比懒汉式单例类稍微的差一些。如果从速度和响应时间来看，饿汉式就会比懒汉式好一些。懒汉式在单例类进行实例化时，必须处理好在多个线程同时首次引用此类时的访问限制问题。 方式三：登记式 123456789101112131415161718192021222324252627282930313233343536373839package com.glmapper.design.singleton;import java.util.HashMap;/** * 单例模式-登记式 * @author glmapper * @date 2017年12月17日下午10:58:36 */public class RegisterSingleton &#123; //提供一个私有的HashMap类型的registerSingletonInstance存储该RegisterSingleton类型的单例 private static HashMap&lt;String,Object&gt; registerSingletonInstance = new HashMap&lt;&gt;(); //通过static静态代码块来进行初始化RegisterSingleton当前类的实例，并将当前实例存入registerSingletonInstance static &#123; RegisterSingleton singleton = new RegisterSingleton(); registerSingletonInstance.put(singleton.getClass().getName(), singleton); &#125; /** * 注意区别，此处提供的是非private类型的，说明当前类可以被继承 */ protected RegisterSingleton()&#123;&#125; /** * 获取实例的方法 */ public static RegisterSingleton getInstance(String name)&#123; //如果name为空，则那么默认为当前类的全限定名 if (name == null) &#123; name =&quot;com.glmapper.design.singleton.RegisterSingleton&quot;; &#125; //如果map中没有查询到指定的单例，则将通过Class.forName(name)来创建一个实例对象，并存入map中 if (registerSingletonInstance.get(name)==null) &#123; try &#123; registerSingletonInstance.put(name, Class.forName(name).newInstance()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; //返回实例 return (RegisterSingleton) registerSingletonInstance.get(name); &#125;&#125; 登记式单例是Gof为了克服饿汉式和懒汉式单例类均不可被继承的缺点而设计的。 12345678910111213141516171819202122232425262728package com.glmapper.design.singleton;/** * 登记式-单例-子类 * @author glmapper * @date 2017年12月17日下午11:14:03 * */public class ChildRegisterSingleton extends RegisterSingleton&#123; /** * 由于子类必须允许父类以构造方法调用产生实例，因此，子类的构造方法必须 * 是public类型的。但是这样一来，就等于说可以允许以new * ChildRegisterSingleton()的方式产生实例，而不必在父类的登记中。 */ public ChildRegisterSingleton()&#123;&#125; //客户端测试获取实例 public static void main(String[] args) &#123; ChildRegisterSingleton crs1 = (ChildRegisterSingleton) getInstance( &quot;com.glmapper.design.singleton.ChildRegisterSingleton&quot;); ChildRegisterSingleton crs2 = (ChildRegisterSingleton) getInstance( &quot;com.glmapper.design.singleton.ChildRegisterSingleton&quot;); System.out.println(crs1 == crs2); &#125;&#125;返回：true 这个同志们可以自行验证，肯定是一样的。但是不能使用new，因为前提约束是，需在父类中登记的才是单例。 方式四：双重检测模式，双重检测方式在某些书上或者文献中说对于java语言来说是不成立的，但是目前确实是通过某种技巧完成了在java中使用双重检测机制的单例模式的实现，；这种技巧后面来说；关于为什么java语言对于双重检测成例不成立，大家可以在[BLOCH01]文献中看下具体情况。先来看一个单线程模式下的情况： 123456789101112131415package com.glmapper.design.singleton;/** * 一个错误的单例例子 * @author glmapper * @date 2017年12月17日下午11:53:04 */public class DoubleCheckSingleton &#123; private static DoubleCheckSingleton instance=null; public static DoubleCheckSingleton getDoubleCheckSingleton()&#123; if (instance == null) &#123; instance = new DoubleCheckSingleton(); &#125; return instance; &#125;&#125; 这个很明显是一个错误的例子，对于A/B两个线程，因为step 1并没有使用同步策略，因此线程A/B可能会同时进行// step 2，这样的话，就会可能创建两个对象。那么正确的方式如下：使用synchronized关键字来保证同步。 12345678910111213141516package com.glmapper.design.singleton;/** * 这是一个正确的打开方式哦。。。 * @author glmapper * @date 2017年12月17日下午11:53:04 */public class DoubleCheckSingleton &#123; private static DoubleCheckSingleton instance=null; //使用synchronized来保证getDoubleCheckSingleton同一时刻只能被一个线程访问 public synchronized static DoubleCheckSingleton getDoubleCheckSingleton()&#123; if (instance == null) &#123; instance = new DoubleCheckSingleton(); &#125; return instance; &#125;&#125; 这种方式虽然保证了线程安全性，但是也存在另外一种问题：同步化操作仅仅在instance首次初始化操作之前会起到作用，如果instance已经完成了初始化，对于getDoubleCheckSingleton每一次调用来说都会阻塞其他线程，造成一个不必要的瓶颈。那我们就通过使用更加细粒度化的锁，来适当的减小额外的开销。OK，下面再来一个错误的例子： 1234567891011121314151617181920212223package com.glmapper.design.singleton;/** * 一个错误的单例例子 * @author glmapper * @date 2017年12月17日下午11:53:04 */public class DoubleCheckSingleton &#123; private static DoubleCheckSingleton instance=null; //使用synchronized来保证getDoubleCheckSingleton同一时刻只能被一个线程访问 public static DoubleCheckSingleton getDoubleCheckSingleton()&#123; if (instance == null) &#123; //1 // B线程检测到uniqueInstance不为空 synchronized (DoubleCheckSingleton.class) &#123; //2 if (instance == null) &#123; //3 instance = new DoubleCheckSingleton();//4 // A线程被指令重排了，刚好先赋值了；但还没执行完构造函数。 &#125; &#125; &#125; // 后面B线程执行时将引发：对象尚未初始化错误。 return instance;//5 &#125;&#125; 看起来没什么毛病呀？我们来分析，两个线程A和B，同时到达1,且都通过了1的检测。此时A到了4，B在2。此时B线程检测到instance不为空，A线程被指令重排了，刚好先赋值了；但还没执行完构造函数；再接下来B线程执行时将引发：对象尚未初始化错误（5）。 对于上面的问题，我们可以通过volatile关键字来修饰instance对象，来保证instance对象的内存可见性和防止指令重排序。这个也就是前面说到的“技巧”。 123private static DoubleCheckSingleton instance=null;改为：private static volatile DoubleCheckSingleton instance=null; 本篇将单例模式的几种情况进行了分析。后面将会对将java中和Spring中所使用的单例场景进行具体的案例分析。 JAVA中的单例模式使用JAVA中对于单例模式的使用最经典的就是RunTime这个类。注释解读：每个Java应用程序都有一个Runtime类的单个实例，允许应用程序与运行应用程序的环境进行交互。 当前运行时可以从getRuntime方法获得。应用程序不能创建它自己的这个类的实例。 看过上篇文章的小伙伴可能比较清楚，这里RunTime使用的是懒汉式单例的方式来创建的。Runtime提供了一个静态工厂方法getRuntime方法用于获取Runtime实例。Runtime这个类的具体源码分析和只能此处不做分析。 Spring中的单例Spring依赖注入Bean实例默认是单例的。Spring中bean的依赖注入都是依赖AbstractBeanFactory的getBean方法来完成的。那我们就来看看在getBean中都发生了什么。 org.springframework.beans.factory.suppor.AbstractBeanFactory从上面这张图中我们啥也看不出，只知道在getBean中又调用了doGetBean方法（Spring中还有java源码中有很多类似的写法，好处在于我们可以通过子类继承，继而编写我们自己的处理逻辑）。OK，再来看看doGetBean方法。 来看下这个方法的注释：返回指定的bean可以共享或独立的实例 （谷歌+有道+百度） name:要检索的bean的名称 requiredType:要检索的bean所需的类型 args:如果使用静态工厂方法的显式参数创建原型，则使用参数。 在其他情况下使用非空args值是无效的。 typeCheckOnly:获得实例是否是为了类型检查，而不是实际的使用 这个方法体内的代码非常的多，那么我们本文不是来学习Spring的，所以我们只看我们关心的部分，为手工注册的singleton检查单例缓存。,从这个注释可以看出，此处就是我们获取实例的地方，再往下看。 此处和上面的getBean一样，也是通过模板方法的方式进行调用的。OK，这里我们看到了获取单例实例的具体实现过程。返回注册在给定名称下的(原始的)singleton对象。检查已经实例化的单例，并且还允许提前引用当前创建的单例（解析循环引用）。这里使用的是饿汉式中的双重检测机制来实现的。 OK，至此单例模式的学习就结束了，下一篇文章将会介绍工厂模式（简单工厂，工厂方法，抽象工厂）。]]></content>
      <categories>
        <category>架构之路</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[怎么写一个死锁？]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-base-thread-deadlock%2F</url>
    <content type="text"><![CDATA[刚把手头上的项目代码撸完，闲来看看博客，然后就看到了线程这块的东西。之前有简单的记录过线程和进行的零碎知识。JAVA基础知识系列—进程、线程安全 看着看着就想着怎么能写一个死锁呢，打开eclipse，突然感觉无从下手；之前都是一直在解决阻塞、死锁这些问题，现在反过来去写一个死锁感觉有点莫名奇妙。。。 ok,写一个死锁就要有一种场景，并且满足死锁的条件。 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放 不剥夺条件:进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 首先要有竞争的资源，并且两个线程要同时都在等待对方释放资源。那我们先弄两个资源： 12Object lock=new Object();Object lock2=new Object(); 然后有两个线程： 12345Tr1 tr1=new Tr1(lock, lock2);Tr2 tr2=new Tr2(lock, lock2); Thread t1=new Thread(tr1);Thread t2=new Thread(tr2); 启动： 12t1.start();t2.start(); 那么对于lock，lock2怎么再线程内部产生竞争关系呢？来看代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.glmapper.base.synchronize;public class Tr1 implements Runnable &#123; Object lock; Object lock2; public Tr1(Object lock,Object lock2)&#123; this.lock= lock; this.lock2= lock2; &#125; @Override public void run() &#123; //获取lock synchronized (lock) &#123; System.out.println(Thread.currentThread().getName()+&quot;获取了lock锁&quot;); try &#123; Thread.sleep(3000); &#125; catch (Exception e) &#123; &#125; //获取lock2 synchronized (lock2) &#123; System.out.println(Thread.currentThread().getName()+&quot;获取了lock2锁&quot;); &#125; &#125; &#125;&#125;public class Tr2 implements Runnable &#123; Object lock; Object lock2; public Tr2(Object lock,Object lock2)&#123; this.lock= lock; this.lock2= lock2; &#125; @Override public void run() &#123; //获取lock2 synchronized (lock2) &#123; System.out.println(Thread.currentThread().getName()+&quot;获取了lock2锁&quot;); try &#123; Thread.sleep(3000); &#125; catch (Exception e) &#123; &#125; //获取lock synchronized (lock) &#123; System.out.println(Thread.currentThread().getName()+&quot;获取了lock锁&quot;); &#125; &#125; &#125;&#125; 分析一下：当线程1获取lock时，线程2获取了lock2锁；然后线程1继续执行，到这里， 123synchronized (lock2) &#123; System.out.println(Thread.currentThread().getName()+&quot;获取了lock2锁&quot;);&#125; 此时需要获取到lock2这个锁，但是lock2现在被线程2持有；同时，线程2也开始执行到： 123synchronized (lock) &#123; System.out.println(Thread.currentThread().getName()+&quot;获取了lock锁&quot;);&#125; 此时线程2也在尝试获取lock这把锁，但是lock又被线程1持有了。两个线程都在等待对方释放资源，造成了死锁。OK，完成了。。。当我准备关机时，发现还在等呢？？？那为什么呢？？我们开看下发生了什么…. 通过jps来看下我们程序进程 使用jstack -l 【pid】 来看下信息 两个线程都处于BLOCKED状态了…,继续往下看found 1 deadlock.如我们所愿，死锁发生了！]]></content>
      <categories>
        <category>java 基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>线程</tag>
        <tag>thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[菜鸟成长系列-面向对象的6种设计原则]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-bird-three%2F</url>
    <content type="text"><![CDATA[菜鸟成长系列拖了一周多了，今天继续之前的思路来进行。按照之前的规划，这篇主要来学习设计原则先关知识。通过本文学习，希望大家一方面能是能够认识这些原则是什么，能够在日常的开发中起到怎样的约束，并且用这些原则来提高代码的复用性和可维护性，另一方面是对后续的设计模式的学习能够有一些基础。 菜鸟成长系列-概述菜鸟成长系列-面向对象的四大基础特性菜鸟成长系列-多态、接口和抽象类 设计原则，在java与模式这本书中有提到，用于提高系统可维护性的同时，也提高系统的可复用性。这本书中主要讲了六种设计原则： “开-闭”原则 里氏替换原则 依赖倒置原则 接口隔离原则 单一职责原则 迪特米法则 这些设计原则首先都是复用的原则，遵循这些原则可以有效的提高系统的复用性，同时也提高了系统的可维护性。 “开-闭”原则网上看到一个人的解释，他是这样来比喻的：一个本子，已经写完了，你不可能撕几张纸粘上去吧，最好的办法是买个新的。道理就是这样，一个已经做好的程序，不支持修改的，因为修改的话，有可能造成程序无法运行或报错，所以，通常程序只支持扩展，不支持修改。 1.为什么会有这样一个原则来作为程序设计的一种约束呢？在软件的生命周期内，由于软件功能或者结构的变化、升级和维护等原因需要对软件原有代码进行修改，在修改的过程中可能会给旧代码中引入错误，也可能会使我们不得不对整个功能进行重构，并且还需要进行软件的重新测试，因此我们希望在软件设计之初，能够用一种原则来进行一些基本的约束，使得在软件后期的功能变更、扩展或者维护更加容易 2.开闭原则解决的问题是什么？当软件需要进行改变时，我们应该尽量通过扩展软件实体的行为来实现变化，而不是通过修改已有的代码来实现变化。通过这样一种原则，可以很好的实现在保证原有功能稳定的前提下扩展新的功能 3.什么是开闭原则呢？一个软件实体(类、模块或函数)应当对扩展开放，对修改关闭。也就是说在扩展或者修改软件功能时，应尽量在不修改原有代码的情况下进行 举个简单的栗子：现在有这样一个需求，系统需要通过QQ来进行验证登录。OK，我们来撸代码： 用户类User 1234567891011121314151617181920212223package com.glmapper.framerwork;/** * 用户信息类 * @author glmapper * @date 2017年12月9日下午10:54:09 * */public class User &#123; private String userName;//用户名 private String passWord;//密码 public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getPassWord() &#123; return passWord; &#125; public void setPassWord(String passWord) &#123; this.passWord = passWord; &#125;&#125; QQ核心验证逻辑 12345678910111213141516package com.glmapper.framerwork;/** * QQ验证器 * @author glmapper * @date 2017年12月9日下午10:49:24 */public class QQAuther &#123; /** * 用于验证QQ登录信息 */ public boolean validateQQ(User user) &#123; //模拟下逻辑 return user.toString()==null?false:true; &#125;&#125; 核心验证服务类 12345678910111213141516171819202122package com.glmapper.framerwork;/** * * 用于验证的核心服务 * @author glmapper * @date 2017年12月9日下午10:47:04 * */public class AuthService &#123; //持有一个QQ验证器对象 private QQAuther qqAuther; //通过构造器注入qqAuther对象 public AuthService(QQAuther qqAuther) &#123; this.qqAuther = qqAuther; &#125; /* * 验证用户合法性 */ public boolean validateUser(User user)&#123; return qqAuther.validateQQ(user); &#125;&#125; 客户端 12345678910111213141516171819package com.glmapper.framerwork;/** * 客户端调用验证 * @author glmapper * @date 2017年12月9日下午10:50:13 * */public class AuthClient &#123; public static void main(String[] args) &#123; //获取用户信息 User user = UserHolder.getUser(); QQAuther qqAuther = new QQAuther(); AuthService authService = new AuthService(qqAuther); //获取验证结果 boolean isOK = authService.validateUser(user); System.out.println(isOK); &#125;&#125; OK,完事了！但是现在需要接入微博的开放平台接口；修改代码…。增加一个微博验证器：123456789101112131415package com.glmapper.framerwork;/** * 微博核心验证器 * @author glmapper * @date 2017年12月9日下午11:01:10 */public class WeiBoAuther &#123; /** * 用于验证QQ登录信息 */ public boolean validateWeiBo(User user) &#123; return user.toString()==null?false:true; &#125;&#125; 核心验证服务修改： 1234567891011121314151617181920212223242526272829package com.glmapper.framerwork;/** * * 用于验证的核心服务 * @author glmapper * @date 2017年12月9日下午10:47:04 * */public class AuthService &#123; //持有一个QQ验证器对象 private Object obj; //通过构造器注入qqAuther对象 public AuthService(Object obj) &#123; this.obj = obj; &#125; /* * 验证用户合法性 */ public boolean validateUser(User user)&#123; //这里仅作为模拟，一般情况下会通过使用定义枚举&amp;工厂模式来完成 if (obj instanceof QQAuther) &#123; return new QQAuther().validateQQ(user); &#125; if(obj instanceof WeiBoAuther)&#123; return new WeiBoAuther().validateWeiBo(user); &#125; return false; &#125;&#125; 客户端改变： 12345678910111213141516171819202122232425package com.glmapper.framerwork;/** * 客户端调用验证 * @author glmapper * @date 2017年12月9日下午10:50:13 * */public class AuthClient &#123; public static void main(String[] args) &#123; //获取用户信息 User user = UserHolder.getUser(); //QQ QQAuther qqAuther = new QQAuther(); boolean isQQOK = new AuthService(qqAuther).validateUser(user); System.out.println(isQQOK); //微博 WeiBoAuther weiBoAuther = new WeiBoAuther(); boolean isWeiBoOK = new AuthService(weiBoAuther).validateUser(user); System.out.println(isWeiBoOK); &#125;&#125; OK，改进完成！但是又有新的需求，接入微信….。假如我们现在把微信开放平台也接入了，然后又来需求要接入支付宝账户、苏宁易购账户等等。。。就需要不断的修改代码。那么这个时候就需要在设计之初用到我们的开闭原则来做一个约束了。继续撸：首先我们需要需要定义一个接口用于约束： 验证器接口，用于被QQ/WEIBO/微信/苏宁易购等开发平台验证器实现 12345678910111213package com.glmapper.framerwork;/** * 定义一个约束接口 * @author glmapper * @date 2017年12月9日下午11:32:32 * */public interface ValidateInteface &#123; /** * 提供一个验证入口 */ boolean validate(User user);&#125; QQ修改之后 123456789101112131415package com.glmapper.framerwork;/** * QQ验证器 * @author glmapper * @date 2017年12月9日下午10:49:24 */public class QQAuther implements ValidateInteface&#123; /** * 用于验证QQ登录信息 */ @Override public boolean validate(User user) &#123; return user.toString()==null?false:true; &#125;&#125; 微博修改之后 12345678910111213141516package com.glmapper.framerwork;/** * 微博核心验证器 * @author glmapper * @date 2017年12月9日下午11:01:10 */public class WeiBoAuther implements ValidateInteface&#123; /** * 用于验证QQ登录信息 */ @Override public boolean validate(User user) &#123; // TODO Auto-generated method stub return user.toString()==null?false:true; &#125;&#125; 核心验证服务 1234567891011121314151617181920package com.glmapper.framerwork;/** * 用于验证的核心服务 * @author glmapper * @date 2017年12月9日下午10:47:04 */public class AuthService &#123; //持有一个QQ验证器对象 private ValidateInteface validate; //通过构造器注入qqAuther对象 public AuthService(ValidateInteface validate) &#123; this.validate = validate; &#125; /* * 验证用户合法性 */ public boolean validateUser(User user)&#123; return validate.validate(user); &#125;&#125; 客户端 123456789101112131415161718192021package com.glmapper.framerwork;/** * 客户端调用验证 * @author glmapper * @date 2017年12月9日下午10:50:13 * */public class AuthClient &#123; public static void main(String[] args) &#123; //获取用户信息 User user = UserHolder.getUser(); //QQ ValidateInteface qqAuther = new QQAuther(); boolean isQQOK = new AuthService(qqAuther).validateUser(user); System.out.println(isQQOK); //微博 ValidateInteface weiBoAuther = new WeiBoAuther(); boolean isWeiBoOK = new AuthService(weiBoAuther).validateUser(user); System.out.println(isWeiBoOK); &#125;&#125; 改进之后我们可以发现，对于原来的核心验证服务类、各验证器类，无论增加什么方式接入，我们都不需要去修改它的代码了。而此时我们需要做的就是新增一个验证器（例如苏宁易购验证器），然后继承ValidateInterface接口就行了。总体来首，开闭原则的核心是： 抽象化 对可变性的封装原则（1.不可变性不应该散落在代码的多处，而应当被封装到一个对象里面；2.一种可变性不应当与另外一种可变性混合在一起） （大家如果有更简单暴力的例子，可以留言；这个例子想了很多都感觉不是很恰当，还是从工作中抽象出来的）。 里氏替换原则任何父类可以出现的地方，子类一定可以出现里氏替换原则算是对“开闭”原则的补充，上面也提到，实现“开闭”原则的关键步骤是抽象化，而父类与子类的继承关系就是抽象化的一种具体体现，所以里氏替换原则是对实现抽象化的具体步骤的规范。 摘自java与模式中的定义:如果对每一个类型为 T1的对象 o1，都有类型为 T2 的对象o2，使得以 T1定义的所有程序 P 在所有的对象 o1 都代换成 o2 时，程序 P 的行为没有发生变化，那么类型 T2 是类型 T1 的子类型。 下图中描述了一种继承关系，从最高层的动物一直衍生出具体的动物。OK，写一段断码来看看： 顶层抽象父类-Animal 12345678910package com.glmapper.framework.model.lsp;/** * 顶层抽象父类动物类 * @author glmapper * @date 2017年12月10日上午10:51:30 */public abstract class Animal &#123; //提供一个抽象方法，以供不同子类来进行具体的实现 public abstract void eatFood(String foodName);&#125; 具体动物类型-Dog 12345678910111213 package com.glmapper.framework.model.lsp;/** *子类-小狗 * @author glmapper * @date 2017年12月10日上午10:54:17 * */public class Dog extends Animal&#123; @Override public void eatFood(String foodName) &#123; System.out.println(&quot;小狗吃&quot;+foodName); &#125;&#125; 具体动物-哈士奇 12345678910111213141516 package com.glmapper.framework.model.lsp;/** * 具体小狗的种类-子类哈士奇 * @author glmapper * @date 2017年12月10日上午10:56:59 * */public class HSQDog extends Dog&#123; /** * 重写父类方法 */ @Override public void eatFood(String foodName) &#123; System.out.println(&quot;哈士奇吃&quot;+foodName); &#125;&#125; 客户端 123456789101112131415package com.glmapper.framework.model.lsp;//客户端程序public class ClientMain &#123; public static void main(String[] args) &#123; //子类 HSQDog hsqdog=new HSQDog(); hsqdog.eatFood(&quot;饼干&quot;); //父类 Dog dog = new HSQDog(); dog.eatFood(&quot;饼干&quot;); //顶层父类 Animal animal = new HSQDog(); animal.eatFood(&quot;饼干&quot;); &#125;&#125; 运行结果 123哈士奇吃饼干哈士奇吃饼干哈士奇吃饼干 可以看出我们最开始说的那句话任何父类可以出现的地方，子类一定可以出现，反过来是不成立的。我的理解是子类通过集成获取的父类的属性和行为，并且子类自身也具有自己的属性和行为；父类可以出现的地方必然是需要用到父类的属性或者行为，而子类都涵盖了父类的这些信息，因此可以做到替换。反过来不行是因为父类在上述的例子中只是充当了一种类型约束，它可能不具有子类的某些特征，因此就无法做到真正的替换。 里氏替换原则是继承复用的基石，只有当子类可以替换掉基类，软件单位的功能不会受到影响时，基类才能被真正的复用，而子类也才能够在基类的基础上增加新的功能。 依赖倒转原则实现“开闭”原则的关键是抽象化，并且从抽象化导出具体化实现。如果说开闭原则是面向对象设计的目标的话，依赖倒转原则就是面向对象设计的主要机制（java与模式）。依赖倒转原则：要依赖与抽象，不依赖于具体实现。 怎么理解呢? 1）高层模块不应该直接依赖于底层模块的具体实现，而应该依赖于底层的抽象。换言之，模块间的依赖是通过抽象发生，实现类之间不发生直接的依赖关系，其依赖关系是通过接口或抽象类产生的。 2）接口和抽象类不应该依赖于实现类，而实现类依赖接口或抽象类。这一点其实不用多说，很好理解，“面向接口编程”思想正是这点的最好体现 首先是第一点，从复用的角度来说，高层次的模块是设计者应当复用的。但是在传统的过程性的设计中，复用却侧重于具体层次模块的复用。比如算法的复用，数据结构的复用，函数库的复用等，都不可避免是具体层次模块里面的复用。较高层次的结构依赖于较低层次的结构，然后较低层次的结构又依赖于更低层次的结构，直到依赖到每一行代码为止。然后对低层次修改也会逐层修改，一直到最高层的设计模块中。 对于一个系统来说，一般抽象层次越高，它的稳定性就越好，因此也是作为复用的重点。 “倒转”，实际上就是指复用应当将复用的重点放在抽象层上，如果抽象层次的模块相对独立于具体层次模块的话，那么抽象层次的模块的复用便是相对较为容易的了。 在很多情况下，一个java程序需要引用一个对象，如果这个对象有一个抽象类型的话，应当使用这个抽象类型作为变量的静态类型。在上面我们画了动物和小狗的类图关系，在客户端调用的时候有三种方式： 123456789//子类(方式1)HSQDog hsqdog=new HSQDog();hsqdog.eatFood(&quot;饼干&quot;);//父类（方式2）Dog dog = new HSQDog();dog.eatFood(&quot;饼干&quot;);//顶层父类（方式3）Animal animal = new HSQDog();animal.eatFood(&quot;饼干&quot;); 如果我们需要一个哈士奇（HSQDog）的话，我们不应当使用方式1，而是应当使用方式2或者方式3。 接口隔离原则接口隔离原则：使用多个专门的接口比使用单一的总接口要好。换句话说，从一个客户类的角度来讲：一个类对另外一个类的依赖性应当是建立在最小的接口上的。这个其实在我们实际的开发中是经常遇到的。比如我们需要编写一个完成一个产品的一些操作接口。12345678910111213141516package com.glmapper.framework.model.isp;/** * 一个产品服务接口 * @author glmapper * @date 2017年12月10日下午12:01:31 */public interface ProductService &#123; //增加产品 public int addProduct(Product p); //删除产产品 public int deleteProduct(int pId); //修改产品 public int updateProduct(Product p); //查询一个产品 public Product queryProduct(int pId);&#125; OK，我们在ProductService中提供了对产品的增删改查；但是随着需求升级，我们需要可以增加对产品新的批量导入和导出。OK，这时在接口中继续新增两个方法：1234//从excel中批量导入public void batchImportFromExcel();//从excel中批量导导出public void batchExportFromExcel(); 然后需求又需要扩展，需要增加增加购买产品、产品订单生产、查询订单、订单详情….；这样一来，我们的ProductService就会慢慢的急速膨胀。与此对应的具体的实现逻辑ProductServiceImpl类也会变得非常的庞大，可能单类会超过数千行代码。 那么我们就需要进行接口隔离，将产品的基本操作如增删改查放在一个接口，将产品订单处理放在一个接口，将产品申购放在一个接口，将批量操作放在一个接口等等…对于每一个接口我们只关心某一类特定的职责，这个其实就是和单一职责原则有点挂钩了。通过这种设计，降低了单个接口的复杂度，使得接口的“内聚性”更高，“耦合性”更低。由此可以看出接口隔离原则的必要性。 迪特米法则迪特米法则：又称为最少知识原则，就是说一个对象应当对其他对象尽可能少的了解；看下迪特米法则的几种表述：1.只与你直接的朋友们通信2.不跟陌生人说话3.每一个软件单位对其他的单位都只有最少知识，而且局限于那些与本单位密切相关的软件单位 也就是说，如果两个雷不必彼此直接通信，那么这两个类就不应当发生直接的相互作用。如果其中一个类需要电泳另一个类的某一个方法的话，可以通过第三者进行消息的转发。代码看下： 某个人 1234567891011121314package com.glmapper.framework.model.isp;/** * 某个人 * @author glmapper * @date 2017年12月10日下午12:39:45 */public class SomeOne &#123; //具体oprateion行为 public void oprateion(Friend friend)&#123; Stranger stranger =friend.provide(); stranger.oprateion3(); &#125;&#125;SomeOne具有一个oprateion方法，该方法接受Friend为参数，根据上面的定义可以知道Friend是SomeOne的“朋友”（直接通信了） 朋友 123456789101112131415package com.glmapper.framework.model.isp;/** * 朋友 * @author glmapper * @date 2017年12月10日下午12:40:09 */public class Friend &#123; private Stranger stranger = new Stranger(); public Stranger provide()&#123; return stranger; &#125; public void opration2()&#123; &#125;&#125;很明显SomeOne的opration方法不满足迪特米法则，因为这个方法中涉及到了陌生人Stranger,Stranger不是SomeOne的朋友 OK，我们来通过迪特米法则进行改造。 改造之后的SomeOne 12345678910111213package com.glmapper.framework.model.isp;/** * 某个人 * @author glmapper * @date 2017年12月10日下午12:39:45 * */public class SomeOne &#123; //具体oprateion行为 public void oprateion(Friend friend)&#123; friend.forward(); &#125;&#125; 改造之后的朋友 1234567891011121314151617package com.glmapper.framework.model.isp;/** * 朋友 * @author glmapper * @date 2017年12月10日下午12:40:09 * */public class Friend &#123; private Stranger stranger = new Stranger(); public void opration2()&#123; &#125; //进行转发 public void forward() &#123; stranger.oprateion3(); &#125;&#125; 由于调用了转发，因此SomeOne中就不会和陌生人Stranger直接的关系就被忽略了。满足了直接和朋友通信、不与陌生人说话的条件。但是迪特米法则带来的问题也是很明显的：即会在系统中造出大量的小方法散落在系统的各个角落，这些方法仅仅是传递消息的调用，与系统的业务逻辑没有任何关系。 单一职责上面在接口隔离中有提到过，单一职责其实很好理解，解释尽量的使得我们的每一个类或者接口只完成本职工作以内的事情，不参与其他任何逻辑。比如说苹果榨汁机我就只用来榨苹果汁，如果你需要榨黄瓜汁的话，你就得买一个黄瓜榨汁机。 总结OK ，至此，设计原则部分就复习完了。总结一下： 单一职责原则要求实现类要职责单一； 里氏替换原则要求不要去破坏继承系统； 依赖倒置原则要求面向接口编程； 接口隔离原则要求在设计接口的时候要精简单一； 迪米特法则要求要降低耦合； 开闭原则是总纲，要求对扩展开发，对修改关闭。 大家周末愉快！（如果有不当之处，希望大家及时指出，多谢！）]]></content>
      <categories>
        <category>架构之路</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从源码来聊一聊hashmap]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-base-hashmap%2F</url>
    <content type="text"><![CDATA[HashMap为什么会是面试中的常客呢？我觉得有以下几点原因：* 考察你阅读源码的能力* 是否了解内部数据结构* 是否了解其存储和查询逻辑* 对非线程安全情况下的使用考虑前段时间一同事面试蚂蚁金服，就被问到了这个问题；其实很多情况下都是从hashMap,hashTable,ConcurrentHahMap三者之间的关系衍生而出，当然也有直接就针对hashMap原理直接进行考察的。实际上本质都一样，就是为了考察你是否对集合中这些常用集合的原理、实现和使用场景是否清楚。一方面是我们开发中用的多，当然用的人也就多，但是用的好的人却不多（我也用的多，用的也不好）。所以就借此机会（强行蹭一波）再来捋一捋这个HashMap。本文基于jdk1.7.0_80；jdk 1.8之后略有改动，这个后面细说。 继承关系123public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable hashMap实现了Map、Cloneable、Serializable三个接口，并且继承了AbstractMap这个抽象类。hashTable继承的是Dictionary这个类，同时也实现了Map、Cloneable、Serializable三个接口。 主要属性 DEFAULT_INITIAL_CAPACITY 默认初始容量 16 （hashtable 是11） 常量 12345/** * The default initial capacity - MUST be a power of two. * 默认初始容量-必须是2的幂。 */ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 MAXIMUM_CAPACITY 默认最大容量 常量 1234567/** * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. *如果有一个更大的值被用于构造HashMap,则使用最大值 */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; DEFAULT_LOAD_FACTOR 负载因子(默认0.75) 常量 12345/** * The load factor used when none specified in constructor. * 加载因子，如果构造函数中没有指定，则使用默认的 */ static final float DEFAULT_LOAD_FACTOR = 0.75f; EMPTY_TABLE 默认的空表 12345/** * An empty table instance to share when the table is not inflated. * 当表不膨胀时共享的空表实例。 */ static final Entry&lt;?,?&gt;[] EMPTY_TABLE = &#123;&#125;; table 表，必要时调整大小。长度必须是两个幂。这个也是hashmap中的核心的存储结构 1234/** * The table, resized as necessary. Length MUST Always be a power of two. */ transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE; size 表示HashMap中存放KV的数量（为链表/树中的KV的总和） 1234/** * The number of key-value mappings contained in this map. */ transient int size; threshold 扩容变量，表示当HashMap的size大于threshold时会执行resize操作。threshold=capacity*loadFactor 1234567/** * The next size value at which to resize (capacity * load factor). * @serial */ // If table == EMPTY_TABLE then this is the initial capacity at which the // table will be created when inflated. int threshold; loadFactor 负载因子 负载因子用来衡量HashMap满的程度。loadFactor的默认值为0.75f。计算HashMap的实时装载因子的方法为：size/capacity，而不是占用桶的数量去除以capacity。（桶的概念后续介绍） 123456/** * The load factor for the hash table. * * @serial */final float loadFactor; modCount这个HashMap的结构修改的次数是那些改变HashMap中的映射数量或修改其内部结构(例如rehash)的那些。这个字段用于使迭代器对HashMap失败快速的集合视图。(见ConcurrentModificationException)。 12345678/** * The number of times this HashMap has been structurally modified * Structural modifications are those that change the number of mappings in * the HashMap or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the HashMap fail-fast. (See ConcurrentModificationException). */ transient int modCount; hashSeed 与此实例相关联的随机值，用于哈希键的散列代码，使散列冲突更难找到。如果0，那么替代哈希是禁用的。 123456/** * A randomizing value associated with this instance that is applied to * hash code of keys to make hash collisions harder to find. If 0 then * alternative hashing is disabled. */ transient int hashSeed = 0; 结构分析1static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; hashmap中是通过使用一个继承自Map中内部类Entry的Entry静态内部类来存储每一个K-V值的。看下具体代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; //键对象 V value; //值对象 Entry&lt;K,V&gt; next; //指向链表中下一个Entry对象，可为null，表示当前Entry对象在链表尾部 int hash; //键对象的hash值 /** * 构造对象 */ Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; /** * 获取key */ public final K getKey() &#123; return key; &#125; /** * 获取value */ public final V getValue() &#123; return value; &#125; /** * 设置value，这里返回的是oldValue(这个不太明白，哪位大佬清楚的可以留言解释下，非常感谢) */ public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; /** * 重写equals方法 */ public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; /** * 重写hashCode方法 */ public final int hashCode() &#123; return Objects.hashCode(getKey()) ^ Objects.hashCode(getValue()); &#125; public final String toString() &#123; return getKey() + &quot;=&quot; + getValue(); &#125; /** * This method is invoked whenever the value in an entry is * overwritten by an invocation of put(k,v) for a key k that&apos;s already * in the HashMap. */ void recordAccess(HashMap&lt;K,V&gt; m) &#123; &#125; /** * This method is invoked whenever the entry is * removed from the table. */ void recordRemoval(HashMap&lt;K,V&gt; m) &#123; &#125; &#125; HashMap是一个用于存储Key-Value键值对的集合，每一个键值对也叫做Entry。这些个键值对（Entry）分散存储在一个数组当中，这个数组就是HashMap的主干（也就是上面的table–桶）。看一张图： hashmap初始化时各个空间的默认值为null，当插入元素时（具体插入下面分析），根据key值来计算出具体的索引位置，如果重复，则使用尾插入法进行插入后面链表中。 尾插法之前我是通过插入17条数据来试验的（具体数据数目随意，越大重复的几率越高）1234567public static void main(String[] args) throws Exception &#123; HashMap&lt;String, Object&gt; map=new HashMap&lt;&gt;(); for (int i = 0; i &lt; 170; i++) &#123; map.put(&quot;key&quot;+i, i); &#125; System.out.println(map); &#125; 通过断点查看next，可以得出我们上面的结论：1.索引冲突时会使用链表来存储；2.插入链表的方式是从尾部开始插入的（官方的解释是一般情况下，后来插入的数据被使用的频次较高），这样的话有利于查找。 主要方法我们平时在开发是最常用的hashMap中的方法无非就是先创建一个HashMap对象，然后存，接着取；对应的方法就是： 构造函数 put函数 get函数 构造函数 12345678910111213141516171819202122232425262728/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and load factor. * * @param initialCapacity the initial capacity 指定的初始化容量大小 * @param loadFactor the load factor 指定的负载因子 * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public HashMap(int initialCapacity, float loadFactor) &#123; //如果初始化容量小于0，则抛出异常 if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); //如果初始化容量大于最大容量，则使用默认最大容量 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //如果负载因子小于0或者非数值类型，则抛出异常 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); //初始化负载因子 this.loadFactor = loadFactor; //初始化threshold threshold = initialCapacity; //这个初始化方法是个空方法，应该是意在HashMap的子类中由使用者自行重写该方法的具体实现 init(); &#125; 另外两个构造方法实际上都是对上面这个构造方法的调用： 12345678//只制定默认容量 public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; //使用HashMap默认的容量大小和负载因子 public HashMap() &#123; this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR); &#125; 还有一个是：1234567public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); inflateTable(threshold); putAllForCreate(m); &#125; 构造一个映射关系与指定 Map 相同的新 HashMap。所创建的 HashMap 具有默认加载因子 (0.75) 和足以容纳指定 Map 中映射关系的初始容量。 put方法首先，我们都知道hashmap中的key是允许为null的，这一点也是面试中最常问到的点。那我先看下为什么可以存null作为key值。 123456789101112131415161718192021222324252627282930public V put(K key, V value) &#123; //如果table是空的 if (table == EMPTY_TABLE) &#123; //inflate：扩容/膨胀的意思 inflateTable(threshold); &#125; //如果key为null 此处敲下桌子，为什么可以存null？ if (key == null) //执行putForNullKey方法，这个方法的作用是如果key为null，就将当前的k-v存放到table[0],即第一个桶。 return putForNullKey(value); //对key进行一次hash运算，获取hash值 int hash = hash(key); //根据key值得hash值和表的长度来计算索引位置 int i = indexFor(hash, table.length); //移动数据，插入数据 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); //上面Entry中的setValue中也有提到，返回的都是旧的数据 return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); return null; &#125; hash方法：检索对象哈希代码，并将附加哈希函数应用于结果哈希，该哈希函数防止质量差的哈希函数。 这是至关重要的，因为HashMap使用两个长度的哈希表，否则会碰到hashCode的冲突，这些hashCodes在低位上没有区别。 注意：空键总是映射到散列0，因此索引为0。1234567891011121314/** final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); //这个函数确保在每个比特位置上仅以恒定倍数不同 //的散列码具有有限数量的冲突（在默认加载因子下大约为8）。 h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; 冲突具体过程描述： 一个空的hashmap表 插入元素，通过hash计算得出索引为3，因为当前3的位置没有元素，因此直接插入进去即可 再次插入元素，通过hash计算得出索引还是3，发生冲突，则将当前新插入的元素放在原来的已有的元素位置，并将其next指向原来已经存在的元素。get方法返回指定键映射到的值;如果此映射不包含键映射，则返回null。123456789public V get(Object key) &#123; //和存null key一样，取的时候也是从table[0]取 if (key == null) return getForNullKey(); //获取entry Entry&lt;K,V&gt; entry = getEntry(key); return null == entry ? null : entry.getValue(); &#125; getEntry方法 12345678910111213141516171819final Entry&lt;K,V&gt; getEntry(Object key) &#123; //size等于0，说明当前hashMap中没有元素，直接返回null（每个entry默认值为null） if (size == 0) &#123; return null; &#125; //根据key值计算hash值 int hash = (key == null) ? 0 : hash(key); //通过hash值获取到索引位置，找到对应的桶链进行遍历查找 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; //如果找到则返回，如果没有链表指针移动到下一个节点继续查找。 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null; &#125; 扩容机制在前面提到过threshold，扩容变量，表示当HashMap的size大于threshold时会执行resize操作。其计算方式是：threshold=capacity*loadFactor。从上面的式子中我们可以得知hashmap的扩容时机是当前当前size的值超过容量乘以负载因子时就会触发扩容。来看下源码： 123456789101112131415161718192021void addEntry(int hash, K key, V value, int bucketIndex) &#123; //如果当前size超过threshold 并且满足桶索引位置不为null的情况下，扩容 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; //扩容之后为原来的两倍 resize(2 * table.length); //重新计算hash值 hash = (null != key) ? hash(key) : 0; //重写计算索引 bucketIndex = indexFor(hash, table.length); &#125; //执行具体的插入操作 createEntry(hash, key, value, bucketIndex); &#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123; //先取到当前桶的entry Entry&lt;K,V&gt; e = table[bucketIndex]; //将新的数据插入到table[bucketIndex]，再将之前的entry通过链表简介到table[bucketIndex]的next指向；前面的图已经进行了描述。 table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++; &#125; 需要注意的是，扩容并不是在hashmap满了之后才进行的，看下面断点： 通过默认构造函数new了一个map对象出来，通过for循环插入12条数据，断点到执行结束，我们看到当前table的容量是16，扩容变量threshold为12（16x0.75），现在我们将12改为13.此时13还是小于16的，但是还是触发了hashmap 的扩容。当前table容量为32（扩容为了之前的两倍），threshold为24（32x0.75），通过这两种图我们得知： 每次扩容之后的容量为原有容量的两倍（2n） 触发扩容并不是因为当前hashmap对象已经满了，而是通过threhold扩容变量来控制触发时机的。 小结本文就单纯的扒了一波源码，并对源码中的注释并结合自己的理解进行了翻译，通过断点调试简单的介绍了尾插法在hashmap的应用。最后通过几张图描述了下hashmap发生索引冲突时的解决方案。hashmap在面试时真的是可深可浅，但是源码的阅读还是很有必要的，下面推荐两篇博客给大家。 1.关于hashmap与hashtable的具体对比可以参考这个博客：HashMap和HashTable到底哪不同？ 2.关于为什么hashmap中的容量必须是2的幂，这篇博客大家可以看下：什么是hashmap？ 3.关于hashmap非线程安全的解释并发安全问题之HashMap]]></content>
      <categories>
        <category>java 基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>hash</tag>
        <tag>map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个朋友圈泛型问题引发的“案子”]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-base-one%2F</url>
    <content type="text"><![CDATA[昨天朋友圈问了一个问题：对于下面的list，何如在list添加一个Integer型整数？1ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(); 有这样几种回答： 1.不知道（非专业回答） 2.硬塞（非专业回答） 3.把String 改成Integer再添加（违背了问题初衷） 4.把String改成Object，可以加任意类型（违背了问题初衷） 5.String换成通配符 6.反射 对于1、2就不说了，属于搞事情的！3、4、5三种方式违背了问题的初衷，如果可以改，那我们直接new三个ArrayList就可以了。6反射，这个是无限接近的，那么这个和反射有什么关系呢？下来看下下面几个例子： 12345678910111213141516171819public static void main(String[] args) &#123; ArrayList list=new ArrayList(); ArrayList&lt;String&gt; str_list=new ArrayList&lt;String&gt;(); ArrayList&lt;Integer&gt; int_list=new ArrayList&lt;Integer&gt;(); ArrayList&lt;Object&gt; obj_list=new ArrayList&lt;Object&gt;(); //对象比较 System.out.println(list == str_list); System.out.println(list == int_list); System.out.println(list == obj_list); //对象的运行时class比较 System.out.println(list.getClass() == str_list.getClass()); System.out.println(list.getClass() == int_list.getClass()); System.out.println(list.getClass() == obj_list.getClass()); &#125; 结果：123456falsefalsefalsetruetruetrue 其实上面三个很容易理解，不同对象在内存中的地址肯定是不同的，因此均为false;下面三个均为true?是的，确实为true,这就引出了朋友圈的那个问题。为什么不同的三个对象，他们的getClass是一样的，不应该是有三个不同的hashCode吗？这个其实就是泛型编译时和运行时的问题。对于泛型来说，泛型只在编译阶段有效，编译之后，集合的泛型是去泛型化的；原因：由于JVM泛型的擦除机制，在运行时JVM是不知道泛型信息的。因此：java集合中的泛型，是来约束用户的错误输入的，只在编译时有效；在回到问题最初，我们怎么才能将一个Integer对像放入上面定义的list中呢？既然集合中的泛型是编译时有效的，那我我们就可以通过绕过编译的方式进行插入。那么如何绕过编译时的校验呢？答案就是用反射；我们知道JAVA反射机制是指：“在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；这种动态获取信息以及动态调用对象方法的功能称为java语言的反射机制。OK，再来看程序： 123456789 ArrayList&lt;String&gt; str_list=new ArrayList&lt;String&gt;();//获取类信息Class c=str_list.getClass();//获取add方法Method m=c.getMethod(&quot;add&quot;, Object.class);//运行时调用add方法m.invoke(str_list, 20);//输出当前str_listSystem.out.println(str_list); 结果：1[20] 从结果可以看出，我们完成了在list中添加Integer的任务。【泛型、反射、编译时、运行时】]]></content>
      <categories>
        <category>java 基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>泛型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-排序算法思想及实现]]></title>
    <url>%2F2018%2F11%2F10%2Fleetcode%2Falg-one%2F</url>
    <content type="text"><![CDATA[排序算法主要有：插入排序，选择排序，冒泡排序，希尔排序，归并排序，快速排序，堆排序。这里的排序指的是内部排序，也就是基于内存的排序，基于内存的排序是基于大O模型的，可以使用大O模型来衡量算法的性能摘自我自己的博客园：http://www.cnblogs.com/myadmin/p/5821158.html 中的部分排序算法。 插入排序基本思想：每一步都将一个待排数据按其大小插入到已经排序的数据中的适当位置，直到全部插入完毕。 1234原始：4 3 1 21) 3 4 1 22) 1 3 4 23) 1 2 3 4 核心代码： 12345678910111213141516/** * 插入排序 */ public static int[] insertSort(int[] arr) &#123; for (int i = 1; i &lt; arr.length; i++) &#123; int index = i;// index当前扫描到的元素下标 int temp = arr[i]; // 寻找插入的位置 while (index &gt; 0 &amp;&amp; temp &lt; arr[index - 1]) &#123; arr[index] = arr[index - 1]; index--; &#125; arr[index] = temp; &#125; return arr; &#125; 选择排序基本思想：从所有序列中先找到最小的，然后放到第一个位置。之后再看剩余元素中最小的，放到第二个位置……以此类推，就可以完成整个的排序工作了。可以很清楚的发现，选择排序是固定位置，找元素。相比于插入排序的固定元素找位置，是两种思维方式。 1234567893 2 1 4 6 5初始化索引位置为0 寻找最小值所在位置交换：1 2 3 4 6 5初始化索引位置为1寻找最小值所在位置交换：1 2 3 4 6 5依次类推！ 核心代码： 123456789101112131415161718/** * 选择排序 */ public static int[] selectSort(int[] arr) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; int minVal = arr[i]; int index = i; for (int j = i + 1; j &lt; arr.length; j++) &#123;// 找到最小元素 if (arr[j] &lt; minVal) &#123; minVal = arr[j]; index = j; &#125; &#125; arr[index] = arr[i]; arr[i] = minVal; &#125; return arr; &#125; 冒泡排序基本思想：原理是临近的数字两两进行比较,按照从小到大或者从大到小的顺序进行交换。核心代码：12345678910111213141516171819/** * 冒泡排序 * * @param arr * 输入的待排数组 * @return 返回排序号的数组 */ public static int[] bubbleSort(int[] arr) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; for (int j = 1; j &lt; arr.length; j++) &#123; if (arr[j - 1] &gt; arr[j]) &#123; int temp = arr[j - 1]; arr[j - 1] = arr[j]; arr[j] = temp; &#125; &#125; &#125; return arr; &#125; 希尔排序基本思想：先取一个小于n的整数d1作为第一个增量，把文件的全部记录分组。所有距离为d1的倍数的记录放在同一个组中。先在各组内进行直接插入排序；然后，取第二个增量d2&lt;d1重复上述的分组和排序，直至所取的增量 =1( &lt; …&lt;d2&lt;d1)，即所有记录放在同一组中进行直接插入排序为止。（下图来自百度图片） 核心代码： 12345678910111213141516171819202122232425262728/** * 希尔排序 * * @author sgl * */public class ShellSort &#123; public static int[] shellSort(int[] arr) &#123; int step = arr.length / 2;// 初始步长 while (1 &lt;= step) &#123; for (int i = step; i &lt; arr.length; i++) &#123; if (arr[i] &lt; arr[i - step]) &#123; int temp = arr[i]; arr[i] = arr[i - step]; arr[i - step] = temp; &#125; &#125; step = step / 2; for (int i = 0; i &lt; arr.length; i++) &#123; System.out.print(arr[i]+&quot;,&quot;); &#125; System.out.println(); &#125; return arr; &#125; 归并排序基本思想：将待排序序列R[0…n-1]看成是n个长度为1的有序序列，将相邻的有序表成对归并，得到n/2个长度为2的有序表；将这些有序序列再次归并，得到n/4个长度为4的有序序列；如此反复进行下去，最后得到一个长度为n的有序序列。归并排序其实要做两件事：（1）“分解”——将序列每次折半划分。（2）“合并”——将划分后的序列段两两合并后排序。 核心代码：12345678910111213141516171819202122232425262728293031323334353637public static int[] sort(int[] nums, int low, int high) &#123; int mid = (low + high) / 2; if (low &lt; high) &#123; sort(nums, low, mid);// 左边 sort(nums, mid + 1, high);// 右边 merge(nums, low, mid, high);// 左右归并 &#125; return nums; &#125; public static void merge(int[] nums, int low, int mid, int high) &#123; int[] temp = new int[high - low + 1]; int i = low;// 左指针 int j = mid + 1;// 右指针 int k = 0; // 把较小的数先移到新数组中 while (i &lt;= mid &amp;&amp; j &lt;= high) &#123; if (nums[i] &lt; nums[j]) &#123; temp[k++] = nums[i++]; &#125; else &#123; temp[k++] = nums[j++]; &#125; &#125; // 把左边剩余的数移入数组 while (i &lt;= mid) &#123; temp[k++] = nums[i++]; &#125; // 把右边边剩余的数移入数组 while (j &lt;= high) &#123; temp[k++] = nums[j++]; &#125; // 把新数组中的数覆盖nums数组 for (int k2 = 0; k2 &lt; temp.length; k2++) &#123; nums[k2 + low] = temp[k2]; &#125; &#125;&#125; 快速排序基本思想：快速排序采用的思想是分治思想。快速排序是找出一个元素（理论上可以随便找一个）作为基准,然后对数组进行分区操作,使基准左边元素的值都不大于基准值,基准右边的元素值 都不小于基准值，如此作为基准的元素调整到排序后的正确位置。递归快速排序，将其他n-1个元素也调整到排序后的正确位置。最后每个元素都是在排序后的正 确位置，排序完成。所以快速排序算法的核心算法是分区操作，即如何调整基准的位置以及调整返回基准的最终位置以便分治递归。 核心代码： 123456789101112131415161718192021222324252627282930313233/** * 快速排序 * * @author sgl * */public class QuickSort &#123; static void quicksort(int n[], int left, int right) &#123; int dp; if (left &lt; right) &#123; dp = partition(n, left, right); quicksort(n, left, dp - 1); quicksort(n, dp + 1, right); &#125; &#125; static int partition(int n[], int left, int right) &#123; int pivot = n[left]; while (left &lt; right) &#123; while (left &lt; right &amp;&amp; n[right] &gt;= pivot) right--; if (left &lt; right) n[left++] = n[right]; while (left &lt; right &amp;&amp; n[left] &lt;= pivot) left++; if (left &lt; right) n[right--] = n[left]; &#125; n[left] = pivot; return left; &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于maven构建多模块化的SSM框架]]></title>
    <url>%2F2018%2F11%2F10%2Fproject-frame-maven-ssm%2F</url>
    <content type="text"><![CDATA[之前写过一篇SSM的框架整合；项目开发框架-SSM；对SSM中的一些点进行了学习记录,那篇文章也是基于maven来创建的，那么为什么又要搞一篇呢？以我当前公司项目A来说，A项目包括前台、后台子项目【前台用于对外，后台用于管理】，如果按照前一篇文章的那种方式来进行，我们就需要建立两个单独的框架来进行开发，一样的拥有一套从dmo实体类包，util包，dao包，service包以及controller包，这种结构非常的紧凑和独立，但是问题在于，我们前后台使用的是同一个库，dmo、util、dao以及service中都会存在大量重复的代码，很多基础方法无法公用；另外一个原因是，我们还需要包装一些接口向外提供服务【不局限于我们自己的这两个系统】，这样一来，我们又需要再去抽离一次service，非常不方便。因此就使用maven来构建多模块项目，对于util、dao、rpc服务接口以及service进行模块化分离，这样一来，这些模块就可以对我们自己的前后台以及外部提供一些公关的服务，避免了大量的代码重复，也方便管理。 Maven多模块项目，通过合理的模块拆分，实现代码的复用，便于维护和管理。尤其是一些开源框架，也是采用多模块的方式，提供插件集成，用户可以根据需要配置指定的模块。 构建多模块化项目基于maven构建多模块化项目主要依赖于maven可以实现父子项目的关系，子项目可以父项目的依赖Jar包，这样也方便我们去共同管理jar依赖，但是由于一个项目中毕竟会有很多人进行协同开发，在此过程中如果没有很好的约束，对于这种多模块化来说，解决jar包的冲突也很繁琐。 新建一个父工程1.创建maven项目 step1:(新建maven项目) step2:(勾选创建一个简单工程) step3:(填写工程配置：主要是打包方式要选择pom方式)点击finish，父项目就创建成功了！2.创建子项目 step1:(右击父项目-&gt;maven-&gt;New Maven Model Project) step2: step3:(一般情况下，我们项目中的util、dao、service都是可以直接分出来的，这里我们选择quickstart来构建,用于生产后面的jar包提供服务。我们的web子项目选择webapp来构建，用于配置文件、jsp文件/ftl/html/js/css等界面资源文件维护)点击finish，完成子模块的构建！构建之后的项目结构为：此时，我们的父模块中已经有了子模块的项目标识，新建的dao模块中不包括webapp此类的文件夹。那么这时就可以将我们的数据访问相关的类和接口都放在这个子模块中，如果其他项目需要使用，我们直接引入就行，引入方式如下（下面截图是从service模块引入dao模块的，这里的groupId，artifactId，version我们可以在dao的pom文件中直接复制使用）：（上面新建的过程只作为演示而用，下面的引入和上面的新建项目并非一个项目）其他的模块构建和dao的构建过程是一样的，这里就不一一构建了。源码地址在下面，解压之后，以maven项目方式导入，修改下数据库配置文件应该就可以直接运行了（当前项目基于jdk1.7写的，有的小伙伴如果用1.8的话，应该会出现jsp无法编译的一个错误）；源码附件中还有一个setting文件,阿里的，个人觉得用起来很不错，也推荐给大家! 源码地址：http://download.csdn.net/download/sinat_25518349/10124726【这个是csdn的地址，现在资源上传还必需要选择C币，小伙伴如果没有csdn账户或者C币不足，可以在文章留言区留言，留下邮箱，我发给你们】]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>ssm</tag>
        <tag>spring</tag>
        <tag>mybatis</tag>
        <tag>web</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多态、接口和抽象类]]></title>
    <url>%2F2018%2F11%2F10%2Fglmapper-bird-two%2F</url>
    <content type="text"><![CDATA[面向对象的三大特性：封装、继承、多态。从一定角度来看，封装和继承几乎都是为多态而准备的。这是我们最后一个概念，也是最重要的知识点。 多态的定义：指允许不同类的对象对同一消息做出响应。即同一消息可以根据发送对象的不同而采用多种不同的行为方式。（发送消息就是函数调用） 动态绑定 静态绑定和动态绑定这里所谓的绑定，即一个方法的调用与方法所在的类（方法主体）关联起来。 静态绑定（前期绑定）：即在程序执行前，即编译的时候已经实现了该方法与所在类的绑定，像C就是静态绑定。 java中只有static，final，private和构造方法是静态绑定，其他的都属于动态绑定，而private的方法其实也是final方法（隐式），而构造 方法其实是一个static方法（隐式），所以可以看出把方法声明为final，第一可以让他不被重写，第二也可以关闭它的动态绑定。 动态绑定（后期绑定）：运行时根据对象的类型进行绑定，java中的大多数方法都是属于动态绑定，也就是实现多态的基础。 java实现了后期绑定，则必须提供一些机制，可在运行期间判断对象的类型，并分别调用适当的方法。 也就是说，编译的时候该方法不与所在类绑定，编译器此时依然不知道对象的类型，但方法调用机制能自己去调查，找到正确的方法主体。java里实现动态绑定的是JVM. 动态绑定是实现多态的技术，是指在执行期间判断所引用对象的实际类型，根据其实际的类型调用其相应的方法。 多态的作用消除类型之间的耦合关系。即：把不同的子类对象都当作父类来看，可以屏蔽不同子类对象之间的差异，写出通用的代码，做出通用的编程，以适应需求的不断变化。 多态存在的三个必要条件一、要有继承；二、要有重写；三、父类引用指向子类对象。 多态的优点1.可替换性（substitutability）。多态对已存在代码具有可替换性。2.可扩充性（extensibility）。多态对代码具有可扩充性。增加新的子类不影响已存在类的多态性、继承性，以及其他特性的运行和操作。实际上新加子类更容易获得多态功能。3.接口性（interface-ability）。多态是超类通过方法签名，向子类提供了一个共同接口，由子类来完善或者覆盖它而实现的。4.灵活性（flexibility）。它在应用中体现了灵活多样的操作，提高了使用效率。5.简化性（simplicity）。多态简化对应用软件的代码编写和修改过程，尤其在处理大量对象的运算和操作时，这个特点尤为突出和重要。 多态的实现方式Java中多态的实现方式： 接口实现 继承父类进行方法重写 同一个类中进行方法重载。例子无论工作还是学习中，笔都是我们经常用到的工具。但是笔的种类又非常的繁多，铅笔、签字笔、水笔、毛笔、钢笔…。现在我们要对“笔”进行抽象，抽象成一个抽象父类“Pen” 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.glmapper.demo.base;/** * 抽象父类：笔 * @author glmapper */public abstract class Pen &#123; //笔的长度 private int length; //颜色 private String color; //类型 private String type; //价格 private double price; //写字 public abstract void write(String cnt); public int getLength() &#123; return length; &#125; public void setLength(int length) &#123; this.length = length; &#125; public String getColor() &#123; return color; &#125; public void setColor(String color) &#123; this.color = color; &#125; public String getType() &#123; return type; &#125; public void setType(String type) &#123; this.type = type; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; &#125; 现在有两个子类，分别是：铅笔和钢笔。 铅笔类，继承父类Pen，并重写write方法1234567891011121314151617package com.glmapper.demo.base;/** * 铅笔类 继承父类 笔（满足必要条件一：有继承【其实如果是接口的话，implement实现也是可以的】） * @author glmapper * */public class Pencil extends Pen&#123; /** * 父类的抽象方法委托子类具体实现：覆盖 */ //满足必要条件二：要有重写【当然，如果是对于write有重载也是可以的，不同的概念而已】 @Override public void write(String cnt) &#123; System.out.println(&quot;这是一只铅笔写的内容，内容是：&quot;+cnt); &#125;&#125; 钢笔类，继承父类Pen，并重写write方法 1234567891011121314package com.glmapper.demo.base;/** * 钢笔类 继承父类 笔 * @author 17070738 * */public class Fountainpen extends Pen&#123; @Override public void write(String cnt) &#123; System.out.println(&quot;这是一支钢笔写的内容，内容是：&quot;+cnt); &#125;&#125; 测试： 12345678910111213package com.glmapper.demo.base;public class MainTest &#123; public static void main(String[] args) &#123; /* Pen pen= new Pencil();*/ //必要条件三：父类引用指向子类对象。 Pen pen= new Fountainpen(); pen.write(&quot;我是一支笔&quot;); &#125;&#125; 输出结果：这是一支钢笔写的内容，内容是：我是一支笔 说明可替换性：多态对笔Pen类工作，对其他任何子类，如铅笔、钢笔，也同样工作。可扩充性：在实现了铅笔、钢笔的多态基础上，很容易增添“笔”类的多态性。 接口一个Java接口，就是一些方法特征的集合。【本文角度并非是java基础角度来说，主要是以设计模式中的应用为背景，因此对于相关定义及用法请自行学习。http://www.runoob.com/java/java-interfaces.html】我们在平时的工作中，提到接口，一般会含有两种不同的含义， 指的是java接口，这是一种java语言中存在的结构，有特定的语法和结构 指一个类所具有的方法特征的集合，是一种逻辑上的抽象。 前者叫做“java接口”，后者叫着“接口”。例如：java.lang.Runnable就是一个java接口。 为什么使用接口我们考虑一下，假如没有接口会怎么样呢？一个类总归是可以通过继承来进行扩展的，这难道不足以我们的实际应用吗？一个对象需要知道其他的一些对象，并且与其他的对象发生相互的作用，这是因为这些对象需要借住于其他对象的行为以便于完成一项工作。这些关于其他对象的知识，以及对其他对象行为的调用，都是使用硬代码写在类里面的，可插入性几乎为0。如：钢笔中需要钢笔水，钢笔水有不同的颜色：钢笔水类：1234567891011121314151617181920212223package com.glmapper.demo.base;/** * 钢笔墨水 * @author glmapper */public class PenInk &#123; //墨水颜色 private String inkColor; public String getInkColor() &#123; return inkColor; &#125; public void setInkColor(String inkColor) &#123; this.inkColor = inkColor; &#125; public PenInk(String inkColor) &#123; super(); this.inkColor = inkColor; &#125; &#125; 钢笔中持有一个墨水类的对象引用： 123456789101112131415package com.glmapper.demo.base;/** * 钢笔类 继承父类 笔 * @author 17070738 * */public class Fountainpen extends Pen&#123; //引用持有 PenInk ink =new PenInk(&quot;black&quot;); @Override public void write(String cnt) &#123; System.out.println(&quot;钢笔墨水颜色是：&quot;+ink.getInkColor()); System.out.println(&quot;这是一支钢笔写的内容，内容是：&quot;+cnt); &#125;&#125; 但是这种时候，我们需要换一种颜色怎么办呢？就必须要对Fountainpen中的代码进行修改，将创建PenInk对象时的inkColor属性进行更改；现在假如我们有一个具体的类，提供某种使用硬代码写在类中的行为；现在，要提供一些类似的行为，并且可以实现动态的可插入，也就是说，要能够动态的决定使用哪一种实现。一种方案就是为这个类提供一个抽象父类，且声明出子类要提供的行为，然后让这个具体类继承自这个抽象父类。同时，为这个抽象父类提供另外一个具体的子类，这个子类以不同的方法实现了父类所声明的行为。客户端可以动态的决定使用哪一个具体的子类，这是否可以提供可插入性呢？改进之后的代码：子类1：黑色墨水1234567891011package com.glmapper.demo.base;/** * 黑色墨水 * @author glmapper */public class BlackInk extends PenInk&#123; public BlackInk() &#123; super(&quot;black&quot;); &#125;&#125; 子类2：蓝色墨水 1234567891011package com.glmapper.demo.base;/** * 蓝色墨水 * @author glmapper */public class BlueInk extends PenInk&#123; public BlueInk() &#123; super(&quot;blue&quot;); &#125;&#125; 钢笔类引用： 123456789101112131415161718package com.glmapper.demo.base;/** * 钢笔类 继承父类 笔 * @author 17070738 * */public class Fountainpen extends Pen&#123; PenInk ink ; //通过构造函数初始化PenInk ，PenInk由具体子类来实现 public Fountainpen(PenInk ink) &#123; this.ink = ink; &#125; @Override public void write(String cnt) &#123; System.out.println(&quot;钢笔墨水颜色是：&quot;+ink.getInkColor()); System.out.println(&quot;这是一支钢笔写的内容，内容是：&quot;+cnt); &#125;&#125; 客户端调用： 12345678public static void main(String[] args) &#123; /** * 使用黑色墨水子类 */ Pen pen= new Fountainpen(new BlackInk()); pen.write(&quot;我是一支笔&quot;); &#125; 从上面代码可以看出，确实可以在简单的情况下提供了动态可插入性。 但是由于java语言是一个单继承的语言，换言之，一个类只能有一个超类，因此，在很多情况下，这个具体类可能已经有了一个超类，这个时候，要给他加上一个新的超类是不可能的。如果硬要做的话，就只好把这个新的超类加到已有的超类上面，形成超超类的情况，如果这个超超类的位置也已经被占用了，就只好继续向上移动，直到移动到类等级结构的最顶端。这样一来，对一个具体类的可插入性设计，就变成了对整个等级结构中所有类的修改。这种还是假设这些超类是我们可以控制的，如果某些超类是由一些软件商提供的，我们无法修改，怎么办呢？因此，假设没有接口，可插入性就没有了保证。 类型java接口（以及java抽象类）用来声明一个新的类型。java设计师应当主要使用java接口和抽象类而不是具体类进行变量的类型声明、参数的类型声明、方法的返还类型声明，以及数据类型的转换等。当然，一个更好的做法是仅仅使用java接口，而不要使用抽象java类来做到上面这些。在理想的情况下，一个具体java类应当只实现java接口和抽象类中声明过的方法，而不应该给出多余的方法。 类型等级结构java接口（以及抽象类）一般用来作为一个类型的等级结构的起点java的类型是以类型等级结构的方式组织起来的，在一个类型等级结构里面，一个类型可以有一系列的超类型，这时这个类型叫做其超类型的子类型。子类型的关系是传递性：类型甲是类型乙的子类型，类型乙是类型丙的子类型，那么类型甲就是类型丙的子类型。 混合类型如果一个类已经有一个主要的超类型，那么通过实现一个接口，这个类可以拥有另一个次要的超类型。这种次要的超类型就叫做混合类型。例如：在java中， TreeMap类有多个类型，它的主要类型是AbstractMap,这是一种java的聚集；而Cloneable接口则给出了一个次要类型，这个类型说明当前类的对象是可以被克隆；同时Serializable也是一个次要类型，它表明当前类的对象是可以被序列化的。而NavigableMap继承了SortedMap,因为之前说到过，子类型是可以传递的，因此对于TreeMap来说，SortedMap（或者说NavigableMap）表明这个聚集类是可以排序的。 接口的一些用法 单接口方法：接口中只有一个方法；java语言中有很多但方法接口的使用，Runnalble接口中的run（）方法。 1234567891011121314public interface Runnable &#123; /** * When an object implementing interface &lt;code&gt;Runnable&lt;/code&gt; is used * to create a thread, starting the thread causes the object&apos;s * &lt;code&gt;run&lt;/code&gt; method to be called in that separately executing * thread. * &lt;p&gt; * The general contract of the method &lt;code&gt;run&lt;/code&gt; is that it may * take any action whatsoever. * * @see java.lang.Thread#run() */ public abstract void run();&#125; 标识接口：没有任何方法和属性的接口；标识接口不对实现它的类有任何语义上的要求，仅仅是表明实现该接口的类属于一个特定的类型。上面说到的Serializable接口就是一种标识接口。 12public interface Serializable &#123;&#125; 常量接口：用java接口来声明一些常量 12345package com.glmapper.demo.base;public interface MyConstants &#123; public static final String USER_NAME=&quot;admin&quot;;&#125;; 这样一来，凡是实现这个接口的类都会自动继承这些常量，并且都可以像使用自己的常量一样，不需要再用MyConstants.USER_NAME来使用。 抽象类在java语言里面，类有两种，一种是具体类，一种是抽象类。在上面给出的代码中，使用absract修饰的类为抽象类。没有被abstract修饰的类是具体类。抽象类通常代表一个抽象概念，它提供一个继承的出发点。而具体类则不同，具体类可以被实例化，应当给出一个有逻辑实现的对象模板。由于抽象类不可以被实例化，因此一个程序员设计一个新的抽象类，一定是用来被继承的。（不建议使用具体类来进行相关的继承）。 关于代码重构假设有两个具体类，类A和类B，类B是类A的子类，那么一个比较简单的方案应该是建立一个抽象类（或者java接口），暂定为C，然后让类A和类B成为抽象类C的子类【没有使用UML的方式来绘制，请见谅哈】。 上面其实就是里氏替换原则，后面会具体介绍到的。这种重构之后，我们需要做的就是如何处理类A和类B的共同代码和共同数据。下面给出相关准则。 抽象类应当拥有尽可能多的共同代码 在一个继承等级结构中，共同的代码应当尽量向结构的顶层移动，将重复的代码从子类中抽离，放在抽象父类中，提高代码的复用率。这样做的另外一个好处是，在代码发生改变时，我们只需要修改一个地方【因为共同代码均在父类中】。 抽象类应当拥有尽可能少的数据数据的移动方向是从抽象类到具体类，也就是从继承等级的顶层到底层的移动。我们知道，一个对象的数据不论是否使用都会占用资源，因此数据应当尽量放到具体类或者继承等级结构的低端。 Has - A 与Is -A当一个类是另外一个类的角色时【我 有一个 玩具】，这种关系就不应当使用继承来描述了，这个将会在后面说到的“合成/聚合复用原则”来描述。Has - A: 我有一只笔（聚合）Is - A:钢笔是一种笔（继承） 关于子类扩展父类的责任子类应当扩展父类的职责，而不是置换掉或者覆盖掉超类的职责。如果一个子类需要将继承自父类的责任取消或者置换后才能使用的话，就很有可能说明这个子类根本不属于当前父类的子类，存在设计上的缺陷。 最后，说明下，我们在平时的工作中会经常使用的工具类，再次特地申明一下，我们也尽可能少的去从工具类进行继承扩展。 参考： 《Java与模式》电子工业出版社出版，作者：阎宏。 http://www.runoob.com/java/java-interfaces.html]]></content>
      <categories>
        <category>java 基础</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向对象的四大基础特性]]></title>
    <url>%2F2018%2F11%2F10%2Fglmapper-bird-one%2F</url>
    <content type="text"><![CDATA[按照之前的学习规划，开始进行第一部分的学习。那么今天就重新认识一下JAVA中的四大特性：抽象、封装、继承、多态 抽象学习面向对象，抽象还是很重要的。面向对象最接近我们人类的思维，你的抽象能力就是你对万物万事总结和归纳的能力。关于抽象，就是从具体事物抽出、概括出它们共同的方面、本质属性与关系等，而将个别的、非本质的方面、属性与关系舍弃，这种思维过程，称为抽象。在JAVA中表现就是使用abstract来修饰类，被abstract修饰的类成为抽象类，一般是为了为子类提供一些共有的属性和行为，不同的子类根据自身的特性再进行具体的行为实现。 封装封装是面向对象的重要原则，就是把对象的属性和行为（方法）结合为一个独立的整体，并尽可能隐藏对象的内部实现细节；在java中，对于对象的内部属性一般用private来实现隐藏，并通过set和get方法对外提供访问接口。封装实际上是一种信息隐藏技术的实现方式。 对象的数据封装特性彻底消除了传统结构方法中数据与操作分离所带来的种种问题，提高了程序的可复用性和可维护性，降低了程序员保持数据与操作内容的负担。 对象的数据封装特性还可以把对象的私有数据和公共数据分离开，保护了私有数据，减少了可能的模块间干扰，达到降低程序复杂性、提高可控性的目的。 例如：对于客观存在的人这个对象进行属性和行为抽象【此处仅仅是部分抽象】；使用private关键字来修饰人的属性，并通过对应的set和get方法对外界提供访问入口；在行为方面，通过public关键字来修饰，对外提供具体的行为描述。外界对象并不知道“人”这个对象在内部发生了什么，仅仅是通过提供的方法来获得具体的描述信息。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 人 * @author glmapper * */public class Person &#123; //姓名 private String name; //年龄 private int age; //性别 private String sex; //身高 private float high; //体重 private float weight; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getSex() &#123; return sex; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public float getHigh() &#123; return high; &#125; public void setHigh(float high) &#123; this.high = high; &#125; public float getWeight() &#123; return weight; &#125; public void setWeight(float weight) &#123; this.weight = weight; &#125; public void eat()&#123; System.out.println(&quot;吃东西&quot;); &#125; public void walk()&#123; System.out.println(&quot;走路&quot;); &#125; public void study()&#123; System.out.println(&quot;学习&quot;); &#125;&#125; 继承继承是面向对象最显著的一个特性，是从已有的类中派生出新的类，我们把它称之为子类，子类继承父类的属性和行为，并能根据自己的需求扩展出新的属性和行为，提高了代码的可复用性。 提高了代码的复用性。 让类与类之间产生了关系，给第三个特征多态提供了前提。 Java的继承通过extends关键字来实现，实现继承的类被称为子类，被继承的类称为父类(有的也称其为基类、超类)，父类和子类的关系，是一种一般和特殊的关系；子类扩展父类，将可以获得父类的全部属性和方法。 男人是人的一种，男人的特征是有胡子，因此也有剪胡子的行为【有胡子和剪胡子并非依赖关系；一个是属性，一个是行为】；但是男人继承了人这个父类，因此，男人也具有例如姓名、性别、身高、体重等属性，同时也具有父类人具有的吃饭、走路和学习的行为。123456789101112131415161718192021/** * 男人 * @author glmapper * */public class Man extends Person&#123; //胡子 private String goatee; public String getGoatee() &#123; return goatee; &#125; public void setGoatee(String goatee) &#123; this.goatee = goatee; &#125; public void shaved()&#123; System.out.println(&quot;剪胡子&quot;); &#125;&#125; 重写父类的方法：大部分的时候，子类总是以父类为基础，额外添加新的属性和方法。但有一种情况例外：子类需要重写父类的方法。例如男人吃东西比较快，女人吃东西比较慢，因此对于eat方法来说，Man可以覆盖父类的eat方法，来描述Man本身的特点。 1234@Overridepublic void eat()&#123; System.out.println(&quot;快速的吃东西&quot;);&#125; 当子父类中出现相同方法时，会先运行子类中的方法。重写的特点：方法名一样，访问修饰符权限不小于父类，返回类型一致，参数列表一致。 多态定义：指允许不同类的对象对同一消息做出响应。即同一消息可以根据发送对象的不同而采用多种不同的行为方式。（发送消息就是函数调用）实现多态的技术称为：动态绑定（dynamic binding），是指在执行期间判断所引用对象的实际类型，根据其实际的类型调用其相应的方法。从语言特点上来说，Java引用变量有两个类型：一个是编译时类型，一个是运行时类型。编译时的类型由声明该变量时使用的类型决定，运行时的类型由实际赋给该变量的对象决定。如果编译时类型和运行时类型不一致，就会出现所谓的多态（Polymorphism）。封装和继承都是为Java语言的多态提供了支撑；多态存在的三个必要条件： 要有继承； 要有重写； 父类引用指向子类对象。 具体的实现方式就是：接口实现，继承父类进行方法重写，同一个类中进行方法重载。 下一篇在说类和接口的时候再用具体的例子来描述覆盖、重载。]]></content>
      <categories>
        <category>java 基础</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[菜鸟成长系列-概述]]></title>
    <url>%2F2018%2F11%2F10%2Fglmapper-bird-overall%2F</url>
    <content type="text"><![CDATA[前言作为渣硕一枚，毕业时一线互联网公司招聘基本没有参与过，等回过神准备参加，春招都快结束了【17年毕业】；跌跌撞撞面了几家公司，虽然被虐，但是基本上都给了Offer，几番比较之后到了苏宁，选择做一枚金融码农。虽然网上关于在苏宁做IT有着不同的看法，有好有坏，褒贬不一，但就我而言，作为新人，苏宁还是给我提供了不错的工作和学习环境【入职快4个月了】。言归正传，作为一个初入职场的新人，面对很多复杂的业务场景，不同的开源技术的使用，一开始确实有点“慌”，但是随着慢慢的渗入，从能够将一个项目成功跑起来，到对着详设给代码加注释，到自己去画某一条业务线的流程图，到第一次独立完成一个需求，再到不断的去发现现有框架或者业务逻辑中的问题并去尝试优化；这个过程还是很“吃鸡”的。技术始终是来支撑业务的，熟悉产品很重要，只有深入了解了某一个业务，然后拓展出与其他业务的关系，这样才可以发现问题，找到切入点去做具体的代码优化和业务优化。所以人人都是产品经理这句话很中肯，特别是程序员，要学会和产品经理“讨价还价”。 为什么要写为什么要写，最开始的想法就是把自己工作中的问题和坑记录下来，以便于自己不会再调到坑里面去。但是后来发现跑偏了，在没有具体深入了解的情况下开始“借鉴”+“总结”。后来再去看，该不会的还是不会，这就很无奈。网上有很多什么“JAVA学习路线一览”、“数据库学习路线一览”，“Spring学习路线一览”。。。，实话实说，尝试过，但是都失败了，走不下去。比如说我想学习java集合，然后就去看，去总结，然后就会发现，线程安全和不安全在集合里面的比较很多，然后就去看线程安全相关，然后再走，就会发现从这个坑跳到了另外一个坑，一方面是没有足够的时间去研究，另一方面没有把自己的思维放进去，结果就是学到的还是很碎的东西。因此放弃别人的成功之路，回来走自己的独木桥。结合自己之前的一些技术积累和实际工作的需求，来整合。看了很多，却发现深入的不多。一开始想的是从java的Object开始写，但是当我去尝试一次之后就放弃了【其实从java基础类库学还是很不错的】；我觉得不适合我这种不按套路出牌的人，因此就给自己定了一个框，在框里学。这个框是什么呢？就是设计模式。无论是java基础类库的设计还是Spring体系的设计基本都离不开设计模式的使用，为什么说不从Object开始，不从Spring的启动开始就是因为当我顺着一条线开始走的时候，就会牵扯出无数条线，直到不知道去往哪一条开始。只有当前站在顶层去看整体的时候，才会对全局有一个把握，才能直到不同分支的关系，才能更好的学抓细节。 写什么我的想法是以设计模式为主线来贯穿，开始重新学习。【从java语言的角度】设计模式中基本上都是围绕六种设计原则来约束的，再利用JAVA中提供抽象、继承、多态提供的机制来进行具体的实现。顺着这个思路简单罗列下我自己的学习路线：因为需要使用JAVA，那么就必须先要对抽象、继承和多态有一个比较清楚的理解，因此第一部分将会从java语言本身的特色来学习，主要包括： 1.抽象，继承，多态的理解 2.类和接口 3.面向接口编程的理解OK，到此就收，第一部分就把后面设计模式中我们需要用的方式的基础定了个基调。第二部分就直接进入设计模式范畴之内： 1.设计原则 2.创建型 3.结构型 4.行为型在学习某个具体的设计模式的时候会结合java语言中某些类库来讨论，穿插学习；第二部分之后，对于设计模式、设计模式在java中的应用、Spring的顶层设计以及java中的一些类库会有一个大体的掌握。这个部分会需要很长时间，会涉及到的知识点会很多，有点慌。第三部分开始数据结构，为什么是数据结构而不是并发或者数据库呢。一方面在java中很多关于并发的问题都会涉及到集合的使用，集合内部就依赖于不同的数据结构；数据库方面，如果都不清楚数据库是怎么存数据的，就不可能知道怎么去优化；如果都不知道数据库中的数据的存储结构是什么，又怎么能知道数据是怎么存的呢？上面三个部分结束之后，关于java差不多也就结束了。那么作为一个程序员，对于网络理解和开源技术的使用才是真正快速解决实际问题和吃饭问题的根本。spring、mybatis、redis，struts2,hibernate,以及相关rpc框架。关于java虚拟机这个东西不会单独的写了，感觉写不出来，等有了实际的经验积累之后再去谈吧。。。结束语其实我们每个人每天都会有想法，好坏不说，要去试试，这篇文章写完之后我也不知道自己能走到哪个部分，但是还是回去尝试走一走。我也不知道这种学习的“野路子”适合不适合，但是就现在【2017.11.5 11:59】看，我觉得对我是可以的。]]></content>
      <categories>
        <category>架构之路</category>
      </categories>
      <tags>
        <tag>规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目开发框架-SSM]]></title>
    <url>%2F2018%2F11%2F10%2Fproject-frame-ssm%2F</url>
    <content type="text"><![CDATA[1.Spring无需多言，作为开源届数一数二的典例，项目开发中无处不在；核心IOC容器，用来装载bean（java中的类）-用Spring的IOC容器来管理Bean的生命周期，有了这样一种机制，我们就可以不用在代码中去重复的做new操作。aop，面向切面编程，spring中最主要的是用于事务方面的使用。 2.Spring MVC作用于web层，相当于controller，与struts中的action一样，都是用来处理用户请求的。同时，相比于struts2来说，更加细粒度，它是基于方法层面的，而struts是基于类层面的。 3.MyBatisMyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生信息，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录。[来自：http://www.mybatis.org/mybatis-3/zh/index.html] 他人总结 Hibernate功能强大，数据库无关性好，O/R映射能力强，如果你对Hibernate相当精通，而且对Hibernate进行了适当的封装，那么你的项目整个持久层代码会相当简单，需要写的代码很少，开发速度很快，非常爽。 Hibernate的缺点就是学习门槛不低，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡取得平衡，以及怎样用好Hibernate方面需要你的经验和能力都很强才行。 MYBATIS入门简单，即学即用，提供了数据库查询的自动对象绑定功能，而且延续了很好的SQL使用经验，对于没有那么高的对象模型要求的项目来说，相当完美。 MYBATIS的缺点就是框架还是比较简陋，功能尚有缺失，虽然简化了数据绑定代码，但是整个底层数据库查询实际还是要自己写的，工作量也比较大，而且不太容易适应快速数据库修改。4.SSM框架整合本项目将以购物为背景，主要包括商品信息及库存【因为想顺便学习一下事务的处理】、订单信息。下面将从数据库创建、项目结构说明、配置文件、业务代码等方面进行一步步说明。4.1 数据库创建1.商品表123456CREATE TABLE `goods` ( `goods_id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;商品ID&apos;, `goodsname` varchar(100) NOT NULL COMMENT &apos;商品名称&apos;, `number` int(11) NOT NULL COMMENT &apos;商品库存&apos;, PRIMARY KEY (`goods_id`)) ENGINE=InnoDB AUTO_INCREMENT=1000 DEFAULT CHARSET=utf8 COMMENT=&apos;商品表&apos; 初始化表数据12INSERT INTO `goods` (`goods_id`, `goodsname`, `number`)VALUES (1001, &apos;SN卫衣&apos;, 15) 2.订单表12345678CREATE TABLE `orderinfo` ( `order_id` varchar(20) NOT NULL COMMENT &apos;订单编号&apos;, `goods_id` bigint(18) NOT NULL COMMENT &apos;商品ID&apos;, `user_id` bigint(10) NOT NULL COMMENT &apos;用户ID&apos;, `order_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &apos;下单时间&apos; , PRIMARY KEY (`order_id`), INDEX `idx_order_id` (`order_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&apos;订单表&apos; OK，至此表结构及初始化数据构建完成，下面说下基于Mavan的项目结构。 项目结构说明因为项目是使用maven来管理jar包的，先来贴一下，pom.xml的配置 pom.xml为了避免学习小伙伴崇尚拿来主义【也就是去除了xmlns之类的东西】，这里只放项目依赖的jar包的dependencies；本案例将本着“需则用”的原则，避免在网上看到的各种乱七八糟的依赖都丢进来的情况，造成资源浪费和干扰阅读。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134&lt;dependencies&gt; &lt;!-- 单元测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 1.日志 slf4j--&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 2.数据库连接驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.37&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- 2.数据库连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 3.MyBatis 以及 spring-mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 4.Servlet 相关依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;taglibs&lt;/groupId&gt; &lt;artifactId&gt;standard&lt;/artifactId&gt; &lt;version&gt;1.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 5.Spring --&gt; &lt;!-- 5.1 Spring核心 ：core bean context --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;4.1.7.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;4.1.7.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.1.7.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 5.2 Spring jdbc依赖，事务依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.1.7.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;4.1.7.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 5.3 Spring web依赖&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;4.1.7.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.1.7.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 5.4 Spring test --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;4.1.7.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 6.redis客户端:Jedis【不使用的话可以直接去除】 --&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff-core&lt;/artifactId&gt; &lt;version&gt;1.0.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff-runtime&lt;/artifactId&gt; &lt;version&gt;1.0.8&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 7.工具类 --&gt; &lt;dependency&gt; &lt;groupId&gt;commons-collections&lt;/groupId&gt; &lt;artifactId&gt;commons-collections&lt;/artifactId&gt; &lt;version&gt;3.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; *项目结构图 src/test/java：用于junit的测试类 src/main/java: dao:数据库处理 service:业务处理 enums:项目枚举 mapper:dao中方法对应mybatis映射文件，Sql就在这里面 web：控制器，controller entity:项目中的实体类，如：商品类和订单类 配置文件 jdbc.properties1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://serverName:port/dbname?useUnicode=true&amp;characterEncoding=utf8jdbc.username=[填写自己的数据库用户名]jdbc.password=[填写自己的数据库登录密码] logback.xml这里直接用的是控制台输出，如果是生产环境，可以根据具体的需求进行配置。 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration debug=&quot;true&quot;&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; ch.qos.logback.classic.encoder.PatternLayoutEncoder --&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level=&quot;debug&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot; /&gt; &lt;/root&gt;&lt;/configuration&gt; mybatis-config这里主要是MyBaties全局配置文件的配置，可以将一些类的别名、主键自增配置、驼峰命名规则配置等。 12345678910111213&lt;configuration&gt; &lt;!-- 配置全局属性 --&gt; &lt;settings&gt; &lt;!-- 使用jdbc的getGeneratedKeys获取数据库自增主键值 --&gt; &lt;setting name=&quot;useGeneratedKeys&quot; value=&quot;true&quot; /&gt; &lt;!-- 使用列别名替换列名 默认:true --&gt; &lt;setting name=&quot;useColumnLabel&quot; value=&quot;true&quot; /&gt; &lt;!-- 开启驼峰命名转换:Table&#123;create_time&#125; -&gt; Entity&#123;createTime&#125; --&gt; &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot; /&gt; &lt;/settings&gt;&lt;/configuration&gt; spring 相关配置文件为了更加清晰的了解spring各个组件的作用，这里将数据源的配置、事务配置和视图解析器的配置分开来。spring-dao.xml这里面主要就是spring配置整合mybatis的具体过程，具体包括：1.引入数据库配置文件2.配置数据源【数据库连接池】3.配置SqlSessionFactory对象4.配置扫描Dao接口包，动态实现Dao接口，注入到spring容器中 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;!-- 1.配置数据库相关参数properties的属性：$&#123;url&#125; --&gt;&lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot; /&gt;&lt;!-- 2.数据库连接池 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;!-- 配置连接池属性 --&gt; &lt;property name=&quot;driverClass&quot; value=&quot;$&#123;jdbc.driver&#125;&quot; /&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;$&#123;jdbc.url&#125;&quot; /&gt; &lt;property name=&quot;user&quot; value=&quot;$&#123;jdbc.username&#125;&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot; /&gt; &lt;!-- c3p0连接池的私有属性 --&gt; &lt;property name=&quot;maxPoolSize&quot; value=&quot;30&quot; /&gt; &lt;property name=&quot;minPoolSize&quot; value=&quot;10&quot; /&gt; &lt;!-- 关闭连接后不自动commit --&gt; &lt;property name=&quot;autoCommitOnClose&quot; value=&quot;false&quot; /&gt; &lt;!-- 获取连接超时时间 --&gt; &lt;property name=&quot;checkoutTimeout&quot; value=&quot;10000&quot; /&gt; &lt;!-- 当获取连接失败重试次数 --&gt; &lt;property name=&quot;acquireRetryAttempts&quot; value=&quot;2&quot; /&gt;&lt;/bean&gt;&lt;!-- 3.配置SqlSessionFactory对象 --&gt;&lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;!-- 注入数据库连接池 --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt; &lt;!-- 配置MyBaties全局配置文件:mybatis-config.xml --&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis-config.xml&quot; /&gt; &lt;!-- 扫描entity包 使用别名 --&gt; &lt;property name=&quot;typeAliasesPackage&quot; value=&quot;com.glmapper.framerwork.entity&quot; /&gt; &lt;!-- 扫描sql配置文件:mapper需要的xml文件 --&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;com.glmapper.framerwork.mapper/*.xml&quot; /&gt;&lt;/bean&gt;&lt;!-- 4.配置扫描Dao接口包，动态实现Dao接口，注入到spring容器中 --&gt;&lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;!-- 注入sqlSessionFactory --&gt; &lt;property name=&quot;sqlSessionFactoryBeanName&quot; value=&quot;sqlSessionFactory&quot; /&gt; &lt;!-- 给出需要扫描Dao接口包 --&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.glmapper.framerwork.dao&quot; /&gt;&lt;/bean&gt; spring-service实际的开发过程中事务一般都是在service层进行操作。因此用一个单独的spring-service.xml来进行事务的相关的配置 12345678910 &lt;!-- 扫描service包下所有使用注解的类型 --&gt;&lt;context:component-scan base-package=&quot;com.glmapper.framerwork.service&quot; /&gt;&lt;!-- 配置事务管理器 --&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!-- 注入数据库连接池 --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;&lt;/bean&gt;&lt;!-- 配置基于注解的声明式事务 --&gt;&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot; /&gt; spring-web.xml配置SpringMVC；需要说明一下，一般我们在实际的开发过程中，会配置json2map解析。这里没有用到就不贴出来，读者可以自行网上搜索一波。 1234567891011121314151617&lt;!-- 1.开启SpringMVC注解模式 --&gt;&lt;mvc:annotation-driven /&gt;&lt;!-- 2.静态资源默认servlet配置 (1)加入对静态资源的处理：js,css,图片等 (2)允许使用&quot;/&quot;做整体映射 --&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!-- 3.配置视图解析器ViewResolver --&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;viewClass&quot; value=&quot;org.springframework.web.servlet.view.JstlView&quot; /&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/jsp/&quot; /&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot; /&gt; &lt;/bean&gt; &lt;!-- 4.扫描web相关的bean --&gt; &lt;context:component-scan base-package=&quot;com.glmapper.framerwork.web&quot; /&gt; web.xml 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!-- 编码过滤器 --&gt; &lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;async-supported&gt;true&lt;/async-supported&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!-- Spring监听器 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 防止Spring内存溢出监听器 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.util.IntrospectorCleanupListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 配置DispatcherServlet --&gt; &lt;servlet&gt; &lt;servlet-name&gt;mvc-dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 配置springMVC需要加载的配置文件 spring-dao.xml,spring-service.xml,spring-web.xml Mybatis - &gt; spring -&gt; springmvc --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/spring-*.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;mvc-dispatcher&lt;/servlet-name&gt; &lt;!-- 默认匹配所有的请求 --&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 至此，所有的配置文件结束，下面将进行具体的代码环节 业务代码这里mapper中的xml文件就不贴了，自行脑补。。。。 实体类：包括商品和订单 商品类1234567891011121314151617181920212223242526272829/** * 商品信息类 * @author glmapper * */public class Goods &#123; private long goodsId;// 商品ID private String goodsName;// 商品名称 private int number;// 商品库存 public long getGoodsId() &#123; return goodsId; &#125; public void setGoodsId(long goodsId) &#123; this.goodsId = goodsId; &#125; public String getGoodsName() &#123; return goodsName; &#125; public void setGoodsName(String goodsName) &#123; this.goodsName = goodsName; &#125; public int getNumber() &#123; return number; &#125; public void setNumber(int number) &#123; this.number = number; &#125;&#125; 订单类 1234567891011121314151617181920212223242526272829303132333435/** * 订单信息类 * @author glmapper * */public class OrderInfo &#123; private String orderId;//订单ID private long goodsId;//商品ID private long userId;//用户ID private Date orderTime;//下单时间 public String getOrderId() &#123; return orderId; &#125; public void setOrderId(String orderId) &#123; this.orderId = orderId; &#125; public long getGoodsId() &#123; return goodsId; &#125; public void setGoodsId(long goodsId) &#123; this.goodsId = goodsId; &#125; public long getUserId() &#123; return userId; &#125; public void setUserId(long userId) &#123; this.userId = userId; &#125; public Date getOrderTime() &#123; return orderTime; &#125; public void setOrderTime(Date orderTime) &#123; this.orderTime = orderTime; &#125;&#125; 商品dao 12345678910111213141516171819202122232425262728public interface GoodsDao &#123; /** * 通过ID查询单件商品信息 * * @param id * @return */ Goods queryById(long id); /** * 查询所有商品信息 * * @param offset 查询起始位置 * @param limit 查询条数 * @return */ List&lt;Goods&gt; queryAll(@Param(&quot;offset&quot;) int offset, @Param(&quot;limit&quot;) int limit); /** * 减少商品库存 * * @param bookId * @return 如果影响行数等于&gt;1，表示更新的记录行数 */ int reduceNumber(long goodsId);&#125; 订单dao 1234567891011121314151617public interface OrderInfoDao &#123; /** * 插入订单记录 * * @param OrderInfo orderInfo * @return 插入的行数 */ int insertOrderInfo(OrderInfo orderInfo); /** * 通过主键查询订单记录，返回订单实体 * @param orderId * @return */ OrderInfo queryByOrderId(String orderId);&#125; 下单服务接口orderService 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566@Service(&quot;orderService&quot;)public class OrderServiceImpl implements OrderService &#123; //log生成器 private Logger logger = LoggerFactory.getLogger(OrderServiceImpl.class); // 注入dao依赖【商品dao，订单dao】 @Autowired private GoodsDao goodsDao; @Autowired private OrderInfoDao orderInfoDao; @Override public Goods getById(long goodsId) &#123; // TODO Auto-generated method stub return goodsDao.queryById(goodsId); &#125; @Override public List&lt;Goods&gt; getList(int offset,int limit) &#123; // TODO Auto-generated method stub return goodsDao.queryAll(offset, limit); &#125; @Override @Transactional public OrderInfo buyGoods(long goodsId, long userId) &#123; //扣减库存，插入订单 =一个事务 如果失败则执行回滚 try &#123; // 减库存 int update = goodsDao.reduceNumber(goodsId); if (update &lt;= 0) &#123;// 库存不足 throw new NoNumberException(&quot;no number&quot;); &#125; else &#123; // 执行预约操作 OrderInfo orderInfo=new OrderInfo(); orderInfo.setGoodsId(goodsId); orderInfo.setUserId(userId); orderInfo.setOrderTime(new Date()); String orderId=getRandomOrderId(goodsId); orderInfo.setOrderId(orderId); int insert = orderInfoDao.insertOrderInfo(orderInfo); if (insert &lt;= 0) &#123;// 重复预约 throw new RepeatAppointException(&quot;repeat appoint&quot;); &#125; else &#123;// 预约成功 return orderInfo; &#125; &#125; &#125; catch (Exception e) &#123; //这里可以丰富下具体的返回信息 logger.error(&quot;下单失败&quot;); &#125; return null; &#125; private String getRandomOrderId(long goodsId) &#123; SimpleDateFormat dateFormater = new SimpleDateFormat(&quot;yyyyMMddhhmmss&quot;); String prefix=dateFormater.format(new Date()); String goodsIdStr=goodsId+&quot;&quot;; String temp=&quot;&quot;; for (int i = 0; i &lt; 6; i++) &#123; Random random=new Random(goodsIdStr.length()-1); temp+=goodsIdStr.charAt(random.nextInt()); &#125; return prefix+temp; &#125;&#125; OK，至此所有核心代码及配置文件罗列完毕；【mapper中的xml和具体的controller就不贴了，相信大家对这个也不陌生。本文主要意图在于梳理下自己学习中的一些点，SSM框架在实际的应用开发中还会有很多其他的开源技术结合进来，如：quartz,redis等。当前本文的列子就是一个空壳子，以备参考吧】]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>ssm</tag>
        <tag>spring</tag>
        <tag>mybatis</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA基础知识系列---进程、线程安全]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-thread-base%2F</url>
    <content type="text"><![CDATA[1.1 临界区保证在某一时刻只有一个线程能访问数据的简便方法，在任意时刻只允许一个线程对资源进行访问。如果有多个线程试图同时访问临界区，那么在有一个线程进入后，其他所有试图访问临界区的线程将被挂起，并一直持续到进入临界区的线程离开。临界区在被释放后，其他线程可以继续抢占，并以此达到用原子方式操作共享资源的目的 1.2 互斥量互斥量和临界区很相似，只能拥有互斥对象的线程才能具有访问资源的权限，由于互斥对象只有一个，因此就决定了任何情况下次共享资源都不会同时被多个线程所访问。当前占据资源的线程在任务处理完后应将拥有的互斥对象交出，以便其他线程在获得后可以访问资源。互斥量比临界区复杂，因为使用互斥不仅仅能够在同一应用程序不同线程中实现资源的安全共享，而且可以在不同应用程序的线程之间实现对资源的安全共享。 1.3 管程/信号量管程和信号量是同一个概念。指一个互斥独占锁定的对象或称为互斥体。在给定的时间，仅有一个线程可以获得管程。当一个线程需要锁定，他必须进入管程。所有其他的试图进入已经锁定的管程的线程必须挂起直到第一个线程退出管程。这些其他的线程被称为等待线程。一个拥有管程的线程如果愿意的话可以再次进入相同的管程（可重入性） 1.4 CAS操作CAS操作（compare and swap）CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则返回V。这是一种乐观锁的思路，它相信在它修改之前，没有其它线程去修改它；而Synchronized是一种悲观锁，它认为在它修改之前，一定会有其它线程去修改它，悲观锁效率很低。下面来看一下AtomicInteger是如何利用CAS实现原子性操作的。 1.5 重排序编译器和处理器为了提高性能，而在程序执行时会对程序进行重排序。他的出现是为了提高程序的并发度。从而提高性能；但是对于多线程程序，重排序可能会导致程序执行的结果不是我们需要的结果，重排序分为编译器和处理器俩个方面。而处理器重排序包括指令级重排序和内存重排序。 小节在java中，所有的变量（实例字段，静态字段，构成数组的元素，不包括局部变量和方法参数）都存储在主内存中，内个线程都有自己的工作内存，线程的工作内存保存被线程使用到的变量的主内存副本拷贝。线程对变量的所有操作都必须在工作内存中进行，为不能直接读写主内存的变量。不同线程之间也不恩能够直接访问对方工作内存中的变量，线程间比变量值的传递通过主内存来完成。 JAVA中线程安全相关关键字及类主要包括：synchronized，Volitile，ThreadLocal，Lock，Condition 2.1 Volitile作用： 1）保证了心智能立即存储到主内存才，每次使用前立即从主内存中刷新 2）禁止指令重排序优化 Volitile关键字不能保证在多线程环境下对共享数据的操作的正确性，可以使用在自己状态改变之后需要立即通知所有线程的情况下，只保证可见性，不保证原子性。即通过刷新变量值确保可见性。 Java中synchronized和final也能保证可见性 synchronized：同步快通过变量锁定前必须清空工作内存中的变量值，重新从主内存中读取变量值，解锁前必须把变量值同步回主内存来确保可见性。 final:被final修饰的字段在构造器中一旦被初始化完成，并且构造器没有把this引用传递进去，那么在其他线程中就能看见final字段的值，无需同步就可以被其他线程正确访问。 2.2 synchronized把代码块声明为synchronized，有俩个作用，通常是指改代码具有原子性和可见性。如果没有同步机制提供的这种可见性，线程看到的共享比那里可能是修改前的值或不一致的值，这将引发许多严重问题。 原理：当对象获取锁是，他首先是自己的高速缓存无效，这样就可以保证直接从主内存中装入变量，同样在对象释放锁之前，他会刷新其高速缓存，强制使已做的任何更改都出现在主内存中，这样会保证在同一个锁上同步的俩个线程看到在synchronized块内修改的变量的相同值。 synchronized释放由JVM自己管理。 存在的问题： 1）无法中断一个正在等待获得锁的线程 2）无法通过投票得到锁，如果不想等待下去，也就没法得到锁 3）同步还需要锁的释放只能在与获得锁所在的堆栈帧相同的堆栈中进行，多数情况下，这没问题（而且与一场处理交互的很好），但是，确实存在一些非块结构的锁定更适合情况。 2.3 LockLock是有JAVA编写而成的，在java这个层面是无关JVM实现的。包括：ReentrantLock，ReadWriteLock。其本质都依赖于AbstractQueueSynchronized类。Lock提供了很多锁的方式，尝试锁，中断锁等。释放锁的过程由JAVA开发人员自己管理。 就性能而言，对于资源冲突不多的情况下synchronized更加合理，但如果资源访问冲突多的情况下，synchronized的性能会快速下降，而Lock可以保持平衡。 2.4 conditionCondition将Object监视器方法（wait，notify,notifyall）分解成截然不同的对象，以便通过这些对象与任意Lock实现组合使用，为每个对象提供多个等待set(wait-set),，其中Lock替代了synchronized方法和语句的使用，condition替代了Object监视器方法的使用。Condition实例实质上被你绑定到一个锁上。要为特定Lock实例获得Condition实例，请使用其newCondition（）方法。 2.5 ThreadLock线程局部变量。 变量是同一个，但是每个线程都使用同一个初始值，也就是使用同一个变量的一个新的副本，这种情况下TreadLocal就非常有用。 应用场景：当很多线程需要多次使用同一个对象，并且需要该对象具有相同初始值的时候，最适合使用TreadLocal。 事实上，从本质上讲，就是每个线程都维持一个MAP，而这个map的key就是TreadLocal,而值就是我们set的那个值，每次线程在get的时候，都从自己的变量中取值，既然从自己的变量中取值，那就肯定不存在线程安全的问题。总体来讲，TreadLocal这个变量的状态根本没有发生变化。它仅仅是充当了一个key的角色，另外提供给每一个线程一个初始值。如果允许的话，我们自己就能实现一个这样的功能，只不过恰好JDK就已经帮助我们做了这个事情。 使用TreadLocal维护变量时，TreadLocal为每个使用该变量的线程提供独立地变量副本，所以每一个线程都可以独立地改变自己的副本，而不会英语其他线程所对应的副本。从线程的角度看，目标变量对象是线程的本地变量，这也是类名中Local所需要表达的意思。 TreadLocal的四个方法： void set(Object val),设置当前线程的线程局部变量的值 Object get（）返回当前线程所对用的线程局部变量。 void remove() 将当前线程局部变量的值删除，目的是为了减少内存的占用，线程结束后，局部变量自动被GC Object initValue() 返回该线程局部变量的初始值，使用protected修饰，显然是为了让子类覆盖而设计的。 线程安全的实现方式3.1 互斥同步在多线程访问的时候，保证同一时间只有一条线程使用。 临界区，互斥量，管程都是同步的一种手段。 java中最基本的互斥同步手段是synchronized，编译之后会形成monitorenter和monitorexit这俩个字节码指令，这俩个字节码都需要一个reference类型的参数来指明要锁定和解锁的对象，还有一个锁的计数器，来记录加锁的次数，加锁几次就要同样解锁几次才能恢复到无锁状态。 java的线程是映射到操作系统的原生线程之上的，不管阻塞还是唤醒都需要操作系统的帮助完成，都需要从用户态转换到核心态，这是很耗费时间的，是java语言中的一个重量级的操作，虽然虚拟机本身会做一点优化的操作，比如通知操作系统阻塞之前会加一段自旋等待的过程，避免频繁切换到核心态。 3.2 非阻塞同步互斥和同步最主要的问题就是阻塞和唤醒所带来的性能的问题，所以这通常叫阻塞同步（悲观的并发策略）.随着硬件指令集的发展，我们有另外的选择：基于冲突检测的乐观并发策略，通俗讲就是先操作，如果没有其他线程争用共享的数据，操作就成功，如果有，则进行其他的补偿（最常见的就是不断的重试）。这种乐观的并发策略许多实现都不需要把线程先挂起，这种同步操作被称为非阻塞同步。 3.3 无同步部分代码天生就是线程安全的，不需要同步。 1）可重入代码：纯代码，具有不依赖存储在堆上的数据和公用的系统资源，用到的状态量都由参数中传入，不调用非可重入的方法等特征，它的返回结果是可以预测的。 2）线程本地存储：把共享数据的可见性范围限制在同一个线程之内，这样就无需同步也能保证线程之间不出现数据争用问题。可以通过java.lang.TreadLocal类来实现线程本地存储的功能。]]></content>
      <categories>
        <category>java 基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>线程</tag>
        <tag>thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊 Web 核心 - Servlet]]></title>
    <url>%2F2018%2F11%2F10%2Ftopic-servlet%2F</url>
    <content type="text"><![CDATA[Servlet实际上是ServerApplet–小服务程序或服务连接器，用Java编写的服务器端程序，主要功能在于交互式地浏览和修改数据，生成动态Web内容。与常用的协议，如DNS，TCP/IP，HTTP类似，Servlet是作为一整套规范存在的；同时作为J2EE标准的一部分，定义了javaweb开发的标准。Servlet制定了java处理WEB请求的一系列标准，我们只需要按照标准规定的去做就可以了。实际上，无论是Struts2的FilterDispatcher还是SpringMvc的DispatcherServlet,其底层都是通过实现Sevlet或者Servlet类型的扩展【如：GenericServlet】来实现的。 1.Servlet接口下图为Servlet3.1中的结构图：因为Servlet是以规范的方式存在的，实际上就是定义一系列规范接口。在Servlet接口中，主要包括以下几个接口： 1)init方法是在容器启动时被容器调用，且只会被调用一次； 2)getServletConfig方法用于获取ServletConfig； 3)service方法用于处理一个具体的请求 4)getServletInfo方法用于获取Servlet相关的信息：版权等。 5)destroy方法用来销毁一个Servlet，和init一样，只会被调用一次，一般在服务器关闭时用于释放一些资源。 init方法调用时会接受一个ServletConfig类型的参数，用于初始化Servlet，由容器传入。ServletConfig，顾名思义，其包含了Serlvet的配置信息。通常情况下，我们在web.xml文件中定义Serlvet时，会通过init-param标签来进行参数配置。在Springmvc的配置中，通常通过以下方式来配置参数： 2.ServletConfig接口1)getServletName用于获取Servlet的名字，也就是我们在web.xml中定义的servlet-name2)getServletContext返回ServletContext，代表我们当前应用本身3)getInitParameter用于获取init-param配置的参数4)getInitParameterNames用于获取所有init-param配置名字的集合ServletContext和ServletConfig最常见的使用就是传递初始化参数。来看下spring中的contextConfigServlet的参数配置通过context-param配置的contextConfigLocation配置到了ServletContext中，再通过Servlet下的init-param配置的contextConfigLocation配置到ServletConfig中,在Servlet中可以通过getInitParameter方法获取具体的信息。 3.GenericServletGenericServlet是Servlet的默认实现，代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package javax.servlet;import java.io.IOException;import java.io.Serializable;import java.util.Enumeration;import java.util.ResourceBundle;public abstract class GenericServlet implements Servlet, ServletConfig, Serializable&#123; private static final String LSTRING_FILE = &quot;javax.servlet.LocalStrings&quot;; private static ResourceBundle lStrings = ResourceBundle.getBundle(&quot;javax.servlet.LocalStrings&quot;); private transient ServletConfig config; public void destroy() &#123; &#125; public String getInitParameter(String name) &#123; ServletConfig sc = getServletConfig(); if (sc == null) &#123; throw new IllegalStateException(lStrings.getString(&quot;err.servlet_config_not_initialized&quot;)); &#125; return sc.getInitParameter(name); &#125; public Enumeration getInitParameterNames() &#123; ServletConfig sc = getServletConfig(); if (sc == null) &#123; throw new IllegalStateException(lStrings.getString(&quot;err.servlet_config_not_initialized&quot;)); &#125; return sc.getInitParameterNames(); &#125; public ServletConfig getServletConfig() &#123; return this.config; &#125; public ServletContext getServletContext() &#123; ServletConfig sc = getServletConfig(); if (sc == null) &#123; throw new IllegalStateException(lStrings.getString(&quot;err.servlet_config_not_initialized&quot;)); &#125; return sc.getServletContext(); &#125; public String getServletInfo() &#123; return &quot;&quot;; &#125; public void init(ServletConfig config) throws ServletException &#123; this.config = config; init(); &#125; public void init() throws ServletException &#123; &#125; public void log(String msg) &#123; getServletContext().log(getServletName() + &quot;: &quot; + msg); &#125; public void log(String message, Throwable t) &#123; getServletContext().log(getServletName() + &quot;: &quot; + message, t); &#125; public abstract void service(ServletRequest paramServletRequest, ServletResponse paramServletResponse) throws ServletException, IOException; public String getServletName() &#123; ServletConfig sc = getServletConfig(); if (sc == null) &#123; throw new IllegalStateException(lStrings.getString(&quot;err.servlet_config_not_initialized&quot;)); &#125; return sc.getServletName(); &#125;&#125; 从其继承和实现关系来看，GenericServlet主要做了3件事：1.实现了ServletConfig接口，这样我们就可以直接调用ServletConfig里面的方法； GenericServlet实现了ServletConfig，可以在需要的时候直接调用ServletConfig中的方法，不需要再先获取ServletConfig对象；比如，获取ServletContext的时候可以直接调用getServletContext,而无需调用getServletConfig().getServletContext(),但是实际上，其底层的内部实现还是在内部还是进行了getServletConfig().getServletContext()的调用。2.提供了无参的init方法 GenericServlet实现了Servlet的init（ServletConfig config）方法，在里面将config设置给了其内部变量config，然后调用了无参的init方法；此方法可以在子类中通过覆盖它来完成初始化工作。这种方式具有的有点包括以下几点： a.config设置为内部属性，这样可以在ServletConfig的接口方法中直接调用Config的相应方法来执行； b.我们在写Serlvet的时候可以不用再关心Config，只需要执行自己的初始化逻辑即可 c.在重写init方法时，不需要再调用super.init(config)。3.提供了Log方法 GenericServlet提供了2个log方法，一个用于记录日志，一个用于记录异常。其具体的实现是通过传给ServletConfig的日志实现的。 GenericServlet是与具体协议无关的。 4.HttpServletHttpServlet是基于Http协议实现的Servlet的基类，写Servlet时直接继承HttpServlet即可,不需要再重头实现Servlet接口，SpringMvc中的dispatcherServlet就是HttpServlet的子类。 HttpServlet是与Http协议相关的，HttpServlet处理请求主要是通过重写父类的service方法来完成具体的请求处理的。在service方法中首先是将ServletRequest和ServletResponse转换成HttpServletRequest和HttpServletResponse，然后根据请求的不同路由到不同的处理过程中去【处理方法就是我们常见的doXXX的方法。最常见的就是doGet和doPost】]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码系列：Spring的启动过程]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-series-starting%2F</url>
    <content type="text"><![CDATA[Spring对于程序员说来说都不陌生；作为一个强大的开源技术，帮助我们能够更好的进行项目的开发与维护。直接进入主题吧。Spring的启动过程实际上就是Ioc容器初始化以及载入Bean的过程；本文主要是学习记录下前半部分（Ioc容器的初始化），新手上路，如有错误，请指正！1.从配置文件说起 12345678&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt; &lt;/listener&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; 在一般的WEB项目中，项目的启动一般是从web.xml文件的载入开始的。如果我们的项目中使用了Spring，那么你肯定会在你的web.xml文件中看到上面的配置。Spring正是通过ContextLoaderListener监听器来进行容器初始化的。下面通过代码进行分析。 2.Spring容器加载的三步走 step1:创建一个WebApplicationContext step2:配置并且刷新Bean step3：将容器初始化到servlet上下文中 3.WebApplicationContext的创建过程1public class ContextLoaderListener extends ContextLoader implements ServletContextListener 从ContextLoaderListener的定义可以看出，该监听器继承了ContextLoader，并且重写了ServletContextListener中的contextInitialized和contextDestroyed方法。 在contextInitialized中，通过调用父类（ContextLoader）的initWebApplicationContext方法进行容器创建：1234@Overridepublic void contextInitialized(ServletContextEvent event) &#123; initWebApplicationContext(event.getServletContext());&#125; 下面来看initWebApplicationContext的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public WebApplicationContext initWebApplicationContext(ServletContext servletContext) &#123; //1：判断当前容器是否存在，如果存在则报容器已经存在的异常信息 if (servletContext.getAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE) != null) &#123; throw new IllegalStateException( "Cannot initialize context because there is already a root application context present - " + "check whether you have multiple ContextLoader* definitions in your web.xml!"); &#125; Log logger = LogFactory.getLog(ContextLoader.class); //下面这个日志就是我们经常在启动Spring项目时看到的日志信息: //Initializing Spring root WebApplicationContext //Root WebApplicationContext: initialization started servletContext.log("Initializing Spring root WebApplicationContext"); if (logger.isInfoEnabled()) &#123; logger.info("Root WebApplicationContext: initialization started"); &#125; long startTime = System.currentTimeMillis(); try &#123; // Store context in local instance variable, to guarantee that // it is available on ServletContext shutdown. //如果当前容器为null,则创建一个容器，并将servletContext上下文作为参数传递进去， if (this.context == null) &#123; this.context = createWebApplicationContext(servletContext); &#125; //判断当前容器是否为可配置的，只有是Configurable的容器，才能进行后续的配置 if (this.context instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) this.context; if (!cwac.isActive()) &#123; // The context has not yet been refreshed -&gt; provide services such as // setting the parent context, setting the application context id, etc if (cwac.getParent() == null) &#123; // The context instance was injected without an explicit parent -&gt; // determine parent for root web application context, if any. ApplicationContext parent = loadParentContext(servletContext); cwac.setParent(parent); &#125; //三步走中的第二步：配置并且刷新当前容器 configureAndRefreshWebApplicationContext(cwac, servletContext); &#125; &#125; //将配置并且刷新过的容器存入servlet上下文中，并以WebApplicationContext的类名作为key值 servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); ClassLoader ccl = Thread.currentThread().getContextClassLoader(); if (ccl == ContextLoader.class.getClassLoader()) &#123; currentContext = this.context; &#125; else if (ccl != null) &#123; currentContextPerThread.put(ccl, this.context); &#125; if (logger.isDebugEnabled()) &#123; logger.debug("Published root WebApplicationContext as ServletContext attribute with name [" + WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE + "]"); &#125; if (logger.isInfoEnabled()) &#123; long elapsedTime = System.currentTimeMillis() - startTime; logger.info("Root WebApplicationContext: initialization completed in " + elapsedTime + " ms"); &#125; //返回创建好的容器 return this.context; &#125; catch (RuntimeException ex) &#123; logger.error("Context initialization failed", ex); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, ex); throw ex; &#125; catch (Error err) &#123; logger.error("Context initialization failed", err); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, err); throw err; &#125; &#125; 下面我们在看下是如何创建WebApplicationContext的 12345678910protected WebApplicationContext createWebApplicationContext(ServletContext sc) &#123; //首先来确定context是由什么类定义的，并且判断当前容器是否为可配置的 Class&lt;?&gt; contextClass = determineContextClass(sc); if (!ConfigurableWebApplicationContext.class.isAssignableFrom(contextClass)) &#123; throw new ApplicationContextException("Custom context class [" + contextClass.getName() + "] is not of type [" + ConfigurableWebApplicationContext.class.getName() + "]"); &#125; //创建可配置的上下文容器 return (ConfigurableWebApplicationContext) BeanUtils.instantiateClass(contextClass); &#125; 最后来看下determineContextClass这个方法 12345678910111213141516171819202122232425protected Class&lt;?&gt; determineContextClass(ServletContext servletContext) &#123; //首先从web.xml中查看用户是否自己定义了context String contextClassName = servletContext.getInitParameter(CONTEXT_CLASS_PARAM); //如果有，则通过反射创建实例 if (contextClassName != null) &#123; try &#123; return ClassUtils.forName(contextClassName, ClassUtils.getDefaultClassLoader()); &#125; catch (ClassNotFoundException ex) &#123; throw new ApplicationContextException( "Failed to load custom context class [" + contextClassName + "]", ex); &#125; &#125; /*如果没有，则去defaultStrategies里面取【defaultStrategies是Propertites类的/对象，在ContextLoader中的静态代码块中初始化的；具体可看下下面的图像】；默认容器是XmlWebApplicationContext*/ else &#123; contextClassName = defaultStrategies.getProperty(WebApplicationContext.class.getName()); try &#123; return ClassUtils.forName(contextClassName, ContextLoader.class.getClassLoader()); &#125; catch (ClassNotFoundException ex) &#123; throw new ApplicationContextException( "Failed to load default context class [" + contextClassName + "]", ex); &#125; &#125; &#125; 总的来说就是：Spring的web工程首先回去检查用户是否自己定义了context，如果有就采用；如果没有就使用Spring默认的。defaultStrategies初始化： 至此，容器创建完成。下面是整个过程的一个流程图（有疏漏，回头补一个时序图）：]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA中的关键字]]></title>
    <url>%2F2018%2F11%2F10%2Fjava-key-word%2F</url>
    <content type="text"><![CDATA[Java中关键字有51个，另外还有俩个保留字；因此总共有53个。本文只将java中的关键字进行罗列和简单介绍，对于部分关键字的理解如sychronized将在其他文章中单独分析。 保留字保留字是指预留的关键字； const用于修饰字段或局部变量的声明；被const修饰的字段或局部变量的值是常数，不能被修改 goto指定跳转到标签，找到标签后，程序将处理从下一行开始的命令。 关键字 数据类型 访问修饰符 类、接口定义相关 流程控制与判断相关 包 变量\类\接口\方法修饰符 异常处理 枚举和断言]]></content>
      <categories>
        <category>java 基础</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一道算法题：无重复字符的最长子串]]></title>
    <url>%2F2018%2F11%2F10%2Fleetcode%2Fleetcode5%2F</url>
    <content type="text"><![CDATA[题目给定一个字符串，找出不含有重复字符的 最长子串 的长度。 示例： 给定 “abcabcbb” ，没有重复字符的最长子串是 “abc” ，那么长度就是3。 给定 “bbbbb” ，最长的子串就是 “b” ，长度是1。 给定 “pwwkew” ，最长子串是 “wke” ，长度是3。请注意答案必须是一个子串，”pwke” 是 子序列 而不是子串。 方案1思路： 字符对应的数字作为下标 初始化一个255的boolean作为所有可能出现的字符对应的存在可能性，不存在重复的均为false，存在重复的，则对应的下标置为true。 两个指针进行移动，前指针先不动，后指针移动并根据当前字符对应整数下标是否为false或者true进行判断。如果是false，则表示没有重复，则指针向后移动一位；如果为true，表示存在重复，则后指针停止移动，并计算当前字串长度，且将boolean数组重置，第一个指针向前移动一位，后指针指向当前前指针。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int len = 0 ; if (s==null || s.length()== 0 ) &#123; return 0; &#125; if (s.length() == 1)&#123; return 1; &#125; int firstPoint = 0; int nextPoint = 0; boolean[] exist=new boolean[255]; while (nextPoint &lt; s.length()&amp;&amp;firstPoint &lt;s.length())&#123; int currMax = 0; int index = s.charAt(nextPoint)-0; while (exist[index] == false&amp;&amp;nextPoint&lt;s.length())&#123; exist[s.charAt(nextPoint)-0] = true; nextPoint++; if (nextPoint &lt; s.length())&#123; index = s.charAt(nextPoint)-0; &#125; &#125; currMax = Math.max(currMax,nextPoint-firstPoint); firstPoint++; nextPoint=firstPoint; len = Math.max(len,currMax); for (int i = 0 ; i &lt; 255 ; i++) &#123; exist[i] = false; &#125; &#125; return len; &#125;&#125; 方案2思路： 以一个hashmap作为辅助，map的key存储的是字符，value存储的是该字符当前的位置，首先设置一个头指针，指向字符串开头，那么从开始遍历字符串，如果map当中不包含这个字符，那么用这个字符当前所在的位置减去头指针的位置，然后与最大长度做比较，选打的成为最大长度，然后把当前字符的以及位置放入map，以abba为例，头指针指向a，当前为a，那么长度为1，map。put（‘a’,0）,当前为b，那么长度为2，map.put(‘b’,1)，如果说map中存在当前字符，那么把头指针指向，头指针当前的位置与map中存储该字符位置的下一个位置当中的较大者，成为新的头指针位置，比如当走到第二个b的时候，那么头指针原来是0，当前map中存放b的位置是1，那么头指针指向2，所以长度为1，比最大长度小不进行替换，最后将当前的字符及位置放入map，现在是map.put(‘b’,2)，然后走到了a，那么当前map中a的位置是0，那么它的下一个位置是1，与当前头指针位置2相比，小于当前头指针的位置，那么头指针不跟新，所以长度为2，与最大长度相等，所以不替换，最后求出最大长度为2. 12345678910111213141516171819public static int lengthOfLongestSubstring(String s) &#123; Map&lt;Character,Integer&gt; map=new HashMap&lt;Character,Integer&gt;(); int maxLength=0; int now=0; for(int i=0;i&lt;s.length();i++)&#123; if(map.containsKey(s.charAt(i)))&#123; now=Math.max(map.get(s.charAt(i))+1,now); if((i-now+1)&gt;maxLength)&#123; maxLength=i-now+1; &#125; &#125;else&#123; if((i-now+1)&gt;maxLength)&#123; maxLength=i-now+1; &#125; &#125; map.put(s.charAt(i), i); &#125; return maxLength; &#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一道算法题：求两个排序数组的中位数]]></title>
    <url>%2F2018%2F11%2F10%2Fleetcode%2Fleetcode4%2F</url>
    <content type="text"><![CDATA[题目有两个大小为 m 和 n 的排序数组 nums1 和 nums2 。 请找出两个排序数组的中位数并且总的运行时间复杂度为 O(log (m+n)) 。 示例1: 1234nums1 = [1, 3]nums2 = [2]中位数是 2.0 示例2: 12345nums1 = [1, 2]nums2 = [3, 4]中位数是 (2 + 3)/2 = 2.5 归并&amp;topK问题 方案1这个思路就是对于两个有序数组进行合并，合并到一个大的有序的数组中去，然后求合并后数组的中位数。下面代码中使用的是归并排序的方式，对于两个有序数组进行归并排序的。从复杂度的角度来说可以满足题目的要求，但是还是存在一些问题，主要是怎么能够使得时间复杂度变成O{MIN(nums1.length,nums2.leng)}。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class Solution &#123; public double findMedianSortedArrays(int[] nums1, int[] nums2) &#123; double result = 0.0d; int[] nums = new int[nums1.length+nums2.length]; int num1index = 0; int num2index = 0; int index = 0; if (nums1.length == 0 &amp;&amp; nums2.length == 0)&#123; return result; &#125; if (nums1.length == 0)&#123; return getResult(nums2); &#125; if (nums2.length == 0)&#123; return getResult(nums1); &#125; while(num1index &lt; nums1.length &amp;&amp; num2index &lt; nums2.length)&#123; if (nums1[num1index] &lt; nums2[num2index])&#123; nums[index]= nums1[num1index]; num1index++; &#125;else&#123; nums[index]= nums2[num2index]; num2index++; &#125; index++; &#125; while (num1index &lt; nums1.length)&#123; nums[index] = nums1[num1index++]; index++; &#125; while (num2index &lt; nums2.length)&#123; nums[index] = nums2[num2index++]; index++; &#125; if (nums.length%2==0) &#123; result = (nums[nums.length/2]+nums[nums.length/2-1])/2.0; &#125; else&#123; result = nums[nums.length/2]; &#125; return result; &#125; private double getResult(int[] nums)&#123; double result = 0.0D; if (nums.length%2==0) &#123; result = (nums[nums.length/2]+nums[nums.length/2-1])/2.0; &#125; else&#123; result = nums[nums.length/2]; &#125; return result; &#125;&#125; 方案2求两个排序数组的中位数 假设nums1.length = m, nums2.length = n; m &lt; n; 若(m + n) % 2 == 0, 表示两数组之和为偶数，应该是有两个中位数，因此最终结果为第9行的代码所示。否则，结果为第7行的代码所示。 为了使得方法的统一，在最初时，对数组进行处理，统一使得传进方法的短数组为nums1，即第14行代码所示。 如果len1-start1 == 0,则表示nums1已经全部加入前k个了，则第k个为nums2[k -1]; 在方法findKth（）中的k是一直变化的，初始时，k为两个数组中排序之后的第k个数的位置；k在方法中的真正含义为“还需要找到多少个数才能达到k个”；因此假设nums1.length ==0;,此时len1-start1 == 0, 则中位数就是nums2[k - 1],即在nums1中找到了0个数，还需要找k个数，第k个数就是nums[k - 1]; 如果k == 1,则表示前k-1小的数已经找过了，则第k个数肯定是nums1[start1]和nums2[start2]中较小的那个数。 下面接着就是常规的情况：即nums1中包含一部分k,nums2中也包含一部分的k,因此就从每个数组的k/2那里开始比较（也相当于每次都会有一半的数被加入前k个，因此时间复杂度为O（log(m + n)））：采用p1和p2分别记录当前nums1和nums2需要比较的那个位，由于nums1比较短，因此有可能2/k的位置已经超出了nums1的长度，因此nums1还需要做特殊处理，即第19行代码所示；由于p1做了特殊处理，那p2也就要做特殊处理。总之，start1~p1和start2~p2的和一定为k。1）若nums1[p1 - 1] &lt; nums[p2 - 1],则表明【start1, p1)之间的值在前k个数中；2）若nums[p1 - 1] &gt; nums2[p2- 1],则表明【start2, p2)之间的值在前k个数中；3）若两值相等，则表明【start1, p1)+【start2， p2）的个数为k,则结果直接返回其中一个即可。为什么比较的p1和p2的前一个位的数，而不是p1和p2位置的数呢？ 举例说明：假设start1== start2 == 0, 则p1 = Math.min(len1, k / 2); p2 = k - p1,即p1 + p2 == k;；假设p1 = 5, p2 = 7;, 则k = 12; 在数组中nums[5]其实是第6个数，nums[7]其实是第8个数，所以我们比较的是nums1[p1 - 1]与nums2[p2 - 1]的值； 1234567891011121314151617181920212223242526272829public class Solution &#123; public double findMedianSortedArrays(int[] nums1, int[] nums2) &#123; int len1 = nums1.length; int len2 = nums2.length; int size = len1 + len2; if(size % 2 == 1) return findKth(nums1, 0, len1, nums2, 0, len2, size / 2 + 1); else return (findKth(nums1, 0, len1, nums2, 0, len2, size / 2) + findKth(nums1, 0, len1, nums2, 0, len2, size / 2 + 1)) /2; &#125; public double findKth(int[] nums1, int start1, int len1, int[] nums2, int start2, int len2, int k) &#123; if(len1 - start1 &gt; len2 -start2) // 传进来的时候统一让短的数组为nums1 return findKth(nums2, start2, len2, nums1, start1, len1, k); if(len1 - start1 == 0) // 表示nums1已经全部加入前K个了，第k个为nums2[k - 1]; return nums2[k - 1]; if(k == 1) return Math.min(nums1[start1], nums2[start2]); // k==1表示已经找到第k-1小的数，下一个数为两个数组start开始的最小值 int p1 = start1 + Math.min(len1 - start1, k / 2); // p1和p2记录当前需要比较的那个位 int p2 = start2 + k - p1 + start1; if(nums1[p1 - 1] &lt; nums2[p2 - 1]) return findKth(nums1, p1, len1, nums2, start2, len2, k - p1 + start1); else if(nums1[p1 - 1] &gt; nums2[p2 -1]) return findKth(nums1, start1, len1, nums2, p2, len2, k - p2 + start2); else return nums1[p1 - 1]; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一道算法题：最长回文子串]]></title>
    <url>%2F2018%2F11%2F10%2Fleetcode%2Fleetcode3%2F</url>
    <content type="text"><![CDATA[##题目给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 长度最长为1000。 示例: 12345输入: &quot;babad&quot;输出: &quot;bab&quot;注意: &quot;aba&quot;也是有效答案 示例: 123输入: &quot;cbbd&quot;输出: &quot;bb&quot; 思路一开始是想用最笨的方法来解的，也就是找出所有的字串，然后再对所有的子串进行回文检测，并记录长度。这种方式时间复杂度可想而知，O(n)O(n)O(n)=O(n^3)。所以这种肯定是不能满足我们要求的。 ok，那我们来分析一下这个问题，先把这个问题特殊化； 假如输入的字符串长度就是1 那么这个字符串的最长回文串就是它自己，长度就是1 假如字符串长度为2，它要是回文串的化，就需要两个字符是相等的。 即：s[i] == s[j] 且i-j=1(此处假定i是较大索引位置) 那么对于i－j&gt;1的情况下呢？是不是只要满足下面的条件就可以了： 即:s[i] == s[j]&amp;&amp;s[i-1] == s[j+1] 其实这种思路就是动态规划。关于动态规划的理论性文字就不码了，有兴趣的小伙伴阔以自行学习。下面就针对这个问题码一下代码： 12345678910111213141516171819202122232425262728293031323334353637public String longestPalindrome(String s) &#123; // 长度为1，返回当前串 if (s.length()==1)&#123; return s; &#125; //长度为2并且两个字符相等则返回 if (s.length()==2&amp;&amp;s.charAt(0)==s.charAt(1))&#123; return s; &#125; //用于标记isLongestPalindrome[j][i]即从j到i是否是回文串； //如isLongestPalindrome[1][5]＝＝true则表示字符串索引位置从1到5的子串是回文串。 boolean[][] isLongestPalindrome = new boolean[s.length()][s.length()]; //最长回文串初始最大为0 int maxlen = 0; //对应的maxlen的开始索引位置 int beginIndex = 0; //对应的maxlen的结束索引位置 int lastIndex = 0; for (int i=0;i&lt;s.length();i++)&#123; int j=i; while(j&gt;=0)&#123; //满足上述的第三个条件，即当前s.charAt(i)==s.charAt(j)并 //且s[j＋1到i－1]也是回文串 if (s.charAt(i)==s.charAt(j)&amp;&amp;(i-j&lt;2||isLongestPalindrome[j+1][i-1]))&#123; isLongestPalindrome[j][i]=true; if (maxlen &lt; i-j+1) &#123; beginIndex = j; lastIndex = i+1; maxlen = i-j+1; &#125; &#125; j--; &#125; &#125; return s.substring(beginIndex,lastIndex);&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一道算法题：颠倒整数]]></title>
    <url>%2F2018%2F11%2F10%2Fleetcode%2Fleetcodeone%2F</url>
    <content type="text"><![CDATA[题目给定一个范围为 32 位 int 的整数，将其颠倒。 例 1: 12输入: 123输出: 321 例 2: 12输入: -123输出: -321 例 3: 12输入: 120输出: 21 注意:假设我们的环境只能处理 32 位 int范围内的整数。根据这个假设，如果颠倒后的结果超过这个范围，则返回 0。 方案这个题目其实挺简单的；思路如下： 判断输入的数字是否大于最大整数，其实这里没有必要判断，因为如果参数输入大于最大整数的话会直接报错。 将整数转换成字符串 判断是否是负数，这个依据就是判断字符串中是否存在‘－’ 从后向前开始遍历，注意的是必须后向遍历且初始为0的情况下保持继续向前迭代。 如果转换之后的值大于最大整数，则会导致string转int失败，抛出异常，那么我们直接在这把异常捕获，并且返回0（偷懒一波，丷）1234567891011121314151617181920212223242526272829303132333435363738394041424344 public int reverse(int x) &#123; int result = 0; if (x &gt;Integer.MAX_VALUE)&#123; return 0 ; &#125; String s =String.valueOf(x); int len = 0; if (s!=null&amp;&amp;s.length()!=0&amp;&amp;s.charAt(0)=='-')&#123; len = 1; &#125;else if(s.length() == 1)&#123; return x; &#125; int lastp = s.length()-1; boolean isStart = true; String ints = ""; while( lastp &gt;= len)&#123; if (isStart &amp;&amp; s.charAt(lastp)=='0')&#123; while (s.charAt(lastp)=='0')&#123; lastp--; &#125; isStart = false; &#125; if (isStart)&#123; isStart = false; &#125; ints = ints+s.charAt(lastp); lastp--; &#125; try&#123; result = Integer.parseInt(ints); &#125;catch (NumberFormatException e)&#123; return 0; &#125; return len==0?result:result*(-1);&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于 rpc 的整理和理解]]></title>
    <url>%2F2018%2F11%2F10%2Fseiri-rpc%2F</url>
    <content type="text"><![CDATA[架构演变 所有的界面和服务均在同一个进程下 基于mvc的视图与服务分离，但是实际上还是在一个应用系统中，只不过在功能层次上划分的更加细致 粒度更细，对于不同的功能服务进行切分，并进行单独的部署 面向服务的架构，将应用程序的不同功能单元（称为服务）通过这些服务之间定义良好的接口和契约联系起来 微服务 此处不支持图片展示，自行脑补！！！ 随着业务量和用户量的增加，架构也是从单一系统走向分布式系统，我能想到的是，这种架构的演变主要解决的问题在于： 通过业务模块的拆分，使得每个模块的职责更加清晰，但是模块的职责边界的划分往往也是很疼头的事情。 细致的划分使得项目在管理上面会更加方面，从代码的角度来说，开发和维护的成本也会降低，不会因为一个bug去跑整个项目了。 提高了系统的容错率，单一系统如果宕机那就真的gg了，另外就是，单个环节出现问题也会导致项目无法正常运行（比如数据库出问题了）。对于分布式系统来说，一般都会使用冗余的方式来提高可用性，个人理解就是可以提供多个一样的服务，它们之间可以进行切换。 分布式系统带来的问题一个是成本，硬件成本，运维成本都会增加。 rpc简介及常用的rpc框架随着集中式架构向分布式架构的转变，应用系统之间的服务调用与通讯问题成为了首要解决的需求。 而RPC 的主要目标就是为了让构建分布式计算（应用）变得更加简单，在提供强大的远程调用能力时不损失本地调用的语义简洁性。 为实现该目标，RPC 框架需提供一种透明调用机制让使用者不必显式的区分本地调用和远程调用。 如下代码： 123456789101112131415 @Autowiredprivate GlRpcAgent glRpcAgent; //rpc代理/** * @param param 此处约定参数以Map键值对的形式传递 */@Overridepublic List&lt;OrderInfo&gt; queryOrdersByUserId(Map&lt;String, Object&gt; param) &#123; //创建远程调用代理（远程服务的类的全限定名） OrderConsumeAgent orderConsumer=glRpcAgent.getAgent(&quot;com.glmapper.rpc.interface.OrderConsumeInterface&quot;); //通过代理获取返回结果 此处getOrders为远程服务器上的com.glmapper.rpc.interface.OrderConsumeInterface接口中的方法，param为参数 Map&lt;String,Object&gt; resultMap=(Map)orderConsumer.call(&quot;getOrders&quot;,param); //解析返回结果（远程方法同样以Map集合的方式放回） List&lt;OrderInfo&gt; orders = parseResultMap(resultMap); return orders;&#125; 为什么要以全限定名来获取呢，这个我们将会在后面来说。 什么是RPCIn distributed computing a remote procedure call (RPC) is when a computer program causes a procedure (subroutine) to execute in another address space(commonly on another computer on a shared network), which is coded as if it were a normal (local) procedure call, without the programmer explicitly coding the details for the remote interaction.RPC 的全称是 Remote Procedure Call 是一种进程间通信方式。 它允许程序调用另一个进程上（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的调用代码基本相同。 从定义中可以得知，RPC主要来解决三件事情： 进程间通讯 提供和本地方法调用一样的调用机制 屏蔽程序员对远程调用的细节实现 首先是进程间的通信问题，对于分布式环境，rpc能够帮助我们解决不同服务器之间的通信及数据传输问题，即做好方法调用到数据的转换，然后借助网络进行数据传递；rpc客户端向rpc服务端发起远程服务调用，通过请求的封装，参数的封装，序列化、编码、约定协议传输、解析请求、处理请求、封装返回消息数据、在进行返回数据的序列化、编码、在通过网络返回给客户端。再者是提供和本地方法调用一样的调用机制，为什么这么说，对于业务系统来说，我们更多的关注点在于如何解决实际的业务需求问题，而不想花更多的时间和心思在诸如上述过程中关于网络传输及编解码过程，因此对于rpc来说，需要将这些编解码、协议约定、网络传输等进行一个整体的封装，然后只向业务系统提供最简单的调用方式。最后一个屏蔽程序员对远程调用的细节实现，其实也就是第二点中提到的那些功能的封装，我们不用去关系rpc到底是如何实现的，也不用关心它是如何运作的，对于业务开发人员来说，通过约定的方式进行类似于本地方法调用的形式来调用远程服务接口就可以了。那么如何实现透明化的远程调用呢？什么样的内部封装才能让我们觉得像以本地调用方式调用远程服务呢？对于java来说就是使用代理。java代理有两种方式：1） jdk 动态代理（接口代理）；2）cglib代理（子类代理）。尽管字节码生成方式实现的代理更为强大和高效，但代码不易维护，大部分公司实现RPC框架时还是选择动态代理方式。这部分也将会在后续的章节中展开来说。 RPC基本原理上面说到，rpc需要对一些远程调用的内部实现进行封装。我们说到有以下几个点： 序列化 编解码 协议 网络 从发起远程调用到接收到数据返回结果，大致过程是： 1）服务消费方（client）调用以本地调用方式调用服务；2）client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体；3）client stub找到服务地址，并将消息发送到服务端；4）server stub收到消息后进行解码；5）server stub根据解码结果调用本地的服务；6）本地服务执行并将结果返回给server stub；7）server stub将返回结果打包成消息并发送至消费方；8）client stub接收到消息，并进行解码；9）服务消费方得到最终结果。 那么rpc就相当于将step2-step8的步骤进行了封装。下面借用一张网上的图片来帮助我们理解这个过程。 RPC模型对于上图，我们进行进一步的拆解得到（来自网络）： RPC 服务端通过 RpcServer 去暴露服务接口，而客户端通过 RpcClient 去获取服务接口。客户端像调用本地方法一样去调用远程接口方法，RPC 框架提供接口的代理实现，实际的调用将委托给代理 RpcProxy。代理封装调用信息并将调用转交给 RpcInvoker 去实际执行。在客户端的 RpcInvoker 通过连接器 RpcConnector 去维持与服务端的通道 RpcChannel，并使用 RpcProtocol 执行协议编码（encode）并将编码后的请求消息通过通道发送给服务端。RPC 服务端接收器 RpcAcceptor接收客户端的调用请求，同样使用 RpcProtocol 执行协议解码（decode）。解码后的调用信息传递给 RpcProcessor 去控制处理调用过程，最后再委托调用给 RpcInvoker 去实际执行并返回调用结果。 通过上述分析可知，这里面包括以下核心组件： 用于暴露服务接口的RpcServer 用于发现服务接口的RpcClient 远程接口的代理实现RpcProxy 负责协议编解码的RpcProtocol（实际的rpc框架中一般会提供多种不同的实现） 网络连接器（之前看过一篇文章说9个组件，对于咱们这个来说，部分模块可以集成在client和server中） 常见的RPC框架目前常见的分布式RPC框架有以下几个： dubbo阿里巴巴公司开源的一个Java高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 Spring框架无缝集成 motan新浪微博开源的一个Java 框架。它诞生的比较晚，起于2013年，2016年5月开源。Motan 在微博平台中已经广泛应用，每天为数百个服务完成近千亿次的调用。 rpcxGo语言生态圈的Dubbo， 比Dubbo更轻量，实现了Dubbo的许多特性，借助于Go语言优秀的并发特性和简洁语法，可以使用较少的代码实现分布式的RPC服务。 gRPCGoogle开发的高性能、通用的开源RPC框架，主要面向移动应用开发并基于HTTP/2协议标准而设计，基于ProtoBuf(Protocol Buffers)序列化协议开发，且支持众多开发语言。本身它不是分布式的，所以要实现上面的框架的功能需要进一步的开发。 thriftApache的一个跨语言的高性能的服务框架 RPC与MQMQ(message queue)消息队列，从某种程度上来说，同样可以实现RPC的功能。从功能特点上来说，MQ可以把消息存储，而RPC不行。关于MQ和RPC做了以下简单的对比，如下图所示： 总结本文对RPC的基本原理、特点以及基本组件进行了简单的说明，让我们可以对RPC有一个基本的了解。关于常见的RPC框架也做了基本认识，对于这些优秀的框架，我们在实现我们自己RPC时可以借鉴一下这些架构里的一些模式以及技术。最后说明了下为什么我们会在分布式架构中要使用RPC而不是MQ，对于MQ来说，在处理同步调用无法满足实际的生产需求，而RPC才更加适合分布式应用的实际需要。]]></content>
      <categories>
        <category>rpc</category>
      </categories>
      <tags>
        <tag>rpc</tag>
        <tag>dubbo</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于 Mybatis 中 SQL 语句的整理]]></title>
    <url>%2F2018%2F11%2F10%2Fseiri-mybatis%2F</url>
    <content type="text"><![CDATA[随着业务的发展，越来越多的应用系统都从一个大的系统分拆成多个小的系统，各个系统之间通过一定的通信协议进行数据交换。这样就会导致一些小的应用系统自己不用去进行数据库的操作，只需要进行一些rpc调用或者缓存就可以拿到数据进行展示。我之前参与的一个项目就是这样的情况，而我也是将近7个多月的时间没有写过一行SQL。 近期参与的一个项目的数据大多都市基于数据库来进行数据交互的，所以免不了的要写大量的SQL，所以本篇就总结一下一些SQL的基本写法，以备后用。 建表12345CREATE TABLE IF NOT EXISTS `user_test` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '自增长id', `user_name` varchar(128) NOT NULL COMMENT '用户名', PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='用户表'; 查询 简单的查询 123&lt;select id="queryUserByName" resultMap="userMap" parameterType="java.lang.String"&gt; SELECT * FROM user_test WHERE user_name = #&#123;userName&#125; &lt;/select&gt; 需要注意的是如果这里不指定parameterType，则默认会识别处理；如果指定了类型，则传入的值就需要和当前指定的类型保持一致，不然就会出现数据类型转换异常。 简单分页查询 1234567&lt;select id="queryUsersList" resultMap="userMap"&gt; SELECT * FROM user_test WHERE 1=1 &lt;if test="keyword != null and keyword != ''" &gt; AND user_name LIKE concat('%',#&#123;keyword&#125;,'%') &lt;/if&gt; LIMIT #&#123;currentPage&#125;,#&#123;pageSize&#125;&lt;/select&gt; left join app_info表和app_verion表分别存储的是应用信息和应用版本信息。现在要根据appId和versionId查出一个应用的具体信息【包括信息信息和版本信息】 12345678&lt;select id=&quot;getAppDetail&quot; resultMap=&quot;appDeatilMap&quot;&gt; select m.id id, m.app_name appName, n.version version, from app_info m LEFT JOIN app_version n ON m.id = n.app_id where m.id = #&#123;appId&#125; and n.id = #&#123;versionId&#125; &lt;/select&gt; 查询条件是list 123456789101112131415&lt;select id=&quot;queryAppByAppNames&quot; resultMap=&quot;AppMap&quot; parameterType=&quot;java.util.List&quot;&gt; select a.app_name appName, b.version version from starter_info a,starter_version b where a.id = b.app_id and a.id in ( select id from app_info where app_name in &lt;foreach collection=&quot;list&quot; item=&quot;item&quot; index=&quot;index&quot; open=&quot;(&quot; close=&quot;)&quot; separator=&quot;,&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; )&lt;/select&gt; 更新 简单的更新 1234567&lt;update id="updateApp" parameterType="java.util.List"&gt; UPDATE app_info SET app_name = #&#123;appName&#125; WHERE app_id = #&#123;appId&#125;&lt;/update&gt; 批量更新 有这样一个需求，把 app_info表中id 为1，2，3的app的app_name改为appName1，appName2，appName3; 使用 case ..when ..then 这样的语法结构来完成： case 是当前的条件，when表示条件值，then后面是当前目前更新字段的值； 下面的说明：当前id=#{item.appId}时,app_name=#{item.appName} 1234567891011&lt;update id="updateApps" parameterType="java.util.List"&gt; UPDATE app_info set app_name = &lt;foreach collection="applList" item="item" index="index" separator=" " open="case ID" close="end"&gt; when #&#123;item.appId,jdbcType=INTEGER&#125; then #&#123;item.appName,jdbcType=INTEGER&#125; &lt;/foreach&gt; where id in &lt;foreach collection="appList" index="index" item="item" separator="," open="(" close=")"&gt; #&#123;item.appId,jdbcType=INTEGER&#125; &lt;/foreach&gt;&lt;/update&gt; OK，现在于这样的需要： 根据应用类型的不同，更新不同的运行环境配置； 123456789101112131415161718192021222324252627282930313233&#123; [ &#123; &quot;appType&quot;:&quot;applet&quot;, &quot;cpu&quot;:5, &quot;memory&quot;:4, &quot;card&quot;:3, &quot;nums&quot;:2, &quot;network&quot;:1, &quot;isInUse&quot;:1 &#125;, &#123; &quot;appType&quot;:&quot;bs&quot;, &quot;cpu&quot;:5, &quot;memory&quot;:4, &quot;card&quot;:3, &quot;nums&quot;:2, &quot;network&quot;:1, &quot;isInUse&quot;:1 &#125;, &#123; &quot;appType&quot;:&quot;cs&quot;, &quot;cpu&quot;:5, &quot;memory&quot;:4, &quot;card&quot;:3, &quot;nums&quot;:2, &quot;network&quot;:1, &quot;isInUse&quot;:1 &#125;, //有几个放几个 ]&#125; trim属性说明 1.prefix,suffix 表示在trim标签包裹的部分的前面或者后面添加内容 2.如果同时有prefixOverrides,suffixOverrides 表示会用prefix,suffix覆盖Overrides中的内容。 3.如果只有prefixOverrides,suffixOverrides 表示删除开头的或结尾的xxxOverides指定的内容。 12345678910111213141516171819202122232425262728293031323334353637383940&lt;update id=&quot;updateBatchApp&quot; parameterType=&quot;java.util.List&quot;&gt; UPDATE app_info &lt;trim prefix=&quot;set&quot; suffixOverrides=&quot;,&quot;&gt; &lt;trim prefix=&quot;cpu = case&quot; suffix=&quot;end,&quot;&gt; &lt;foreach collection=&quot;modelList&quot; item=&quot;item&quot; index=&quot;index&quot;&gt; &lt;if test=&quot;item != null&quot;&gt; when app_type =#&#123;item.appType&#125; then #&#123;item.cpu&#125; &lt;/if&gt; &lt;/foreach&gt; &lt;/trim&gt; &lt;trim prefix=&quot;memory = case&quot; suffix=&quot;end,&quot;&gt; &lt;foreach collection=&quot;modelList&quot; item=&quot;item&quot; index=&quot;index&quot;&gt; &lt;if test=&quot;item != null&quot;&gt; when app_type =#&#123;item.appType&#125; then #&#123;item.memory&#125; &lt;/if&gt; &lt;/foreach&gt; &lt;/trim&gt; &lt;trim prefix=&quot;card = case&quot; suffix=&quot;end,&quot;&gt; &lt;foreach collection=&quot;modelList&quot; item=&quot;item&quot; index=&quot;index&quot;&gt; when app_type =#&#123;item.appType&#125; then #&#123;item.card&#125; &lt;/foreach&gt; &lt;/trim&gt; &lt;trim prefix=&quot;nums = case&quot; suffix=&quot;end,&quot;&gt; &lt;foreach collection=&quot;modelList&quot; item=&quot;item&quot; index=&quot;index&quot;&gt; when app_type =#&#123;item.appType&#125; then #&#123;item.nums&#125; &lt;/foreach&gt; &lt;/trim&gt; &lt;trim prefix=&quot;network = case&quot; suffix=&quot;end,&quot;&gt; &lt;foreach collection=&quot;modelList&quot; item=&quot;item&quot; index=&quot;index&quot;&gt; when app_type =#&#123;item.appType&#125; then #&#123;item.network&#125; &lt;/foreach&gt; &lt;/trim&gt; &lt;trim prefix=&quot;is_in_use = case&quot; suffix=&quot;end,&quot;&gt; &lt;foreach collection=&quot;modelList&quot; item=&quot;item&quot; index=&quot;index&quot;&gt; when app_type =#&#123;item.appType&#125; then #&#123;item.isInUse&#125; &lt;/foreach&gt; &lt;/trim&gt; &lt;/trim&gt; where app_id = #&#123;appId&#125;&lt;/update&gt; 关于性能问题没做研究，之前看过关于不同更新语句写法的一篇性能的分析，大家有兴趣可以看下：批量更新数据两种方法效率对比 删除 简单删除 1DELETE FROM app_info where id = #&#123;id&#125; 批量删除 123456&lt;delete id=&quot;deleteApps&quot; parameterType=&quot;java.util.List&quot;&gt; DELETE FROM app_info where app_id in &lt;foreach item=&quot;item&quot; collection=&quot;appIds&quot; open=&quot;(&quot; separator=&quot;,&quot; close=&quot;)&quot;&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/delete&gt; 时间字符串 order by不知道各位是否遇到过，之前的前辈们在项目中将时间用字符串的方式存在DB中，而不是使用DATE,然后有一天你的前辈走了，你的主管说查出来按时间来排序….；呵呵，好！！！ 1234567&lt;select id=&quot;querySysParamList&quot; resultMap=&quot;sysParamDO&quot;&gt; SELECT * FROM app_info WHERE 1=1 &lt;if test=&quot;keyword != null and keyword != &apos;&apos;&quot; &gt; AND app_name LIKE concat(&apos;%&apos;,#&#123;keyword&#125;,&apos;%&apos;) &lt;/if&gt; ORDER BY DATE_FORMAT(update_time,&apos;%H %k %I %r %T %S %w&apos;) DESC&lt;/select&gt; 字符串转为日期格式SELECT DATE_FORMAT(‘2011-09-20 08:30:45’, ‘%Y-%m-%d %H:%i:%S’); 把日期转为字符串格式SELECT DATE_FORMAT(NOW(), ‘%Y-%m-%d %H:%i:%S’); 附： 123456789101112131415161718192021222324252627%M 月名字(January……December) %W 星期名字(Sunday……Saturday) %D 有英语前缀的月份的日期(1st, 2nd, 3rd, 等等。） %Y 年, 数字, 4 位 %y 年, 数字, 2 位 %a 缩写的星期名字(Sun……Sat) %d 月份中的天数, 数字(00……31) %e 月份中的天数, 数字(0……31) %m 月, 数字(01……12) %c 月, 数字(1……12) %b 缩写的月份名字(Jan……Dec) %j 一年中的天数(001……366) %H 小时(00……23) %k 小时(0……23) %h 小时(01……12) %I 小时(01……12) %l 小时(1……12) %i 分钟, 数字(00……59) %r 时间,12 小时(hh:mm:ss [AP]M) %T 时间,24 小时(hh:mm:ss) %S 秒(00……59) %s 秒(00……59) %p AM或PM %w 一个星期中的天数(0=Sunday ……6=Saturday ） %U 星期(0……52), 这里星期天是星期的第一天 %u 星期(0……52), 这里星期一是星期的第一天 %% 一个文字“%”。 先记录这些，有坑再补！ 参考：http://www.runoob.com/sql/sql-tutorial.html]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
        <tag>sql</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 反向代理和负载均衡策略实战案例]]></title>
    <url>%2F2018%2F11%2F10%2Fnginxone%2F</url>
    <content type="text"><![CDATA[欢迎关注：glmapper_2018 引言先来看下nginx在web服务器排名上的趋势： 存在即合理，那为什么要使用nginx呢？这得看看nginx能帮我们做些什么。 首先，nginx能做反向代理【关于反向代理和正向代理此处不做说明了，感兴趣的小伙伴自行谷歌】；比方说，我想在本地使用 www.glmapper1.com 的域名去访问www.taobao.com。那么这个时候我们就可以通过nginx去实现。 再者，nginx能实现负载均衡，什么是负载均衡呢？就是说应用部署在不同的服务器上，但是通过统一的域名进入，nginx则对请求进行分发，将请求分发到不同的服务器上去处理，这样就可以有效的减轻了单台服务器的压力。 在上面这两种情况下，nginx服务器的作用都只是作为分发服务器，真正的内容，我们可以放在其他的服务器上，这样来，还能起到一层安全隔壁的作用，nginx作为隔离层。 解决跨域问题 同源：URL由协议、域名、端口和路径组成，如果两个URL的协议、域名和端口相同，则表示他们同源。 浏览器的同源策略：浏览器的同源策略，限制了来自不同源的”document”或脚本，对当前”document”读取或设置某些属性。从一个域上加载的脚本不允许访问另外一个域的文档属性。 因为nginx和tomcat不能共用同一端口,url一样，端口不同，这样就会有跨域问题。 PS：点到为止，这里本次测试没有涉及，就不妄自菲薄了！！! 配置文件解析配置文件主要由四部分组成： main(全区设置) server(主机配置) http(控制着nginx http处理的所有核心特性) location(URL匹配特定位置设置)。 upstream(负载均衡服务器设置) 下面以默认的配置文件来说明下具体的配置文件属性含义： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151#Nginx的worker进程运行用户以及用户组#user nobody;#Nginx开启的进程数worker_processes 1;#定义全局错误日志定义类型，[debug|info|notice|warn|crit]#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#指定进程ID存储文件位置#pid logs/nginx.pid;#事件配置events &#123; #use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; #epoll模型是Linux内核中的高性能网络I/O模型，如果在mac上面，就用kqueue模型。 use kqueue; #每个进程可以处理的最大连接数，理论上每台nginx服务器的最大连接数为worker_processes*worker_connections。理论值：worker_rlimit_nofile/worker_processes worker_connections 1024;&#125;#http参数http &#123; #文件扩展名与文件类型映射表 include mime.types; #默认文件类型 default_type application/octet-stream; #日志相关定义 #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #连接日志的路径，指定的日志格式放在最后。 #access_log logs/access.log main; #开启高效传输模式 sendfile on; #防止网络阻塞 #tcp_nopush on; #客户端连接超时时间，单位是秒 #keepalive_timeout 0; keepalive_timeout 65; #开启gzip压缩输出 #gzip on; #虚拟主机基本设置 server &#123; #监听的端口号 listen 80; #访问域名 server_name localhost; #编码格式，如果网页格式与当前配置的不同的话将会被自动转码 #charset koi8-r; #虚拟主机访问日志定义 #access_log logs/host.access.log main; #对URL进行匹配 location / &#123; #访问路径，可相对也可绝对路径 root html; #首页文件，匹配顺序按照配置顺序匹配 index index.html index.htm; &#125; #错误信息返回页面 #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; #访问URL以.php结尾则自动转交给127.0.0.1 # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; #php脚本请求全部转发给FastCGI处理 # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; #禁止访问.ht页面 # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\.ht &#123; # deny all; #&#125; &#125; #第二个虚拟主机配置 # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; #HTTPS虚拟主机定义 # HTTPS server # #server &#123; # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; include servers/*;&#125; 反向代理实例假设我现在需要本地访问www.baidu.com;配置如下： 1234567891011server &#123; #监听80端口 listen 80; server_name localhost; # individual nginx logs for this web vhost access_log /tmp/access.log; error_log /tmp/error.log ; location / &#123; proxy_pass http://www.baidu.com; &#125; 验证结果： 可以看到，我在浏览器中使用localhost打开了百度的首页… 负载均衡实例下面主要验证最常用的三种负载策略。虚拟主机配置：123456789101112131415161718192021222324server &#123; #监听80端口 listen 80; server_name localhost; # individual nginx logs for this web vhost access_log /tmp/access.log; error_log /tmp/error.log ; location / &#123; #负载均衡 #轮询 #proxy_pass http://polling_strategy; #weight权重 #proxy_pass http://weight_strategy; #ip_hash # proxy_pass http://ip_hash_strategy; #fair # proxy_pass http://fair_strategy; #url_hash # proxy_pass http://url_hash_strategy; #重定向 #rewrite ^ http://localhost:8080; &#125; 轮询策略123456# 1、轮询（默认）# 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 upstream polling_strategy &#123; server glmapper.net:8080; # 应用服务器1 server glmapper.net:8081; # 应用服务器2&#125; 测试结果（通过端口号来区分当前访问）： 12348081：hello8080：hello8081：hello8080：hello 权重策略123456#2、指定权重#指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 upstream weight_strategy &#123; server glmapper.net:8080 weight=1; # 应用服务器1 server glmapper.net:8081 weight=9; # 应用服务器2&#125; 测试结果：总访问次数15次，根据上面的权重配置，两台机器的访问比重：2：13；满足预期！ ip hash策略123456789#3、IP绑定 ip_hash#每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，#可以解决session的问题;在不考虑引入分布式session的情况下，#原生HttpSession只对当前servlet容器的上下文环境有效upstream ip_hash_strategy &#123; ip_hash; server glmapper.net:8080; # 应用服务器1 server glmapper.net:8081; # 应用服务器2&#125; iphash 算法:ip是基本的点分十进制，将ip的前三个端作为参数加入hash函数。这样做的目的是保证ip地址前三位相同的用户经过hash计算将分配到相同的后端server。作者的这个考虑是极为可取的，因此ip地址前三位相同通常意味着来着同一个局域网或者相邻区域，使用相同的后端服务让nginx在一定程度上更具有一致性。 为什么说要解释下iphash,因为采坑了；和猪弟在进行这个策略测试时使用了5台机器来测试的，5台机器均在同一个局域网内【192.168.3.X】;测试时发现5台机器每次都路由到了同一个服务器上，一开始以为是配置问题，但是排查之后也排除了这个可能性。最后考虑到可能是对于同网段的ip做了特殊处理，验证之后确认了猜测。 其他负载均衡策略这里因为需要安装三方插件，时间有限就不验证了，知悉即可！1234567891011121314151617#4、fair（第三方）#按后端服务器的响应时间来分配请求，响应时间短的优先分配。 upstream fair_strategy &#123; server glmapper.net:8080; # 应用服务器1 server glmapper.net:8081; # 应用服务器2 fair; &#125; #5、url_hash（第三方）#按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，#后端服务器为缓存时比较有效。 upstream url_hash_strategy &#123; server glmapper.net:8080; # 应用服务器1 server glmapper.net:8081; # 应用服务器2 hash $request_uri; hash_method crc32; &#125; 重定向rewrite1234location / &#123; #重定向 #rewrite ^ http://localhost:8080;&#125; 验证思路：本地使用localhost:80端口进行访问，根据nginx的配置，如果重定向没有生效，则最后会停留在当前localhost:80这个路径，浏览器中的地址栏地址不会发生改变；如果生效了则地址栏地址变为localhost:8080； 通过验证，满足预期！ 总结本文先对nginx的作用和基本的配置做了简单说明；然后通过负载均衡的实例测试了不同负载均衡算法的具体应用反馈结果。帮助自己更加深刻的理解nginx服务器中的一些配置细节。感谢刘秘提供的helloworld程序【基于springboot的脚手架，有需要的可以联系他获取；还有就是刘秘是个男的…😜】 参考 http://nginx.org/ https://www.nginx.com/ http://www.sohu.com/a/161411719_324809]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>代理模式</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊 session和cookie]]></title>
    <url>%2F2018%2F11%2F10%2Ftopic-session-cookie%2F</url>
    <content type="text"><![CDATA[本来是想写aop设计机制的，但是最近被session这个东西搞得有点头大，所以就抽点时间来整理下关于session的一些东西。 目录 从http协议的无状态性说起 无连接和无状态 持久连接 http无状态 如何保持状态信息 Cookie Cookie机制原理 Cookie在servlet-api中的定义 Cookie属性 创建Cookie Cookie更新 Cookie删除 从请求中获取Cookie Cookie同源与跨域 Cookie数量&amp;大小限制及处理策略 Session session机制原理 HttpSession 创建session 生命周期 session的有效期 分布式session从http协议的无状态性说起 HTTP是一种无状态协议。关于这个无状态之前我也不太理解，因为HTTP底层是TCP，既然是TCP，就是长连接，这个过程是保持连接状态的，又为什么说http是无状态的呢？先来搞清楚这两个概念： 无连接和无状态 无连接 每次连接只处理一个请求，服务端处理完客户端一次请求，等到客户端作出回应之后便断开连接； 无状态 是指服务端对于客户端每次发送的请求都认为它是一个新的请求，上一次会话和下一次会话没有联系； 无连接的维度是连接，无状态的维度是请求；http是基于tcp的，而从http1.1开始默认使用持久连接；在这个连接过程中，客户端可以向服务端发送多次请求，但是各个请求之间的并没有什么联系；这样来考虑，就很好理解无状态这个概念了。 持久连接持久连接，本质上是客户端与服务器通信的时候，建立一个持久化的TCP连接，这个连接不会随着请求结束而关闭，通常会保持连接一段时间。 现有的持久连接类型有两种：HTTP/1.0+的keep-alive和HTTP/1.1的persistent。 HTTP/1.0+的keep-alive 先来开一张图： 这张图是请求www.baidu.com时的请求头信息。这里面我们需要注意的是： 1connection: keep-alive 我们每次发送一个HTTP请求，会附带一个connection:keep-alive，这个参数就是声明一个持久连接。 HTTP/1.1的persistent HTTP/1.1的持久连接默认是开启的，只有首部中包含connection：close，才会事务结束之后关闭连接。当然服务器和客户端仍可以随时关闭持久连接。 当发送了connection：close首部之后客户端就没有办法在那条连接上发送更多的请求了。当然根据持久连接的特性，一定要传输正确的content-length。 还有根据HTTP/1.1的特性，是不应该和HTTP/1.0客户端建立持久连接的。最后，一定要做好重发的准备。 http无状态OK，首先来明确下，这个状态的主体指的是什么？应该是信息，这些信息是由服务端所维护的与客户端交互的信息（也称为状态信息）；因为HTTP本身是不保存任何用户的状态信息的，所以HTTP是无状态的协议。 如何保持状态信息在聊这个这个问题之前，我们来考虑下为什么http自己不来做这个事情：也就是让http变成有状态的。 http本身来实现状态维护 从上面关于无状态的理解，如果现在需要让http自己变成有状态的，就意味着http协议需要保存交互的状态信息；暂且不说这种方式是否合适，但从维护状态信息这一点来说，代价就很高，因为既然保存了状态信息，那后续的一些行为必定也会受到状态信息的影响。 从历史角度来说，最初的http协议只是用来浏览静态文件的，无状态协议已经足够，这样实现的负担也很轻。但是随着web技术的不断发展，越来越多的场景需要状态信息能够得以保存；一方面是http本身不会去改变它的这种无状态的特性（至少目前是这样的），另一方面业务场景又迫切的需要保持状态；那么这个时候就需要来“装饰”一下http，引入一些其他机制来实现有状态。 cookie和session体系 通过引入cookie和session体系机制来维护状态信息。即用户第一次访问服务器的时候，服务器响应报头通常会出现一个Set-Cookie响应头，这里其实就是在本地设置一个cookie，当用户再次访问服务器的时候，http会附带这个cookie过去，cookie中存有sessionId这样的信息来到服务器这边确认是否属于同一次会话。 Cookie cookie是由服务器发送给客户端（浏览器）的小量信息，以{key：value}的形式存在。 Cookie机制原理 客户端请求服务器时，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。而客户端浏览器会把Cookie保存起来。当浏览器再请求 服务器时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器通过检查该Cookie来获取用户状态。 我们通过看下servlet-api中Cookie类的定义及属性，来更加具体的了解Cookie。 Cookie在servlet-api中的定义123456789101112131415161718public class Cookie implements Cloneable, Serializable &#123; private static final long serialVersionUID = -6454587001725327448L; private static final String TSPECIALS; private static final String LSTRING_FILE = "javax.servlet.http.LocalStrings"; private static ResourceBundle lStrings = ResourceBundle.getBundle("javax.servlet.http.LocalStrings"); private String name; private String value; private String comment; private String domain; private int maxAge = -1; private String path; private boolean secure; private int version = 0; private boolean isHttpOnly = false; //....省略其他方法&#125; Cookie属性 name cookie的名字，Cookie一旦创建，名称便不可更改 value cookie值 comment 该Cookie的用处说明。浏览器显示Cookie信息的时候显示该说明 domain 可以访问该Cookie的域名。如果设置为“.baidu.com”，则所有以“baidu.com”结尾的域名都可以访问该Cookie；第一个字符必须为“.” maxAge Cookie失效的时间，单位秒。 正数，则超过maxAge秒之后失效。 负数，该Cookie为临时Cookie，关闭浏览器即失效，浏览器也不会以任何形式保存该Cookie。 为0，表示删除该Cookie。 path 该Cookie的使用路径。例如： path=/，说明本域名下contextPath都可以访问该Cookie。 path=/app/，则只有contextPath为“/app”的程序可以访问该Cookie path设置时，其以“/”结尾. secure 该Cookie是否仅被使用安全协议传输。这里的安全协议包括HTTPS，SSL等。默认为false。 version 该Cookie使用的版本号。 0 表示遵循Netscape的Cookie规范，目前大多数用的都是这种规范； 1 表示遵循W3C的RFC2109规范；规范过于严格，实施起来很难。 在servlet规范中默认是0； isHttpOnly HttpOnly属性是用来限制非HTTP协议程序接口对客户端Cookie进行访问；也就是说如果想要在客户端取到httponly的Cookie的唯一方法就是使用AJAX，将取Cookie的操作放到服务端，接收客户端发送的ajax请求后将取值结果通过HTTP返回客户端。这样能有效的防止XSS攻击。 上述的这些属性，除了name与value属性会被提交外，其他的属性对于客户端来说都是不可读的，也是不可被提交的。 创建Cookie12345Cookie cookie = new Cookie("cookieSessionId","qwertyuiop");cookie.setDomain(".baidu.com"); // 设置域名cookie.setPath("/"); // 设置路径cookie.setMaxAge(Integer.MAX_VALUE); // 设置有效期为永久response.addCookie(cookie); // 回写到客户端 创建Cookie只能通过上述方式来创建，因为在Cookie类中只提供了这样一个构造函数。 123456789101112131415161718192021222324252627//Cookie的构造函数public Cookie(String name, String value) &#123; if (name != null &amp;&amp; name.length() != 0) &#123; //判断下是不是token //判断是不是和Cookie的属性字段重复 if (this.isToken(name) &amp;&amp; !name.equalsIgnoreCase("Comment") &amp;&amp; !name.equalsIgnoreCase("Discard") &amp;&amp; !name.equalsIgnoreCase("Domain") &amp;&amp; !name.equalsIgnoreCase("Expires") &amp;&amp; !name.equalsIgnoreCase("Max-Age") &amp;&amp; !name.equalsIgnoreCase("Path") &amp;&amp; !name.equalsIgnoreCase("Secure") &amp;&amp; !name.equalsIgnoreCase("Version") &amp;&amp; !name.startsWith("$")) &#123; this.name = name; this.value = value; &#125; else &#123; String errMsg = lStrings.getString("err.cookie_name_is_token"); Object[] errArgs = new Object[]&#123;name&#125;; errMsg = MessageFormat.format(errMsg, errArgs); throw new IllegalArgumentException(errMsg); &#125; &#125; else &#123; throw new IllegalArgumentException(lStrings.getString ("err.cookie_name_blank")); &#125;&#125; Cookie更新在源码中可以知道，Cookie本身并没有提供修改的方法；在实际应用中，一般通过使用相同name的Cookie来覆盖原来的Cookie,以达到更新的目的。 但是这个修改的前提是需要具有相同domain，path的 Set-Cookie 消息头 12Cookie cookie = new Cookie("cookieSessionId","new-qwertyuiop");response.addCookie(cookie); Cookie删除与Cookie更新一样，Cookie本身也没有提供删除的方法；但是从前面分析Cookie属性时了解到，删除Cookie可以通过将maxAge设置为0即可。 123Cookie cookie = new Cookie("cookieSessionId","new-qwertyuiop");cookie.setMaxAge(0);response.addCookie(cookie); 上面的删除是我们自己可控的；但是也存在一些我们不可控或者说无意识情况下的删除操作： 如果maxAge是负值，则cookie在浏览器关闭时被删除 持久化cookie在到达失效日期时会被删除 浏览器中的 cookie 数量达到上限，那么 cookie 会被删除以为新建的 cookie 创建空间。 其实很多情况下，我们关注的都是后者。关于数量上限后面会说到。 从请求中获取Cookie1Cookie[] cookies = request.getCookies(); Cookie同源与跨域我们知道浏览器的同源策略： URL由协议、域名、端口和路径组成，如果两个URL的协议、域名和端口相同，则表示他们同源。浏览器的同源策略，限制了来自不同源的”document”或脚本，对当前”document”读取或设置某些属性。 对于Cookie来说，Cookie的同源只关注域名，是忽略协议和端口的。所以一般情况下，https://localhost:80/和http://localhost:8080/的Cookie是共享的。 Cookie是不可跨域的；在没有经过任何处理的情况下，二级域名不同也是不行的。(wenku.baidu.com和baike.baidu.com)。 Cookie数量&amp;大小限制及处理策略 IE6.0 IE7.0/8.0 Opera FF Safari Chrome 个数/个 20/域 50/域 30/域 50/域 无限制 53/域 大小/Byte 4095 4095 4096 4097 4097 4097 注：数据来自网络，仅供参考 因为浏览器对于Cookie在数量上是有限制的，如果超过了自然会有一些剔除策略。在这篇文章中Browser cookie restrictions提到的剔除策略如下： The least recently used (LRU) approach automatically kicks out the oldest cookie when the cookie limit has been reached in order to allow the newest cookie some space. Internet Explorer and Opera use this approach. 最近最少使用（LRU）方法：在达到cookie限制时自动地剔除最老的cookie，以便腾出空间给许最新的cookie。Internet Explorer和Opera使用这种方法。 Firefox does something strange: it seems to randomly decide which cookies to keep although the last cookie set is always kept. There doesn’t seem to be any scheme it’s following at all. The takeaway? Don’t go above the cookie limit in Firefox. Firefox决定随机删除Cookie集中的一个Cookie，并没有什么章法。所以最好不要超过Firefox中的Cookie限制。 超过大小长度的话就是直接被截取丢弃； SessionCookie机制弥补了HTTP协议无状态的不足。在Session出现之前，基本上所有的网站都采用Cookie来跟踪会话。 与Cookie不同的是，session是以服务端保存状态的。 session机制原理当客户端请求创建一个session的时候，服务器会先检查这个客户端的请求里是否已包含了一个session标识 - sessionId， 如果已包含这个sessionId，则说明以前已经为此客户端创建过session，服务器就按照sessionId把这个session检索出来使用（如果检索不到，可能会新建一个） 如果客户端请求不包含sessionId，则为此客户端创建一个session并且生成一个与此session相关联的sessionId sessionId的值一般是一个既不会重复，又不容易被仿造的字符串，这个sessionId将被在本次响应中返回给客户端保存。保存sessionId的方式大多情况下用的是cookie。 HttpSessionHttpSession和Cookie一样，都是javax.servlet.http下面的；Cookie是一个类，它描述了Cookie的很多内部细节。而HttpSession是一个接口，它为session的实现提供了一些行为约束。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public interface HttpSession &#123; /** * 返回session的创建时间 */ public long getCreationTime(); /** * 返回一个sessionId,唯一标识 */ public String getId(); /** *返回客户端最后一次发送与该 session 会话相关的请求的时间 *自格林尼治标准时间 1970 年 1 月 1 日午夜算起，以毫秒为单位。 */ public long getLastAccessedTime(); /** * 返回当前session所在的ServletContext */ public ServletContext getServletContext(); public void setMaxInactiveInterval(int interval); /** * 返回 Servlet 容器在客户端访问时保持 session * 会话打开的最大时间间隔 */ public int getMaxInactiveInterval(); public HttpSessionContext getSessionContext(); /** * 返回在该 session会话中具有指定名称的对象， * 如果没有指定名称的对象，则返回 null。 */ public Object getAttribute(String name); public Object getValue(String name); /** * 返回 String 对象的枚举，String 对象包含所有绑定到该 session * 会话的对象的名称。 */ public Enumeration&lt;String&gt; getAttributeNames(); public String[] getValueNames(); public void setAttribute(String name, Object value); public void putValue(String name, Object value); public void removeAttribute(String name); public void removeValue(String name); /** * 指示该 session 会话无效，并解除绑定到它上面的任何对象。 */ public void invalidate(); /** * 如果客户端不知道该 session 会话，或者如果客户选择不参入该 * session 会话，则该方法返回 true。 */ public boolean isNew();&#125; 创建session创建session的方式是通过request来创建；1234// 1、创建Session对象HttpSession session = request.getSession(); // 2、创建Session对象HttpSession session = request.getSession(true); 这两种是一样的；如果session不存在，就新建一个；如果是false的话，标识如果不存在就返回null； 生命周期session的生命周期指的是从Servlet容器创建session对象到销毁的过程。Servlet容器会依据session对象设置的存活时间，在达到session时间后将session对象销毁。session生成后，只要用户继续访问，服务器就会更新session的最后访问时间，并维护该session。 之前在单进程应用中，session我一般是存在内存中的，不会做持久化操作或者说使用三方的服务来存session信息，如redis。但是在分布式场景下，这种存在本机内存中的方式显然是不适用的，因为session无法共享。这个后面说。 session的有效期session一般在内存中存放，内存空间本身大小就有一定的局限性，因此session需要采用一种过期删除的机制来确保session信息不会一直累积，来防止内存溢出的发生。 session的超时时间可以通过maxInactiveInterval属性来设置。 如果我们想让session失效的话，也可以当通过调用session的invalidate()来完成。 分布式session首先是为什么会有这样的概念出现？ 先考虑这样一个问题，现在我的应用需要部署在3台机器上。是不是出现这样一种情况，我第一次登陆，请求去了机器1，然后再机器1上创建了一个session；但是我第二次访问时，请求被路由到机器2了，但是机器2上并没有我的session信息，所以得重新登录。当然这种可以通过nginx的IP HASH负载策略来解决。对于同一个IP请求都会去同一个机器。 但是业务发展的越来越大，拆分的越来越多，机器数不断增加；很显然那种方案就不行了。那么这个时候就需要考虑是不是应该将session信息放在一个独立的机器上，所以分布式session要解决的问题其实就是分布式环境下的session共享的问题。 上图中的关于session独立部署的方式有很多种，可以是一个独立的数据库服务，也可以是一个缓存服务(redis，目前比较常用的一种方式，即使用Redis来作为session缓存服务器)。 参考 https://www.cnblogs.com/icelin/p/3974935.html https://www.nczonline.net/blog/2008/05/17/browser-cookie-restrictions/ https://zh.wikipedia.org/wiki/%E8%B6%85%E6%96%87%E6%9C%AC%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AE]]></content>
      <categories>
        <category>session</category>
      </categories>
      <tags>
        <tag>聊一聊</tag>
        <tag>session</tag>
        <tag>cookie</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊 AOP：Advice 源码解析]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-aop-advice%2F</url>
    <content type="text"><![CDATA[在第一篇中的例子和概念介绍中我们对 Advice 有了一个初步的认知。在Spring AOP中，Advice的作用就是用来描述 Spring AOP 围绕方法调用而注入的切面行为。 本篇文章将从源码的角度来看一看 Advice 到底是什么样的？又是怎么完成通知的？ Advice 接口1234567891011package org.aopalliance.aop;/** * Tag interface for Advice. Implementations can be any type * of advice, such as Interceptors. * @author Rod Johnson * @version $Id: Advice.java,v 1.1 2004/03/19 17:02:16 johnsonr Exp $ */public interface Advice &#123;&#125; Advice 接口的定义是在 org.aopalliance.aop 包下面的；从上面的代码中我们可以知道，Advice 接口并没有提供任何的方法；类似的接口定义还有java 中的如Serializable接口，这类接口一般称之为标识接口；标识接口对实现它的类没有任何的语义要求,仅仅是充当一个标示的作用,用来表明实现它的类属于一个特定的类型（从这种标识性角度来说，和注解其实挺像的）； Spring AOP中通过定义和使用这样一个统一的接口，为的就是能够为切面增强的织入功能做更多的细化和扩展。下面就对常见的三个Advice进行分析。 BeforeAdvice12public interface BeforeAdvice extends Advice &#123;&#125; 这个接口也是一个标识接口。看下 BeforeAdvice 的继承关系： MethodBeforeAdvice 是 BeforeAdvice 为待增强的目标方法设置的前置增强接口。 1234public interface MethodBeforeAdvice extends BeforeAdvice &#123; void before(Method method, Object[] args, Object target) throws Throwable;&#125; MethodBeforeAdvice 中提供了一个回调函数 before(…) ； 作为回调函数，before 方法的实现在 Advice 中被配置到目标方法后，会在调用目标方法时被回调。来看下before方法的几个参数： Method method ：（ method being invoked）这个参数是目标方法的反射对象； Object[] args ：（arguments to the method）目标方法的输入参数； Object target ：（target of the method invocation）方法调用的目标 AspectJMethodBeforeAdviceAspectJMethodBeforeAdvice 继承了 AbstractAspectJAdvice 抽象类，并实现了 MethodBeforeAdvice 接口。从 AspectJMethodBeforeAdvice 类中代码可以得知，AspectJMethodBeforeAdvice 重写 before 方法的实现是 通过调用父类的 invokeAdviceMethod 方法完成的。也就是说Spring AOP 的Advice包装了AspectJ的before方法。 Spring AOP的实现后面再说，我们先自己来实现一个简单的通知。 自定义 Advice实现 MethodBeforeAdvice定义我们自己的 GlmapperBeforeMethodAdvice ；这里实现 MethodBeforeAdvice 接口，然后重写 before 这个方法。 1234567891011121314151617181920212223/** * @description: 自定义的 GlmapperBeforeMethodAdvice * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: glmapper@leishu * @date: 18/6/23 */public class GlmapperBeforeMethodAdvice implementsMethodBeforeAdvice,MethodInterceptor &#123; private static final Logger LOGGER = LoggerFactory.getLogger(GlmapperBeforeMethodAdvice.class.getSimpleName()); @Override public void before(Method method, Object[] args, Object target) throws Throwable &#123; LOGGER.info("invoke BeforeAdvice successfully..."); &#125; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; Object result=invocation.proceed(); return result; &#125;&#125; OK，有了这个 GlmapperBeforeMethodAdvice ，再来看看怎么用它；同样本篇文章所使用的案例均使用前一篇博客中的那个脚手架来完成。 12345678910111213141516171819202122232425262728293031&lt;!--我们的目标类--&gt;&lt;bean id="goodsService" class="com.glmapper.framerwork.service.impl.GoodsServiceImpl"/&gt;&lt;!--我们自定义的Advice--&gt;&lt;bean id="glmapperBeforeMethodAdvice" class="com.glmapper.framerwork.Advice.GlmapperBeforeMethodAdvice"&gt;&lt;/bean&gt;&lt;!-- 声明切入点adviser --&gt;&lt;bean id="adviser" class="org.springframework.aop.support.RegexpMethodPointcutAdvisor"&gt; &lt;!--这里使用我们自定义的advice--&gt; &lt;property name="advice" ref="glmapperBeforeMethodAdvice"&gt;&lt;/property&gt; &lt;!-- pattern指定queryAll方法作为切入点； \. 这个是转义使用--&gt; &lt;property name="pattern" value="com\.glmapper\.framerwork\.service\.impl\.GoodsServiceImpl\.queryAll"&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 定义代理对象 返回实例是目标对象 target属性指定的goodsService对象--&gt;&lt;bean id="proxyService"class="org.springframework.aop.framework.ProxyFactoryBean"&gt; &lt;property name="target"&gt; &lt;ref bean="goodsService" /&gt; &lt;/property&gt; &lt;!--源码内固定的属性private String[] interceptorNames; --&gt; &lt;property name="interceptorNames"&gt; &lt;value&gt;adviser&lt;/value&gt; &lt;/property&gt;&lt;/bean&gt; 客户端部分，通过SpringContextUtil来拿代理对象； 1234567891011@RequestMapping("/initPage")public ModelAndView initPage(HttpServletRequest request, HttpServletResponse response, ModelAndView view) &#123; //获取代理bean GoodsService proxyService= (GoodsService) SpringContextUtil.getBean("proxyService"); //调用 List&lt;Goods&gt; goods = proxyService.queryAll(10,10); view.addObject("goodsList", goods); view.setViewName("goodslist"); return view;&#125; 日志输出满足我们的期望（如下）： 同样的，在GlmapperBeforeMethodAdvice基础上再实现 AfterReturningAdvice 接口，重写afterReturning方法，就能实现后置通知。 12345@Overridepublic void afterReturning(Object returnValue, Method method, Object[]args, Object target) throws Throwable &#123; LOGGER.info("invoke AfterAdvice successfully...");&#125; 这个方式在聊一聊 AOP ：表现形式与基础概念中有说道。 Advice 在 Aop 中的实现原理这里感觉没什么好说的，上面的案例其实就是Spring提供给我们使用的接口。因为MethodBeforeAdvice等都是继承自 AbstractAspectJAdvice 这个抽象类；我们就来看下这个抽象类里面的一些核心逻辑吧。我们按照AspectJMethodBeforeAdvice这里这个类里面before提供的线索来一步步分析。 首先在AspectJMethodBeforeAdvice里before方法中调用的是这个逻辑： 12345678910111213/** * Invoke the advice method. * @param jpMatch the JoinPointMatch that matched this execution join point * @param returnValue the return value from the method execution (may be null) * @param ex the exception thrown by the method execution (may be null) * @return the invocation result * @throws Throwable in case of invocation failure */protected Object invokeAdviceMethod(JoinPointMatch jpMatch, ObjectreturnValue, Throwable ex) throws Throwable &#123; return invokeAdviceMethodWithGivenArgs(argBinding(getJoinPoint(), jpMatch, returnValue, ex));&#125; 这里 argBinding 方法的作用是获取方法执行连接点上的参数，并将一组参数输出给Advice方法。 继续来看invokeAdviceMethodWithGivenArgs这个方法： 1234567891011121314151617181920212223protected Object invokeAdviceMethodWithGivenArgs(Object[] args) throwsThrowable &#123; //保存一份参数副本 Object[] actualArgs = args; //验证下参数是否不存在 if (this.aspectJAdviceMethod.getParameterTypes().length == 0) &#123; actualArgs = null; &#125; try &#123; //设置下方法的访问权限 ReflectionUtils.makeAccessible(this.aspectJAdviceMethod); // invoke执行；这里先通过aspectInstanceFactory对像拿到我们的目标对象实例，然后再进行invoke调用执行 return this.aspectJAdviceMethod.invoke(this.aspectInstanceFactory.getAspectInstance(), actualArgs); &#125; catch (IllegalArgumentException ex) &#123; throw new AopInvocationException("Mismatch on arguments to advice method [" + this.aspectJAdviceMethod + "]; pointcut expression [" + this.pointcut.getPointcutExpression() + "]", ex); &#125; catch (InvocationTargetException ex) &#123; throw ex.getTargetException(); &#125;&#125; 上面这段代码其实就是通过反射的方式执行了我们的目标方法。我们再回过头来看下我们的目标方法到底在哪里去进行增强的；这里我们通过配置文件来看： 123456789101112&lt;!-- 代理对象 返回实例是目标对象 target属性指定的AOPservice对象--&gt;&lt;bean id="proxyService"class="org.springframework.aop.framework.ProxyFactoryBean"&gt; &lt;property name="target"&gt; &lt;ref bean="goodsService" /&gt; &lt;/property&gt; &lt;!--源码内固定的属性private String[] interceptorNames; --&gt; &lt;property name="interceptorNames"&gt; &lt;value&gt;adviser&lt;/value&gt; &lt;/property&gt;&lt;/bean&gt; 代理对象proxyService实现上是ProxyFactoryBean产生的；这里就不在阐述BeanFactory和FactoryBean的区别了。 从上面的配置文件我们可以简单的了解到，代理对象实际上是我们目标对象+adviser共同组成；而在adviser里面又包括了我们的通知。 ProxyFactoryBean继承了FactoryBean，我们知道FactoryBean也是用来生成bean的，但是它生成的bean是通过其getObject方法来获取的。OK，那我们来看下ProxyFactoryBean的getObject方法： 1234567891011121314151617181920212223/** * Return a proxy. Invoked when clients obtain beans from this factory bean. * Create an instance of the AOP proxy to be returned by this factory. * The instance will be cached for a singleton, and create on each call to * &#123;@code getObject()&#125; for a proxy. * @return a fresh AOP proxy reflecting the current state of this factory */@Overridepublic Object getObject() throws BeansException &#123; //初始化Advisor链 initializeAdvisorChain(); //如果是单例，则获取单例对象 if (isSingleton()) &#123; return getSingletonInstance(); &#125; else &#123; if (this.targetName == null) &#123; logger.warn("Using non-singleton proxies with singleton targets is often undesirable. " + "Enable prototype proxies by setting the 'targetName' property."); &#125; return newPrototypeInstance(); &#125;&#125; 返回一个代理。当客户端从这个工厂bean获取bean时调用。创建该工厂返回的AOP代理的一个实例。该实例将被缓存为一个单例，并在每次调用时创建。 initializeAdvisorChain：创建 advisor（拦截器）链。每次添加新的 prototype 实例时，源自 BeanFactory 的 Advisor 都将被刷新。通过工厂 API 以编程方式添加的拦截器不受此类更改的影响。（译注）；其实就是根据我们配置的interceptorNames来初始化我们的advisor（拦截器）链，用来增强我们的目标调用方法。 下面是getSingletonInstance这个方法： 123456789101112131415161718192021222324/** * Return the singleton instance of this class's proxy object, * lazily creating it if it hasn't been created already. * @return the shared singleton proxy */private synchronized Object getSingletonInstance() &#123; if (this.singletonInstance == null) &#123; //创建目标对象的代理 this.targetSource = freshTargetSource(); if (this.autodetectInterfaces &amp;&amp; getProxiedInterfaces().length == 0 &amp;&amp; !isProxyTargetClass()) &#123; // Rely on AOP infrastructure to tell us what interfaces to proxy. //获取目标类 Class&lt;?&gt; targetClass = getTargetClass(); if (targetClass == null) &#123; throw new FactoryBeanNotInitializedException("Cannot determine target class for proxy"); &#125; setInterfaces(ClassUtils.getAllInterfacesForClass(targetClass, this.proxyClassLoader)); &#125; // Initialize the shared singleton instance. super.setFrozen(this.freezeProxy); this.singletonInstance = getProxy(createAopProxy()); &#125; return this.singletonInstance;&#125; 上面代码最核心的是getProxy这个方法，这里方式有两个方式，一个是cglib，另外一种是jdk动态代理： 这里我们以默认的动态代理的方式来说：(org.springframework.aop.framework.JdkDynamicAopProxy类中) 1234567891011@Overridepublic Object getProxy(ClassLoader classLoader) &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Creating JDK dynamic proxy: target source is " + this.advised.getTargetSource()); &#125; Class&lt;?&gt;[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this);&#125; 这个方法返回的就是指定接口的代理类实例，该接口将方法调用分派给指定的调用处理程序。 到此整个AOP代理生成逻辑就完了。 总结一下就是我们的代理类中其实包括了我们AOP增强的那部分逻辑的，这个其实从上面的配置文件中就很清楚的可以看出来；所以从Adivce介个角度来说，它其实会被抱在advisor中，然后在被传递到代理对象中，代理对象除了拥有我们目标对象的能力之外，还包括了Adivce的能力；通过这种方式就实现了增强。 关于Advice就到这里了，下一章会来单独说一下 PointCut 。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>聊一聊</tag>
        <tag>aop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊 AOP ：表现形式与基础概念]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-aop-base%2F</url>
    <content type="text"><![CDATA[aop 终于提上日程来写一写了。 系列目录本系列分为 上、中、下三篇。上篇主要是介绍如果使用 AOP ，提供了demo和配置方式说明；中篇来对实现 AOP 的技术原理进行分析；下篇主要针对Spring中对于AOP的实现进行源码分析。 从一个例子说起 基于代理的方式 纯POJO切面 配置方式 AspectJ 注解方式 AspectJ XML 配置方式 表达式说明 基础概念 AOP概念 Target Object 织入（Weave Proxy Introduction Aspect Joinpoint Pointcut Advice 概念 分类 关系 一些坑 项目地址项目地址：glmapper-ssm-parent 这个项目里面包含了下面几种 AOP 实现方式的所有代码，有兴趣的同学可以fork跑一下。这个demo中列举了4中方式的实现： 基于代码的方式 基于纯POJO类的方式 基于Aspect注解的方式 基于注入式Aspect的方式 目前我们经常用到的是基于Aspect注解的方式的方式。下面来一个个了解下不同方式的表现形式。 基于代理的方式这种方式看起来很好理解，但是配置起来相当麻烦；小伙伴们可以参考项目来看，这里只贴出比较关键的流程代码。 1、首先定义一个接口：GoodsService12345678910public interface GoodsService &#123; /** * 查询所有商品信息 * * @param offset 查询起始位置 * @param limit 查询条数 * @return */ List&lt;Goods&gt; queryAll(int offset,int limit);&#125; 2、GoodsService 实现类123456789101112@Service@Qualifier("goodsService")public class GoodsServiceImpl implements GoodsService &#123; @Autowired private GoodsDao goodsDao; public List&lt;Goods&gt; queryAll(int offset, int limit) &#123; System.out.println("执行了queryAll方法"); List&lt;Goods&gt; list = new ArrayList&lt;Goods&gt;(); return list; &#125;&#125; 3、定义一个通知类 LoggerHelper，该类继承 MethodBeforeAdvice和 AfterReturningAdvice。123456789101112131415//通知类 LoggerHelperpublic class LoggerHelper implements MethodBeforeAdvice,AfterReturningAdvice &#123; private static final Logger LOGGER = LoggerFactory.getLogger(LoggerHelper.class); //MethodBeforeAdvice的before方法实现 public void before(Method method, Object[] objects, Object o) throws Throwable &#123; LOGGER.info("before current time:"+System.currentTimeMillis()); &#125; //AfterReturningAdvice的afterReturning方法实现 public void afterReturning(Object o, Method method, Object[] objects, Object o1) throws Throwable &#123; LOGGER.info("afterReturning current time:"+System.currentTimeMillis()); &#125;&#125; 4、重点，这个配置需要关注下。这个项目里面我是配置在applicationContext.xml文件中的。12345678910111213141516171819202122232425262728&lt;!-- 定义被代理者 --&gt;&lt;bean id="goodsServiceImpl" class="com.glmapper.framerwork.service.impl.GoodsServiceImpl"&gt;&lt;/bean&gt;&lt;!-- 定义通知内容，也就是切入点执行前后需要做的事情 --&gt;&lt;bean id="loggerHelper" class="com.glmapper.framerwork.aspect.LoggerHelper"&gt;&lt;/bean&gt;&lt;!-- 定义切入点位置 --&gt;&lt;bean id="loggerPointcut" class="org.springframework.aop.support.JdkRegexpMethodPointcut"&gt; &lt;property name="pattern" value=".*query.*"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 使切入点与通知相关联，完成切面配置 --&gt;&lt;!-- 从这里可以帮助我们理解Advisor，advice和pointcut之间的关系--&gt;&lt;!--adivce和pointcut是Advisor的两个属性--&gt;&lt;bean id="loggerHelperAdvisor" class="org.springframework.aop.support.DefaultPointcutAdvisor"&gt; &lt;property name="advice" ref="loggerHelper"&gt;&lt;/property&gt; &lt;property name="pointcut" ref="loggerPointcut"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 设置代理 --&gt;&lt;bean id="proxy" class="org.springframework.aop.framework.ProxyFactoryBean"&gt; &lt;!-- 代理的对象 ，也就是目标类--&gt; &lt;property name="target" ref="goodsServiceImpl"&gt;&lt;/property&gt; &lt;!-- 使用切面 --&gt; &lt;property name="interceptorNames" value="loggerHelperAdvisor"&gt;&lt;/property&gt; &lt;!-- 代理接口，商品接口 --&gt; &lt;property name="proxyInterfaces" value="com.glmapper.framerwork.service.GoodsService"&gt;&lt;/property&gt;&lt;/bean&gt; 5、使用：注解注入方式1234567891011121314151617181920@Controller@RequestMapping("/buy")public class BuyController &#123; @Autowired private OrderService orderService; //因为我们已经在配置文件中配置了proxy， //所以这里可以直接注入拿到我们的代理类 @Autowired private GoodsService proxy; @RequestMapping("/initPage") public ModelAndView initPage(HttpServletRequest request, HttpServletResponse response, ModelAndView view) &#123; //这里使用proxy执行了*query*, List&lt;Goods&gt; goods = proxy.queryAll(10,10); view.addObject("goodsList", goods); view.setViewName("goodslist"); return view; &#125;&#125; 6、使用：工具类方式手动获取bean这个方式是通过一个SpringContextUtil工具类来获取代理对象的。12345678910@RequestMapping(&quot;/initPage&quot;)public ModelAndView initPage(HttpServletRequest request, HttpServletResponse response, ModelAndView view) &#123; //这里通过工具类来拿，效果一样的。 GoodsService proxy= (GoodsService) SpringContextUtil.getBean(&quot;proxy&quot;); List&lt;Goods&gt; goods = proxy.queryAll(10,10); view.addObject(&quot;goodsList&quot;, goods); view.setViewName(&quot;goodslist&quot;); return view;&#125; 7、SpringContextUtil 类的定义这个还是有点坑的，首先SpringContextUtil是继承ApplicationContextAware这个接口，我们希望能够SpringContextUtil可以被Spring容器直接管理，所以，需要使用 @Component 标注。标注了之后最关键的是它得能够被我们配置的注入扫描扫到（亲自踩的坑，我把它放在一个扫不到的包下面，一直debug都是null；差点砸电脑…） 123456789101112131415161718192021222324252627282930313233@Componentpublic class SpringContextUtil implements ApplicationContextAware &#123; // Spring应用上下文环境 private static ApplicationContext applicationContext; /** * 实现ApplicationContextAware接口的回调方法，设置上下文环境 * * @param applicationContext */ public void setApplicationContext(ApplicationContext applicationContext) &#123; SpringContextUtil.applicationContext = applicationContext; &#125; /** * @return ApplicationContext */ public static ApplicationContext getApplicationContext() &#123; return applicationContext; &#125; /** * 获取对象 * 这里重写了bean方法，起主要作用 * @param name * @return Object 一个以所给名字注册的bean的实例 * @throws BeansException */ public static Object getBean(String name) throws BeansException &#123; return applicationContext.getBean(name); &#125;&#125; 8、运行结果12345678921:04:47.940 [http-nio-8080-exec-7] INFO c.g.framerwork.aspect.LoggerHelper - before currenttime:1529413487940执行了queryAll方法21:04:47.940 [http-nio-8080-exec-7] INFO c.g.framerwork.aspect.LoggerHelper - afterReturning currenttime:1529413487940 上面就是最最经典的方式，就是通过代理的方式来实现AOP的过程。 纯POJO切面 aop:config注意这里和LoggerHelper的区别，这里的LoggerAspect并没有继承任何接口或者抽象类。 1、POJO 类定义123456789101112131415161718/** * @description: [描述文本] * @email: &lt;a href="guolei.sgl@antfin.com"&gt;&lt;/a&gt; * @author: guolei.sgl * @date: 18/6/20 */public class LoggerAspect &#123; private static final Logger LOGGER = LoggerFactory.getLogger(LoggerHelper.class); public void before()&#123; LOGGER.info("before current time:"+System.currentTimeMillis()); &#125; public void afterReturning() &#123; LOGGER.info("afterReturning current time:"+System.currentTimeMillis()); &#125;&#125; 2、配置文件123456789101112131415161718&lt;!-- 定义通知内容，也就是切入点执行前后需要做的事情 --&gt;&lt;bean id="loggerAspect" class="com.glmapper.framerwork.aspect.LoggerAspect"&gt;&lt;/bean&gt;&lt;aop:config&gt; &lt;!--定义切面--&gt; &lt;aop:aspect ref="loggerAspect"&gt; &lt;aop:pointcut id="loggerPointCut" expression= "execution(* com.glmapper.framerwork.service.impl.*.*(..)) " /&gt; &lt;!-- 定义 Advice --&gt; &lt;!-- 前置通知 --&gt; &lt;aop:before pointcut-ref="loggerPointCut" method="before" /&gt; &lt;!-- 后置通知 --&gt; &lt;aop:after-returning pointcut-ref="loggerPointCut" method="afterReturning"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 注意这里LoggerAspect中的before和afterReturning如果有参数，这里需要处理下，否则会报 0 formal unbound in pointcut 异常。 @AspectJ 注解驱动方式这种方式是最简单的一种实现，直接使用 @Aspect 注解标注我们的切面类即可。 1、定义切面类，并使用 @Aspect 进行标注123456789101112131415161718192021222324/** * @description: 使用Aspect注解驱动的方式 * @email: &lt;a href="guolei.sgl@antfin.com"&gt;&lt;/a&gt; * @author: guolei.sgl * @date: 18/6/20 */@Aspectpublic class LoggerAspectInject &#123; private static final Logger LOGGER = LoggerFactory.getLogger(LoggerAspectInject.class); @Pointcut("execution(* com.glmapper.framerwork.service.impl.*.*(..))") public void cutIn()&#123;&#125; @Before("cutIn()") public void before()&#123; LOGGER.info("before current time:"+System.currentTimeMillis()); &#125; @AfterReturning("cutIn()") public void AfterReturning()&#123; LOGGER.info("afterReturning current time:"+System.currentTimeMillis()); &#125;&#125; 2、使用方式1：配置文件方式声明 bean123456789&lt;aop:aspectj-autoproxy /&gt;&lt;!-- 定义通知内容，也就是切入点执行前后需要做的事情 --&gt;&lt;bean id="loggerAspectInject" class="com.glmapper.framerwork.aspect.LoggerAspectInject"&gt;&lt;/bean&gt;&lt;!-- 定义被代理者 --&gt;&lt;bean id="goodsServiceImpl" class="com.glmapper.framerwork.service.impl.GoodsServiceImpl"&gt;&lt;/bean&gt; 3、客户端使用：1234567891011121314151617181920@Controller@RequestMapping("/buy")public class BuyController &#123; @Autowired private OrderService orderService; @RequestMapping("/initPage") public ModelAndView initPage(HttpServletRequest request, HttpServletResponse response, ModelAndView view) &#123; //通过SpringContextUtil手动获取 代理bean GoodsService goodsService=(GoodsService) SpringContextUtil.getBean("goodsServiceImpl"); List&lt;Goods&gt; goods = goodsService.queryAll(10,10); view.addObject("goodsList", goods); view.setViewName("goodslist"); return view; &#125;&#125; 4、使用方式2：使用@component注解托管给IOC1234567891011121314151617181920@Aspect@Component //这里加上了Component注解，就不需要在xml中配置了public class LoggerAspectInject &#123; private static final Logger LOGGER = LoggerFactory.getLogger(LoggerAspectInject.class); @Pointcut("execution(* com.glmapper.framerwork.service.impl.*.*(..))") public void cutIn()&#123;&#125; @Before("cutIn()") public void before()&#123; LOGGER.info("before current time:"+System.currentTimeMillis()); &#125; @AfterReturning("cutIn()") public void AfterReturning()&#123; LOGGER.info("afterReturning current time:"+System.currentTimeMillis()); &#125;&#125; 5、客户端代码：1234567891011121314151617181920@Controller@RequestMapping("/buy")public class BuyController &#123; @Autowired private OrderService orderService; //直接注入 @Autowired private GoodsService goodsService; @RequestMapping("/initPage") public ModelAndView initPage(HttpServletRequest request, HttpServletResponse response, ModelAndView view) &#123; List&lt;Goods&gt; goods = goodsService.queryAll(10,10); view.addObject("goodsList", goods); view.setViewName("goodslist"); return view; &#125;&#125; 6、比较完整的一个LoggerAspectInject，在实际工程中可以直接参考123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * @description: aop * @email: &lt;a href="henugl@1992.163.com"&gt;&lt;/a&gt; * @author: glmapper@磊叔 * @date: 18/6/4 */@Aspect@Componentpublic class LoggerAspectInject &#123; private static final Logger LOGGER= LoggerFactory.getLogger(LoggerAspectInject.class); @Pointcut("execution(* com.glmapper.book.web.controller.*.*(..))") public void cutIn()&#123; &#125; @Around("cutIn()") // 定义Pointcut，名称即下面的标识"aroundAdvice public Object aroundAdvice(ProceedingJoinPoint poin)&#123; System.out.println("环绕通知"); Object object = null; try&#123; object = poin.proceed(); &#125;catch (Throwable e)&#123; e.printStackTrace(); &#125; return object; &#125; // 定义 advise //这个方法只是一个标识，相当于在配置文件中定义了pointcut的id,此方法没有返回值和参数 @Before("cutIn()") public void beforeAdvice()&#123; System.out.println("前置通知"); &#125; @After("cutIn()") public void afterAdvice()&#123; System.out.println("后置通知"); &#125; @AfterReturning("cutIn()") public void afterReturning()&#123; System.out.println("后置返回 "); &#125; @AfterThrowing("cutIn()") public void afterThrowing()&#123; System.out.println("后置异常"); &#125;&#125; 关于命名切入点：上面的例子中cutIn方法可以被称之为命名切入点，命名切入点可以被其他切入点引用，而匿名切入点是不可以的。只有@AspectJ支持命名切入点，而Schema风格不支持命名切入点。如下所示，@AspectJ使用如下方式引用命名切入点： 12345678@Pointcut("execution(* com.glmapper.book.web.controller.*.*(..))")public void cutIn()&#123;&#125;//引入命名切入点@Before("cutIn()")public void beforeAdvice()&#123; System.out.println("前置通知");&#125; 注入式 AspectJ 切面这种方式我感觉是第二种和第三种的结合的一种方式。 1、定义切面类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/*** @description: 注入式 也是一种通过XML方式配置的方式* @email: &lt;a href="guolei.sgl@antfin.com"&gt;&lt;/a&gt;* @author: guolei.sgl* @date: 18/6/20*/public class LoggerAspectHelper &#123; private static final Logger LOGGER = LoggerFactory.getLogger(LoggerAspectHelper.class); /** * 调动方法前执行 * @param point * @throws Throwable */ public void doBefore(JoinPoint point) throws Throwable &#123; LOGGER.info("before current time:"+System.currentTimeMillis()); &#125; /** * 在调用方法前后执行 * @param point * @return * @throws Throwable */ public Object doAround(ProceedingJoinPoint point) throws Throwable &#123; LOGGER.info("around current time:"+System.currentTimeMillis()); if(point.getArgs().length&gt;0) &#123; return point.proceed(point.getArgs()); &#125;else&#123; return point.proceed(); &#125; &#125; /** * 在调用方法之后执行 * @param point * @throws Throwable */ public void doAfter(JoinPoint point) throws Throwable &#123; LOGGER.info("after current time:"+System.currentTimeMillis()); &#125; /** * 异常通知 * @param point * @param ex */ public void doThrowing(JoinPoint point, Throwable ex) &#123; LOGGER.info("throwing current time:"+System.currentTimeMillis()); &#125;&#125; 2、XML 配置12345678910111213141516171819&lt;bean id="loggerAspectHelper" class="com.glmapper.framerwork.aspect.LoggerAspectHelper"&gt;&lt;/bean&gt;&lt;aop:config&gt; &lt;aop:aspect id="configAspect" ref="loggerAspectHelper"&gt; &lt;!--配置com.glmapper.framerwork.service.imp 包下所有类或接口的所有方法 --&gt; &lt;aop:pointcut id="cutIn" expression= "execution(* com.glmapper.framerwork.service.impl.*.*(..))" /&gt; &lt;aop:before pointcut-ref="cutIn" method="doBefore" /&gt; &lt;aop:after pointcut-ref="cutIn" method="doAfter" /&gt; &lt;aop:around pointcut-ref="cutIn" method="doAround" /&gt; &lt;aop:after-throwing pointcut-ref="cutIn" method="doThrowing" throwing="ex" /&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 3、结果123456723:39:48.756 [http-nio-8080-exec-4] INFO c.g.f.aspect.LoggerAspectHelper- before current time:152950918875623:39:48.757 [http-nio-8080-exec-4] INFO c.g.f.aspect.LoggerAspectHelper- around current time:1529509188757excute queryAll method...23:39:48.757 [http-nio-8080-exec-4] INFO c.g.f.aspect.LoggerAspectHelper- after current time:1529509188757 表达式 从上面的例子中我们都是使用一些正则表达式来指定我们的切入点的。在实际的使用中，不仅仅是execution，还有其他很多种类型的表达式。下面就列举一些： 1、execution用于匹配方法执行的连接点; 1execution(* com.glmapper.book.web.controller.*.*(..)) execution（）表达式的主体； 第一个 “*” 符号表示返回值的类型任意； com.glmapper.book.web.controller AOP所切的服务的包名，即，我们的业务部分 包名后面的”.” 表示当前包及子包 第二个”*” 表示类名，即所有类 .*(..) 表示任何方法名，括号表示参数，两个点表示任何参数类型 2、within用于匹配指定类型内的方法执行; 123456//如果在com.glmapper.book.web.controller包或其下的任何子包中//定义了该类型，则在Web层中有一个连接点。within(com.glmapper.book.web.controller..*)@Pointcut("within(com.glmapper.book.web.controller..*)")public void cutIn()&#123;&#125; @within：用于匹配所以持有指定注解类型内的方法； 12345678910/** * @description: 注解定义 * @email: &lt;a href="henugl@1992.163.com"&gt;&lt;/a&gt; * @author: glmapper@磊叔 * @date: 18/6/4 */@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.METHOD,ElementType.FIELD&#125;)public @interface AuthAnnotation &#123;&#125; 任何目标对象对应的类型持有AuthAnnotation注解的类方法；必须是在目标对象上声明这个注解，在接口上声明的对它不起作用。12345@within(com.glmapper.book.common.annotaion.AuthAnnotation)//所有被@AdviceAnnotation标注的类都将匹配@Pointcut("@within(com.glmapper.book.common.annotaion.AuthAnnotation)") public void cutIn()&#123;&#125; 3、this用于匹配当前AOP代理对象类型的执行方法；注意是AOP代理对象的类型匹配，这样就可能包括引入接口也类型匹配；this中使用的表达式必须是类型全限定名，不支持通配符； 12345678//当前目标对象（非AOP对象）实现了 UserService 接口的任何方法this(com.glmapper.book.web.service.UserService)//用于向通知方法中传入代理对象的引用。@Before("cutIn() &amp;&amp; this(proxy)")public void beforeAdvice(ProceedingJoinPoint poin,Object proxy)&#123; System.out.println("前置通知");&#125; 4、target用于匹配当前目标对象类型的执行方法；注意是目标对象的类型匹配，这样就不包括引入接口也类型匹配；target中使用的表达式必须是类型全限定名，不支持通配符；12345678//当前目标对象（非AOP对象）实现了 UserService 接口的任何方法target(com.glmapper.book.web.service.UserService)//用于向通知方法中传入代理对象的引用。@Before("cutIn() &amp;&amp; target(proxy)")public void beforeAdvice(ProceedingJoinPoint poin,Object proxy)&#123; System.out.println("前置通知");&#125; @target：用于匹配当前目标对象类型的执行方法，其中目标对象持有指定的注解；任何目标对象持有Secure注解的类方法；这个和@within一样必须是在目标对象上声明这个注解，在接口上声明的对它同样不起作用。1234@target(com.glmapper.book.common.annotaion.AuthAnnotation)@Pointcut("@target(com.glmapper.book.common.annotaion.AuthAnnotation)")public void cutIn()&#123;&#125; 5、args用于匹配当前执行的方法传入的参数为指定类型的执行方法；参数类型列表中的参数必须是类型全限定名，通配符不支持；args属于动态切入点，这种切入点开销非常大，非特殊情况最好不要使用； 12345678910//任何一个以接受“传入参数类型为java.io.Serializable”开头，//且其后可跟任意个任意类型的参数的方法执行，//args指定的参数类型是在运行时动态匹配的args (java.io.Serializable,..)//用于将参数传入到通知方法中。@Before("cutIn() &amp;&amp; args(age,username)")public void beforeAdvide(JoinPoint point, int age, String username)&#123; //...&#125; @args：用于匹配当前执行的方法传入的参数持有指定注解的执行；任何一个只接受一个参数的方法，且方法运行时传入的参数持有注解AuthAnnotation；动态切入点，类似于arg指示符； 123456@args (com.glmapper.book.common.annotaion.AuthAnnotation)@Before("@args(com.glmapper.book.common.annotaion.AuthAnnotation)")public void beforeAdvide(JoinPoint point)&#123; //...&#125; 6、@annotation使用“@annotation(注解类型)”匹配当前执行方法持有指定注解的方法；注解类型也必须是全限定类型名；1234567//当前执行方法上持有注解 AuthAnnotation将被匹配@annotation(com.glmapper.book.common.annotaion.AuthAnnotation)//匹配连接点被它参数指定的AuthAnnotation注解的方法。//也就是说，所有被指定注解标注的方法都将匹配。@Pointcut("@annotation(com.glmapper.book.common.annotaion.AuthAnnotation)")public void cutIn()&#123;&#125; 还有一种是bean的方式，没用过。有兴趣可以看看。 例子在下面说到的基础概念部分对应给出。 基础概念基础概念部分主要将 AOP 中的一些概念点捋一捋，这部分主要参考了官网上的一些解释。 AOPAOP(Aspect-Oriented Programming)， 即 面向切面编程, 它与 OOP( Object-Oriented Programming, 面向对象编程) 相辅相成, 提供了与 OOP 不同的抽象软件结构的视角。在 OOP 中,我们以类(class)作为我们的基本单元, 而 AOP 中的基本单元是 Aspect(切面)。 横切关注点(Cross Cutting Concern)：独立服务，如系统日志。如果不是独立服务（就是与业务耦合比较强的服务）就不能横切了。通常这种独立服务需要遍布系统各个角落，遍布在业务流程之中。 Target Object目标对象。织入 advice 的目标对象。 目标对象也被称为 advised object。因为 Spring AOP 使用运行时代理的方式来实现 aspect, 因此 adviced object 总是一个代理对象(proxied object)；注意， adviced object 指的不是原来的类, 而是织入 advice 后所产生的代理类。 织入（Weave）即Advice应用在JoinPoint的过程，这个过程叫织入。从另外一个角度老说就是将 aspect 和其他对象连接起来, 并创建 adviced object 的过程。 根据不同的实现技术， AOP织入有三种方式: 编译器织入，这要求有特殊的Java编译器 类装载期织入， 这需要有特殊的类装载器 动态代理织入, 在运行期为目标类添加增强( Advice )生成子类的方式。 Spring 采用动态代理织入, 而AspectJ采用编译器织入和类装载期 代理Spring AOP默认使用代理的是标准的JDK动态代理。这使得任何接口（或一组接口）都可以代理。 Spring AOP也可以使用CGLIB代理。如果业务对象不实现接口，则默认使用CGLIB。对接口编程而不是对类编程是一种很好的做法；业务类通常会实现一个或多个业务接口。在一些特殊的情况下，即需要通知的接口上没有声明的方法，或者需要将代理对象传递给具体类型的方法，有可能强制使用CGLIB。 Introductions我们知道Java语言本身并非是动态的，就是我们的类一旦编译完成，就很难再为他添加新的功能。但是在一开始给出的例子中，虽然我们没有向对象中添加新的方法，但是已经向其中添加了新的功能。这种属于向现有的方法添加新的功能，那能不能为一个对象添加新的方法呢？答案肯定是可以的，使用introduction就能够实现。 introduction：动态为某个类增加或减少方法。为一个类型添加额外的方法或字段。Spring AOP 允许我们为 目标对象 引入新的接口(和对应的实现)。 Aspect切面：通知和切入点的结合。 切面实现了cross-cutting（横切）功能。最常见的是logging模块、方法执行耗时模块，这样，程序按功能被分为好几层，如果按传统的继承的话，商业模型继承日志模块的话需要插入修改的地方太多，而通过创建一个切面就可以使用AOP来实现相同的功能了，我们可以针对不同的需求做出不同的切面。 而将散落于各个业务对象之中的Cross-cutting concerns 收集起来，设计各个独立可重用的对象，这些对象称之为Aspect；在上面的例子中我们根据不同的配置方式，定义了四种不同形式的切面。 JoinpointAspect 在应用程序执行时加入业务流程的点或时机称之为 Joinpoint ，具体来说，就是 Advice 在应用程序中被呼叫执行的时机，这个时机可能是某个方法被呼叫之前或之后（或两者都有），或是某个异常发生的时候。 Joinpoint &amp; ProceedingJoinPoint环绕通知 = 前置+目标方法执行+后置通知，proceed方法就是用于启动目标方法执行的。 环绕通知 ProceedingJoinPoint 执行 proceed 方法 的作用是让目标方法执行 ，这 也是环绕通知和前置、后置通知方法的一个最大区别。 Proceedingjoinpoint 继承了 JoinPoint 。是在JoinPoint的基础上暴露出 proceed 这个方法。proceed很重要，这个是aop代理链执行的方法；暴露出这个方法，就能支持aop:around 这种切面（其他的几种切面只需要用到JoinPoint，这跟切面类型有关）， 能决定是否走代理链还是走自己拦截的其他逻辑。 在环绕通知的方法中是需要返回一个Object类型对象的，如果把环绕通知的方法返回类型是void，将会导致一些无法预估的情况，比如：404。 Pointcut匹配 join points的谓词。Advice与切入点表达式相关联, 并在切入点匹配的任何连接点上运行。（例如，具有特定名称的方法的执行）。由切入点表达式匹配的连接点的概念是AOP的核心，Spring默认使用AspectJ切入点表达式语言。 在 Spring 中, 所有的方法都可以认为是Joinpoint, 但是我们并不希望在所有的方法上都添加 Advice, 而 Pointcut 的作用就是提供一组规则(使用 AspectJ pointcut expression language 来描述) 来匹配Joinpoint, 给满足规则的Joinpoint 添加 Advice。 Pointcut 和 Joinpoint在Spring AOP 中, 所有的方法执行都是 join point。 而 point cut 是一个描述信息，它修饰的是 join point， 通过 point cut，我们就可以确定哪些 join point 可以被织入Advice。 因此join point 和 point cut本质上就是两个不同维度上的东西。 advice 是在 join point 上执行的, 而 point cut 规定了哪些 join point 可以执行哪些 advice。 Advice概念Advice 是我们切面功能的实现，它是切点的真正执行的地方。比如像前面例子中打印时间的几个方法（被@Before等注解标注的方法都是一个通知）；Advice 在 Jointpoint 处插入代码到应用程序中。 分类BeforeAdvice，AfterAdvice，区别在于Advice在目标方法之前调用还是之后调用，Throw Advice 表示当目标发生异常时调用Advice。 before advice： 在 join point 前被执行的 advice. 虽然 before advice 是在 join point 前被执行, 但是它并不能够阻止 join point 的执行, 除非发生了异常(即我们在 before advice 代码中, 不能人为地决定是否继续执行 join point 中的代码) after return advice： 在一个 join point 正常返回后执行的 advice after throwing advice： 当一个 join point 抛出异常后执行的 advice after(final) advice： 无论一个 join point 是正常退出还是发生了异常, 都会被执行的 advice. around advice：在 join point 前和 joint point 退出后都执行的 advice. 这个是最常用的 advice. Advice、JoinPoint、PointCut 关系 下面这张图是在网上一位大佬的博客里发现的，可以帮助我们更好的理解这些概念之间的关系。 上面是对于AOP中涉及到的一些基本概念及它们之间的关系做了简单的梳理。 一些坑在调试程序过程中出现的一些问题记录 1、使用AOP拦截controller层的服务成功，但是页面报错4041234@Around("cutIn()")public void aroundAdvice(ProceedingJoinPoint poin) &#123; System.out.println("环绕通知");&#125; 这里需要注意的是再使用环绕通知时，需要给方法一个返回值。 12345@Around("cutIn()")public Object aroundAdvice(ProceedingJoinPoint poin) throws Throwable &#123; System.out.println("环绕通知"); return poin.proceed();&#125; 2、0 formal unbound in pointcut在spring 4.x中 提供了aop注解方式 带参数的方式。看下面例子： 1234567@Pointcut(value = "execution(* com.glmapper.framerwork.service.impl.*(int,int)) &amp;&amp; args(i,j)") public void cutIn(int i, int j) &#123;&#125; @Before(value="cutIn(i, j)",argNames = "i,j") public void beforeMethod( int i, int j) &#123; System.out.println("---------begins with " + i + "-" +j); &#125; 比如说这里，Before中有两个int类型的参数，如果此时我们在使用时没有给其指定参数，那么就会抛出：Caused by: java.lang.IllegalArgumentException: error at ::0 formal unbound in pointcut 异常信息。 本来是想放在一篇里面的，但是实在太长了，就分开吧；周末更新下]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>聊一聊</tag>
        <tag>aop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Guava 在实际场景中的应用封装]]></title>
    <url>%2F2018%2F11%2F10%2Fguavacacheone%2F</url>
    <content type="text"><![CDATA[毕竟西湖六月中，风光不与四时同。 接天莲叶无穷碧，映日荷花别样红。 晓出净慈寺送林子方-杨万里 周末与小伙伴约了一波西湖，这个时间荷花开的正好…，在开始文章之前先放一张“佛系”美图来镇楼！！！ 最近这段时间用了下谷歌的guava，自己封了一个缓存模板方案，特此记录，以备后续所需。 一个缓存定时清除任务带来的GC问题为什么要从这个来说起，因为不说这个就没guava什么事了！ 最近项目中需要使用缓存来对一查查询频繁的数据做缓存处理；首先我们也不希望引入三方的如redis或者memcache这样的服务进来，其次是我们对于数据一致性的要求并不是很高，不需要集群内的查询接口共享到一份缓存数据；所以这样一来我们只要实现一个基于内存的缓存即可。 最开始我并没有考虑使用guava来做这个事情，而是自己写了一套基于CurrentHashMap的缓存方案；这里需要明确一点，因为缓存在这个场景里面希望提供超时清除的能力，而基于所以在自己缓存框架中增加了定时清除过期数据的能力。 这里我就直接把定时清楚的这段代码放上来： 123456789101112131415161718192021222324252627 /** * 静态内部类来进行超时处理 */private class ClearCacheThread extends Thread &#123; @Override public void run() &#123; while (true)&#123; try &#123; long now = System.currentTimeMillis(); Object[] keys = map.keySet().toArray(); for (Object key : keys) &#123; CacheEntry entry = map.get(key); if (now - entry.time &gt;= cacheTimeout) &#123; synchronized (map) &#123; map.remove(key); if (LOGGER.isDebugEnabled())&#123; LOGGER.debug("language cache timeout clear"); &#125; &#125; &#125; &#125; &#125;catch (Exception e)&#123; LOGGER.error("clear out time cache value error;",e); &#125; &#125; &#125;&#125; 这个线程是用来单独处理过期数据的。缓存初始化时就会触发这个线程的start方法开始执行。 正式由于这段代码的不合理导致我在发布dev环境之后，机器GC触发的频次高的离谱。在尝试了不同的修复方案之后，最后选择放弃了；改用guava了！ 小伙伴们可以在下面留言来讨论下这里为什么会存在频繁GC的问题；我会把结论放在评论回复里面。 guava为什么选用guava呢，很显然，是大佬推荐的！！！ guava是谷歌提供的一个基于内存的缓存工具包，Guava Cache 提供了一种把数据（key-value对）缓存到本地（JVM）内存中的机制，适用于很少会改动的数据。Guava Cache 与 ConcurrentMap 很相似，但也不完全一样。最基本的区别是 ConcurrentMap 会一直保存所有添加的元素，直到显式地移除。相对地，Guava Cache 为了限制内存占用，通常都设定为自动回收元素。 对于我们的场景，guava 提供的能力满足了我们的需要： 数据改动小 基于内存 可以自动回收 既然选择它了，我们还是有必要来先对它有个大致的了解；先来看看它提供的一些类和接口： 接口/类 详细解释 Cache 【I】;定义get、put、invalidate等操作，这里只有缓存增删改的操作，没有数据加载的操作。 AbstractCache 【C】;实现Cache接口。其中批量操作都是循环执行单次行为，而单次行为都没有具体定义。 LoadingCache 【I】;继承自Cache。定义get、getUnchecked、getAll等操作，这些操作都会从数据源load数据。 AbstractLoadingCache 【C】;继承自AbstractCache，实现LoadingCache接口。 LocalCache 【C】;整个guava cache的核心类，包含了guava cache的数据结构以及基本的缓存的操作方法。 LocalManualCache 【C】;LocalCache内部静态类，实现Cache接口。其内部的增删改缓存操作全部调用成员变量localCache（LocalCache类型）的相应方法。 LocalLoadingCache 【C】;LocalCache内部静态类，继承自LocalManualCache类，实现LoadingCache接口。其所有操作也是调用成员变量localCache（LocalCache类型）的相应方法 CacheBuilder 【C】;缓存构建器。构建缓存的入口，指定缓存配置参数并初始化本地缓存。CacheBuilder在build方法中，会把前面设置的参数，全部传递给LocalCache，它自己实际不参与任何计算 CacheLoader 【C】;用于从数据源加载数据，定义load、reload、loadAll等操作。 整个来看的话，guava里面最核心的应该算是 LocalCache 这个类了。 123@GwtCompatible(emulated = true)class LocalCache&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implementsConcurrentMap&lt;K, V&gt; 关于这个类的源码这里就不细说了，直接来看下在实际应用中我的封装思路【封装满足我当前的需求，如果有小伙伴需要借鉴，可以自己在做扩展】 12345678910111213141516private static final int MAX_SIZE = 1000;private static final int EXPIRE_TIME = 10;private static final int DEFAULT_SIZE = 100;private int maxSize = MAX_SIZE;private int expireTime = EXPIRE_TIME;/** 时间单位（分钟） */private TimeUnit timeUnit = TimeUnit.MINUTES;/** Cache初始化或被重置的时间 */private Date resetTime;/** 分别记录历史最多缓存个数及时间点*/private long highestSize = 0;private Date highestTime;private volatile LoadingCache&lt;K, V&gt; cache; 这里先是定义了一些常量和基本的属性信息，当然这些属性会提供set&amp;get方法，供实际使用时去自行设置。 1234567891011121314151617181920212223242526272829303132333435363738public LoadingCache&lt;K, V&gt; getCache() &#123; //使用双重校验锁保证只有一个cache实例 if(cache == null)&#123; synchronized (this) &#123; if(cache == null)&#123; //CacheBuilder的构造函数是私有的，只能通过其静态方法newBuilder()来获得CacheBuilder的实例 cache = CacheBuilder.newBuilder() //设置缓存容器的初始容量为100 .initialCapacity(DEFAULT_SIZE) //缓存数据的最大条目 .maximumSize(maxSize) //定时回收:缓存项在给定时间内没有被写访问（创建或覆盖），则回收。 .expireAfterWrite(expireTime, timeUnit) //启用统计-&gt;统计缓存的命中率等 .recordStats() //设置缓存的移除通知 .removalListener((notification)-&gt; &#123; if (LOGGER.isDebugEnabled())&#123; LOGGER.debug("&#123;&#125; was removed, cause is &#123;&#125;" ,notification.getKey(), notification.getCause()); &#125; &#125;) .build(new CacheLoader&lt;K, V&gt;() &#123; @Override public V load(K key) throws Exception &#123; return fetchData(key); &#125; &#125;); this.resetTime = new Date(); this.highestTime = new Date(); if (LOGGER.isInfoEnabled())&#123; LOGGER.info("本地缓存&#123;&#125;初始化成功.", this.getClass().getSimpleName()); &#125; &#125; &#125; &#125; return cache;&#125; 上面这段代码是整个缓存的核心，通过这段代码来生成我们的缓存对象【使用了单例模式】。具体的属性参数看注释。 因为上面的那些都是封装在一个抽象类AbstractGuavaCache里面的，所以我又封装了一个CacheManger用来管理缓存，并对外提供具体的功能接口；在CacheManger中，我使用了一个静态内部类来创建当前默认的缓存。 12345678910111213141516171819202122232425262728/** * 使用静态内部类实现一个默认的缓存，委托给manager来管理 * * DefaultGuavaCache 使用一个简单的单例模式 * @param &lt;String&gt; * @param &lt;Object&gt; */private static class DefaultGuavaCache&lt;String, Object&gt; extendsAbstractGuavaCache&lt;String, Object&gt; &#123; private static AbstractGuavaCache cache = new DefaultGuavaCache(); /** * 处理自动载入缓存，按实际情况载入 * 这里 * @param key * @return */ @Override protected Object fetchData(String key) &#123; return null; &#125; public static AbstractGuavaCache getInstance() &#123; return DefaultGuavaCache.cache; &#125;&#125; 大概思路就是这样，如果需要扩展，我们只需要按照实际的需求去扩展AbstractGuavaCache这个抽象类就可以了。具体的代码贴在下面了。 完整的两个类AbstractGuavaCache123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137public abstract class AbstractGuavaCache&lt;K, V&gt; &#123; protected final Logger LOGGER = LoggerFactory.getLogger(AbstractGuavaCache.class); private static final int MAX_SIZE = 1000; private static final int EXPIRE_TIME = 10; /** 用于初始化cache的参数及其缺省值 */ private static final int DEFAULT_SIZE = 100; private int maxSize = MAX_SIZE; private int expireTime = EXPIRE_TIME; /** 时间单位（分钟） */ private TimeUnit timeUnit = TimeUnit.MINUTES; /** Cache初始化或被重置的时间 */ private Date resetTime; /** 分别记录历史最多缓存个数及时间点*/ private long highestSize = 0; private Date highestTime; private volatile LoadingCache&lt;K, V&gt; cache; public LoadingCache&lt;K, V&gt; getCache() &#123; //使用双重校验锁保证只有一个cache实例 if(cache == null)&#123; synchronized (this) &#123; if(cache == null)&#123; //CacheBuilder的构造函数是私有的，只能通过其静态方法ne //wBuilder()来获得CacheBuilder的实例 cache = CacheBuilder.newBuilder() //设置缓存容器的初始容量为100 .initialCapacity(DEFAULT_SIZE) //缓存数据的最大条目 .maximumSize(maxSize) //定时回收:缓存项在给定时间内没有被写访问 //（创建或覆盖），则回收。 .expireAfterWrite(expireTime, timeUnit) //启用统计-&gt;统计缓存的命中率等 .recordStats() //设置缓存的移除通知 .removalListener((notification)-&gt; &#123; if (LOGGER.isDebugEnabled())&#123; //... &#125; &#125;) .build(new CacheLoader&lt;K, V&gt;() &#123; @Override public V load(K key) throws Exception &#123; return fetchData(key); &#125; &#125;); this.resetTime = new Date(); this.highestTime = new Date(); if (LOGGER.isInfoEnabled())&#123; //... &#125; &#125; &#125; &#125; return cache; &#125; /** * 根据key从数据库或其他数据源中获取一个value，并被自动保存到缓存中。 * * 改方法是模板方法，子类需要实现 * * @param key * @return value,连同key一起被加载到缓存中的。 */ protected abstract V fetchData(K key); /** * 从缓存中获取数据（第一次自动调用fetchData从外部获取数据），并处理异常 * @param key * @return Value * @throws ExecutionException */ protected V getValue(K key) throws ExecutionException &#123; V result = getCache().get(key); if (getCache().size() &gt; highestSize) &#123; highestSize = getCache().size(); highestTime = new Date(); &#125; return result; &#125; public int getMaxSize() &#123; return maxSize; &#125; public void setMaxSize(int maxSize) &#123; this.maxSize = maxSize; &#125; public int getExpireTime() &#123; return expireTime; &#125; public void setExpireTime(int expireTime) &#123; this.expireTime = expireTime; &#125; public TimeUnit getTimeUnit() &#123; return timeUnit; &#125; public void setTimeUnit(TimeUnit timeUnit) &#123; this.timeUnit = timeUnit; &#125; public Date getResetTime() &#123; return resetTime; &#125; public void setResetTime(Date resetTime) &#123; this.resetTime = resetTime; &#125; public long getHighestSize() &#123; return highestSize; &#125; public void setHighestSize(long highestSize) &#123; this.highestSize = highestSize; &#125; public Date getHighestTime() &#123; return highestTime; &#125; public void setHighestTime(Date highestTime) &#123; this.highestTime = highestTime; &#125;&#125; DefaultGuavaCacheManager123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public class DefaultGuavaCacheManager &#123; private static final Logger LOGGER = LoggerFactory.getLogger(DefaultGuavaCacheManager.class); //缓存包装类 private static AbstractGuavaCache&lt;String, Object&gt; cacheWrapper; /** * 初始化缓存容器 */ public static boolean initGuavaCache() &#123; try &#123; cacheWrapper = DefaultGuavaCache.getInstance(); if (cacheWrapper != null) &#123; return true; &#125; &#125; catch (Exception e) &#123; LOGGER.error("Failed to init Guava cache;", e); &#125; return false; &#125; public static void put(String key, Object value) &#123; cacheWrapper.getCache().put(key, value); &#125; /** * 指定缓存时效 * @param key */ public static void invalidate(String key) &#123; cacheWrapper.getCache().invalidate(key); &#125; /** * 批量清除 * @param keys */ public static void invalidateAll(Iterable&lt;?&gt; keys) &#123; cacheWrapper.getCache().invalidateAll(keys); &#125; /** * 清除所有缓存项 ： 慎用 */ public static void invalidateAll() &#123; cacheWrapper.getCache().invalidateAll(); &#125; public static Object get(String key) &#123; try &#123; return cacheWrapper.getCache().get(key); &#125; catch (Exception e) &#123; LOGGER.error("Failed to get value from guava cache;", e); &#125; return null; &#125; /** * 使用静态内部类实现一个默认的缓存，委托给manager来管理 * * DefaultGuavaCache 使用一个简单的单例模式 * @param &lt;String&gt; * @param &lt;Object&gt; */ private static class DefaultGuavaCache&lt;String, Object&gt; extends AbstractGuavaCache&lt;String, Object&gt; &#123; private static AbstractGuavaCache cache = new DefaultGuavaCache(); /** * 处理自动载入缓存，按实际情况载入 * @param key * @return */ @Override protected Object fetchData(String key) &#123; return null; &#125; public static AbstractGuavaCache getInstance() &#123; return DefaultGuavaCache.cache; &#125; &#125;&#125; 参考Google Guava官方教程（中文版）]]></content>
      <categories>
        <category>缓存</category>
      </categories>
      <tags>
        <tag>cache</tag>
        <tag>guava</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式链路跟踪组件 SOFATracer 和 Zipkin 模型转换原理]]></title>
    <url>%2F2018%2F11%2F10%2Fsofa-tracer-zipkin%2F</url>
    <content type="text"><![CDATA[SOFATracer 是一个用于分布式系统调用跟踪的组件，通过统一的 traceId 将调用链路中的各种网络调用情况以日志的方式记录下来或者上报到 zipkin，以达到透视化网络调用的目的。这种以日志的方式记录下来或者上报到zipkin通常称为 Report，即数据上报 SOFATracer 的数据上报是在遵循 OpenTracing 规范基础上扩展出来的能力，OpenTracing 规范本身只是约定了数据模型和行为。本文主要目的在于分析 SOFATracer 的数据上报功能部分，主要内容如下： 基于 OpenTracing 规范的分布式链路跟踪解决方案 SOFATracer Report 数据上报模型 SOFATracer 和 Zipkin 模型转换原理 基于 OpenTracing 规范的分布式链路跟踪解决方案 OpenTracing 是一个轻量级的标准化层，它位于应用程序/类库和追踪或日志分析程序之间。为了解决不同的分布式追踪系统 API 不兼容的问题，OpenTracing 提供了一套平台无关、厂商无关的 API，同时也提供了统一的概念和数据标准。关于对 OpenTracing 标准的版本化描述可以参考 specification.md（https://github.com/opentracing/specification/blob/master/specification.md）。一些具体的概念下面将结合SOFATracer 的实现来一一说明。 目前基于 OpenTracing 规范实现的链路跟踪组件有 Jaeger，Appdash，Apache SkyWalking ，Datadog 等。像谷歌的 StackDriver Tracer 实际上并不是遵循 OpenTracing 规范的，但是都源自于 Dapper 这篇论文。 规范其实就是模型和行为的约束，在 OpenTracing 规范中有三种关键和相互关联的模型：Tracer、Span 和SpanContext；并且在规范中对于每个模型的行为也做了约定。 Tracer Tracer 可以被认为是一个由多个 Span 组成的有向无环图。一个 Tracer 可以用来描述一个请求从发出到收到响应整个链路过程。前提是需要在适当的地方进行埋点。下图就是一条完整的链路的展示： 在 SOFATracer 中 ，SofaTracer 实现了 Tracer 接口，实现了构建 span，数据载入（Inject）和 数据提取（Extract ) 的能力。 Start a new Span ：创建一个新的 Span 。通过指定的 operationName 来创建一个新的 Span。operationName 表示由 Span 完成的具体的工作 ( 例如，RPC 方法名称、函数名称或一个较大的计算任务中的阶段的名称)。 Inject a SpanContext：将 SpanContext 注入到给定类型的 “carrier” 中，用于进行跨进程的传输。 Extract a SpanContext ：从载体中提取中 spanContext 实例对象。这个过程是注入的逆过程。spanContext 中包括了贯穿整个链路的 traceId ，变化的 spanId ，父 spanId 以及透传数据等。 Span 一个 span 代表系统中具有开始时间和执行时长的逻辑运行单元。span 之间通过嵌套或者顺序排列建立逻辑因果关系，然后再通过这种关系来构建整个调用链路（Tracer）。 OpenTracing 规范 API 约定 Span 的模型如下（实际上就是 Span 接口中对应的方法，需要由遵循该规范的实现者必须提供的最小能力的集合）： Get the Span’s SpanContext： 通过 Span 获取 SpanContext （即使 span 已经结束，或者即将结束） Finish：结束一个 Span 。Finish 必须是 span 实例的最后一个被调用的方法。但是在主线程处理失败或者其他程序错误发生时，Finish 方法可能不会被调用。在这种情况下，实现者应该明确的记录 Span，保证数据的持久化（这一点 SOFATracer 其实是没有做的）。 Set a K:V tag on the Span：为 Span 设置 tag 。tag 的 key 必须是 string 类型；value 必须是 string、boolean 或数字类型。通常会使用 Tag 来记录跟踪系统感兴趣的一些指标数据。 Add a new log event：为 Span 增加一个 log 事件，用于记录 Span 生命周期中发生的事件。 Set a Baggage item： 设置一个 string:string 类型的键值对，一般是业务数据在全链路数据透明传输，存储在 SpanContext 中。 Get a Baggage item： 通过 key 获取 Baggage 中的元素。 SpanContext Span 上下文，几乎包含了需要在链路中传递的全部信息。另外，Span 间 References 就是通过 SpanContext 来建立关系的。根据 OpenTracing 规范要求，SpanContext 是不可变的，目的是防止由于 Span 的结束和相互关系，造成的复杂生命周期问题。 SpanContext 表示必须传播到后代 Spans 和跨进程边界的 Span 状态。SpanContext 在逻辑上分为两部分： 跨 Span 边界传播的用户级 “Baggage” 识别或以其他方式关联 Span 实例所需的任何 Tracer 实现特定字段（例如，trace_id，span_id，sampling，元组） Opentracing 中 SpanContext 接口中只有一个 baggageItems 方法，通过这个方法来遍历所有的 baggage 元素。 123public interface SpanContext &#123; Iterable&lt;Map.Entry&lt;String, String&gt;&gt; baggageItems();&#125; SOFATracer 扩展的 Tracer 的能力上面简单介绍了 OpenTracing 规范 API 对于 Tracer、Span、SpanContext 三个核心模型的规范定义。下面来看下 SOFATracer 是如何遵循规范并做扩展的。 在 OpenTracing 规范 基础上，SOFATracer 提供了实现，并在规范基础上提供了扩展功能。本文主要介绍上图中标绿色的部分，即数据上报功能。 SOFATracer 中提供了 Report 接口，然后基于此接口扩展了两个实现： 第一种 Report 扩展是基于 Disruptor（https://github.com/LMAX-Exchange/disruptor） 高性能无锁循环队列的异步落地磁盘的日志打印。 第二种 Report 扩展是提供远程上报，能够将 SOFATracer 的链路数据模型汇报到 Zipkin 中做调用链路的展示。 当然，SOFATracer 也允许用户自定义上报功能，只需要在自己的工程代码中实现 Report 接口即可，下面是 Report 接口的定义： 123456789101112public interface Reporter &#123; // 上报到远程服务器的持久化类型 String REMOTE_REPORTER = "REMOTE_REPORTER"; // 组合类型 String COMPOSITE_REPORTER = "COMPOSITE_REPORTER"; // 获取 Reporter 实例类型 String getReporterType(); // 上报 span void report(SofaTracerSpan span); // 关闭上报 span 的能力 void close();&#125; SOFATracer Report 数据上报模型 上面提到 SOFATracer 的 Report 有两种机制，一种是落到磁盘，另外一种是上报到 zipkin。SOFATracer 中这两种方案并不是二选一的，而是可以同时使用多个实现。例如，我们希望上报数据到 zipkin，先引入 tracer-sofa-boot-starter 这个依赖，并进行相关 zipkin 的配置之后就可以将链路数据上报到 zipkin，如果没有引入依赖则不会上报。本节来分析下 SOFATracer 上报数据过程的具体逻辑。 上面这张图描述了数据上报的几种方式： 绿色部分，上报 zipkin：这里其实就是实现上报 zipkin 的一个回调，当进行 reportSpan 操作时，会执行一个invokeReportListeners ，这个方法就是通知所有实现了 SpanReportListener 接口的类执行回调方法，然后在这个回调方法中将 span 数据上报到 zipkin。 红色部分，输出到磁盘：SOFATracer 为了提供更好的扩展能力，将输出日志的 Report 细分为 client 和 server 两种；并在 Tracer 基类中提供 generateClientStatReporter 和 generateServerStatReporter 两个抽象方法，供不同的组件自己来实现一些特殊化的定制。 关于何时进行上报，其实这个在 Opentracing API 的规范中已经给出了明确的时机。在上面的介绍中提到，“Finish必须是 span 实例的最后一个被调用的方法”，当 finish 方法被调用时也就意味着一个 span 生命周期的结束，为了保证 span 数据的完整性和正确性，SOFATracer reportSpan 的逻辑就是在 finish 方法被调用时触发执行。 数据落地磁盘 SOFATracer 日志落盘是基于Disruptor高性能无锁循环队列实现的，提供了异步打印日志到本地磁盘的能力。 append : 追溯 Report，无论是 clientReport 还是 serverReport ，底层均依赖 DiskReporterImpl 的实现。DiskReporterImpl 是 SOFATracer 统筹处理日志落盘的类。clientReport 和 serverReport 的最终调用都会走到DiskReporterImpl 中的 digestReport 这个方法。digestReport 中会将当前 span append 到环形缓冲队列中，append 操作就是发布一个事件的过程。 consume：consume 是 Disruptor 中的对应的消费模型；SOFATracer 中这个消费者就是将 SofaTracerSpan 中的数据写到日志文件中的。 事件发布过程： 数据上报 zipkin 前面提到，上报 zipkin 的是通过 onSpanReport 这个回调函数完成的。tracer-sofa-boot-starter 这个依赖中提供了 SpanReportListener 接口实现 ZipkinSofaTracerSpanRemoteReporter 。而在 onSpanReport 这个回调函数中，又将具体上报委托给了 AsyncReporter 来处理。 123456789@Overridepublic void onSpanReport(SofaTracerSpan span) &#123; if (span == null) &#123; return; &#125; //convert Span zipkinSpan = convertToZipkinSpan(span); this.delegate.report(zipkinSpan);&#125; 构建 AsyncReporter 对象需要两个参数： sender： 数据发送器，SOFATracer 中，sender 的是通过 RestTemplate 以 http 方式 来与 zipkin 进行通信传输的。 url：Zipkin 默认的 Collector 使用 http 协议里收集 Trace 信息，客户端调用 /api/v1/spans 或 /api/v2/spans 来上报 tracer 信息。这里我们使用的是 Zipkin V2 的 API。 AsyncReporter 中实际构建的是 BoundedAsyncReporter 对象 ， 并且在构建一个异步报告器是，会根据messageTimeoutNanos 是否大于 0 来决定是否起一个守护线程 flushThread；flushThread 作用是一直循环调用 BoundedAsyncReporter 的 flush 方法，将内存中的 Span 信息上报给 Zipkin。具体细节这里不展开分析。 SOFATracer 和 Zipkin 模型转换原理 在上小节中贴出的小段代码中，除了构建 delegate 对象用于执行上报外；另一个关键就是 SOFATracer 的 Span 模型转换成 Zipkin Span 模型。SOFATracer 从 2.2.0 版本之后支持 Zipkin v2 的模型 ，对于 Zipkin v1 的模型不在提供支持。 Zipkin v2的模型 下面是 zipkin GitHub 上提供的 Zipkin v2 的模型的结构化数据 Demo。关于 Zipkin 的 Span 模型支持可以查看 Simplified span2 format #1499 12345678910111213141516171819202122&#123; "kind": "CLIENT", "traceId": "5af7183fb1d4cf5f", "parentId": "6b221d5bc9e6496c", "id": "352bff9a74ca9ad2", "name": "query", "timestamp": 1461750040359000, "duration": 5000, "localEndpoint": &#123; "serviceName": "zipkin-server", "ipv4": "172.19.0.3", "port": 9411 &#125;, "remoteEndpoint": &#123; "serviceName": "mysql", "ipv4": "172.19.0.2", "port": 3306 &#125;, "tags": &#123; "jdbc.query": "//....discard" &#125;&#125; Zipkin v2 的模型结构较为简洁，整体看起来并没有什么繁重，这种对于使用者来说是很友好的，方便理解。其实在Zipkin v1 模型时，其整个模型也是比较复杂的，zipkin 社区对于 Zipkin 数据模型的变更也有讨论，见 Zipkin v2 span model #939 ；像现在 v2 模型中的 tags，替换了原本 v1 中的 binaryAnnotations，binaryAnnotations 的存在是 v1 模型复杂的重要原因。详见 去除原因。 SofaTracerSpan 模型SofaTracerSpan 是基于 Opentracing 标准来的。但是 Opentracing 标准并没有规定一个 Span 模型必须有哪些属性。所以各个基于该标准的产品在于 Span 的模型上是不统一的，大多会基于其本身产生的场景带有一些特殊的属性。 12345678910111213141516171819202122232425&#123; "client":true, "server":false, "durationMicroseconds":775, "endTime":1536288243446, "logType":"httpclient-digest.log", "operationName":"GET", "logs":[ // ... ], "sofaTracer":&#123; "clientReporter":&#123;&#125;, "tracerTags":&#123;&#125;, "tracerType":"httpclient" &#125;, "sofaTracerSpanContext":&#123; // sofaTracerSpanContext info &#125;, "spanReferences":[], "startTime":1536288242671, "tagsWithBool":&#123;&#125;, "tagsWithNumber":&#123;&#125;, "tagsWithStr":&#123;&#125;, "thisAsParentWhenExceedLayer":&#123;&#125;&#125; SOFATracer 的 Span 模型相较于 Opentracing 规范模型和 Zipkin v2 的模型来说，记录的数据信息更加丰富，且在 Opentracing 规范的基础上扩展了一套自己的 API，可以让使用者能够更加方便的在自己的代码中来获取链路中的信息；在日志中展示更多的 span 信息，能够帮助我们去了解一些调用细节，在发生问题时，也提供了更多排查问题的依据信息。 模型转换对照为了使得 SOFATracer 的数据能够被 zipkin 解析，需要将 SOFATracer 的 Span 模型转换成 zipkin v2 的数据模型。 Zipkin v2 Span Model SOFATracer Span Model 备注 traceId traceId traceId id spanId spanId parentId parentId 父spanId name operationName span 名，用来描述当前span 的行为 duration - 当前span的时间跨度;这里通过span的（结束时间-开始时间）获取 timestamp timestamp 当前span的开始时间 localEndPoint operationName&amp;host&amp;logData 标明这个span的来源 remoteEndPoint - 被调用方的服务名和地址 tags bizBaggage &amp; tags 额外的用于描述span的信息 整体来看，Span 模型相似度是很高，但是实际上并不能直接将某些相同的字段直接进行值复制；这里有一个 案例：ISSUE#57 。 traceId 和 spanId 处理zipkin 在自己的模型里做了很多特殊的处理。比如 traceId 需满足16 或者 32 位，长度不够的会高位补 0；所以在使用 SOFATracer 时，日志中的 traceId 和上报到 zipkin 的 traceId 长度不一致是合理的。 关于 spanId，我们期望在 zipkin 中展示是以（0.1,0.1.1,…）这种形式来描述，能够直观的看到 span 之间的依赖关系。但是目前使用的 zipkin 模型并不能满足我们的需求，主要原因在于虽然 zipkin 在 v2 模型中虽然支持 string 类型的 id ，但是其长度限制是16位，对于 SOFATracer 来说，如果存在较长的链路调用，会导致层次丢失。另外，如果上报 zipkin 的 span 的 parentId 为 0，那么 zipkin 将会不进行设置；而 SOFATracer 的第一个 span 的 id 就是从 0 开始的，所以会导致链路构建失败，如果我们尝试通过改变起始 id 来改变，会对整个模型产生影响。经过验证测试，我们最终采用的方案是使用冲突较小的 FNV64 Hash 算法将 String 类型转换成 long 型来描述我们的 spanId。 SOFARPC 上报的数据处理在整个模型转换中，比较核心的就是如何兼容 SOFARPC 上报的数据。Zipkin 在构建链路数时，其基本的模型是 client-server-client-server-.. 这种模式；不会出现 a server calling a server 这种情况，也就是带有kind = server 的 span 的 父span 应该是 kind = client。 SOFARPC 对于一个 rpc span 上报了两个 span 信息，这两个 span 除了 kind 类型不同之外，其他的信息是一样的。当数据上报给 zipkin 之后，zipkin 通过自己的算法来构建依赖树时，会对上报的 SOFARPC 数据处理有问题。下图是没有适配 SOFARPC 生成的链路: 这里可以看出，从 mvc 到 rpc 之间的关系被‘切断’了。 造成上述问题的原因在于，SOFATracer 上报数据到 zipkin 时，在 v2 模型中，zipkin 会通过广度优先遍历来构建依赖树，实际上在展示 services 或者 dependencies 时，zipkin ui 中的展示会依赖 endpiont 中的 serviceName ；两个条件： SOFARPC 的 span 有两个（client&amp;server），但是这两个 span 具有相同的 spanId 和 parentId，span.kind 不同。 zipkin 在构建依赖树时，依赖于 endpiont 中的 serviceName。该 servieName 依赖于 idToNode（Node.TreeBuilder 中的属性，Map 结构，映射关系为 spanId -&gt; span）。 123Node&lt;V&gt; previous = idToNode.put(id, node);if (previous != null) node.setValue(mergeFunction.merge(previous.value, node.value)); 这里当前 node 为 rpc server 类型时，previous 返回结果不为 null，会执行 merge 操作，该 merge 操作的核心就是设置当前 rpc node 的 remoteEndpoint，值为 rpc client 的 localEndpoint。 这样会有一个问题，就是 RPC 的 client 和 server Span 在 Zipkin 模型中的会被合并成一个 span；这样就会导致 server -&gt; server 的情况，与 zipkin 的 client -&gt; server 链路模型有冲突。如下图（绿色为SOFATracer span，黄色为 zipkin span）： 通过分析 zipkin 的构建过程，适配 SOFARPC 上报数据时，SOFARPC server span 的 remoteEndpoint 不能依赖 SOFARPC client span 的 localEndpoint，而应该依赖 SOFARPC client parentSpan 的 localEndpoint。下图为 SOFARPC 适配之后的依赖关系图： 总结 本文从 OpenTracing 规范说起，对 OpenTracing 规范中的模型和行为进行了简单的描述。结合 OpenTracing 规范，介绍了蚂蚁金服 SOFATracer 分布式链路跟踪的模型实现。在此基础上，对 SOFATracer 的数据上报功能进行了详细的分析，包括基于 disruptor 实现的异步日志落盘和上报数据到zipkin；最后对 SOFATracer 和 Zipkin 模型转换原理进行了说明，并对 SOFARPC 模型数据的上报处理进行了解析。 相关文档链接 SOFATracer GitHub Zipkin 官网 Zipkin GitHub opentracing 规范 opentracing 官网 disruptor]]></content>
      <categories>
        <category>SOFA</category>
      </categories>
      <tags>
        <tag>OpenTracing</tag>
        <tag>链路跟踪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[看完这个不会配置 logback ，请你吃瓜！]]></title>
    <url>%2F2018%2F11%2F10%2Flogone%2F</url>
    <content type="text"><![CDATA[之前在 日志？聊一聊slf4j吧 这篇文章中聊了下slf4j。本文也从实际的例子出发，针对logback的日志配置进行学习。 logack 简介 logback 官网：https://logback.qos.ch/ 目前还没有看过日志类框架的源码，仅限于如何使用。所以就不说那些“空话”了。最直观的认知是： logback和log4j是一个人写的 springboot默认使用的日志框架是logback。 三个模块组成 logback-core logback-classic logback-access 其他的关于性能，关于内存占用，关于测试，关于文档详见源码及官网说明 logback-core 是其它模块的基础设施，其它模块基于它构建，显然，logback-core 提供了一些关键的通用机制。logback-classic 的地位和作用等同于 Log4J，它也被认为是 Log4J 的一个改进版，并且它实现了简单日志门面 SLF4J；而 logback-access 主要作为一个与 Servlet 容器交互的模块，比如说tomcat或者 jetty，提供一些与 HTTP 访问相关的功能。 配置文件详解这部分主要来学习下logback配置文件的一些配置项。 configuration先来看这张图，这个结构就是整个logback.xml配置文件的结构。 对应来看下配置文件： 1234567891011121314151617&lt;configuration scan="true" scanPeriod="60 seconds" debug="false"&gt; &lt;property name="glmapper-name" value="glmapper-demo" /&gt; &lt;contextName&gt;$&#123;glmapper-name&#125;&lt;/contextName&gt; &lt;appender&gt; //xxxx &lt;/appender&gt; &lt;logger&gt; //xxxx &lt;/logger&gt; &lt;root&gt; //xxxx &lt;/root&gt; &lt;/configuration&gt; ps：想使用spring扩展profile支持，要以logback-spring.xml命名，其他如property需要改为springProperty scan:当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。 scanPeriod:设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。 debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 contextName每个logger都关联到logger上下文，默认上下文名称为“default”。但可以使用contextName标签设置成其他名字，用于区分不同应用程序的记录 property用来定义变量值的标签，property标签有两个属性，name和value；其中name的值是变量的名称，value的值时变量定义的值。通过property定义的值会被插入到logger上下文中。定义变量后，可以使“${name}”来使用变量。如上面的xml所示。 logger用来设置某一个包或者具体的某一个类的日志打印级别以及指定appender。 root根logger，也是一种logger，且只有一个level属性 appender负责写日志的组件，下面会细说 filterfilter其实是appender里面的子元素。它作为过滤器存在，执行一个过滤器会有返回DENY，NEUTRAL，ACCEPT三个枚举值中的一个。 DENY：日志将立即被抛弃不再经过其他过滤器 NEUTRAL：有序列表里的下个过滤器过接着处理日志 ACCEPT：日志会被立即处理，不再经过剩余过滤器 案例分析首先来配置一个非常简单的文件。这里申请下，我使用的是 logback-spring.xml。和 logback.xml 在properties上有略微差别。其他都一样。 工程：springboot+web 先来看下项目目录 properties中就是指定了日志的打印级别和日志的输出位置： 1234#设置应用的日志级别logging.level.com.glmapper.spring.boot=INFO#路径logging.path=./logs 通过控制台输出的loglogback-spring.xml的配置如下：123456789101112&lt;configuration&gt; &lt;!-- 默认的控制台日志输出，一般生产环境都是后台启动，这个没太大作用 --&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;Pattern&gt;%d&#123;HH:mm:ss.SSS&#125; %-5level %logger&#123;80&#125; - %msg%n&lt;/Pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level="info"&gt; &lt;appender-ref ref="STDOUT"/&gt; &lt;/root&gt;&lt;/configuration&gt; 打印日志的controller123456789101112private static final Logger LOGGER =LoggerFactory.getLogger(HelloController.class);@Autowiredprivate TestLogService testLogService;@GetMapping("/hello")public String hello()&#123; LOGGER.info("GLMAPPER-SERVICE:info"); LOGGER.error("GLMAPPER-SERVICE:error"); testLogService.printLogToSpecialPackage(); return "hello spring boot";&#125; 验证结果：123401:50:39.633 INFO com.glmapper.spring.boot.controller.HelloController- GLMAPPER-SERVICE:info01:50:39.633 ERROR com.glmapper.spring.boot.controller.HelloController- GLMAPPER-SERVICE:error 上面的就是通过控制台打印出来的，这个时候因为我们没有指定日志文件的输出，因为不会在工程目录下生产logs文件夹。 控制台不打印，直接输出到日志文件先来看下配置文件： 1234567891011121314151617181920212223242526272829303132333435&lt;configuration&gt; &lt;!-- 属性文件:在properties文件中找到对应的配置项 --&gt; &lt;springProperty scope="context" name="logging.path" source="logging.path"/&gt; &lt;springProperty scope="context" name="logging.level" source="logging.level.com.glmapper.spring.boot"/&gt; &lt;!-- 默认的控制台日志输出，一般生产环境都是后台启动，这个没太大作用 --&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;Pattern&gt;%d&#123;HH:mm:ss.SSS&#125; %-5level %logger&#123;80&#125; - %msg%n&lt;/Pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="GLMAPPER-LOGGERONE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/glmapper-spring-boot/glmapper-loggerone.log &lt;/file&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/glmapper-spring-boot/glmapper-loggerone.log.%d&#123;yyyy-MM-dd&#125;&lt;/FileNamePattern&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level="info"&gt; &lt;appender-ref ref="GLMAPPER-LOGGERONE"/&gt; &lt;/root&gt;&lt;/configuration&gt; 这里我们appender-ref指定的appender是GLMAPPER-LOGGERONE，因为之前没有名字为GLMAPPER-LOGGERONE的appender，所以要增加一个name为GLMAPPER-LOGGERONE的appender。 注意上面这个配置，我们是直接接将root的appender-ref直接指定到我们的GLMAPPER-LOGGERONE这个appender的。所以控制台中将只会打印出bannar之后就啥也不打印了，所有的启动信息都会被打印在日志文件glmapper-loggerone.log中。 但是实际上我们不希望我的业务日志中会包括这些启动信息。所以这个时候我们就需要通过logger标签来搞事情了。将上面的配置文件进行简单修改： 12345678&lt;logger name="com.glmapper.spring.boot.controller" level="$&#123;logging.level&#125;" additivity="false"&gt; &lt;appender-ref ref="GLMAPPER-LOGGERONE" /&gt;&lt;/logger&gt;&lt;root level="$&#123;logging.level&#125;"&gt; &lt;appender-ref ref="STDOUT"/&gt;&lt;/root&gt; 让root指向控制台输出；logger负责打印包com.glmapper.spring.boot.controller下的日志。 验证结果还是通过我们的测试controller来打印日志为例，但是这里不会在控制台出现日志信息了。期望的日志文件在./logs/glmapper-spring-boot/glmapper-loggerone.log。 logger和appender的关系上面两种是一个基本的配置方式，通过上面两个案例，我们先来了解下logger/appender/root之间的关系，然后再详细的说下logger和appender的配置细节。 在最前面介绍中提到，root是根logger,所以他两是一回事；只不过root中不能有name和additivity属性，是有一个level。 appender是一个日志打印的组件，这里组件里面定义了打印过滤的条件、打印输出方式、滚动策略、编码方式、打印格式等等。但是它仅仅是一个打印组件，如果我们不使用一个logger或者root的appender-ref指定某个具体的appender时，它就没有什么意义。 因此appender让我们的应用知道怎么打、打印到哪里、打印成什么样；而logger则是告诉应用哪些可以这么打。例如某个类下的日志可以使用这个appender打印或者某个包下的日志可以这么打印。 appender 配置详解这里以上面案例中的名为GLMAPPER-LOGGERONE的appender说明： 123456789101112131415161718&lt;appender name="GLMAPPER-LOGGERONE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/glmapper-spring-boot/glmapper-loggerone.log &lt;/file&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/glmapper-spring-boot/glmapper-loggerone.log.%d&#123;yyyy-MM-dd&#125;&lt;/FileNamePattern&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt; appender 有两个属性 name和class;name指定appender名称，class指定appender的全限定名。上面声明的是名为GLMAPPER-LOGGERONE，class为ch.qos.logback.core.rolling.RollingFileAppender的一个appender。 appender 的种类 ConsoleAppender：把日志添加到控制台 FileAppender：把日志添加到文件 RollingFileAppender：滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件。它是FileAppender的子类 append 子标签1&lt;append&gt;true&lt;/append&gt; 如果是 true，日志被追加到文件结尾，如果是false，清空现存文件，默认是true。 filter 子标签在简介中提到了filter；作用就是上面说的。可以为appender 添加一个或多个过滤器，可以用任意条件对日志进行过滤。appender 有多个过滤器时，按照配置顺序执行。 ThresholdFilter临界值过滤器，过滤掉低于指定临界值的日志。当日志级别等于或高于临界值时，过滤器返回NEUTRAL；当日志级别低于临界值时，日志会被拒绝。 123&lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;INFO&lt;/level&gt;&lt;/filter&gt; LevelFilter级别过滤器，根据日志级别进行过滤。如果日志级别等于配置级别，过滤器会根据onMath(用于配置符合过滤条件的操作) 和 onMismatch(用于配置不符合过滤条件的操作)接收或拒绝日志。 12345&lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; 关于NEUTRAL、ACCEPT、DENY 见上文简介中关于filter的介绍。 file 子标签file 标签用于指定被写入的文件名，可以是相对目录，也可以是绝对目录，如果上级目录不存在会自动创建，没有默认值。 123&lt;file&gt; $&#123;logging.path&#125;/glmapper-spring-boot/glmapper-loggerone.log&lt;/file&gt; 这个表示当前appender将会将日志写入到${logging.path}/glmapper-spring-boot/glmapper-loggerone.log这个目录下。 rollingPolicy 子标签这个子标签用来描述滚动策略的。这个只有appender的class是RollingFileAppender时才需要配置。这个也会涉及文件的移动和重命名（a.log-&gt;a.log.2018.07.22）。 TimeBasedRollingPolicy最常用的滚动策略，它根据时间来制定滚动策略，既负责滚动也负责出发滚动。这个下面又包括了两个属性： FileNamePattern maxHistory 123456789&lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!--日志文件输出的文件名:按天回滚 daily --&gt; &lt;FileNamePattern&gt; $&#123;logging.path&#125;/glmapper-spring-boot/glmapper-loggerone.log.%d&#123;yyyy-MM-dd&#125; &lt;/FileNamePattern&gt; &lt;!--日志文件保留天数--&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt;&lt;/rollingPolicy&gt; 上面的这段配置表明每天生成一个日志文件，保存30天的日志文件 FixedWindowRollingPolicy根据固定窗口算法重命名文件的滚动策略。 encoder 子标签对记录事件进行格式化。它干了两件事： 把日志信息转换成字节数组 把字节数组写入到输出流 12345&lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt;&lt;/encoder&gt; 目前encoder只有PatternLayoutEncoder一种类型。 定义一个只打印error级别日志的appcener1234567891011121314151617181920212223 &lt;!-- 错误日志 appender ： 按照每天生成日志文件 --&gt;&lt;appender name="ERROR-APPENDER" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;append&gt;true&lt;/append&gt; &lt;!-- 过滤器，只记录 error 级别的日志 --&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;error&lt;/level&gt; &lt;/filter&gt; &lt;!-- 日志名称 --&gt; &lt;file&gt;$&#123;logging.path&#125;/glmapper-spring-boot/glmapper-error.log&lt;/file&gt; &lt;!-- 每天生成一个日志文件，保存30天的日志文件 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!--日志文件输出的文件名:按天回滚 daily --&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/glmapper-spring-boot/glmapper-error.log.%d&#123;yyyy-MM-dd&#125;&lt;/FileNamePattern&gt; &lt;!--日志文件保留天数--&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;!-- 编码 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt; 定义一个输出到控制台的appender123456&lt;!-- 默认的控制台日志输出，一般生产环境都是后台启动，这个没太大作用 --&gt;&lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;Pattern&gt;%d&#123;HH:mm:ss.SSS&#125; %-5level %logger&#123;80&#125; - %msg%n&lt;/Pattern&gt; &lt;/encoder&gt;&lt;/appender&gt; logger 配置详解1234&lt;logger name="com.glmapper.spring.boot.controller" level="$&#123;logging.level&#125;" additivity="false"&gt; &lt;appender-ref ref="GLMAPPER-LOGGERONE" /&gt;&lt;/logger&gt; 上面的这个配置文件描述的是：com.glmapper.spring.boot.controller这个包下的${logging.level}级别的日志将会使用GLMAPPER-LOGGERONE来打印。logger有三个属性和一个子标签： name:用来指定受此logger约束的某一个包或者具体的某一个类。 level:用来设置打印级别（TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF），还有一个值INHERITED或者同义词NULL，代表强制执行上级的级别。如果没有设置此属性，那么当前logger将会继承上级的级别。 addtivity:用来描述是否向上级logger传递打印信息。默认是true。 appender-ref则是用来指定具体appender的。 不同日志隔离打印案例在前面的例子中我们有三种appender,一个是指定包约束的，一个是控制error级别的，一个是控制台的。然后这小节我们就来实现下不同日志打印到不同的log文件中。 根据包进行日志文件隔离这个例子里我们将com.glmapper.spring.boot.controller中的日志输出到glmapper-controller.log；将com.glmapper.spring.boot.service中的日志输出到glmapper-service.log。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;!--打印日志到glmapper-service.log的appender--&gt;&lt;appender name="GLMAPPER-SERVICE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/glmapper-spring-boot/glmapper-service.log &lt;/file&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/glmapper-spring-boot/glmapper-service.log.%d&#123;yyyy-MM-dd&#125;&lt;/FileNamePattern&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt;&lt;!--打印日志到glmapper-controller.log的appender--&gt;&lt;appender name="GLMAPPER-CONTROLLER" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt; $&#123;logging.path&#125;/glmapper-spring-boot/glmapper-controller.log &lt;/file&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/glmapper-spring-boot/glmapper-controller.log.%d&#123;yyyy-MM-dd&#125;&lt;/FileNamePattern&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt;&lt;!--此logger约束将.controller包下的日志输出到GLMAPPER-CONTROLLER，错误日志输出到GERROR-APPENDE；GERROR-APPENDE见上面--&gt;&lt;logger name="com.glmapper.spring.boot.controller" level="$&#123;logging.level&#125;" additivity="false"&gt; &lt;appender-ref ref="GLMAPPER-CONTROLLER" /&gt; &lt;appender-ref ref="GERROR-APPENDER" /&gt;&lt;/logger&gt;&lt;!--此logger约束将.service包下的日志输出到GLMAPPER-SERVICE，错误日志输出到GERROR-APPENDE；GERROR-APPENDE见上面--&gt;&lt;logger name="com.glmapper.spring.boot.service" level="$&#123;logging.level&#125;" additivity="false"&gt; &lt;appender-ref ref="GLMAPPER-SERVICE" /&gt; &lt;appender-ref ref="GERROR-APPENDER" /&gt;&lt;/logger&gt; 来看运行结果 1、glmaper-controller 2、glmapper-service 3、glmapper-error 满足我们的预期，但是这里有个小问题。在info日志里出现了error,当然这是正常的。假如我们不想在info里面出现error怎么办呢？很简单，我们以APPENDER-SERVICE为例，将filter过滤器进行修改： 将下面的：123&lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt;&lt;/filter&gt; 修改为： 1234567&lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;!-- 如果命中就禁止这条日志 --&gt; &lt;onMatch&gt;DENY&lt;/onMatch&gt; &lt;!-- 如果没有命中就使用这条规则 --&gt; &lt;onMismatch&gt;ACCEPT&lt;/onMismatch&gt; &lt;/filter&gt; 这里同时要注意的是，在logger中level需要设置为info级别。 根据类进行日志文件隔离这个其实也是和上面那个差不过，只不过粒度更细一点，一般情况下比如说我们有个定时任务类需要单独来记录其日志信息，这样我们就可以考虑使用基于类维度来约束打印。 123456789101112131415161718192021222324252627&lt;!--特殊功能单独appender 例如调度类的日志--&gt;&lt;appender name="SCHEDULERTASKLOCK-APPENDER" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt;$&#123;logging.path&#125;/glmapper-spring-boot/scheduler-task-lock.log&lt;/file&gt; &lt;!-- 每天生成一个日志文件，保存30天的日志文件 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!--日志文件输出的文件名:按天回滚 daily --&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/glmapper-spring-boot/scheduler-task-lock.log.%d&#123;yyyy-MM-dd&#125;&lt;/FileNamePattern&gt; &lt;!--日志文件保留天数--&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;!-- 编码 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt;&lt;!--这里指定到了具体的某一个类--&gt;&lt;logger name="com.glmapper.spring.boot.task.TestLogTask" level="$&#123;logging.level&#125;" additivity="true"&gt; &lt;appender-ref ref="SCHEDULERTASKLOCK-APPENDER" /&gt; &lt;appender-ref ref="ERROR-APPENDER" /&gt; &lt;/logger&gt; 最终TestLogTask中的日志将会被打印到这个自己独立的log文件中。如下所示： 根据自定义 logger 的 name 进行日志文件隔离logger的name除了类、包等约束之外，当然还可以这样来玩。。。 在进行案例之前，这里先把前面案例中logger声明的代码贴一下，以作对比,以TestLogTask类中的日志为例： 12private static final Logger LOGGER =LoggerFactory.getLogger(TestLogTask.class); 在getLogger中我们是将当前对象的class作为参数的，这个是为了打印时获取其全限定名的（见下面3-）。 12341-2018-07-21 11:15:42.003 [pool-1-thread-1] 2-INFO 3-com.glmapper.spring.boot.task.TestLogTask -4-com.glmapper.spring.boot.task:info 业务类定义我们同样是service包下定义一个类TestLogNameServiceImpl 1234567891011121314package com.glmapper.spring.boot.service;@Service("testLogNameService")public class TestLogNameServiceImpl implements TestLogNameService &#123; private static final Logger LOGGER = LoggerFactory.getLogger("GLMAPPER-TEST-LOG"); @Override public void print() &#123; LOGGER.info("GLMAPPER-TEST-LOG:this is special logger-----info"); LOGGER.error("GLMAPPER-TEST-LOG:this is special logger-------error"); &#125;&#125; appender和logger配置123456789101112131415161718192021222324252627&lt;appender name="ROOT-APPENDER" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;append&gt;true&lt;/append&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;file&gt;$&#123;logging.path&#125;/glmapper-spring-boot/glmapper-test.log&lt;/file&gt; &lt;!-- 每天生成一个日志文件，保存30天的日志文件 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!--日志文件输出的文件名:按天回滚 daily --&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/glmapper-spring-boot/glmapper-test.log.%d&#123;yyyy-MM-dd&#125; &lt;/FileNamePattern&gt; &lt;!--日志文件保留天数--&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;!-- 编码 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt;&lt;!--这里的name和业务类中的getLogger中的字符串是一样的--&gt;&lt;logger name="GLMAPPER-TEST-LOG" level="$&#123;logging.level&#125;" additivity="true"&gt; &lt;appender-ref ref="ROOT-APPENDER" /&gt; &lt;appender-ref ref="ERROR-APPENDER" /&gt; &lt;/logger&gt; 我们这个预期的是TestLogNameServiceImpl中的日志不打印到glmapper-service.log中，而是打印到glmapper-test.log中。 1、glmapper-test.log 2、glmapper-service.log 满足我们的预期。 如何使用logback打印mybatis的sql语句这个还是比较坑的。为什么。看下这个： 123&lt;settings&gt; &lt;setting name="logImpl" value="slf4j" /&gt;&lt;/settings&gt; 在mybatis-configration.xml中，我们通过这样一个配置项来关联到具体的日志组件。但是logImpl的实现中是没有logback的。那么怎么办呢？这里只能通过slf4j的方式桥接到logback。 然后在我们的logback-spring.xml中进行如下配置： 1234 &lt;!-- 将sql语句输出到具体的日志文件中 --&gt;&lt;logger name="com.alipay.sofa.cloudplatform.common.dao" level="$&#123;logging.sql.level&#125;" additivity="false"&gt; &lt;appender-ref ref="SQL-APPENDER"/&gt;&lt;/logger&gt; 这里有几个点需要注意的。首先是${logging.sql.level}这个必须是debug，这个是由mybatis本身实现决定的。而这里的name设定的com.alipay.sofa.cloudplatform.common.dao值就是我们dao接口的包路径。 网上看了一个比较典型的案例，这种方式只能输出到控制台，并不能将文件输出到日志文件；它是根据内部的一个实现机制偷了个懒。mybatis用logback日志不显示sql的解决办法。 总结本篇博客主要是整理最近工作中的一些日志配置积累，将每个细节进行总结一下，以作备忘。如果有时间的话会考虑看一个日志框架的源码。其实我觉得还是很有必要的，日志组件毕竟是需要进行日志文件落盘的，这个会涉及到许多的性能问题、缓冲区问题、队列问题、当然还有一些锁的问题、同步打印或者异步打印等问题。有兴趣的小伙伴可以看看，然后分享给我们。 后面准备写一写蚂蚁金服SOFABoot和SpringBoot的一些文章，如果有兴趣可以先看一波。 SOFABoot GitHub 传送门]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>logback</tag>
        <tag>slf4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SOFATracer 中 Disruptor 实践]]></title>
    <url>%2F2018%2F11%2F10%2Fsofa-tracer-disruptor%2F</url>
    <content type="text"><![CDATA[OpenTraceing 规范 OpenTracing语义标准 语义惯例 官方文档 SOFATracer 对 OpenTraceing 的实现 SOFATracer 就是根据 OpenTracing 规范 衍生出来的分布式 链路跟 踪的解决方案。 GitHub SOFATrcer 概念OpenTracing 标准中有三个重要的相互关联的类型，分别是Tracer, Span和 SpanContext。 【下面的概念说明过程中，如不做说明，所使用的案例代码均以SOFATracer中的实现为例。】 Tracer一个 trace 代表一个潜在的，分布式的，存在并行数据或并行执行轨迹（潜在的分布式、并行）的系统。一个trace可以认为是多个span的有向无环图（DAG）。 Tracer接口用来创建Span，以及处理如何处理Inject(serialize) 和 Extract (deserialize)，用于跨进程边界传递。 SOFATracer 中 SofaTracer这个类实现了 opentracing 的 Tracer 接口，并在此规范接口上做了一些扩展。看下Tracer 中声明的方法： 123456789101112public interface Tracer &#123; //启动一个新的span SpanBuilder buildSpan(String operationName); //将SpanContext上下文Inject（注入）到carrier &lt;C&gt; void inject(SpanContext spanContext, Format&lt;C&gt; format, C carrier); //将SpanContext上下文从carrier中Extract（提取） &lt;C&gt; SpanContext extract(Format&lt;C&gt; format, C carrier); interface SpanBuilder &#123; // 省略 &#125; &#125; 所以从接口定义来看，要实现一个Tracer，必须要实现其以下的几个能力： 启动一个新的spanSOFATracer 实现了 Tracer 中 buildSpan 方法： 1234@Overridepublic SpanBuilder buildSpan(String operationName) &#123; return new SofaTracerSpanBuilder(operationName);&#125; operationName :操作名称，字符串类型，表示由Span完成的工作 (例如，RPC方法名称、函数名称或一个较大的计算任务中的阶段的名称)。操作名称应该用泛化的字符串形式标识出一个Span实例。 何为泛化的字符串形式，比如现在有一个操作：获取用户 ；下面有几种标识方式： 1、/get 2、/get/user 3、/get/user/123 方式1过于抽象，方式3过于具体。方式2是正确的操作名。 将SpanContext上下文Inject（注入）到carrier12345678@Overridepublic &lt;C&gt; void inject(SpanContext spanContext, Format&lt;C&gt; format, C carrier) &#123; RegistryExtractorInjector&lt;C&gt; registryInjector = TracerFormatRegistry.getRegistry(format); if (registryInjector == null) &#123; throw new IllegalArgumentException("Unsupported injector format: " + format); &#125; registryInjector.inject((SofaTracerSpanContext) spanContext, carrier);&#125; SpanContext :实例 format（格式化）描述，一般会是一个字符串常量，但不做强制要求。通过此描述，通知Tracer实现，如何对SpanContext进行编码放入到carrier中。carrier，根据format确定。Tracer实现根据format声明的格式，将SpanContext序列化到carrier对象中。 RegistryExtractorInjector 见后面 将SpanContext上下文从carrier中Extract（提取）12345678@Overridepublic &lt;C&gt; SpanContext extract(Format&lt;C&gt; format, C carrier) &#123; RegistryExtractorInjector&lt;C&gt; registryExtractor = TracerFormatRegistry.getRegistry(format); if (registryExtractor == null) &#123; throw new IllegalArgumentException("Unsupported extractor format: " + format); &#125; return registryExtractor.extract(carrier);&#125; 格式描述符(format descriptor)(通常但不一定是字符串常量)，告诉Tracer的实现如何在载体对象中对SpanContext进行编码 载体(carrier)，其类型由格式描述符指定。Tracer的实现将根据格式描述对此载体对象中的SpanContext进行编码 返回一个SpanContext实例，可以使用这个SpanContext实例，通过Tracer创建新的Span。 Format从Tracer的注入和提取来看，format都是必须的。 Inject（注入）和Extract（提取）依赖于可扩展的format参数。format参数规定了另一个参数&quot;carrier&quot;的类型，同时约束了&quot;carrier&quot;中SpanContext是如何编码的。所有的Tracer实现，都必须支持下面的format。 Text Map: 基于字符串：字符串的map,对于key和value不约束字符集。 HTTP Headers: 适合作为HTTP头信息的，基于字符串：字符串的map。（RFC 7230.在工程实践中，如何处理HTTP头具有多样性，强烈建议tracer的使用者谨慎使用HTTP头的键值空间和转义符） Binary: 一个简单的二进制大对象，记录SpanContext的信息。 在上面的注入和提取代码中，有如下代码片段： 123456//注入RegistryExtractorInjector&lt;C&gt; registryInjector = TracerFormatRegistry.getRegistry(format);//提取RegistryExtractorInjector&lt;C&gt; registryExtractor = TracerFormatRegistry.getRegistry(format); 来通过TracerFormatRegistry这个类来来看下 SOFATracer 中的 Format 的具体实现。 X-B3在看Format之前，先了解下X-B3。 12Access-Control-Expose-Headers: X-B3-TraceId,X-B3-ParentSpanId,X-B3-SpanId HTTP请求时其span参数通过http headers来传递追踪信息；header中对应的key分别是: X-B3-TraceId: 64 encoded bits（id被encode为hex Strings） X-B3-SpanId : 64 encoded bits X-B3-ParentSpanId: 64 encoded bits X-B3-Sampled:(是否采样) Boolean (either “1” or “0”)（下面的调用是否进行采样） X-B3-Flags:a Long SOFATracer 中的 Format具体代码在 tracer-core -&gt; com.alipay.common.tracer.core.registy 包下: TextMapFormatter TextMapB3Formatter HttpHeadersFormatter HttpHeadersB3Formatter BinaryFormater BinaryFormater：这个的注入和提取实现没有编解码一说；本身就是基于二进制流的操作。 TextMapB3Formatter/TextMapFormatter 和 HttpHeadersB3Formatter/HttpHeadersFormatter 区别就在于编解码不同。HttpHeadersB3Formatter使用的是 URLDecoder.decode &amp;&amp; URLDecoder.encode ; TextMapB3Formatter 返回的是值本身（如果为空或者null则返回空字符串）。 TextMapFormatter和TextMapB3Formatter区别在于注入或者提取是使用的key不用。TextMapB3Formatter中使用的是 x-b3-{} 的字符串作为key。 Span一个span代表系统中具有开始时间和执行时长的逻辑运行单元。span之间通过嵌套或者顺序排列建立逻辑因果关系。当Span结束后(span.finish())，除了通过Span获取SpanContext外，下列其他所有方法都不允许被调用。 同样先来看下opentracing规范api 定义的 span 的定义及方法： 123456789101112131415161718public interface Span extends Closeable &#123; SpanContext context(); void finish(); void finish(long finishMicros); void close(); Span setTag(String key, String value); Span setTag(String key, boolean value); Span setTag(String key, Number value); Span log(Map&lt;String, ?&gt; fields); Span log(long timestampMicroseconds, Map&lt;String, ?&gt; fields); Span log(String event); Span log(long timestampMicroseconds, String event); Span setBaggageItem(String key, String value); String getBaggageItem(String key); Span setOperationName(String operationName); Span log(String eventName, /* @Nullable */ Object payload); Span log(long timestampMicroseconds, String eventName, /* @Nullable */ Object payload);&#125; 通过Span获取SpanContext12345//SOFATracerSpan@Overridepublic SpanContext context() &#123; return this.sofaTracerSpanContext;&#125; 返回值，Span构建时传入的SpanContext。这个返回值在Span结束后(span.finish())，依然可以使用。 复写操作名12345@Overridepublic Span setOperationName(String operationName) &#123; this.operationName = operationName; return this;&#125; operationName:新的操作名，覆盖构建Span时，传入的操作名。 结束Span123456789101112@Overridepublic void finish() &#123; this.finish(System.currentTimeMillis());&#125;@Overridepublic void finish(long endTime) &#123; this.setEndTime(endTime); //关键记录:report span this.sofaTracer.reportSpan(this); SpanExtensionFactory.logStoppedSpan(this);&#125; 有一个可选参数，如果指定完成时间则使用当前指定的时间；如果省略此参数，使用当前时间作为完成时间。finish方法中会将当前span进行report操作。 为Span设置tagTag是一个key:value格式的数据。key必须是String类型，value可以是字符串、布尔或者数字。 字符串类型的value 设置tag 1234567891011121314151617181920@Overridepublic Span setTag(String key, String value) &#123; if (StringUtils.isBlank(key) || StringUtils.isBlank(value)) &#123; return this; &#125; this.tagsWithStr.put(key, value); //注意:server 还是 client 在 OpenTracing 标准中是用 tags 标识的,所以在这里进行判断 if (isServer()) &#123; Reporter serverReporter = this.sofaTracer.getServerReporter(); if (serverReporter != null) &#123; this.setLogType(serverReporter.getReporterType()); &#125; &#125; else if (isClient()) &#123; Reporter clientReporter = this.sofaTracer.getClientReporter(); if (clientReporter != null) &#123; this.setLogType(clientReporter.getReporterType()); &#125; &#125; return this;&#125; 布尔类型的value 设置tag 1234public Span setTag(String key, boolean value) &#123; this.tagsWithBool.put(key, value); return this;&#125; 数字类型的value 设置tag 1234567public Span setTag(String key, Number number) &#123; if (number == null) &#123; return this; &#125; this.tagsWithNumber.put(key, number); return this;&#125; Log结构化数据1234567891011@Overridepublic Span log(long currentTime, Map&lt;String, ?&gt; map) &#123; AssertUtils.isTrue(currentTime &gt;= startTime, "current time must greater than start time"); this.logs.add(new LogData(currentTime, map)); return this;&#125;@Overridepublic Span log(Map&lt;String, ?&gt; map) &#123; return this.log(System.currentTimeMillis(), map);&#125; Map&lt;String, ?&gt; map : 键必须是字符串类型，值可以是任意类型 currentTime : 时间戳。如果指定时间戳，那么它必须在span的开始和结束时间之内。 设置一个baggage（随行数据）元素Baggage元素是一个键值对集合，将这些值设置给给定的Span，Span的SpanContext，以及所有和此Span有直接或者间接关系的本地Span。 也就是说，baggage元素随trace一起保持在带内传递。（译者注：带内传递，在这里指，随应用程序调用过程一起传递） Baggage元素为OpenTracing的实现全栈集成，提供了强大的功能 （例如：任意的应用程序数据，可以在移动端创建它，显然的，它会一直传递了系统最底层的存储系统。由于它如此强大的功能，他也会产生巨大的开销，请小心使用此特性。 再次强调，请谨慎使用此特性。每一个键值都会被拷贝到每一个本地和远程的下级相关的span中，因此，总体上，他会有明显的网络和CPU开销。 12345@Overridepublic Span setBaggageItem(String key, String value) &#123; this.sofaTracerSpanContext.setBizBaggageItem(key, value); return this;&#125; SofaTracerSpan 中的属性 sofaTracer : 当前 tracer spanReferences : 当前span的关系，ChildOf(引用) or FollowsFrom（跟随） tagsWithStr : String 类型的tag 集合 tagsWithBool : 布尔类型的tag集合 tagsWithNumber : 数值类型的tag集合 logs : log结构化数据列表，通过span.log（map）操作的map,均存储在logs中。 operationName：当前span的操作名 sofaTracerSpanContext：当前 spanContext startTime : 当前span 开始时间 endTime : 当前span 结束时间，在finish方法中传入。 logType : report时才有意义:摘要日志类型,日志能够正确打印的关键信息；当前 span 的日志类型,如:客户端为 rpc-client-digest.log,服务端为 rpc-server-digest.log parentSofaTracerSpan：父亲 span,当作为客户端结束并弹出线程上下文时,需要将父亲 span 再放入 SpanContextopentracing 中 SpanContext 接口中只有一个baggageItems方法，通过这个方法来遍历所有的baggage元素。 123public interface SpanContext &#123; Iterable&lt;Map.Entry&lt;String, String&gt;&gt; baggageItems();&#125; 相对于OpenTracing中其他的功能，SpanContext更多的是一个“概念”。也就是说，OpenTracing实现中，需要重点考虑，并提供一套自己的API。 OpenTracing的使用者仅仅需要，在创建span、向传输协议Inject（注入）和从传输协议中Extract（提取）时，使用SpanContext和references， OpenTracing要求，SpanContext是不可变的，目的是防止由于Span的结束和相互关系，造成的复杂生命周期问题。 Disruptor 简介 A High Performance Inter-Thread Messaging Library 高性能的线程间消息传递库 关于 Disruptor 的 一些原理分析可以参考：disruptor 案例先通过 Disruptor 的一个小例子来有个直观的认识；先看下它的构造函数： 1234567891011public Disruptor( final EventFactory&lt;T&gt; eventFactory, final int ringBufferSize, final ThreadFactory threadFactory, final ProducerType producerType, final WaitStrategy waitStrategy)&#123; this( RingBuffer.create(producerType, eventFactory, ringBufferSize, waitStrategy), new BasicExecutor(threadFactory));&#125; eventFactory : 在环形缓冲区中创建事件的 factory ringBufferSize:环形缓冲区的大小，必须是2的幂。 threadFactory：用于为处理器创建线程。 producerType：生成器类型以支持使用正确的sequencer和publisher创建RingBuffer；枚举类型，SINGLE、MULTI两个项。对应于 SingleProducerSequencer和MultiProducerSequencer两种Sequencer。 waitStrategy : 等待策略； 如果我们想构造一个disruptor,那么我们就需要上面的这些组件。从eventFactory来看，还需要一个具体的Event来作为消息事件的载体。【下面按照官方给的案例进行简单的修改作为示例】 消息事件 LongEvent ，能够被消费的数据载体123456789public class LongEvent &#123; private long value; public void set(long value) &#123; this.value = value; &#125; public long getValue() &#123; return value; &#125;&#125; 创建消息事件的factory123456public class LongEventFactory implements EventFactory&lt;LongEvent&gt; &#123; @Override public LongEvent newInstance() &#123; return new LongEvent(); &#125;&#125; ConsumerThreadFactory1234567public class ConsumerThreadFactory implements ThreadFactory &#123; private final AtomicInteger index = new AtomicInteger(1); @Override public Thread newThread(Runnable r) &#123; return new Thread(r, "disruptor-thread-" + index.getAndIncrement()); &#125;&#125; OK ，上面的这些可以满足创建一个disruptor了： 123456789101112131415private int ringBufferCapacity = 8;//消息事件生产FactoryLongEventFactory longEventFactory = new LongEventFactory();//执行事件处理器线程FactoryConsumerThreadFactory consumerThreadFactory = new ConsumerThreadFactory();//用于环形缓冲区的等待策略。WaitStrategy waitStrategy = new BlockingWaitStrategy();//构建disruptorDisruptor&lt;LongEvent&gt; disruptor = new Disruptor&lt;&gt;( longEventFactory, ringBufferCapacity, longEventThreadFactory, ProducerType.SINGLE, waitStrategy); 现在是已经有了 disruptor 了，然后通过：start 来启动： 12//启动 disruptor disruptor.start(); 到这里，已经构建了一个disruptor；但是目前怎么使用它来发布消息和消费消息呢？ 发布消息下面在 for 循环中 发布 5 条数据： 12345678910RingBuffer&lt;LongEvent&gt; ringBuffer = disruptor.getRingBuffer();for (long l = 0; l &lt; 5; l++)&#123; long sequence = ringBuffer.next(); LongEvent event = ringBuffer.get(sequence); event.set(100+l); System.out.println("publish event :" + l); ringBuffer.publish(sequence); Thread.sleep(1000);&#125; 消息已经发布，下面需要设定当前disruptor的消费处理器。前面已经有个LongEvent 和 EventFactory ; 在disruptor中是通过 EventHandler 来进行消息消费的。 编写消费者代码1234567public class LongEventHandler implements EventHandler&lt;LongEvent&gt; &#123; @Override public void onEvent(LongEvent event, long sequence, boolean endOfBatch) throws Exception &#123; System.out.println("Event: " + event.getValue()+" -&gt; " + Thread.currentThread().getName()); Thread.sleep(2000); &#125;&#125; 将 eventHandler 设置到 disruptor 的处理链上 123//将处理事件的事件处理程序 -&gt; 消费事件的处理程序LongEventHandler longEventHandler = new LongEventHandler();disruptor.handleEventsWith(longEventHandler); 运行结果（这里）：123456789101112131415publish event :0Event: 0 -&gt; disruptor-thread-1--------------------------------&gt;publish event :1Event: 1 -&gt; disruptor-thread-1--------------------------------&gt;publish event :2Event: 2 -&gt; disruptor-thread-1--------------------------------&gt;publish event :3Event: 3 -&gt; disruptor-thread-1--------------------------------&gt;publish event :4Event: 4 -&gt; disruptor-thread-1--------------------------------&gt; 基本概念和原理Disruptor整个基于ringBuffer实现的生产者消费者模式的容器。主要属性 12345private final RingBuffer&lt;T&gt; ringBuffer;private final Executor executor;private final ConsumerRepository&lt;T&gt; consumerRepository = new ConsumerRepository&lt;&gt;();private final AtomicBoolean started = new AtomicBoolean(false);private ExceptionHandler&lt;? super T&gt; exceptionHandler = new ExceptionHandlerWrapper&lt;&gt;(); ringBuffer：内部持有一个 RingBuffer 对象，Disruptor 内部的事件发布都是依赖这个RingBuffer对象完成的。 executor：消费事件的线程池 consumerRepository：提供存储库机制，用于将EventHandler与EventProcessor关联起来 started : 用于标志当前Disruptor是否已经启动 exceptionHandler : 异常处理器，用于处理BatchEventProcessor事件周期中 uncaught exceptions 。 RingBuffer环形队列[实现上是一个数组]，可以类比为BlockingQueue之类的队列，ringBuffer的使用，使得内存被循环使用，减少了某些场景的内存分配回收扩容等耗时操作。 12public final class RingBuffer&lt;E&gt; extends RingBufferFields&lt;E&gt; implements Cursored, EventSequencer&lt;E&gt;, EventSink&lt;E&gt; E：在事件的交换或并行协调期间存储用于共享的数据的实现 -&gt; 消息事件 Sequencer RingBuffer 中 生产者的顶级父接口，其直接实现有SingleProducerSequencer和MultiProducerSequencer；对应 SINGLE、MULTI 两个枚举值。 EventHandler事件处置器，改接口用于对外扩展来实现具体的消费逻辑。如上面 demo 中的 LongEventHandler ; 1234//回调接口，用于处理&#123;@link RingBuffer&#125;中可用的事件public interface EventHandler&lt;T&gt; &#123; void onEvent(T event, long sequence, boolean endOfBatch) throws Exception;&#125; event : RingBuffer 已经发布的事件 sequence : 正在处理的事件 的序列号 endOfBatch : 用来标识否是来自 RingBuffer 的批次中的最后一个事件 SequenceBarrier消费者路障。规定了消费者如何向下走。事实上，该路障算是变向的锁。 12345678910111213final class ProcessingSequenceBarrier implements SequenceBarrier &#123; //当等待（探测）的需要不可用时，等待的策略 private final WaitStrategy waitStrategy; //依赖的其它Consumer的序号，这个用于依赖的消费的情况， //比如A、B两个消费者，只有A消费完，B才能消费。 private final Sequence dependentSequence; private volatile boolean alerted = false; //Ringbuffer的写入指针 private final Sequence cursorSequence; //RingBuffer对应的Sequencer private final Sequencer sequencer; //exclude method&#125; waitStrategy 决定了消费者采用何种等待策略。 WaitStrategy Strategy employed for making {@link EventProcessor}s wait on a cursor {@link Sequence}. EventProcessor 的等待策略；具体实现在 disruptor 中有8种， 这些等待策略不同的核心体现是在如何实现 waitFor 这个方法上。 EventProcessor事件处理器，实际上可以理解为消费者模型的框架，实现了线程Runnable的run方法，将循环判断等操作封在了里面。该接口有三个实现类: 1、BatchEventProcessor 12345678910public final class BatchEventProcessor&lt;T&gt; implements EventProcessor &#123; private final AtomicBoolean running = new AtomicBoolean(false); private ExceptionHandler&lt;? super T&gt; exceptionHandler = new FatalExceptionHandler(); private final DataProvider&lt;T&gt; dataProvider; private final SequenceBarrier sequenceBarrier; private final EventHandler&lt;? super T&gt; eventHandler; private final Sequence sequence = new Sequence( Sequencer.INITIAL_CURSOR_VALUE); private final TimeoutHandler timeoutHandler; //exclude method&#125; ExceptionHandler：异常处理器 DataProvider：数据来源，对应 RingBuffer EventHandler：处理 Event 的回调对象 SequenceBarrier：对应的序号屏障 TimeoutHandler：超时处理器，默认情况为空，如果要设置，只需要要将关联的EventHandler实现TimeOutHandler即可。 如果我们选择使用 EventHandler 的时候，默认使用的就是 BatchEventProcessor，它与EventHandler是一一对应，并且是单线程执行。 如果某个RingBuffer有多个BatchEventProcessor，那么就会每个BatchEventProcessor对应一个线程。 2、WorkProcessor 1234567891011121314151617public final class WorkProcessor&lt;T&gt; implements EventProcessor &#123; private final AtomicBoolean running = new AtomicBoolean(false); private final Sequence sequence = new Sequence(Sequencer.INITIAL_CURSOR_VALUE); private final RingBuffer&lt;T&gt; ringBuffer; private final SequenceBarrier sequenceBarrier; private final WorkHandler&lt;? super T&gt; workHandler; private final ExceptionHandler&lt;? super T&gt; exceptionHandler; private final Sequence workSequence; private final EventReleaser eventReleaser = new EventReleaser() &#123; @Override public void release() &#123; sequence.set(Long.MAX_VALUE); &#125; &#125;; private final TimeoutHandler timeoutHandler;&#125; 基本和 BatchEventProcessor 类似，不同在于，用于处理Event的回调对象是WorkHandler。 原理图 无消费者情况下，生产者保持生产，但是 remainingCapacity 保持不变在写demo的过程中，本来想通过不设定 消费者 来观察 RingBuffer 可用容量变化的。但是验证过程中，一直得不到预期的结果，(注：没有设置消费者，只有生产者)，先看结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950publish event :0bufferSie:8remainingCapacity:8cursor:0--------------------------------&gt;publish event :1bufferSie:8remainingCapacity:8cursor:1--------------------------------&gt;publish event :2bufferSie:8remainingCapacity:8cursor:2--------------------------------&gt;publish event :3bufferSie:8remainingCapacity:8cursor:3--------------------------------&gt;publish event :4bufferSie:8remainingCapacity:8cursor:4--------------------------------&gt;publish event :5bufferSie:8remainingCapacity:8cursor:5--------------------------------&gt;publish event :6bufferSie:8remainingCapacity:8cursor:6--------------------------------&gt;publish event :7bufferSie:8remainingCapacity:8cursor:7--------------------------------&gt;publish event :8bufferSie:8remainingCapacity:8cursor:8--------------------------------&gt;publish event :9bufferSie:8remainingCapacity:8cursor:9--------------------------------&gt; 从结果来看，remainingCapacity 的值应该随着 发布的数量 递减的；但是实际上它并没有发生任何变化。 来看下ringBuffer.remainingCapacity() 这个方法： 123456789/** * Get the remaining capacity for this ringBuffer. * * @return The number of slots remaining. */public long remainingCapacity()&#123; return sequencer.remainingCapacity();&#125; 这里面又使用 sequencer.remainingCapacity()这个方法来计算的。上面的例子中使用的是ProducerType.SINGLE，那来看SingleProducerSequencer 这个里面remainingCapacity的实现。 1234567891011@Overridepublic long remainingCapacity()&#123; //上次申请完毕的序列值 long nextValue = this.nextValue; //计算当前已经消费到的序列值 long consumed = Util.getMinimumSequence(gatingSequences, nextValue); //当前生产到的序列值 long produced = nextValue; return getBufferSize() - (produced - consumed);&#125; 来解释下这段代码的含义： 假设当前 ringBuffer 的 bufferSize 是 8 ；上次申请到的序列号是 5，其实也就是说已经生产过占用的序列号是5；假设当前已经消费到的序列号是 3，那么剩余的容量为： 8-（5-2） = 5； 因为这里我们可以确定 bufferSize 和 produced 的值了，那么 remainingCapacity 的结果就取决于getMinimumSequence的计算结果了。 123456789public static long getMinimumSequence(final Sequence[] sequences, long minimum)&#123; for (int i = 0, n = sequences.length; i &lt; n; i++) &#123; long value = sequences[i].get(); minimum = Math.min(minimum, value); &#125; return minimum;&#125; 这个方法是从 Sequence 数组中获取最小序列 。如果sequences 为空，则返回 minimum。回到上一步，看下sequences这个数组是从哪里过来的，它的值在哪里设置的。 1long consumed = Util.getMinimumSequence(gatingSequences, nextValue); gatingSequences是 SingleProducerSequencer父类 AbstractSequencer 中的成员变量： 1protected volatile Sequence[] gatingSequences = new Sequence[0]; gatingSequences 是在下面这个方法里面来管理的。 12345678/** * @see Sequencer#addGatingSequences(Sequence...) */@Overridepublic final void addGatingSequences(Sequence... gatingSequences)&#123; SequenceGroups.addSequences(this, SEQUENCE_UPDATER, this, gatingSequences);&#125; 这个方法的调用栈向前追溯有这几个地方调用了： WorkerPool来管理多个消费者；hangdlerEventsWith 这个方法也是用来设置消费者的。但是在上面的测试案例中我们是想通过不设定消费者 只设定生成者 来观察 环形队列的占用情况，所以gatingSequences 会一直是空的，因此在计算时会把 produced 的值作为 minimum 返回。这样每次计算就相当于： 1return getBufferSize() - (produced - produced) === getBufferSize(); 也就验证了为何在不设定消费者的情况下，remainingCapacity 的值会一直保持不变。 SOFATracer 中 Disruptor 实践SOFATracer中，AsyncCommonDigestAppenderManager 对 disruptor 进行了封装，用于处理外部组件的Tracer摘要日志。该部分借助 AsyncCommonDigestAppenderManager 的源码来分析下SOFATracer如何使用disruptor的。 SOFATracer中使用了两种不同的事件模型，一种是SOFATracer内部使用的 StringEvent , 一种是 外部扩展使用的 SofaTacerSpanEvent。这里以 SofaTacerSpanEvent 这种事件模型来分析。StringEvent 消息事件模型对应的是 AsyncCommonAppenderManager 类封装的disruptor。 SofaTracerSpanEvent ( -&gt; LongEvent)定义消息事件模型，SofaTacerSpanEvent 和 前面 demo 中的 LongEvent 基本结构是一样的，主要是内部持有的消息数据不同，LongEvent 中是一个long类型的数据，SofaTacerSpanEvent中持有的是 SofaTracerSpan 。 123456789public class SofaTracerSpanEvent &#123; private volatile SofaTracerSpan sofaTracerSpan; public SofaTracerSpan getSofaTracerSpan() &#123; return sofaTracerSpan; &#125; public void setSofaTracerSpan(SofaTracerSpan sofaTracerSpan) &#123; this.sofaTracerSpan = sofaTracerSpan; &#125;&#125; Consumer ( -&gt; LongEventHandler)Consumer 是 AsyncCommonDigestAppenderManager 的内部类;实现了 EventHandler 接口，这个consumer就是作为消费者存在的。 在AsyncCommonAppenderManager中也有一个，这个地方个人觉得可以抽出去，这样可以使得AsyncCommonDigestAppenderManager/AsyncCommonAppenderManager的代码看起来更干净； 123456789101112131415161718192021222324252627282930313233343536373839private class Consumer implements EventHandler&lt;SofaTracerSpanEvent&gt; &#123; //日志类型集合，非该集合内的日志类型将不会被处理 protected Set&lt;String&gt; logTypes = Collections.synchronizedSet(new HashSet&lt;String&gt;()); @Override public void onEvent(SofaTracerSpanEvent event, long sequence, boolean endOfBatch) throws Exception &#123; // 拿到具体的消息数据 sofaTracerSpan SofaTracerSpan sofaTracerSpan = event.getSofaTracerSpan(); // 如果没有数据，则不做任何处理 if (sofaTracerSpan != null) &#123; try &#123; String logType = sofaTracerSpan.getLogType(); // 验证当前日志类型是否可以被当前consumer消费 if (logTypes.contains(logType)) &#123; // 获取编码类型 SpanEncoder encoder = contextEncoders.get(logType); //获取 appender TraceAppender appender = appenders.get(logType); // 对数据进行编码处理 String encodedStr = encoder.encode(sofaTracerSpan); if (appender instanceof LoadTestAwareAppender) &#123; ((LoadTestAwareAppender) appender).append(encodedStr, TracerUtils.isLoadTest(sofaTracerSpan)); &#125; else &#123; appender.append(encodedStr); &#125; // 刷新缓冲区，日志输出 appender.flush(); &#125; &#125; catch (Exception e) &#123; // 异常省略 &#125; &#125; &#125; public void addLogType(String logType) &#123; logTypes.add(logType); &#125; &#125; SofaTracerSpanEventFactory （-&gt; LongEventFactory）用于产生消息事件的 Factory 123456public class SofaTracerSpanEventFactory implements EventFactory&lt;SofaTracerSpanEvent&gt; &#123; @Override public SofaTracerSpanEvent newInstance() &#123; return new SofaTracerSpanEvent(); &#125;&#125; ConsumerThreadFactory (-&gt; LongEventThreadFactory )用来产生消费线程的 Factory。 123456789101112131415public class ConsumerThreadFactory implements ThreadFactory &#123; private String workName; public String getWorkName() &#123; return workName; &#125; public void setWorkName(String workName) &#123; this.workName = workName; &#125; @Override public Thread newThread(Runnable runnable) &#123; Thread worker = new Thread(runnable, "Tracer-AsyncConsumer-Thread-" + workName); worker.setDaemon(true); return worker; &#125;&#125; 构建disruptordisruptor 的构建是在 AsyncCommonDigestAppenderManager 的构造函数中完成的。 1234567891011121314151617181920212223242526272829303132333435363738394041public AsyncCommonDigestAppenderManager(int queueSize, int consumerNumber) &#123; // 使用这个计算来保证realQueueSize是2的次幂（返回当前 大于等于queueSize的最小的2的次幂数 ） int realQueueSize = 1 &lt;&lt; (32 - Integer.numberOfLeadingZeros(queueSize - 1)); //构建disruptor，使用的是 ProducerType.MULTI //等待策略是 BlockingWaitStrategy disruptor = new Disruptor&lt;SofaTracerSpanEvent&gt;(new SofaTracerSpanEventFactory(), realQueueSize, threadFactory, ProducerType.MULTI, new BlockingWaitStrategy()); //消费者列表 this.consumers = new ArrayList&lt;Consumer&gt;(consumerNumber); for (int i = 0; i &lt; consumerNumber; i++) &#123; Consumer consumer = new Consumer(); consumers.add(consumer); //设置异常处理程序 disruptor.setDefaultExceptionHandler(new ConsumerExceptionHandler()); //绑定消费者 disruptor.handleEventsWith(consumer); &#125; //是否允许丢弃，从配置文件获取 this.allowDiscard = Boolean.parseBoolean(SofaTracerConfiguration.getProperty( SofaTracerConfiguration.TRACER_ASYNC_APPENDER_ALLOW_DISCARD, DEFAULT_ALLOW_DISCARD)); if (allowDiscard) &#123; //是否记录丢失日志的数量 this.isOutDiscardNumber = Boolean.parseBoolean(SofaTracerConfiguration.getProperty( SofaTracerConfiguration.TRACER_ASYNC_APPENDER_IS_OUT_DISCARD_NUMBER, DEFAULT_IS_OUT_DISCARD_NUMBER)); //是否记录丢失日志的TraceId和RpcId this.isOutDiscardId = Boolean.parseBoolean(SofaTracerConfiguration.getProperty( SofaTracerConfiguration.TRACER_ASYNC_APPENDER_IS_OUT_DISCARD_ID, DEFAULT_IS_OUT_DISCARD_ID)); //丢失日志的数量达到该阈值进行一次日志输出 this.discardOutThreshold = Long.parseLong(SofaTracerConfiguration.getProperty( SofaTracerConfiguration.TRACER_ASYNC_APPENDER_DISCARD_OUT_THRESHOLD, DEFAULT_DISCARD_OUT_THRESHOLD)); if (isOutDiscardNumber) &#123; this.discardCount = new PaddedAtomicLong(0L); &#125; &#125;&#125; 启动 disruptordisruptor的启动委托给了AsyncCommonDigestAppenderManager 的start方法来执行。 1234public void start(final String workerName) &#123; this.threadFactory.setWorkName(workerName); this.ringBuffer = this.disruptor.start();&#125; 来看下，SOFATracer 中 具体是在哪里调用这个start 的： CommonTracerManager : 这个里面持有了AsyncCommonDigestAppenderManager 类的一个单例对象，并且是static 静态代码块中调用了start方法；这个用来输出普通日志。 SofaTracerDigestReporterAsyncManager：这里类里面也是持有了AsyncCommonDigestAppenderManager 类的一个单例对像，并且提供了getSofaTracerDigestReporterAsyncManager方法来获取该单例，在这个方法中调用了start方法；该对象用来输出摘要日志。 发布事件前面的demo中是通过一个for循环来发布事件的，在 SOFATracer 中 的事件发布无非就是当有Tracer日志需要输出时会触发发布，那么对应的就是日志的 append 操作，将日志 append 到环形缓冲区。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public boolean append(SofaTracerSpan sofaTracerSpan) &#123; long sequence = 0L; //是否允许丢弃 if (allowDiscard) &#123; try &#123; //允许丢弃就使用tryNext尝试申请序列，申请不到抛出异常 sequence = ringBuffer.tryNext(); &#125; catch (InsufficientCapacityException e) &#123; //是否输出丢失日志的TraceId和RpcId if (isOutDiscardId) &#123; SofaTracerSpanContext sofaTracerSpanContext = sofaTracerSpan .getSofaTracerSpanContext(); if (sofaTracerSpanContext != null) &#123; SynchronizingSelfLog.warn("discarded tracer: traceId[" + sofaTracerSpanContext.getTraceId() + "];spanId[" + sofaTracerSpanContext.getSpanId() + "]"); &#125; &#125; //是否输出丢失日志的数量 if ((isOutDiscardNumber) &amp;&amp; discardCount.incrementAndGet() == discardOutThreshold) &#123; discardCount.set(0); if (isOutDiscardNumber) &#123; SynchronizingSelfLog.warn("discarded " + discardOutThreshold + " logs"); &#125; &#125; return false; &#125; &#125; else &#123; // 不允许丢弃则使用next方法 sequence = ringBuffer.next(); &#125; try &#123; SofaTracerSpanEvent event = ringBuffer.get(sequence); event.setSofaTracerSpan(sofaTracerSpan); &#125; catch (Exception e) &#123; SynchronizingSelfLog.error("fail to add event"); return false; &#125; //发布 ringBuffer.publish(sequence); return true;&#125; SOFATracer 事件发布的调用逻辑： 追溯调用的流程，可以知道当前 span 调用 finish时或者 SOFATracer中调用reportSpan时 就相当于发布了一个消息事件。 小结本文对 SOFATracer 中使用 Disruptor 来进行日志输出的代码进行了简单的分析，更多内部细节原理可以自行看下SOFATracer的代码。SOFATracer 作为一种比较底层的中间件组件，在实际的业务开发中基本是无法感知的。但是作为技术来学习，还是有很多点可以挖一挖。 SOFATracer GitHub 传送门。 如果有小伙伴对中间件感兴趣，欢迎加入我们团队，欢迎来撩；对 SOFA 技术体系有兴趣的可以关注我们 ALIPAY SOFA 社区；附团队镇楼图。]]></content>
      <categories>
        <category>SOFA</category>
      </categories>
      <tags>
        <tag>SOFATracer</tag>
        <tag>Disruptor</tag>
        <tag>OpenTracing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊 RestTemplate]]></title>
    <url>%2F2018%2F11%2F10%2Ftopic-resttemplate%2F</url>
    <content type="text"><![CDATA[最近这段时间用了下 RestTemplate 这个类，抽点时间总结下一些东西，希望对大家有所帮助。 从 3.0 版本开始，Spring 提供了 RestTemplate 作为用于访问 Rest 服务的客户端，RestTemplate 提供了多种便捷访问远程 Http 服务的方法，能够大大提高客户端的编写效率。 本篇文章将从 RestTemplate 提供的 API 入手，先来了解下 RestTemplate 的具体使用，然后再对其中涉及到的几个核心类进行分析，最后再来分析下 RestTemplate 执行的整个流程，篇幅比较长，建议先码为快！ 核心 API在平时的使用中，我们通常都是使用包装好的getForObject/getForEntity，postForObject/postForEntity/postForLocation，put以及delete。 get 请求处理getForEntity方法的返回值是一个ResponseEntity，ResponseEntity是Spring对HTTP请求响应的封装，包括了几个重要的元素，如响应码、contentType、contentLength、响应消息体等。 url：调用的服务的地址 responseType：返回的body类型 uriVariables：有两种形式: 可以用一个数字做占位符，最后是一个可变长度的参数，来一一替换前面的占位符 也可以前面使用name={name}这种形式，最后一个参数是一个map，map的key即为前边占位符的名字，map的value为参数值 responseType 测试案例定义的一个controller资源： 这里分别使用不同的 responseType 进行测试： 结果：12getForEntity(responseType=Map.class):&#123;glmapper=hello glmapper&#125;getForEntity(responseType=String.class):&#123;&quot;glmapper&quot;:&quot;hello glmapper&quot;&#125; uriVariables 测试案例先来看下非map方式的，两个controller，两种不同方式的参数获取（本质上是一样的） 使用占位符的方式： 使用 map 的方式： getForObjectgetForObject 函数实际上是对 getForEntity 函数的进一步封装，如果只关注返回的消息体的内容，对其他信息都不关注，那么就可以使用 getForObject。 这里调用就比getForEntity要简单一点了，可以直接拿到对象： getForObject 的几个重载方法和 getForEntity 基本是一样的。 post 请求处理在RestTemplate中，POST请求可以通过如下三个方法来发起：postForEntity，postForObject，postForLocation。 postForEntity 案例调用获取： 1postForEntity(URI url, @Nullable Object request, Class&lt;T&gt; responseType) 方法的第一参数表示要调用的服务的地址 方法的第二个参数表示上传的参数 方法的第三个参数表示返回的消息体的数据类型 postForObject 案例和 getForObject 相对应，只关注返回的消息体。 postForLocation 案例postForLocation也是提交新资源，提交成功之后，返回新资源的URI，postForLocation的参数和前面两种的参数基本一致，只不过该方法的返回值为Uri，这个只需要服务提供者返回一个Uri即可，该Uri表示新资源的位置。 这里有点坑，我们需要把这个uri添加到response的header中，不然后面拿到的是null。 exchangeexchange 方法和上述这些方法差别在于需要多一个请求类型的参数： AsyncRestTemplate 异步客户端RestTemplate的异步实现方式。所涉及到的API和RestTemplate基本一致。区别在于RestTemplate直接返回结果，而AsyncRestTemplate返回的是ListenableFuture。 RestTemplate 拦截器Spring提供了ClientHttpRequestInterceptor和AsyncClientHttpRequestInterceptor两个接口，分别可以对RestTemplate和AsyncRestTemplate发起的请求进行拦截，并在其被发送至服务端之前修改请求或是增强相应的信息。 ClientHttpRequestInterceptor 拦截 RestTemplate AsyncClientHttpRequestInterceptor 拦截AsyncRestTemplate 设置拦截器就是通过提供的 setInterceptors 设置即可： 自定义 ResponseErrorHandlerResponseErrorHandler 接口定义了当response发生错误时需要进行的操作。这里我们自定义一个CustomResponseErrorHandler，当返回的code不是200时，就表示执行出错了。 设置 ResponseErrorHandler： 执行结果： 处理流程下面来梳理下 RestTemplate 中请求处理的流程。下图中 XXXX 表示我们调用的 API 方法。大体流程就是：api 内部做一些请求相关的处理封装，然后交给 execute 方法执行，最后真正处理则是在 doExecute 方法中完成。 下面以 getForEntity 方法的执行过程来分析： getForEntity 方法： 基于给定响应类型，返回一个请求回调实现，准备请求。 基于给定响应类型，返回 ResponseEntity 的响应提取器。 execute 方法： 这个方法里面是对url进行urlencode编码处理的，统一转为URL。这里我们也可以手动把参数进行网络编码。 doExecute是请求真正处理的方法，这里来重点看下这个方法的执行过程： createRequest doWithRequest execute handleResponse 1、createRequest这个方法的作用就是创建一个 ClientHttpRequest 对象。RestTemplate集成了 HttpAccessor这个抽象类，创建ClientHttpRequest的过程就是在其父类HttpAccessor中通过默认的 ClientHttpRequestFactory 实现类 SimpleClientHttpRequestFactory 完成具体的请求创建。 1、创建 java.net.HttpURLConnection 对象 2、设置 connection，包括 connectTimeout、setDoInput 等。 3、bufferRequestBody 用于标志是否使用缓存流的形式，默认是 true。缺点是当发送大量数据时，比如 put/post，存在内存消耗严重。该值可以通过 SimpleClientHttpRequestFactory#setBufferRequestBody来修改。 不同版本的变更还是比较大的，大家在阅读源码时，还是从最新的代码来看。 2、doWithRequestRequestCallback 封装了请求体和请求头对象。这里会遍历所有的 HttpMessageConverter，解析成所有支持的MediaType，放在allSupportedMediaTypes中。 1request.getHeaders().setAccept(allSupportedMediaTypes); RestTemplate中对应了两个内部类的实现： AcceptHeaderRequestCallback.doWithRequest的处理。发送请求时，Http头部需要设置Accept字段，该字段表明了发送请求的这方接受的媒体类型（消息格式），也是响应端要返回的信息的媒体类型（消息格式）。根据postForEntity方法的第三个参数responseType，程序将选择适合的解析器XXXConverter，并依据该解析器找出所有支持的媒体类型。 HttpEntityRequestCallback.doWithRequest的处理。如果是POST请求并且消息体存在时，除了设置Accept字段，还可能需要设置Content-Type字段，该字段表明了所发送请求的媒体类型（消息格式），也是响应端接受的媒体类型（消息格式）。根据postForEntity方法的第二个参数request，程序将选择适合的解析器XXXConverter，将请求消息写入输出流。 3、execute这里会把请求头/体封装到connect，然后发送请求。跟踪 execute 方法执行，定位到SimpleBufferingClientHttpRequest#executeInternal方法： 这里是使用实例 SimpleBufferingClientHttpRequest 封装请求体和请求头。从代码中可以看到： delete 时通过前面设置的 DoOutput参数和是否可以设置输出流来判断是否需要发送请求体如果是 delete 请求，那么很明显 DoOutput = false，不会有封装请求体的过程，即不执行FileCopyUtils.copy(bufferedOutput, this.connection.getOutputStream())。 4、handleResponse最后就是 response 的解析了，从代码来看，主要还是 Error 的解析。这里的ErrorHandler我们前面也提到，可以通过实现 ResponseErrorHandler 来自定义 异常处理。 小结本篇先介绍了RestTemplate的API使用，挑了几个介绍了下，更多使用细节还是要针对不同的场景来决定。接着对拦截器，异步RestTemplate以及错误处理器做了简单的介绍并给出了案例。最后分析了下RestTemplate的执行流程，篇幅原因执行流程部分只是大概捋了捋，其中还是很多细节有时间再补充，这部分主要就是看底层是如何通信的，已经请求参数的传递等。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>web</tag>
        <tag>restful</tag>
        <tag>聊一聊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊 Spring 中的扩展机制(二) - NamespaceHandler]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-extention-namespacehandler%2F</url>
    <content type="text"><![CDATA[前一篇 聊一聊 Spring 中的扩展机制（一） 中聊到了ApplicationListener、ApplicationContextAware、BeanFactoryAware三种机制。本篇将介绍 NamespaceHandler 的扩展使用。 相信很多小伙伴对于这几个类都不陌生，基本基于java实现的RPC框架都会使用，比如 Dubbo , SOFARpc 等。本文先从几个小demo入手，了解下基本的概念和编程流程，然后分析下 SOFARpc 中是如何使用的。 NamespaceHandlerNamespaceHandler 是 Spring 提供的 命名空间处理器。下面这张图中，除了乱入的本篇 demo 中涉及到的 BridgeNameSpaceHandler 之外，其他均为 Spring 自身提供的。因为这里我只引入了 bean 和 context 依赖，所以这也仅仅是一部分。图中我们常用的应该算是 AopNamespaceHandler。 我们使用基于xml的spring配置时，可能需要配置如&lt;aop:config /&gt;这样的标签，在配置这个标签之前，通常我们需要引入这个aop所在的命名空间： 1234567891011&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsdhttp://www.springframework.org/schema/aophttp://www.springframework.org/schema/aop/spring-aop.xsdhttp://www.springframework.org/schema/contexthttp://www.springframework.org/schema/context/spring-context.xsd" /&gt; 关于AOP 可以了解下 聊一聊 AOP ：表现形式与基础概念，这里不过多解释，下面就按照 官方文档的流程 来写一个自定义xml，最终效果如下： 12345&lt;bridge:application id="bridgeTestApplication" name="bridgeTestApplication" version="1.0" organization="bridge.glmapper.com" owner="leishu@glmapper"/&gt; 1、定义 xsd 文件关于 xsd 文件的语法规则不在本篇范围之内，有兴趣的同学可以自行google。下面这个文件很简单，定义的element name 为application，对应于 bridge:application中的application。attribute就是上面效果展示中对应的几个属性名。 12345678910111213141516171819&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;&lt;xsd:schema xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:beans="http://www.springframework.org/schema/beans" xmlns:tool="http://www.springframework.org/schema/tool" xmlns="http://bridge.glmapper.com/schema/bridge" targetNamespace="http://bridge.glmapper.com/schema/bridge"&gt; &lt;xsd:import namespace="http://www.springframework.org/schema/beans"/&gt; &lt;xsd:complexType name="applicationType"&gt; &lt;xsd:attribute name="id" type="xsd:ID"/&gt; &lt;xsd:attribute name="name" type="xsd:string" use="required"/&gt; &lt;xsd:attribute name="version" type="xsd:string"/&gt; &lt;xsd:attribute name="owner" type="xsd:string"/&gt; &lt;xsd:attribute name="organization" type="xsd:string"/&gt; &lt;/xsd:complexType&gt; &lt;xsd:element name="application" type="applicationType"/&gt;&lt;/xsd:schema&gt; 2、编写 NamespaceHandler In addition to the schema, we need a NamespaceHandler that will parse all elements of this specific namespace Spring encounters while parsing configuration files. 用编写的这个 NamespaceHandler 来解析配置文件。 具体说来NamespaceHandler会根据schema和节点名找到某个BeanDefinitionParser，然后由BeanDefinitionParser完成具体的解析工作。 Spring提供了默认实现类NamespaceHandlerSupport和AbstractSingleBeanDefinitionParser，最简单的方式就是去继承这两个类。 这里通过继承 NamespaceHandlerSupport 这个抽象类来完成。 123456public class BridgeNamespaceHandler extends NamespaceHandlerSupport &#123; public void init() &#123; registerBeanDefinitionParser("application", new ApplicationBeanDefinitionParser()); &#125;&#125; 这里实际上只是注册了一个解析器，具体的 BeanDefinitionParser 才是将 XML元素映射到特定bean的。 3、编写 BeanDefinitionParser这里直接通过实现BeanDefinitionParser接口的方式定义我们的BeanDefinitionParser实现类。关于AbstractSingleBeanDefinitionParser 的使用在 SPFARpc 中会涉及到。 123456789101112131415161718192021222324252627public class ApplicationBeanDefinitionParser implements BeanDefinitionParser &#123; public BeanDefinition parse(Element element, ParserContext parserContext) &#123; //beanDefinition RootBeanDefinition beanDefinition = new RootBeanDefinition(); beanDefinition.setBeanClass(ApplicationConfig.class); beanDefinition.setLazyInit(false); //解析id String id = element.getAttribute("id"); beanDefinition.getPropertyValues().add("id", id); //解析name beanDefinition.getPropertyValues().add("name", element.getAttribute("name")); //解析version beanDefinition.getPropertyValues().add("version", element.getAttribute("version")); //owner beanDefinition.getPropertyValues().add("owner", element.getAttribute("owner")); //organization beanDefinition.getPropertyValues().add("organization", element.getAttribute("organization")); parserContext.getRegistry().registerBeanDefinition(id, beanDefinition); return beanDefinition; &#125;&#125; 这里我们需要了解的是开始解析自定义标签的时候，是通过BeanDefinitionParserDelegate-&gt;parseCustomElement方法来处理的，如下图所示： 通过ele元素拿到当前namespaceUri，也就是在xsd中定义的命名空间，接着委托给 DefaultNamespaceResolver 得到具体的handler（BridgenamspaceHandler） ,然后执行parse 解析。 4、配置 spring.handlers 和 spring.schmas1234http\://bridge.glmapper.com/schema/bridge=com.glmapper.extention.namespacehandler.BridgeNamespaceHandlerhttp\://bridge.glmapper.com/schema/bridge.xsd=META-INF/bridge.xsd 配置这个其实是为了让Spring在解析xml的时候能够感知到我们的自定义元素，我们需要把NamespaceHandler和xsd文件放到位于META-INF目录下的spring.handlers 和 spring.schmas文件中。这样就可以在spring配置文件中使用我们自定义的标签了。如下： 123456789101112131415&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:bridge="http://bridge.glmapper.com/schema/bridge" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://bridge.glmapper.com/schema/bridge http://bridge.glmapper.com/schema/bridge.xsd"&gt; &lt;bridge:application id="bridgeTestApplication" name="bridgeTestApplication" version="1.0" organization="bridge.glmapper.com" owner="leishu@glmapper"/&gt;&lt;/beans&gt; 验证下从容器中获取我们的bean： 123456789public static void main(String[] args) &#123; ApplicationContext applicationContext = new ClassPathXmlApplicationContext("classpath:bean.xml"); ApplicationConfig applicationConfig = (ApplicationConfig) applicationContext.getBean("bridgeTestApplication"); System.out.println("applicationConfig = "+applicationConfig);&#125; 输出示例：1234567applicationConfig = ApplicationConfig &#123; id=bridgeTestApplication, name=&apos;bridgeTestApplication&apos;, version=&apos;1.0&apos;, owner=&apos;leishu@glmapper&apos;, organization=&apos;bridge.glmapper.com&apos;&#125; 整体来看，如果我们要实现自己的 xml 标签，仅需完成以下几步即可： 1、定义 xsd 文件 2、编写 NamespaceHandler 3、编写 BeanDefinitionParser 4、配置 spring.handlers 和 spring.schmas SOFARpc 中使用分析SOFARpc 中的 rpc.xsd 文件是集成在 sofaboot.xsd 文件中的，详细可见：sofa-boot xsd 文件这里不贴了，有点长 spring.handlers 和 spring.schmas先看下 spring.handlers 和 spring.schmas 配置： 12345678http\://sofastack.io/schema/sofaboot=com.alipay.sofa.infra.config.spring.namespace.handler.SofaBootNamespaceHandlerhttp\://sofastack.io/schema/sofaboot.xsd=META-INF/com/alipay/sofa/infra/config/spring/namespace/schema/sofaboot.xsdhttp\://sofastack.io/schema/rpc.xsd=META-INF/com/alipay/sofa/infra/config/spring/namespace/schema/rpc.xsd 从 spring.handlers找到 NamespaceHandler : SofaBootNamespaceHandler。 SofaBootNamespaceHandler源码如下，这里看出来，并不是像上面我们自己写的那种方式那样，会有一个 BeanDefinitionParser。这里其实设计的很巧妙，通过spi的方式来载入具体的BeanDefinitionParser。 12345678910111213141516171819202122public class SofaBootNamespaceHandler extends NamespaceHandlerSupport &#123; @Override public void init() &#123; ServiceLoader&lt;SofaBootTagNameSupport&gt; serviceLoaderSofaBoot = ServiceLoader.load(SofaBootTagNameSupport.class); //SOFABoot for (SofaBootTagNameSupport tagNameSupport : serviceLoaderSofaBoot) &#123; this.registerTagParser(tagNameSupport); &#125; &#125; private void registerTagParser(SofaBootTagNameSupport tagNameSupport) &#123; if (!(tagNameSupport instanceof BeanDefinitionParser)) &#123; // log return; &#125; String tagName = tagNameSupport.supportTagName(); registerBeanDefinitionParser(tagName, (BeanDefinitionParser) tagNameSupport); &#125;&#125; 这里可以看出有 ReferenceDefinitionParser 和 ServiceDefinitionParser 两个解析类，分别对应服务引用和服务暴露。 下面以ReferenceDefinitionParser为例，先看下它的类图： 解析工作都是在 AbstractContractDefinitionParser 类中完成， ReferenceDefinitionParser 自己只是做了一些特殊处理【jvm-first，jvm服务】。 小结本篇通过 NamespaceHandler 了解了如何去编写我们自定义的xml标签，从NamespaceHandler的角度可以很好的理解一些 RPC 框架中最基础的基于xml 方式的服务引用和暴露的实现思路。另外通过分析 SOFARpc ，也了解了在实际的工程组件中对于NamespaceHandler的扩展使用。 本文代码：glmapper-spring-extention]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>聊一聊</tag>
        <tag>spring 扩展机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊 Spring 中的扩展机制（一）]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-extention-one%2F</url>
    <content type="text"><![CDATA[之前 Spring 源码系列文章中大多是底层源码的分析，通过源码可以让我们能够清晰的了解 Spring 到底是什么，而不是停留于表面的认知。比如当我们要使用 @Autowired 注解时，可以拿到我们想要的 bean ,但是为什么可以是值得思考的。– 关于阅读源码 Spring源码的阅读结合日常的使用，可以帮助我们更好的掌握这个庞大的技术体系，实际的开发工作中有很多地方可以借鉴它的一些思想来帮助我们更好的实现自己的业务逻辑。本篇将以扩展点为切入点，来了解下在Spring生命周期中扩展Spring中的Bean功能。 ApplicationListener 扩展ApplicationListener 其实是 spring 事件通知机制中核心概念；在java的事件机制中，一般会有三个概念： event object : 事件对象 event source ：事件源，产生事件的地方 event listener ：监听事件并处理 ApplicationListener 继承自 java.util.EventListener ，提供了对于Spring中事件机制的扩展。 ApplicationListener 在实际的业务场景中使用的非常多，比如我一般喜欢在容器初始化完成之后来做一些资源载入或者一些组件的初始化。这里的容器指的就是Ioc容器，对应的事件是ContextRefreshedEvent 。 1234567891011@Componentpublic class StartApplicationListener implementsApplicationListener&lt;ContextRefreshedEvent&gt; &#123; @Override public void onApplicationEvent(ContextRefreshedEvent contextRefreshedEvent) &#123; //初始化资源文件 //初始化组件 如：cache &#125;&#125; 上面这段代码会在容器刷新完成之后来做一些事情。下面通过自定义事件来看看怎么使用，在看具体的demo之前，先来了解下一些关注点。 日常工作了，如果要使用 Spring 事件传播机制，我们需要关注的点有以下几点： 事件类，这个用来描述事件本身一些属性，一般继承ApplicationEvent 监听类，用来监听具体的事件并作出响应。需要实现 ApplicationListener 接口 事件发布类，需要通过这个类将时间发布出去，这样才能被监听者监听到，需要实现ApplicationContextAware接口。 将事件类和监听类交给Spring容器。 那么下面就按照这个思路来看下demo的具体实现。 事件类：UserRegisterEventUserRegisterEvent ，用户注册事件；这里作为事件对象，继承自 ApplicationEvent。 12345678910111213141516171819/** * @description: 用户注册事件 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: guolei.sgl * @date: 18/7/25 */public class UserRegisterEvent extends ApplicationEvent &#123; public String name; public UserRegisterEvent(Object o) &#123; super(o); &#125; public UserRegisterEvent(Object o, String name) &#123; super(o); this.name=name; &#125;&#125; 事件发布类：UserService用户注册服务，这里需要在用户注册时将注册事件发布出去，所以通过实现ApplicationEventPublisherAware接口，使UserService具有事件发布能力。 ApplicationEventPublisherAware:发布事件，也就是把某个事件告诉的所有与这个事件相关的监听器。 123456789101112131415161718192021/** * @description: 用户注册服务，实现ApplicationEventPublisherAware接口 ，表明本身具有事件发布能力 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: guolei.sgl * @date: 18/7/25 */public class UserService implements ApplicationEventPublisherAware &#123; private ApplicationEventPublisher applicationEventPublisher; public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) &#123; this.applicationEventPublisher = applicationEventPublisher; &#125; public void register(String name) &#123; System.out.println("用户：" + name + " 已注册！"); applicationEventPublisher.publishEvent(new UserRegisterEvent(name)); &#125;&#125; 这里的UserService实际上是作为事件源存在的，通过register将用户注册事件传播出去。那么下面就是需要定义如何来监听这个事件，并且将事件进行消费处理掉，这里就是通过ApplicationListener来完成。 监听类：BonusServerListener当用户触发注册操作时，向积分服务发送消息，为用户初始化积分。 1234567891011121314/** * @description: BonusServerListener 积分处理，当用户注册时，给当前用户增加初始化积分 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: guolei.sgl * @date: 18/7/25 */public class BonusServerListener implementsApplicationListener&lt;UserRegisterEvent&gt; &#123; public void onApplicationEvent(UserRegisterEvent event) &#123; System.out.println("积分服务接到通知，给 " + event.getSource() + " 增加积分..."); &#125;&#125; 注册到容器中123456789101112131415&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"&gt; &lt;bean id="userService" class="com.glmapper.extention.UserService"/&gt; &lt;bean id="bonusServerListener" class="com.glmapper.extention.BonusServerListener"/&gt; &lt;/beans&gt; 客户端类12345678910111213141516/** * @description: 客户端类 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: guolei.sgl * @date: 18/7/25 */public class MainTest &#123; public static void main(String[] args) &#123; ApplicationContext context =new ClassPathXmlApplicationContext("beans.xml"); UserService userService = (UserService) context.getBean("userService"); //注册事件触发 userService.register("glmapper"); &#125;&#125; 客户端类中，注册一个name为glmapper的用户，执行结果： 12用户：glmapper 已注册！积分服务接到通知，给 glmapper 增加积分... 现在来考虑另外一个问题，增加一个功能，用户注册之后给用户发一个邮件。这个其实就是增加一个监听类就可以，前提是这个监听者是监听当前事件的。 123456789101112/** * @description: 邮件服务监听器，当监听到用户的注册行为时， 给用户发送邮件通知 * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: guolei.sgl * @date: 18/7/25 */public class EmailServerListener implementsApplicationListener&lt;UserRegisterEvent&gt; &#123; public void onApplicationEvent(UserRegisterEvent event) &#123; System.out.println("邮件服务接到通知，给 " + event.getSource() + " 发送邮件..."); 这里如果将UserRegisterEvent换成UserLoginEvent，那么邮件服务将不会有任何行为。 增加发送邮件监听类之后的执行结果：123用户：glmapper 已注册！邮件服务接到通知，给 glmapper 发送邮件...积分服务接到通知，给 glmapper 增加积分... Spring 的事件传播机制是基于观察者模式（Observer）实现的，它可以将 Spring Bean的改变定义为事件 ApplicationEvent，通过 ApplicationListener 监听 ApplicationEvent 事件，一旦Spring Bean 使用 ApplicationContext.publishEvent( ApplicationEvent event )发布事件后，Spring 容器会通知注册在 容器中所有 ApplicationListener 接口的实现类，最后 ApplicationListener 接口实现类判断是否处理刚发布出来的 ApplicationEvent 事件。 ApplicationContextAware 扩展ApplicationContextAware中只有一个setApplicationContext方法。实现了ApplicationContextAware接口的类，可以在该Bean被加载的过程中获取Spring的应用上下文ApplicationContext，通过ApplicationContext可以获取Spring容器内的很多信息。 这种一般在需要手动获取Bean的注入实例对象时会使用到。下面通过一个简单的demo来了解下。 GlmapperApplicationContext 持有ApplicationContext对象，通过实现 ApplicationContextAware接口来给ApplicationContext做赋值。12345678910111213141516171819/** * @description: GlmapperApplicationContext * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: guolei.sgl * @date: 18/7/29 */public class GlmapperApplicationContext implementsApplicationContextAware &#123; private ApplicationContext applicationContext; public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext=applicationContext; &#125; public ApplicationContext getApplicationContext()&#123; return applicationContext; &#125;&#125; 需要手动获取的bean: 1234567891011/** * @description: HelloService * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: guolei.sgl * @date: 18/7/29 */public class HelloService &#123; public void sayHello()&#123; System.out.println("Hello Glmapper"); &#125;&#125; 在配置文件中进行配置： 12345&lt;bean id="helloService"class="com.glmapper.extention.applicationcontextaware.HelloService"/&gt;&lt;bean id="glmapperApplicationContext"class="com.glmapper.extention.applicationcontextaware.GlmapperApplicationContext"/&gt; 客户端类调用： 12345678910111213141516171819202122public class MainTest &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); HelloService helloService = (HelloService) context.getBean("helloService"); helloService.sayHello(); //这里通过实现ApplicationContextAware接口的类来完成bean的获取 GlmapperApplicationContext glmapperApplicationContext = (GlmapperApplicationContext) context.getBean("glmapperApplicationContext"); ApplicationContext applicationContext = glmapperApplicationContext.getApplicationContext(); HelloService glmapperHelloService = (HelloService) applicationContext.getBean("helloService"); glmapperHelloService.sayHello(); &#125;&#125; BeanFactoryAware 扩展我们知道BeanFactory是整个Ioc容器最顶层的接口，它规定了容器的基本行为。实现BeanFactoryAware接口就表明当前类具体BeanFactory的能力。 BeanFactoryAware接口中只有一个setBeanFactory方法。实现了BeanFactoryAware接口的类，可以在该Bean被加载的过程中获取加载该Bean的BeanFactory，同时也可以获取这个BeanFactory中加载的其它Bean。 来想一个问题，我们为什么需要通过BeanFactory的getBean来获取Bean呢？Spring已经提供了很多便捷的注入方式，那么通过BeanFactory的getBean来获取Bean有什么好处呢？来看一个场景。 现在有一个HelloService，这个HelloService就是打招呼，我们需要通过不同的语言来实现打招呼，比如用中文，用英文。一般的做法是： 1234567891011121314151617public interface HelloService &#123; void sayHello();&#125;//英文打招呼实现public class GlmapperHelloServiceImpl implements HelloService &#123; public void sayHello() &#123; System.out.println("Hello Glmapper"); &#125;&#125;//中文打招呼实现public class LeishuHelloServiceImpl implements HelloService &#123; public void sayHello() &#123; System.out.println("你好，磊叔"); &#125;&#125; 客户端类来调用务必会出现下面的方式： 123456if (condition==&quot;英文&quot;)&#123; glmapperHelloService.sayHello();&#125;if (condition==&quot;中文&quot;)&#123; leishuHelloService.sayHello();&#125; 如果有一天，老板说我们要做国际化，要实现全球所有的语言来问候。你是说好的，还是控制不住要动手呢？ 那么有没有什么方式可以动态的去决定我的客户端类到底去调用哪一种语言实现，而不是用过if-else方式来罗列呢？是的，对于这些需要动态的去获取对象的场景，BeanFactoryAware就可以很好的搞定。OK，来看代码改造： 引入BeanFactoryAware： 123456789101112131415161718192021222324252627282930/** * @description: 实现BeanFactoryAware ，让当前bean本身具有 BeanFactory 的能力 * * 实现 BeanFactoηAware 接口的 bean 可以直接访问 Spring 容器，被容器创建以后， * 它会拥有一个指向 Spring 容器的引用，可以利用该bean根据传入参数动态获取被spring工厂加载的bean * * @email: &lt;a href="glmapper_2018@163.com"&gt;&lt;/a&gt; * @author: guolei.sgl * @date: 18/7/29 */public class GlmapperBeanFactory implements BeanFactoryAware &#123; private BeanFactory beanFactory; public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; this.beanFactory=beanFactory; &#125; /** * 提供一个execute 方法来实现不同业务实现类的调度器方案。 * @param beanName */ public void execute(String beanName)&#123; HelloService helloService=(HelloService) beanFactory.getBean(beanName); helloService.sayHello(); &#125;&#125; 这里为了逻辑方便理解，再加入一个HelloFacade 类,这个类的作用就是持有一个BeanFactoryAware的实例对象，然后通过HelloFacade实例对象的方法来屏蔽底层BeanFactoryAware实例的实现细节。 12345678910public class HelloFacade &#123; private GlmapperBeanFactory glmapperBeanFactory; //调用glmapperBeanFactory的execute方法 public void sayHello(String beanName)&#123; glmapperBeanFactory.execute(beanName); &#125; public void setGlmapperBeanFactory(GlmapperBeanFactory beanFactory)&#123; this.glmapperBeanFactory = beanFactory; &#125;&#125; 客户端类 1234567891011121314151617181920212223public class MainTest &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); HelloFacade helloFacade = (HelloFacade) context.getBean("helloFacade"); GlmapperBeanFactory glmapperBeanFactory = (GlmapperBeanFactory) context.getBean("glmapperBeanFactory"); //这里其实可以不通过set方法注入到helloFacade中， //可以在helloFacade中通过autowired //注入；这里在使用main方法来执行验证，所以就手动set进入了 helloFacade.setGlmapperBeanFactory(glmapperBeanFactory); //这个只需要传入不同HelloService的实现类的beanName， //就可以执行不同的业务逻辑 helloFacade.sayHello("glmapperHelloService"); helloFacade.sayHello("leishuHelloService"); &#125;&#125; 可以看到在调用者（客户端）类中，只需要通过一个beanName就可以实现不同实现类的切换，而不是通过一堆if-else来判断。另外有的小伙伴可能会说，程序怎么知道用哪个beanName呢？其实这个也很简单，这个参数我们可以通过一些途径来拼接得到，比如使用一个prefix用来指定语言，prefix+HelloService就可以确定唯一的beanName。 小结本来想着在一篇文章里面把扩展点都写一下的，但是实在太长了。后面差不多还有两篇。本系列中所有的demo可以在github获取，也欢迎小伙伴把能够想到的扩展点pr过来。 glmapper-spring-extention]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>聊一聊</tag>
        <tag>spring 扩展机制</tag>
        <tag>spring 事件机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[：SpringSession系列-集成SpringBoot]]></title>
    <url>%2F2018%2F11%2F10%2Fspring%2Fspring-session-integration-springboot%2F</url>
    <content type="text"><![CDATA[springSession是 spring 旗下的一个项目，把 servlet 容器实现的 httpSession替换为springSession，专注于解决session管理问题。可简单快速且无缝的集成到我们的应用中。本文通过一个案例，使用SpringBoot来集成 SpringSession，并且使用Redis作为存储来实践下SpringSession 的使用。 环境准备因为需要使用Redis作为底层Session的存储介质，实现分布式session，因此需要安装Redis。 Redis 安装1、从官网下载最新版的Redis 2、解压 1tar zxvf redis-5.0.0.tar.gz 3、编译测试 1sudo make test 4、编译安装 1sudo make install 5、安装问题 如果您之前安装过，重复安装且没有卸载干净的话，会报下面的错 12make[1]: *** [test] Error 1 make: *** [test] Error 2 解决这个错误，执行下面的语句即可： 123make distclean make make test 正确安装姿势如下： 6、启动Redis在您的Redis安装目录下，有 redis-server ，执行该脚本命令： OK，到这里，Redis的安装工作完毕。 SpringBoot 工程准备这里我们直接通过Idea来构建我们的SpringBoot工程。 1File-&gt;New-&gt;Project : Spring Initializr OK，SpringBoot 工程准备完毕，这里选择创建的是一个Web工程。 集成集成主要是依赖引入，这里需要redis和session的依赖 依赖引入123456789101112&lt;dependencies&gt; &lt;!--redis 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--sessions 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 配置application.properties123456789#服务端口server.port=8080#redi主机地址spring.redis.host=localhost#redis服务端口spring.redis.port=6379# spring session使用存储类型，spirngboot默认就是使用redis方式，如果不想用可以填none。spring.session.store-type=redis 在启动类中加入@EnableRedisHttpSession 注解1234567@SpringBootApplication@EnableRedisHttpSessionpublic class SpringBootSessionApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootSessionApplication.class, args); &#125;&#125; 测试先来编写一个Controller 123456789101112131415161718192021222324252627/** * SessionController * * @author: glmapper@leishu * @since: 18/11/3 下午3:16 * @version 1.0 **/@Controller@RequestMapping(value = "/")public class SessionController &#123; @ResponseBody @RequestMapping(value = "/session") public Map&lt;String, Object&gt; getSession(HttpServletRequest request) &#123; request.getSession().setAttribute("userName", "glmapper"); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put("sessionId", request.getSession().getId()); return map; &#125; @ResponseBody @RequestMapping(value = "/get") public String get(HttpServletRequest request) &#123; String userName = (String) request.getSession().getAttribute("userName"); return userName; &#125;&#125; 测试结果启动SpringBoot 工程；然后浏览器中输入地址 http://localhost:8080/session； 这里对应执行的是我们上面Controller中的第一个方法getSession，这个方法向session中设置了一个值。 下面我们执行：http://localhost:8080/get 这里是从session中取值: 到此，SpringBoot 整合 SpringSession 的过程就完成了。这里我们只是引入了依赖，然后做了简单的配置，那么我们的请求是如何被 SpringSession 处理的呢？从我们一贯的认知来看，对于基于Servlet规范的容器（SpringBoot 使用的是嵌入式Tomcat）的应用，请求最先被处理的是Filter。我们在基于Spring+SpringMvc这套技术栈开发时,如果我们需要做权限管理，通过会基于Filter或者拦截器。但是这里貌似我们什么也没做，但是请求确实被SpringSession处理了。OK，我们来扒一扒。 SpringSession 是如何处理请求的？上面这张截图想必大家都不陌生，是SpringBoot的启动日志；上图红色框内的是当前应用注册是Filter信息，从这里可以看到有个和 session 有关的Filter：sessionRepositoryFilter；这个bean对应的类是： 12org.springframework.boot.autoconfigure.session.SessionRepositoryFilterConfiguration.ConditionalOnBean=org.springframework.session.web.http.SessionRepositoryFilter 在这里找到了 这里涉及到SpringBoot的自动配置，从spring-boot-autoconfig包下加载spring-autoconfigure-metadata.properties 配置文件，然后获取所有支持自动配置的信息；SpringSession 也在其中。关于如何加载并且注册不在本文的范畴之内，我们继续来分析SpringSession的处理过程。 SpringSession 的处理过程从上面SpringBoot的启动过程我们找到了处理session的Filter，然后知道了它是通过自动配置的方式被注册到当前的容器并且来处理请求。123@Order(SessionRepositoryFilter.DEFAULT_ORDER)public class SessionRepositoryFilter&lt;S extends Session&gt; extends OncePerRequestFilter &#123; 从SessionRepositoryFilter的定义来看： 1、使用了Order，并且配置了一个很小的值（Integer.MIN_VALUE + 50），以此来确保session的Filter在Filter链中被优先执行。 2、集成了OncePerRequestFilter，确保在一次请求只通过一次filter，而不需要重复执行 为什么 session 的 Filter 要被优先执行呢？因为我们的请求被包装了，如果SessionRepositoryFilter不优先处理请求，可能会导致后续的请求行为不一致，这里涉及到 springSession无缝替换应用服务器的request的原理： 1.自定义个Filter，实现doFilter方法 2.继承 HttpServletRequestWrapper 、HttpServletResponseWrapper 类，重写getSession等相关方法(在这些方法里调用相关的 session存储容器操作类)。 3.自定义request和response类；并把它们分别传递到过滤器链 4.把该filter配置到过滤器链的第一个位置上 OK，了解了这些背景，我们来跟踪下整个处理流程。 1、断点到 doFilterInternal 从这里可以看到request和response类被包装了。 2、断点到 getSession这里是从Redis中拿我们session数据的地方 先从我们当前servlet容器中去拿，如果拿到则直接返回 去Redis中取 这里会有一个缓存处理，并非是每次都到Reids中去查一次，避免一次与Reids的交互。 如果缓存当前应用容器缓存中有，则直接返回当前被缓存的session 如果没有，则从请求中获取sessionId，并且根据当前sessionId去Reids中查找session数据 更新缓存session，sessionId,requestedSessionCached等数据状态 如果Redis中有，则更新session相关信息并返回 如果Reids中没有找到，则根据 create 来判断是否创建新的session。 断点到 readCookieValuesSpringSession提供了两种保存和传递SessionId的方式，一种是基于Cookie的，一种是基于Header的。SpringSession中默认使用的是基于Cookie的方式。readCookieValues 就是实现如何从Cookie中获取sessionId的。 这个过程其实很简单，先是从request中获取当前请求携带的所以的Cookie信息，然后将匹配到的 cookieName 为 “SESSION” 的Cookie进行解析。 断点到 RedisOperationsSessionRepository -&gt; getSession这里是从Redis中取session数据的地方 根据sessionId从 Redis中取到 entries 数据 构建 RedisSession 并返回 断点到 commitSessioncommitSession作用是通过HttpSessionIdResolver 将sessionId写到response，并且进行持久化。 这里的 session 其实是已经更新过状态的，比如重新设置了 session 的过期时间等。session 提交实际上就意味着当前请求已经处理完毕了。 小结本文先介绍了如何使用 SpringBoot 集成 SpringSession，并且以 Redis 作为存储。然后简单分析了 SpringSession 的处理过程，本文对 SpringSession 的原理部分没有进行深入分析，下一篇分析下SpringSession的原理。]]></content>
      <categories>
        <category>spring</category>
        <category>session</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>session</tag>
      </tags>
  </entry>
</search>

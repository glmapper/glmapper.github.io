{"title":"神经网络浅讲：从神经元到深度学习","uid":"b66d839268b41dec0c2b4caf81d61b35","slug":"llm/shenduxuexi-shenjingwangluo","date":"2024-12-25T09:46:44.000Z","updated":"2024-12-25T09:49:08.848Z","comments":true,"path":"api/articles/llm/shenduxuexi-shenjingwangluo.json","keywords":"宋国磊, glmapper, 卫恒, 分享, 开源","cover":[],"content":"<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>本篇是笔者最近在学习机器学习时阅读到的一篇文章，刚好和笔者最近在阅读《深度学习革命》这本书里的内容有较多的重叠之处，结合来看，个人认为它比较清楚的帮我捋顺了一些机器学习领域发展的时间线和一些关键技术概念。此外因为作者提到可以转发，因此在公众号转发以便于个人收藏阅读，也期望分享给相关对此方向感兴趣的同学。我保留了原文的标题，对于其中的一些内容做了适当的修改、删除和补充，以便更符合公众号读者的阅读习惯。当然如果你期望转发本篇，请也同时备注原文链接。<br>原文：<a href=\"https://www.cnblogs.com/subconscious/p/5058741.html\">https://www.cnblogs.com/subconscious/p/5058741.html</a></p></blockquote>\n<p>神经网络是一门重要的机器学习技术。它是目前最为火热的研究方向–深度学习的基础。学习神经网络不仅可以让你掌握一门强大的机器学习方法，同时也可以更好地帮助你理解深度学习技术。本文以一种简单的，循序的方式讲解神经网络。适合对神经网络了解不多的同学。本文对阅读没有一定的前提要求，但是懂一些<a href=\"http://www.cnblogs.com/subconscious/p/4107357.html\">机器学习</a>基础会更好地帮助理解本文神经网络是一种模拟人脑的神经网络以期能够实现类人工智能的机器学习技术。人脑中的神经网络是一个非常复杂的组织。成人的大脑中估计有1000亿个神经元之多。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151229110952448-2017198041.jpg\" alt=\"人脑神经网络\"></p>\n<p>那么机器学习中的神经网络是如何实现这种模拟的，并且达到一个惊人的良好效果的？通过本文，你可以了解到这些问题的答案，同时还能知道神经网络的历史，以及如何较好地学习它。由于本文较长，为方便读者，可以收藏转发保存。</p>\n<h1 id=\"一-前言\"><a href=\"#一-前言\" class=\"headerlink\" title=\"一. 前言\"></a>一. 前言</h1><p>让我们来看一个经典的神经网络。这是一个包含三个层次的神经网络。红色的是<strong>输入层</strong>，绿色的是<strong>输出层</strong>，紫色的是<strong>中间层</strong>（也叫<strong>隐藏层</strong>）。输入层有3个输入单元，隐藏层有4个单元，输出层有2个单元。后文中，我们统一使用这种颜色来表达神经网络的结构。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151219151604318-1557737289.jpg\" alt=\"神经网络结构图\"></p>\n<p>在开始介绍前，有一些知识可以先记在心里：</p>\n<ul>\n<li>1、设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定；</li>\n<li>2、神经网络结构图中的拓扑与箭头代表着<strong>预测</strong>过程时数据的流向，跟<strong>训练</strong>时的数据流有一定的区别；</li>\n<li>3、结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的<strong>权重</strong>（其值称为<strong>权值</strong>），这是需要训练得到的。</li>\n</ul>\n<p>除了从左到右的形式表达的结构图，还有一种常见的表达形式是从下到上来表示一个神经网络。这时候，输入层在图的最下方。输出层则在图的最上方，如下图：</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151219174631693-775181930.jpg\" alt=\"从下到上的神经网络结构图\"></p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>从左到右的表达形式以 Andrew Ng 和 LeCun 的文献使用较多，Caffe 里使用的则是从下到上的表达。在本文中使用 Andrew Ng 代表的从左到右的表达形式。下面从简单的神经元开始说起，一步一步介绍神经网络复杂结构的形成。</p></blockquote>\n<h1 id=\"二-神经元\"><a href=\"#二-神经元\" class=\"headerlink\" title=\"二. 神经元\"></a>二. 神经元</h1><h2 id=\"1、引子\"><a href=\"#1、引子\" class=\"headerlink\" title=\"1、引子\"></a>1、引子</h2><p>对于神经元的研究由来已久，1904 年生物学家就已经知晓了神经元的组成结构。一个神经元通常具有多个<strong>树突</strong>，主要用来接受传入信息；而<strong>轴突</strong>只有一条，轴突尾端有许多<strong>轴突末梢</strong>可以给其他多个神经元传递信息<strong>轴突末梢</strong>跟其他神经元的<strong>树突</strong>产生连接，从而传递信号。这个连接的位置在生物学上叫做“<strong>突触</strong>”。人脑中的神经元形状可以用下图做简单的说明：</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151229121248198-818698949.jpg\" alt=\"神经元\"></p>\n<p>1943年，心理学家 McCulloch 和数学家 Pitts 参考了生物神经元的结构，发表了抽象的神经元模型 MP。在下文中，我们会具体介绍神经元模型。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151219175550990-1730772549.jpg\" alt=\"img\">  <img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151219175600006-1000051743.jpg\" alt=\"img\"></p>\n<p>图5 Warren McCulloch（左）和 Walter Pitts（右） </p>\n<h2 id=\"2、结构\"><a href=\"#2、结构\" class=\"headerlink\" title=\"2、结构\"></a>2、结构</h2><p>神经元模型是一个包含<strong>输入，输出与计算功能的模型</strong>。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。下图是一个典型的神经元模型：包含有3个输入，1个输出，以及2个计算功能。注意中间的箭头线。这些线称为“连接”。每个上有一个“权值”。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151219153856802-307732621.jpg\" alt=\"神经元模型 \"></p>\n<p> 连接是神经元中最重要的东西，每一个连接上都有一个权重。<strong>一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好</strong>。我们使用<code>a</code>来表示输入，用<code>w</code>来表示权值。一个表示连接的有向箭头可以这样理解：<code>在初端，传递的信号大小仍然是a，端中间有加权参数w，经过这个加权后的信号会变成a*w，因此在连接的末端，信号的大小就变成了a*w</code>。</p>\n<p>在其他绘图模型里，有向箭头可能表示的是值的不变传递。而在神经元模型里，每个有向箭头表示的是值的加权传递。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151219180614819-1652574235.jpg\" alt=\"连接（connection）\"></p>\n<p> 如果我们将神经元图中的所有变量用符号表示，并且写出输出的计算公式的话，就是下图。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151230201441792-1505283920.jpg\" alt=\"神经元计算 \"></p>\n<p>可见<code>z</code>是在输入和权值的线性加权和叠加了一个 <strong>函数g</strong> 的值。在<code>MP模型</code>里，函数 <code>g</code> 是 <code>sgn</code> 函数，也就是取符号函数。这个函数当输入大于 0 时，输出 1，否则输出 0。</p>\n<p>下面对神经元模型的图进行一些扩展。首先将<code>sum</code>函数与<code>sgn</code>函数合并到一个圆圈里，代表神经元的内部计算。其次，把输入<code>a</code>与输出<code>z</code>写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151230204036479-461440948.jpg\" alt=\"神经元扩展 \"></p>\n<p>当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用“<strong>单元</strong>”（unit）来指代。同时由于神经网络的表现形式是一个有向图(DAG)，有时也会用“<strong>节点</strong>”（node）来表达同样的意思。 </p>\n<h2 id=\"3、效果\"><a href=\"#3、效果\" class=\"headerlink\" title=\"3、效果\"></a>3、效果</h2><p>神经元模型的使用可以这样理解：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>我们有一个数据，称之为 <strong>样本</strong>。样本有四个属性，其中三个属性已知，一个属性未知。我们需要做的就是通过三个已知属性<strong>预测</strong>未知属性。具体办法就是使用神经元的公式进行计算。三个已知属性的值是 a1，a2，a3，未知属性的值是 z。z 可以通过公式计算出来。</p></blockquote>\n<p>这里，已知的属性称之为<strong>特征</strong>，未知的属性称之为<strong>目标</strong>。假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值 w1，w2，w3。那么，我们就可以通过神经元模型预测新样本的目标。</p>\n<h2 id=\"4、影响\"><a href=\"#4、影响\" class=\"headerlink\" title=\"4、影响\"></a>4、影响</h2><p>1943 年发布的MP模型，虽然简单，但已经建立了神经网络大厦的地基。但是，<strong>MP模型中，权重的值都是预先设置的，因此不能学习</strong>。1949 年心理学家 Hebb 提出了 Hebb 学习率，认为人脑神经细胞的<strong>突触</strong>（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151219175742677-1785435491.gif\" alt=\"Donald Olding Hebb\"></p>\n<p>尽管神经元模型与Hebb学习律都已诞生，但限于当时的计算机能力，直到接近 10 年后，第一个真正意义的神经网络才诞生。</p>\n<h1 id=\"三-单层神经网络（感知器）\"><a href=\"#三-单层神经网络（感知器）\" class=\"headerlink\" title=\"三. 单层神经网络（感知器）\"></a>三. 单层神经网络（感知器）</h1><h2 id=\"1、引子-1\"><a href=\"#1、引子-1\" class=\"headerlink\" title=\"1、引子\"></a>1、引子</h2><p>1958年，计算科学家 Rosenblatt 提出了由两层神经元组成的神经网络。他给它起了一个名字–<strong>“感知器”（Perceptron）</strong>（有的文献翻译成“感知机”，下文统一用“感知器”来指代）。感知器是当时首个可以学习的人工神经网络。Rosenblatt 现场演示了其学习识别简单图像的过程，在当时的社会引起了轰动。人们认为已经发现了智能的奥秘，许多学者和科研机构纷纷投入到神经网络的研究中。美国军方大力资助了神经网络的研究，并认为神经网络比“原子弹工程”更重要。这段时间直到 1969 年才结束，这个时期可以看作神经网络的第一次高潮。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151221153812609-1784157068.jpg\" alt=\"Rosenblat与感知器\"></p>\n<h2 id=\"2、结构-1\"><a href=\"#2、结构-1\" class=\"headerlink\" title=\"2、结构\"></a>2、结构</h2><p>在原来 MP 模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变，于是我们就有了下图：从本图开始，我们将权值 <code>w1, w2, w3</code> 写到“连接线”的中间。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151221151959015-1876891081.jpg\" alt=\"单层神经网络\"></p>\n<p>在<strong>“感知器”</strong>中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。</p>\n<p>我们把需要计算的层次称之为“计算层”，并<strong>把拥有一个计算层的网络称之为“单层神经网络”</strong>。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。但在本文里，我们根据计算层的数量来命名。</p>\n<p>假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。下图显示了带有两个输出单元的单层神经网络，其中输出单元<code>z1</code>的计算公式如下图。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151230204223917-579926148.jpg\" alt=\"单层神经网络(Z1)\"></p>\n<p>可以看到，<code>z1</code>的计算跟原先的<code>z</code>并没有区别。我们已知一个神经元的输出可以向多个神经元传递，因此<code>z2</code>的计算公式如下图。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151230204258057-82126781.jpg\" alt=\"单层神经网络(Z2)\"></p>\n<p>可以看到，<code>z2</code>的计算中除了三个新的权值：<code>w4，w5，w6</code>以外，其他与<code>z1</code>是一样的。整个网络的输出如下图。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151230204606760-610264555.jpg\" alt=\"单层神经网络(Z1和Z2)\"></p>\n<p>目前的表达公式有一点不让人满意的就是：<code>w4，w5，w6</code> 是后来加的，很难表现出跟原先的<code>w1，w2，w3</code>的关系。因此我们改用二维的下标，用 w<sub>xy</sub> 来表达一个权值。下标中的 x 代表后一层神经元的序号，而 y 代表前一层神经元的序号（序号的顺序从上到下）。例如，w<sub>1,2</sub>代表后一层的第1个神经元与前一层的第2个神经元的连接的权值（这种标记方式参照了Andrew Ng的课件）。根据以上方法标记，我们有了下图。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151230205437995-673856644.jpg\" alt=\"单层神经网络(扩展)\"></p>\n<p> 如果我们仔细看输出的计算公式，会发现这两个公式就是线性代数方程组。因此可以用矩阵乘法来表达这两个公式。例如，输入的变量是[a1，a2，a3]<sup>T</sup>（代表由a1，a2，a3 组成的列向量），用向量 <strong>a</strong> 来表示。方程的左边是[z1，z2]<sup>T</sup>，用向量 <strong>z</strong> 来表示。系数则是矩阵<strong>W</strong>（2行3列的矩阵，排列形式与公式中的一样）。于是，输出公式可以改写成：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>g(<strong>W</strong> * <strong>a</strong>) &#x3D; <strong>z</strong></p></blockquote>\n<p> 这个公式就是神经网络中从前一层计算后一层的<strong>矩阵运算。</strong></p>\n<h2 id=\"3、效果-1\"><a href=\"#3、效果-1\" class=\"headerlink\" title=\"3、效果\"></a>3、效果</h2><p>与神经元模型不同，感知器中的权值是通过训练得到的。因此，根据以前的知识我们知道，感知器类似一个<strong>逻辑回归</strong>模型，可以做线性分类任务。我们可以用<strong>决策分界</strong>来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是三维的时候，就是划出一个平面，当数据的维度是<code>N</code>维时，就是划出一个<code>N-1</code>维的超平面。下图显示了在二维平面中划出决策分界的效果，也就是感知器的分类效果。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151231073138323-962584420.png\" alt=\"单层神经网络（决策分界）\"></p>\n<h2 id=\"4、影响-1\"><a href=\"#4、影响-1\" class=\"headerlink\" title=\"4、影响\"></a>4、影响</h2><p>感知器只能做简单的线性分类任务。但是当时的人们热情太过于高涨，并没有人清醒的认识到这点。于是，当人工智能领域的巨擘 Minsky 指出这点时，事态就发生了变化。Minsky 在1969年出版了一本叫《Perceptron》的书，里面用详细的数学证明了感知器的弱点，尤其是感知器对 XOR（异或）这样的简单分类任务都无法解决。</p>\n<p>Minsky 认为，如果将计算层增加到两层，计算量则过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。（本文成文后一个月，即2016年1月，<a href=\"http://cacm.acm.org/news/197529-in-memoriam-marvin-minsky-1927-2016/fulltext\">Minsky在美国去世</a>。谨在本文中纪念这位著名的计算机研究专家与大拿。）</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151221213056031-351364155.jpg\" alt=\"Marvin Minsky\">  </p>\n<p>由于 Minsky 的巨大影响力以及书中呈现的悲观态度，让很多学者和实验室纷纷放弃了神经网络的研究。神经网络的研究陷入了冰河期。这个时期又被称为<strong>AI winter</strong>。接近10年以后，对于两层神经网络的研究才带来神经网络的复苏。</p>\n<h1 id=\"四-两层神经网络（多层感知器）\"><a href=\"#四-两层神经网络（多层感知器）\" class=\"headerlink\" title=\"四. 两层神经网络（多层感知器）\"></a>四. 两层神经网络（多层感知器）</h1><h2 id=\"1、引子-2\"><a href=\"#1、引子-2\" class=\"headerlink\" title=\"1、引子\"></a>1、引子</h2><p>两层神经网络是本文的重点，因为正是在这时候，神经网络开始了大范围的推广与使用。Minsky 说过单层神经网络无法解决异或问题，但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。</p>\n<p>1986 年，Rumelhar 和 Hinton 等人提出了<strong>反向传播（Backpropagation，BP）算法</strong>，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。目前，大量的教授神经网络的教材，都是重点介绍两层（带一个隐藏层）神经网络的内容。这时候的 Hinton 还很年轻，30 年以后，正是他重新定义了神经网络，带来了神经网络复苏的又一春。</p>\n<p>  <img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151222162306171-1025091923.jpg\" alt=\"David Rumelhart\">  <img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151222163144687-406696976.jpg\" alt=\"Geoffery Hinton\"></p>\n<h2 id=\"2、结构-2\"><a href=\"#2、结构-2\" class=\"headerlink\" title=\"2、结构\"></a>2、结构</h2><p>两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。现在，我们的权值矩阵增加到了两个，我们用上标来区分不同层次之间的变量。</p>\n<p>例如a<sub>x</sub><sup>(y)</sup>代表第 <code>y </code>层的第 <code>x</code> 个节点。z<sub>1</sub>，z<sub>2</sub> 变成了a<sub>1</sub><sup>(2)</sup>，a<sub>2</sub><sup>(2)</sup>。下图给出了a<sub>1</sub><sup>(2)</sup>，a<sub>2</sub><sup>(2)</sup>的计算公式。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151222164731249-360921014.jpg\" alt=\"两层神经网络（中间层计算）\"></p>\n<p>计算最终输出<code>z</code>的方式是利用了中间层的a<sub>1</sub><sup>(2)</sup>，a<sub>2</sub><sup>(2)</sup>和第二个权值矩阵计算得到的，如下图。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151222171056156-387680541.jpg\" alt=\"两层神经网络（输出层计算）\"></p>\n<p>假设我们的预测目标是一个向量，那么与前面类似，只需要在“输出层”再增加节点即可。我们使用向量和矩阵来表示层次中的变量。<strong>a</strong><sup>(1)</sup>，<strong>a</strong><sup>(2)</sup>，<strong>z</strong> 是网络中传输的向量数据。<strong>W</strong><sup>(1)</sup>和<strong>W</strong><sup>(2)</sup>是网络的矩阵参数。如下图。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151222171328140-1303075636.jpg\" alt=\"两层神经网络（向量形式）\"></p>\n<p>使用矩阵运算来表达整个计算公式的话如下：</p>\n<center>g(W<sup>(1)</sup> * a<sup>(1)</sup>) =a<sup>(2)</sup></center>\n\n<center>g(W<sup>(2)</sup> * a<sup>(2)</sup>) = z</center>\n\n \n\n<p>由此可见，使用矩阵运算来表达是很简洁的，而且也不会受到节点数增多的影响（无论有多少节点参与运算，乘法两端都只有一个变量）。因此神经网络的教程中大量使用矩阵运算来描述。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>需要说明的是，至今为止，我们对神经网络的结构图的讨论中都没有提到偏置节点（<strong>bias unit</strong>）。事实上，这些节点是默认存在的。它本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。</p></blockquote>\n<p>偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量 <strong>b</strong>，称之为<strong>偏置</strong>。如下图。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151226111144687-604911384.jpg\" alt=\"两层神经网络（考虑偏置节点）\"></p>\n<p>可以看出，偏置节点很好认，因为其没有输入（前一层中没有箭头指向它）。有些神经网络的结构图中会把偏置节点明显画出来，有些不会。一般情况下，我们都不会明确画出偏置节点。 在考虑了偏置以后的一个神经网络的矩阵运算如下：</p>\n<center>g(W<sup>(1)</sup> * a<sup>(1)</sup> + b<sup>(1)</sup>) =a<sup>(2)</sup></center>\n\n <center>g(W<sup>(2)</sup> * a<sup>(2)</sup> + b<sup>(2)</sup>) = z</center>\n\n<p>需要说明的是，在两层神经网络中，我们不再使用<code>sgn</code>函数作为函数<code>g</code>，而是使用平滑函数<code>sigmoid</code>作为函数<code>g</code>。我们把<code>函数g</code>也称作<strong>激活函数（active function）</strong>。<strong>事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系</strong>。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象，实现一个神经网络最需要的是 <strong>线性代数库</strong>。</p>\n<h2 id=\"3、效果-2\"><a href=\"#3、效果-2\" class=\"headerlink\" title=\"3、效果\"></a>3、效果</h2><blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p> 与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。</p></blockquote>\n<p>这是什么意思呢？也就是说，面对复杂的非线性分类任务，两层（带一个隐藏层）神经网络可以分类的很好。下面就是一个例子（此两图来自colah 的<a href=\"\">博客</a><a href=\"http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/%EF%BC%89%EF%BC%8C%E7%BA%A2%E8%89%B2%E7%9A%84%E7%BA%BF%E4%B8%8E%E8%93%9D%E8%89%B2%E7%9A%84%E7%BA%BF%E4%BB%A3%E8%A1%A8%E6%95%B0%E6%8D%AE%E3%80%82%E8%80%8C%E7%BA%A2%E8%89%B2%E5%8C%BA%E5%9F%9F%E5%92%8C%E8%93%9D%E8%89%B2%E5%8C%BA%E5%9F%9F%E4%BB%A3%E8%A1%A8%E7%94%B1%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%92%E5%BC%80%E7%9A%84%E5%8C%BA%E5%9F%9F%EF%BC%8C%E4%B8%A4%E8%80%85%E7%9A%84%E5%88%86%E7%95%8C%E7%BA%BF%E5%B0%B1%E6%98%AF**%E5%86%B3%E7%AD%96%E5%88%86%E7%95%8C**%E3%80%82\">http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/），红色的线与蓝色的线代表数据。而红色区域和蓝色区域代表由神经网络划开的区域，两者的分界线就是**决策分界**。</a></p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151231073619073-461403542.png\" alt=\"两层神经网络（决策分界）\"></p>\n<p>　　</p>\n<p>可以看到，这个两层神经网络的决策分界是非常平滑的曲线，而且分类的很好。有趣的是，前面已经学到过，单层网络只能做线性分类任务。而两层神经网络中的后一层也是线性分类层，应该只能做线性分类任务。<strong>为什么两个线性分类任务结合就可以做非线性分类任务？</strong>我们可以把输出层的决策分界单独拿出来看一下：</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151231074314604-2050732128.png\" alt=\"两层神经网络（空间变换）\"></p>\n<p>可以看到，输出层的决策分界仍然是直线。关键就是，从输入层到隐藏层时，数据发生了空间变换。也就是说，两层神经网络中，隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。这样就导出了<strong>两层神经网络可以做非线性分类的关键–隐藏层</strong>。联想到我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，<strong>隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分</strong>。</p>\n<p><span style=\"color: red\">两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，<strong>多层的神经网络的本质就是复杂函数拟合</strong>。</span></p>\n<p>下面来讨论一下隐藏层的节点数设计。在设计一个神经网络时，<strong>输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配</strong>。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做 <strong>Grid Search（网格搜索）</strong>。了解了两层神经网络的结构以后，我们就可以看懂其它类似的结构图。例如<code>EasyPR</code>字符识别网络架构（下图）。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151226122337406-1923048422.jpg\" alt=\"EasyPR字符识别网络\"></p>\n<p><code>EasyPR</code> 使用了字符的图像去进行字符文字的识别。输入是 120 维的向量，输出是要预测的文字类别，共有 65 类。根据实验，我们测试了一些隐藏层数目，发现当值为 40 时，整个网络在测试集上的效果较好，因此选择网络的最终结构就是<code>120，40，65</code>。</p>\n<h2 id=\"4、训练\"><a href=\"#4、训练\" class=\"headerlink\" title=\"4、训练\"></a>4、训练</h2><p>在 Rosenblat 提出的感知器模型中，模型中的参数可以被训练，但是使用的方法较为简单，并没有使用目前机器学习中通用的方法，这导致其扩展性与适用性非常有限。从两层神经网络开始，神经网络的研究人员开始使用机器学习相关的技术进行神经网络的训练。例如用大量的数据（1000-10000左右），使用算法进行优化等等，从而使得模型训练可以获得性能与数据利用上的双重优势。<strong>机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。</strong>具体做法是这样的：</p>\n<p>首先给所有参数赋上随机值。我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为y<sub>p</sub>，真实目标为<code>y</code>。那么，定义一个值 loss，计算公式如下:</p>\n<center>loss = (y<sub>p</sub> - y)<sup>2</sup></center>\n\n \n\n<p>这个值称之为 <strong>损失</strong>（loss），我们的目标就是使对所有训练数据的损失和尽可能的小。</p>\n<p>如果将先前的神经网络预测的矩阵公式带入到y<sub>p</sub>中（因为有z&#x3D;y<sub>p</sub>），那么我们可以把损失写为关于参数（parameter）的函数，这个函数称之为 <strong>损失函数</strong>（loss function）。下面的问题就是求：<strong>如何优化参数，能够让损失函数的值最小</strong>。</p>\n<p>此时这个问题就被转化为一个优化问题。一个常用方法就是高等数学中的<strong>求导</strong>，但是这里的问题由于参数不止一个，求导后计算导数等于0的运算量很大，所以一般来说解决这个优化问题使用的是 <strong>梯度下降</strong> 算法。梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。</p>\n<p>在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用 <strong>反向传播</strong> 算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。反向传播算法可以直观的理解为下图。梯度的计算从后往前，一层层反向传播，前缀<code>E</code> 代表着相对导数的意思。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151229120754198-2003498733.jpg\" alt=\"反向传播算法\"></p>\n<p>反向传播算法的启示是数学中的 <strong>链式法则</strong>。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从<code>BP算法</code>开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。</p>\n<p>优化问题只是训练中的一个部分。机器学习问题之所以称为学习问题，而不是优化问题，就是因为它不仅要求数据在训练集上求得一个较小的误差，在测试集上也要表现好。因为模型最终是要部署到没有见过训练数据的真实场景。提升模型在测试集上的预测效果的主题叫做 <strong>泛化</strong>（generalization），相关方法被称作正则化（regularization），神经网络中常用的泛化技术有 <strong>权重衰减</strong> 等。</p>\n<h2 id=\"5、影响\"><a href=\"#5、影响\" class=\"headerlink\" title=\"5、影响\"></a>5、影响</h2><p>两层神经网络在多个地方的应用说明了其效用与价值。10 年前困扰神经网络界的异或问题被轻松解决。神经网络在这个时候，已经可以发力于语音识别，图像识别，自动驾驶等多个领域。历史总是惊人的相似，神经网络的学者们再次登上了《纽约时报》的专访。人们认为神经网络可以解决许多问题。就连娱乐界都开始受到了影响，当年的《终结者》电影中的阿诺都赶时髦地说一句：我的 CPU 是一个神经网络处理器，一个会学习的计算机。</p>\n<p>但是神经网络仍然存在若干的问题：尽管使用了<code>BP</code>算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。</p>\n<p>90 年代中期，由<code>Vapnik</code>等人发明的 <code>SVM（Support Vector Machines，支持向量机）</code>算法诞生，很快就在若干个方面体现出了对比神经网络的优势：<code>无需调参；高效；全局最优解</code>。基于以上种种理由，<code>SVM</code> 迅速打败了神经网络算法成为主流。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151224114218312-1112152935.jpg\" alt=\"Vladimir Vapnik\"></p>\n<p>神经网络的研究再次陷入了冰河期。当时，只要你的论文中包含神经网络相关的字眼，非常容易被会议和期刊拒收，研究界那时对神经网络的不待见可想而知。</p>\n<h1 id=\"五、多层神经网络（深度学习）\"><a href=\"#五、多层神经网络（深度学习）\" class=\"headerlink\" title=\"五、多层神经网络（深度学习）\"></a>五、多层神经网络（深度学习）</h1><h2 id=\"1、引子-3\"><a href=\"#1、引子-3\" class=\"headerlink\" title=\"1、引子\"></a>1、引子</h2><blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>在被人摒弃的 10 年中，有几个学者仍然在坚持研究。这其中的棋手就是加拿大多伦多大学的 Geoffery Hinton 教授。</p></blockquote>\n<p>2006 年，Hinton 在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“<strong>预训练</strong>”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“<strong>微调</strong>”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词–“<strong>深度学习</strong>”。</p>\n<p>很快，深度学习在语音识别领域暂露头角。接着，2012 年，深度学习技术又在图像识别领域大展拳脚。Hinton 与他的学生在 ImageNet 竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率  15% 的好成绩，这个成绩比第二名高了近 11 个百分点，充分证明了多层神经网络识别效果的优越性。在这之后，关于深度神经网络的研究与应用不断涌现。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151224145544656-1225191900.jpg\" alt=\"Geoffery Hinton\"></p>\n<h2 id=\"2、结构-3\"><a href=\"#2、结构-3\" class=\"headerlink\" title=\"2、结构\"></a>2、结构</h2><p>我们延续两层神经网络的方式来设计一个多层神经网络。在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151224204339234-1994620313.jpg\" alt=\"多层神经网络\"></p>\n<p>依照这样的方式不断添加，我们可以得到更多层的多层神经网络。公式推导的话其实跟两层神经网络类似，使用矩阵运算的话就仅仅是加一个公式而已。在已知输入<strong>a</strong><sup>(1)</sup>，参数<strong>W</strong><sup>(1)</sup>，<strong>W</strong><sup>(2)</sup>，<strong>W</strong><sup>(3)</sup> 的情况下，输出 <strong>z</strong> 的推导公式如下：</p>\n<center>g(W<sup>(1)</sup> * a<sup>(1)</sup>) =a<sup>(2)</sup></center>\n\n<center>g(W<sup>(2)</sup> * a<sup>(2)</sup>) =a<sup>(3)</sup></center>\n\n<center>g(W<sup>(3)</sup> * a<sup>(3)</sup>) = z</center> \n\n \n\n<p>多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做<strong>“正向传播”</strong>。下面讨论一下多层神经网络中的参数。</p>\n<p>首先我们看第一张图，可以看出<strong>W</strong><sup>(1)</sup>中有6个参数，<strong>W</strong><sup>(2)</sup>中有4个参数，<strong>W</strong><sup>(3)</sup>中有6个参数，所以整个神经网络中的参数有16个（这里我们不考虑偏置节点，下同）。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151224212531484-570745053.jpg\" alt=\"多层神经网络（较少参数）\"> 假设我们将中间层的节点数做一下调整。第一个中间层改为3个单元，第二个中间层改为4个单元。经过调整以后，整个网络的参数变成了33个。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151224213620234-1075501325.jpg\" alt=\"多层神经网络（较多参数）\"> </p>\n<p>虽然层数保持不变，但是第二个神经网络的参数数量却是第一个神经网络的接近两倍之多，从而带来了更好的表示（represention）能力。表示能力是多层神经网络的一个重要性质。在参数一致的情况下，我们也可以获得一个“更深”的网络。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151224213703109-813423001.jpg\" alt=\"多层神经网络（更深的层次）\"> </p>\n<p>上图的网络中，虽然参数数量仍然是 33，但却有 4 个中间层，是原来层数的接近两倍。这意味着一样的参数数量，可以用更深的层次去表达。</p>\n<h2 id=\"3、效果-3\"><a href=\"#3、效果-3\" class=\"headerlink\" title=\"3、效果\"></a>3、效果</h2><blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>与两层神经网络不同，多层神经网络中的层数增加了很多。增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。</p></blockquote>\n<p>更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。</p>\n<p>关于逐层特征学习的例子，可以参考下图。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151231075103229-1126297331.png\" alt=\"多层神经网络（特征学习）\"> </p>\n<p>更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的 <strong>容量</strong>（capcity）去拟合真正的关系。</p>\n<p>通过研究发现，在参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。这点也在 <code>ImageNet</code> 的多次大赛中得到了证实。从 2012 年起，每年获得 <code>ImageNet</code> 冠军的深度神经网络的层数逐年增加，2015 年最好的方法 <code>GoogleNet</code> 是一个多达 22 层的神经网络。</p>\n<p>在最新一届的 <code>ImageNet</code> 大赛上，目前拿到最好成绩的<code>MSRA</code>团队的方法使用的更是一个深达<code>152</code>层的网络！关于这个方法更多的信息有兴趣的可以查阅<code>ImageNet</code>网站。</p>\n<h2 id=\"4、训练-1\"><a href=\"#4、训练-1\" class=\"headerlink\" title=\"4、训练\"></a>4、训练</h2><p>在单层神经网络时，我们使用的激活函数是<code>sgn</code>函数。到了两层神经网络时，我们使用的最多的是<code>sigmoid</code>函数。而到了多层神经网络时，通过一系列的研究发现，<code>ReLU</code>函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是<code>ReLU</code>函数。<code>ReLU</code>函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是y&#x3D;max(x,0)。简而言之，在 x 大于0，输出就是输入，而在 x 小于 0 时，输出就保持为 0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。</p>\n<p>在多层神经网络中，<strong>训练的主题仍然是优化和泛化</strong>。当使用足够强的计算芯片（例如 GPU 图形加速卡）时，梯度下降算法以及反向传播算法在多层神经网络中的训练中仍然工作的很好。目前学术界主要的研究既在于开发新的算法，也在于对这两个算法进行不断的优化，例如，增加了一种带动量因子（momentum）的梯度下降算法。　</p>\n<p>在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现 <strong>过拟合现象</strong>。因此正则化技术就显得十分重要。目前，Dropout 技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。</p>\n<h2 id=\"5、影响-1\"><a href=\"#5、影响-1\" class=\"headerlink\" title=\"5、影响\"></a>5、影响</h2><p>目前，深度神经网络在人工智能界占据统治地位。但凡有关人工智能的产业报道，必然离不开深度学习。神经网络界当下的四位引领者除了前文所说的 Ng，Hinton 以外，还有 CNN 的发明人 Yann Lecun（法国人，杨立坤），以及《Deep Learning》的作者 Bengio。</p>\n<p>前段时间一直对人工智能持谨慎态度的马斯克，搞了一个 <a href=\"\">OpenAI项目</a><a href=\"http://news.cnblogs.com/n/534878/%EF%BC%8C%E9%82%80%E8%AF%B7\">http://news.cnblogs.com/n/534878/，邀请</a> Bengio 作为高级顾问。马斯克认为，人工智能技术不应该掌握在大公司如 Google，Facebook 的手里，更应该作为一种开放技术，让所有人都可以参与研究。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151228131815214-368589404.png\" alt=\"Yann LeCun\">  <img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151228131718948-1881649621.png\" alt=\" Yoshua Bengio\"></p>\n<p>多层神经网络的研究仍在进行中。现在最为火热的研究技术包括 RNN，LSTM 等（目前是更大规模参数模型的天下了），研究方向则是图像理解方面（现在有很多，图像理解也是多模态 LLM 中必不可少的组成 ）。图像理解技术是给计算机一幅图片，让它用语言来表达这幅图片的意思。<code>ImageNet</code> 竞赛也在不断召开，有更多的方法涌现出来，刷新以往的正确率。</p>\n<h1 id=\"六、回顾\"><a href=\"#六、回顾\" class=\"headerlink\" title=\"六、回顾\"></a>六、回顾</h1><h2 id=\"1、影响\"><a href=\"#1、影响\" class=\"headerlink\" title=\"1、影响\"></a>1、影响</h2><p>回顾一下神经网络发展的历程，神经网络的发展历史曲折荡漾，既有被人捧上天的时刻，也有摔落在街头无人问津的时段，中间经历了数次大起大落。从单层神经网络（感知器）开始，到包含一个隐藏层的两层神经网络，再到多层的深度神经网络，一共有三次兴起过程。详见下图。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151228170208120-1856567090.jpg\" alt=\"三起三落的神经网络\"> </p>\n<p>上图中的顶点与谷底可以看作神经网络发展的高峰与低谷，图中的横轴是时间，以年为单位。纵轴是一个神经网络影响力的示意表示。如果把 1949 年 Hebb 模型提出到 1958 年的感知机诞生这个 10 年视为落下（没有兴起）的话，那么神经网络算是经历了“三起三落”这样一个过程。俗话说，天将降大任于斯人也，必先苦其心志，劳其筋骨。经历过如此多波折的神经网络能够在现阶段取得成功也可以被看做是磨砺的积累吧。</p>\n<p>历史最大的好处是可以给现在做参考。科学的研究呈现螺旋形上升的过程，不可能一帆风顺。同时，这也给现在过分热衷深度学习与人工智能的人敲响警钟，因为这不是第一次人们因为神经网络而疯狂了。1958 年到 1969 年，以及 1985 年到 1995，这两个十年间人们对于神经网络以及人工智能的期待并不现在低，可结果如何大家也能看的很清楚。</p>\n<p>因此，冷静才是对待目前深度学习热潮的最好办法。如果因为深度学习火热，或者可以有“钱景”就一窝蜂的涌入，那么最终的受害人只能是自己。神经网络界已经两次有被人们捧上天了的境况，相信也对于捧得越高，摔得越惨这句话深有体会。因此，神经网络界的学者也必须给这股热潮浇上一盆水，不要让媒体以及投资家们过分的高看这门技术。很有可能，三十年河东，三十年河西，在几年后，神经网络就再次陷入谷底。根据上图的历史曲线图，这是很有可能的。</p>\n<h2 id=\"2、效果\"><a href=\"#2、效果\" class=\"headerlink\" title=\"2、效果\"></a>2、效果</h2><blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>下面说一下神经网络为什么能这么火热？简而言之，就是其学习效果的强大。随着神经网络的发展，其表示性能越来越强。</p></blockquote>\n<p>从单层神经网络，到两层神经网络，再到多层神经网络，下图说明了，随着网络层数的增加，以及激活函数的调整，神经网络所能拟合的决策分界平面的能力。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151228134016120-1091351096.jpg\" alt=\"表示能力不断增强\"> </p>\n<p>可以看出，随着层数增加，其非线性分界拟合能力不断增强。图中的分界线并不代表真实训练出的效果，更多的是示意效果。神经网络的研究与应用之所以能够不断地火热发展下去，与其强大的函数拟合能力是分不开关系的。</p>\n<h2 id=\"3、外因\"><a href=\"#3、外因\" class=\"headerlink\" title=\"3、外因\"></a>3、外因</h2><p>当然，光有强大的内在能力，并不一定能成功。一个成功的技术与方法，不仅需要内因的作用，还需要时势与环境的配合。神经网络的发展背后的外在原因可以被总结为：<strong>更强的计算性能，更多的数据，以及更好的训练方法</strong>。只有满足这些条件时，神经网络的函数拟合能力才能得已体现，见下图。</p>\n<p><img src=\"https://vspicgo.oss-cn-shanghai.aliyuncs.com/typora/673793-20151228170149135-2107087462.jpg\" alt=\"发展的外在原因\"> </p>\n<p>之所以在单层神经网络年代，Rosenblat 无法制作一个双层分类器，就在于当时的计算性能不足，Minsky 也以此来打压神经网络。但是 Minsky 没有料到，仅仅10 年以后，计算机 CPU 的快速发展已经使得我们可以做两层神经网络的训练，并且还有快速的学习算法BP。</p>\n<p>但是在两层神经网络快速流行的年代。更高层的神经网络由于计算性能的问题，以及一些计算方法的问题，其优势无法得到体现。直到 2012 年，研究人员发现，用于高性能计算的图形加速卡（GPU）可以极佳地匹配神经网络训练所需要的要求：高并行性，高存储，没有太多的控制需求，配合预训练等算法，神经网络才得以大放光彩。</p>\n<p>互联网时代，大量的数据被收集整理，更好的训练方法不断被发现。所有这一切都满足了多层神经网络发挥能力的条件。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>“时势造英雄”，正如Hinton在2006年的论文里说道的</p>\n<p>“**… provided that computers were fast enough, data sets were big enough, and the initial weights were close enough to a good solution. All three conditions are now satisfied.**”，</p></blockquote>\n<p> 外在条件的满足也是神经网络从神经元得以发展到目前的深度神经网络的重要因素。除此以外，一门技术的发扬没有“伯乐”也是不行的。在神经网络漫长的历史中，正是由于许多研究人员的锲而不舍，不断钻研，才能有了现在的成就。前期的 Rosenblat，Rumelhart 没有见证到神经网络如今的流行与地位。但是在那个时代，他们为神经网络的发展所打下的基础，却会永远流传下去，不会退色。</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p>　　1.<a href=\"https://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html\">Neural Networks</a></p>\n<p>　　2.<a href=\"http://ufldl.stanford.edu/wiki/index.php/Neural_Networks\">Andrew Ng Neural Networks </a></p>\n<p>　　3.<a href=\"http://www.36dsj.com/archives/20804\">神经网络简史</a><a href=\"http://www.36dsj.com/archives/20804\"></a></p>\n<p>　　4.<a href=\"http://www.intsci.ac.cn/shizz/course/kd08.ppt\">中科院 史忠植 神经网络 讲义</a></p>\n<p>　　5.<a href=\"http://caai.cn/contents/118/1934.html\">深度学习 胡晓林</a></p>\n","text":" 本篇是笔者最近在学习机器学习时阅读到的一篇文章，刚好和笔者最近在阅读《深度学习革命》这本书里的内容有较多的重叠之处，结合来看，个人认为它比较清楚的帮我捋顺了一...","permalink":"/post/llm/shenduxuexi-shenjingwangluo","photos":[],"count_time":{"symbolsCount":"14k","symbolsTime":"12 mins."},"categories":[{"name":"LLM","slug":"LLM","count":6,"path":"api/categories/LLM.json"}],"tags":[{"name":"neural network","slug":"neural-network","count":1,"path":"api/tags/neural-network.json"},{"name":"marchine learning","slug":"marchine-learning","count":1,"path":"api/tags/marchine-learning.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%80-%E5%89%8D%E8%A8%80\"><span class=\"toc-text\">一. 前言</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%BA%8C-%E7%A5%9E%E7%BB%8F%E5%85%83\"><span class=\"toc-text\">二. 神经元</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1%E3%80%81%E5%BC%95%E5%AD%90\"><span class=\"toc-text\">1、引子</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2%E3%80%81%E7%BB%93%E6%9E%84\"><span class=\"toc-text\">2、结构</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3%E3%80%81%E6%95%88%E6%9E%9C\"><span class=\"toc-text\">3、效果</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4%E3%80%81%E5%BD%B1%E5%93%8D\"><span class=\"toc-text\">4、影响</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%89-%E5%8D%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%84%9F%E7%9F%A5%E5%99%A8%EF%BC%89\"><span class=\"toc-text\">三. 单层神经网络（感知器）</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1%E3%80%81%E5%BC%95%E5%AD%90-1\"><span class=\"toc-text\">1、引子</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2%E3%80%81%E7%BB%93%E6%9E%84-1\"><span class=\"toc-text\">2、结构</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3%E3%80%81%E6%95%88%E6%9E%9C-1\"><span class=\"toc-text\">3、效果</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4%E3%80%81%E5%BD%B1%E5%93%8D-1\"><span class=\"toc-text\">4、影响</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%9B%9B-%E4%B8%A4%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%EF%BC%89\"><span class=\"toc-text\">四. 两层神经网络（多层感知器）</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1%E3%80%81%E5%BC%95%E5%AD%90-2\"><span class=\"toc-text\">1、引子</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2%E3%80%81%E7%BB%93%E6%9E%84-2\"><span class=\"toc-text\">2、结构</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3%E3%80%81%E6%95%88%E6%9E%9C-2\"><span class=\"toc-text\">3、效果</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4%E3%80%81%E8%AE%AD%E7%BB%83\"><span class=\"toc-text\">4、训练</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5%E3%80%81%E5%BD%B1%E5%93%8D\"><span class=\"toc-text\">5、影响</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%BA%94%E3%80%81%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89\"><span class=\"toc-text\">五、多层神经网络（深度学习）</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1%E3%80%81%E5%BC%95%E5%AD%90-3\"><span class=\"toc-text\">1、引子</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2%E3%80%81%E7%BB%93%E6%9E%84-3\"><span class=\"toc-text\">2、结构</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3%E3%80%81%E6%95%88%E6%9E%9C-3\"><span class=\"toc-text\">3、效果</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4%E3%80%81%E8%AE%AD%E7%BB%83-1\"><span class=\"toc-text\">4、训练</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5%E3%80%81%E5%BD%B1%E5%93%8D-1\"><span class=\"toc-text\">5、影响</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%85%AD%E3%80%81%E5%9B%9E%E9%A1%BE\"><span class=\"toc-text\">六、回顾</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1%E3%80%81%E5%BD%B1%E5%93%8D\"><span class=\"toc-text\">1、影响</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2%E3%80%81%E6%95%88%E6%9E%9C\"><span class=\"toc-text\">2、效果</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3%E3%80%81%E5%A4%96%E5%9B%A0\"><span class=\"toc-text\">3、外因</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE\"><span class=\"toc-text\">参考文献</span></a></li></ol>","author":{"name":"glmapper","slug":"blog-author","avatar":"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/common/favicon.ico","link":"/","description":"开放，开源，分享，共享","socials":{"github":"https://github.com/glmapper","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/2412872703","zhihu":"","csdn":"","juejin":"https://juejin.cn/user/3227821827961806","customs":{"sofastack":{"icon":"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/common/sofastack.svg","link":"https://www.sofastack.tech/"}}}},"mapped":true,"hidden":false,"prev_post":{"title":"CentOS 7 编译和部署 ZLMediaKit","uid":"112cbc38ea4b5162ae16f2ac3f23f8d7","slug":"middleware/middleware-zkmediakit-deploy-record","date":"2025-02-15T05:48:05.000Z","updated":"2025-02-15T05:51:05.618Z","comments":true,"path":"api/articles/middleware/middleware-zkmediakit-deploy-record.json","keywords":"宋国磊, glmapper, 卫恒, 分享, 开源","cover":null,"text":"官方建议是在 Ubuntu 上去编译 ZLMediaKit；条件有限，我们的场景用的是 CentOS 7.9，所以就参考着官方文档的 QuickStart: h...","permalink":"/post/middleware/middleware-zkmediakit-deploy-record","photos":[],"count_time":{"symbolsCount":"9.9k","symbolsTime":"9 mins."},"categories":[{"name":"Middleware","slug":"Middleware","count":14,"path":"api/categories/Middleware.json"}],"tags":[{"name":"ZLMediaKit","slug":"ZLMediaKit","count":1,"path":"api/tags/ZLMediaKit.json"}],"author":{"name":"glmapper","slug":"blog-author","avatar":"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/common/favicon.ico","link":"/","description":"开放，开源，分享，共享","socials":{"github":"https://github.com/glmapper","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/2412872703","zhihu":"","csdn":"","juejin":"https://juejin.cn/user/3227821827961806","customs":{"sofastack":{"icon":"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/common/sofastack.svg","link":"https://www.sofastack.tech/"}}}}},"next_post":{"title":"OTeL & Micrometer 在 Spring Boot 中的应用与分析","uid":"7bb9924a7e1cee47f31fec8a03bac95f","slug":"middleware/middleware-micrometer-otel","date":"2024-09-09T07:44:30.000Z","updated":"2024-09-09T07:53:45.343Z","comments":true,"path":"api/articles/middleware/middleware-micrometer-otel.json","keywords":"宋国磊, glmapper, 卫恒, 分享, 开源","cover":[],"text":"之前在 聊聊 SpringBoot3 的 Micrometer Tracing 这篇文章中我介绍了 SpringBoot3 使用 Micrometer Trac...","permalink":"/post/middleware/middleware-micrometer-otel","photos":[],"count_time":{"symbolsCount":"12k","symbolsTime":"11 mins."},"categories":[{"name":"Middleware","slug":"Middleware","count":14,"path":"api/categories/Middleware.json"}],"tags":[{"name":"Metrics","slug":"Metrics","count":2,"path":"api/tags/Metrics.json"},{"name":"OTLP","slug":"OTLP","count":2,"path":"api/tags/OTLP.json"}],"author":{"name":"glmapper","slug":"blog-author","avatar":"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/common/favicon.ico","link":"/","description":"开放，开源，分享，共享","socials":{"github":"https://github.com/glmapper","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/2412872703","zhihu":"","csdn":"","juejin":"https://juejin.cn/user/3227821827961806","customs":{"sofastack":{"icon":"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/common/sofastack.svg","link":"https://www.sofastack.tech/"}}}}}}
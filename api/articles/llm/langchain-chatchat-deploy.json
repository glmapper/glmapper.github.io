{"title":"win 10 部署 langchain-chatchat gpu 版","uid":"815e5fe8003f703e73f556f106d850e3","slug":"llm/langchain-chatchat-deploy","date":"2024-01-05T09:32:34.000Z","updated":"2024-07-05T04:09:05.779Z","comments":true,"path":"api/articles/llm/langchain-chatchat-deploy.json","keywords":null,"cover":[],"content":"<p>基于 ChatGLM 等大语言模型与 Langchain 等应用框架实现，开源、可离线部署的检索增强生成(RAG)大模型知识库项目。</p>\n<span id=\"more\"></span>\n<h2 id=\"本地环境\"><a href=\"#本地环境\" class=\"headerlink\" title=\"本地环境\"></a>本地环境</h2><ul>\n<li>品牌：戴尔 optiplex 7000</li>\n<li>操作系统：win11  家庭版 x64</li>\n<li>处理器：12th Gen Intel(R) Core(TM) i9-12900 2.4 GHz</li>\n<li>显卡：RTX 3060  12g</li>\n</ul>\n<p>nvidia-smi  查看 GPU 信息如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Fri Jan  5 14:38:11 2024</span><br><span class=\"line\">+---------------------------------------------------------------------------------------+</span><br><span class=\"line\">| NVIDIA-SMI 536.19                 Driver Version: 536.19       CUDA Version: 12.2     |</span><br><span class=\"line\">|-----------------------------------------+----------------------+----------------------+</span><br><span class=\"line\">| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class=\"line\">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class=\"line\">|                                         |                      |               MIG M. |</span><br><span class=\"line\">|=========================================+======================+======================|</span><br><span class=\"line\">|   0  NVIDIA GeForce RTX 3060      WDDM  | 00000000:01:00.0  On |                  N/A |</span><br><span class=\"line\">|  0%   35C    P8               7W / 170W |  12063MiB / 12288MiB |      7%      Default |</span><br><span class=\"line\">|                                         |                      |                  N/A |</span><br><span class=\"line\">+-----------------------------------------+----------------------+----------------------+</span><br><span class=\"line\"></span><br><span class=\"line\">+---------------------------------------------------------------------------------------+</span><br><span class=\"line\">| Processes:                                                                            |</span><br><span class=\"line\">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class=\"line\">|        ID   ID                                                             Usage      |</span><br><span class=\"line\">|=======================================================================================|</span><br><span class=\"line\">|    0   N/A  N/A      6788    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |</span><br><span class=\"line\">|    0   N/A  N/A      6940    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |</span><br><span class=\"line\">|    0   N/A  N/A      7876    C+G   C:\\Windows\\explorer.exe                   N/A      |</span><br><span class=\"line\">|    0   N/A  N/A      8816    C+G   ...on\\120.0.2210.91\\msedgewebview2.exe    N/A      |</span><br><span class=\"line\">|    0   N/A  N/A      8988    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |</span><br><span class=\"line\">|    0   N/A  N/A      9124    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |</span><br><span class=\"line\">|    0   N/A  N/A     15560    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |</span><br><span class=\"line\">|    0   N/A  N/A     16528    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |</span><br><span class=\"line\">|    0   N/A  N/A     17260      C   ...\\anaconda3\\envs\\Chatchat\\python.exe    N/A      |</span><br><span class=\"line\">|    0   N/A  N/A     21584    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |</span><br><span class=\"line\">+---------------------------------------------------------------------------------------+</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"软件环境安装\"><a href=\"#软件环境安装\" class=\"headerlink\" title=\"软件环境安装\"></a>软件环境安装</h2><p>Python 版本使用的是官方 wiki 中建议的版本区别： 3.11.5</p>\n<h3 id=\"Anaconda-安装\"><a href=\"#Anaconda-安装\" class=\"headerlink\" title=\"Anaconda 安装\"></a>Anaconda 安装</h3><p>Anaconda 用于管理 python 虚拟环境。Conda 是一个开源的包管理系统和环境管理系统，主要用于数据科学、机器学习和科学计算等领域。它是 Anaconda 发行版的一部分，也可以作为 Miniconda 的一部分单独安装。Conda可以帮助用户轻松地安装、管理和更新软件包、库和依赖项，而无需担心不同软件包之间的冲突。它还允许用户创建、管理和切换不同的虚拟环境，使得在同一台机器上同时进行多个项目开发变得更加便捷。Conda也提供了对Python和非Python软件包的支持，这使得它成为一个广泛使用的工具，不仅局限于Python生态系统。</p>\n<ul>\n<li><p>下载地址：<a href=\"https://www.anaconda.com/download\">https://www.anaconda.com/download</a></p>\n</li>\n<li><p>版本  23.11.0</p>\n</li>\n</ul>\n<p>按安装指引默认安装即可，安装完测试：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~ conda -V</span><br><span class=\"line\">~ conda 23.11.0</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"cuda-安装\"><a href=\"#cuda-安装\" class=\"headerlink\" title=\"cuda 安装\"></a>cuda 安装</h3><p>CUDA（Compute Unified Device Architecture）是由NVIDIA开发的并行计算平台和应用程序编程接口（API）。它允许开发人员使用C&#x2F;C++、Python等编程语言来利用NVIDIA GPU的并行计算能力。CUDA使开发人员能够在GPU上编写高性能的通用并行程序，用于加速科学计算、深度学习、图形渲染和其他需要大量并行计算的领域。通过CUDA，开发人员可以利用GPU的大量计算核心并行处理数据，加快计算速度。</p>\n<p>cuda 所有版本归档地址：<a href=\"https://developer.nvidia.com/cuda-toolkit-archive%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%9C%A8%E8%BF%99%E9%87%8C%E4%B8%8B%E8%BD%BD%E9%9C%80%E8%A6%81%E7%9A%84%E7%89%88%E6%9C%AC%EF%BC%9B%E5%9C%A8%E4%B8%8A%E8%BF%B0\">https://developer.nvidia.com/cuda-toolkit-archive，可以在这里下载需要的版本；在上述</a> nvidia-smi  查看的 GPU 信息看到，我本机支持的 cuda 版本最高是 12.2。</p>\n<h3 id=\"pytorch-安装\"><a href=\"#pytorch-安装\" class=\"headerlink\" title=\"pytorch 安装\"></a>pytorch 安装</h3><p>PyTorch是一个开源的机器学习库，专注于深度学习任务。它由Facebook的人工智能研究团队开发并维护，提供了丰富的工具和接口，用于构建和训练神经网络模型。</p>\n<p>安装时可以在这里 <a href=\"https://download.pytorch.org/whl/torch_stable.html\">https://download.pytorch.org/whl/torch_stable.html</a> 下载。根据需要按照不同操作系统、是否是gpu 环境等找对应的版本即可。</p>\n<ul>\n<li><p>gpu 版本安装</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># torch</span><br><span class=\"line\">pip install https://download.pytorch.org/whl/cu121/torch-2.1.2%2Bcu121-cp310-cp310-win_amd64.whl </span><br><span class=\"line\"># torchaudio</span><br><span class=\"line\">pip install https://download.pytorch.org/whl/cu121/torchaudio-2.1.2%2Bcu121-cp310-cp310-win_amd64.whl  </span><br><span class=\"line\"># torchvision</span><br><span class=\"line\">pip install https://download.pytorch.org/whl/cu121/torchvision-0.16.2%2Bcu121-cp310-cp310-win_amd64.whl</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>cpu 版本安装（这里用了清华源）</p>\n</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install torch torchvision torchaudio --index-url https://download.pytorch.org/wh1/cull8 -i https://pypi.tuna.tsinghua.edu.cn/simple/</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"环境测试\"><a href=\"#环境测试\" class=\"headerlink\" title=\"环境测试\"></a>环境测试</h2><h3 id=\"创建-python-运行虚拟环境\"><a href=\"#创建-python-运行虚拟环境\" class=\"headerlink\" title=\"创建 python 运行虚拟环境\"></a>创建 python 运行虚拟环境</h3><p>打开 Anaconda Prompt 终端命令行工具，使用 conda 创建 python 运行虚拟环境</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(base) ~ conda create -n Chatchat python=3.11.5 </span><br><span class=\"line\">(base) ~ done</span><br><span class=\"line\">(base) ~ #</span><br><span class=\"line\">(base) ~ # To activate this environment, use</span><br><span class=\"line\">(base) ~ #</span><br><span class=\"line\">(base) ~ #     $ conda activate Chatchat</span><br><span class=\"line\">(base) ~ #</span><br><span class=\"line\">(base) ~ # To deactivate an active environment, use</span><br><span class=\"line\">(base) ~ #</span><br><span class=\"line\">(base) ~ #     $ conda deactivate</span><br><span class=\"line\">(base) ~ conda activate Chatchat  # 激活虚拟环境</span><br><span class=\"line\">(Chatchat) ~ </span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"测试-pytorch\"><a href=\"#测试-pytorch\" class=\"headerlink\" title=\"测试 pytorch\"></a>测试 pytorch</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(Chatchat) ~ python</span><br><span class=\"line\">Python 3.11.5 # 省略 .... </span><br><span class=\"line\">&gt;&gt;&gt; import torch</span><br><span class=\"line\">&gt;&gt;&gt; torch.cuda.is_available()</span><br><span class=\"line\">False</span><br><span class=\"line\">&gt;&gt;&gt; x = torch.rand(5, 3)</span><br><span class=\"line\">&gt;&gt;&gt; print(x)</span><br><span class=\"line\">tensor([[0.8827, 0.8297, 0.5390],</span><br><span class=\"line\">        [0.4590, 0.8473, 0.4074],</span><br><span class=\"line\">        [0.4045, 0.0082, 0.4121],</span><br><span class=\"line\">        [0.7649, 0.3901, 0.1535],</span><br><span class=\"line\">        [0.5408, 0.8168, 0.8615]])</span><br><span class=\"line\">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Langchain-Chatchat-下载\"><a href=\"#Langchain-Chatchat-下载\" class=\"headerlink\" title=\"Langchain-Chatchat 下载\"></a>Langchain-Chatchat 下载</h2><p>本文是基于 <a href=\"https://github.com/chatchat-space/Langchain-Chatchat/wiki/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2#%E5%B8%B8%E8%A7%84%E6%A8%A1%E5%BC%8F%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88\">常规模式本地部署方案</a> 进行部署的。代码拉取 &amp; 安装依赖</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 拉取仓库</span><br><span class=\"line\">$ git clone --recursive https://github.com/chatchat-space/Langchain-Chatchat.git</span><br><span class=\"line\"></span><br><span class=\"line\"># 进入目录</span><br><span class=\"line\">$ cd Langchain-Chatchat</span><br><span class=\"line\"></span><br><span class=\"line\"># 安装全部依赖</span><br><span class=\"line\">$ pip install -r requirements.txt</span><br><span class=\"line\"></span><br><span class=\"line\"># 默认依赖包括基本运行环境（FAISS向量库）。以下是可选依赖：</span><br><span class=\"line\">- 如果要使用 milvus/pg_vector 等向量库，请将 requirements.txt 中相应依赖取消注释再安装。</span><br><span class=\"line\">- 如果要开启 OCR GPU 加速，请安装 rapidocr_paddle[gpu]</span><br><span class=\"line\">- 如果要使用在线 API 模型，请安装对用的 SDK</span><br></pre></td></tr></table></figure>\n\n<p>在执行安装依赖时遇到 <a href=\"https://github.com/chatchat-space/Langchain-Chatchat/issues/2533\">https://github.com/chatchat-space/Langchain-Chatchat/issues/2533</a> 这个问题，采用 issue 中提的方式，使用 conda 进行安装的。这里可以先将 requirements.txt 中的 jq 依赖先注释调</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># rapidocr_paddle[gpu]&gt;=1.3.0.post5 # gpu accelleration for ocr of pdf and image files</span><br><span class=\"line\"># jq&gt;=1.6.0 # for .json and .jsonl files. suggest `conda install jq` on windows</span><br></pre></td></tr></table></figure>\n\n<p>然后再执行 <code>pip install -r requirements.txt</code>，然后再使用 <code>conda install jq</code> 安装 jq。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Ps: 笔者之前没有做过 python 开发，最大体验是，一直在装各种包….</p></blockquote>\n<p>此外，为方便用户 API 与 webui 分离运行，可单独根据运行需求安装依赖包。</p>\n<ul>\n<li><p>如果只需运行 API，可执行：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ pip install -r requirements_api.txt</span><br><span class=\"line\"># 默认依赖包括基本运行环境（FAISS向量库）。如果要使用 milvus/pg_vector 等向量库，请将 requirements.txt 中相应依赖取消注释再安装。</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>如果只需运行 WebUI，可执行：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ pip install -r requirements_webui.txt</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>其他建议直接参考官方 wiki 即可，如果有问题先查官方 <a href=\"https://github.com/chatchat-space/Langchain-Chatchat/issues\">issue</a>。</p>\n<h2 id=\"模型下载\"><a href=\"#模型下载\" class=\"headerlink\" title=\"模型下载\"></a>模型下载</h2><p>安装官方手册，下载两个模型：m3e-base 内置模型和 chatglm2-6b-int4 模型。额，实际上  <code>https://huggingface.co/moka-ai/m3e-base</code> 和 <code>https://huggingface.co/THUDM/chatglm2-6b</code> 通过 git 直接 拉是拉不下来的，最后是在 <a href=\"https://huggingface.co/moka-ai/m3e-base/tree/main\">files and versions</a> 下手动下载的。chatglm2-6b 也是一样。即使下载了 lfs 也是不行的，可能打开方式不对？</p>\n<h2 id=\"配置修改和文件修改\"><a href=\"#配置修改和文件修改\" class=\"headerlink\" title=\"配置修改和文件修改\"></a>配置修改和文件修改</h2><h3 id=\"批量修改配置文件名\"><a href=\"#批量修改配置文件名\" class=\"headerlink\" title=\"批量修改配置文件名\"></a>批量修改配置文件名</h3><p>针对 Langchain-Chatchat，批量复制 configs 目录下所有的配置文件，去掉 example 后缀：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python copy_config_example.py</span><br></pre></td></tr></table></figure>\n\n<p>截图如下：</p>\n<p><img src=\"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/llm/image-20240105165310483.png\" alt=\"image-20240105165310483\"></p>\n<h3 id=\"修改-model-config-py-文件\"><a href=\"#修改-model-config-py-文件\" class=\"headerlink\" title=\"修改 model_config.py 文件\"></a>修改 model_config.py 文件</h3><ul>\n<li><p>修改 m3e-base 的模型本地路径</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># &quot;m3e-base&quot;: &quot;moka-ai/m3e-base&quot;,</span><br><span class=\"line\"># 修改为本地的 路径</span><br><span class=\"line\">&quot;m3e-base&quot;: &quot;D:\\\\llm\\\\m3e-base&quot;,</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>修改 chatglm2-6b-int4 模型本地路径</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;llm_model&quot;: &#123;</span><br><span class=\"line\">        # 以下部分模型并未完全测试，仅根据 fastchat和vllm模型的模型列表推定支持</span><br><span class=\"line\">        &quot;chatglm-6b&quot;: &quot;THUDM/chatglm-6b&quot;,</span><br><span class=\"line\">        &quot;chatglm2-6b&quot;: &quot;THUDM/chatglm2-6b&quot;,</span><br><span class=\"line\">        # &quot;chatglm2-6b-int4&quot;: &quot;THUDM/chatglm2-6b-int4&quot;,</span><br><span class=\"line\">        &quot;chatglm2-6b-int4&quot;: &quot;D:\\\\llm\\\\chatglm2-6b&quot;, </span><br><span class=\"line\">        &quot;chatglm2-6b-32k&quot;: &quot;THUDM/chatglm2-6b-32k&quot;,</span><br><span class=\"line\">        # ...</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>修改LLM模型为chatglm2-6b-int4</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># LLM 名称 ,修改为 chatglm2-6b</span><br><span class=\"line\">LLM_MODEL = &quot;chatglm2-6b</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>修改 LLM_DEVICE 运行设备，看使用cpu、cuda（带GPU）或mps( mac本）</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 选用的 Embedding 名称</span><br><span class=\"line\">EMBEDDING_MODEL = &quot;m3e-base&quot; # 可以尝试最新的嵌入式sota模型：bge-large-zh-v1.5</span><br><span class=\"line\"># Embedding 模型运行设备。设为&quot;auto&quot;会自动检测，也可手动设定为&quot;cuda&quot;,&quot;mps&quot;,&quot;cpu&quot;其中之一。</span><br><span class=\"line\">EMBEDDING_DEVICE = &quot;auto&quot;</span><br><span class=\"line\"># LLM 名称</span><br><span class=\"line\">LLM_MODEL = &quot;chatglm2-6b&quot;</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h3 id=\"修改-server-config-py\"><a href=\"#修改-server-config-py\" class=\"headerlink\" title=\"修改 server_config.py\"></a>修改 server_config.py</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 各服务器默认绑定 host。如改为&quot;0.0.0.0&quot;需要修改下方所有XX_SERVER的host</span><br><span class=\"line\">DEFAULT_BIND_HOST = “127.0.0.1&quot;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h2><h3 id=\"知识库初始化\"><a href=\"#知识库初始化\" class=\"headerlink\" title=\"知识库初始化\"></a>知识库初始化</h3><ul>\n<li><p>创建过知识库，可以先执行以下命令创建或更新数据库表，如果可以正常运行，则无需再重建知识库。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python init_database.py --create-tables</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>如果是第一次运行本项目，知识库尚未建立，或者之前使用的是低于最新master分支版本的框架，或者配置文件中的知识库类型、嵌入模型发生变化，或者之前的向量库没有开启 <code>normalize_L2</code>，需要以下命令初始化或重建知识库：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python init_database.py --recreate-vs</span><br></pre></td></tr></table></figure></li>\n</ul>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>这里因为前面安装依赖时一直出现 jq 报错问题，导致很多依赖没有安装成功，在执行数据库初始化时报了一堆依赖找不到，也是一个个 install 的，😭….</p></blockquote>\n<h3 id=\"启动项目\"><a href=\"#启动项目\" class=\"headerlink\" title=\"启动项目\"></a>启动项目</h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python startup.py -a</span><br></pre></td></tr></table></figure>\n\n<p>执行日志大致如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">==============================Langchain-Chatchat Configuration==============================</span><br><span class=\"line\">操作系统：Windows-10-10.0.22000-SP0.</span><br><span class=\"line\">python版本：3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]</span><br><span class=\"line\">项目版本：v0.2.9</span><br><span class=\"line\">langchain版本：0.0.353. fastchat版本：0.2.34</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">当前使用的分词器：ChineseRecursiveTextSplitter</span><br><span class=\"line\">当前启动的LLM模型：[&#x27;chatglm2-6b&#x27;] @ cuda</span><br><span class=\"line\">&#123;&#x27;device&#x27;: &#x27;cuda&#x27;,</span><br><span class=\"line\"> &#x27;host&#x27;: &#x27;127.0.0.1&#x27;,</span><br><span class=\"line\"> &#x27;infer_turbo&#x27;: False,</span><br><span class=\"line\"> &#x27;model_path&#x27;: &#x27;D:\\\\llm\\\\chatglm3-6b&#x27;,</span><br><span class=\"line\"> &#x27;model_path_exists&#x27;: True,</span><br><span class=\"line\"> &#x27;port&#x27;: 20002&#125;</span><br><span class=\"line\">当前Embbedings模型： m3e-base @ cuda</span><br><span class=\"line\">==============================Langchain-Chatchat Configuration==============================</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">2024-01-05 14:17:15,651 - startup.py[line:651] - INFO: 正在启动服务：</span><br><span class=\"line\">2024-01-05 14:17:15,651 - startup.py[line:652] - INFO: 如需查看 llm_api 日志，请前往 D:\\llm\\Langchain-Chatchat\\logs</span><br><span class=\"line\">2024-01-05 14:17:20 | ERROR | stderr | INFO:     Started server process [16928]</span><br><span class=\"line\">2024-01-05 14:17:20 | ERROR | stderr | INFO:     Waiting for application startup.</span><br><span class=\"line\">2024-01-05 14:17:20 | ERROR | stderr | INFO:     Application startup complete.</span><br><span class=\"line\">2024-01-05 14:17:20 | ERROR | stderr | INFO:     Uvicorn running on http://127.0.0.1:20000 (Press CTRL+C to quit)</span><br><span class=\"line\">2024-01-05 14:17:21 | INFO | model_worker | Loading the model [&#x27;chatglm2-6b&#x27;] on worker 2ed3839d ...</span><br><span class=\"line\">Loading checkpoint shards:   0%|                                                                                                                                           | 0/7 [00:00&lt;?, ?it/s]</span><br><span class=\"line\">Loading checkpoint shards:  14%|██████████████████▋                                                                                                                | 1/7 [01:05&lt;06:31, 65.24s/it]</span><br><span class=\"line\">Loading checkpoint shards:  29%|█████████████████████████████████████▍                                                                                             | 2/7 [01:41&lt;04:00, 48.07s/it]</span><br><span class=\"line\">Loading checkpoint shards:  43%|████████████████████████████████████████████████████████▏                                                                          | 3/7 [01:41&lt;01:45, 26.36s/it]</span><br><span class=\"line\">Loading checkpoint shards:  57%|██████████████████████████████████████████████████████████████████████████▊                                                        | 4/7 [01:42&lt;00:48, 16.14s/it]</span><br><span class=\"line\">Loading checkpoint shards:  71%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                     | 5/7 [01:42&lt;00:21, 10.51s/it]</span><br><span class=\"line\">Loading checkpoint shards:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                  | 6/7 [02:13&lt;00:17, 17.35s/it]</span><br><span class=\"line\">Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [02:50&lt;00:00, 23.71s/it]</span><br><span class=\"line\">Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [02:50&lt;00:00, 24.32s/it]</span><br><span class=\"line\">2024-01-05 14:20:11 | ERROR | stderr |</span><br><span class=\"line\">2024-01-05 14:20:16 | INFO | model_worker | Register to controller</span><br><span class=\"line\">INFO:     Started server process [7956]</span><br><span class=\"line\">INFO:     Waiting for application startup.</span><br><span class=\"line\">INFO:     Application startup complete.</span><br><span class=\"line\">INFO:     Uvicorn running on http://127.0.0.1:7861 (Press CTRL+C to quit)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">==============================Langchain-Chatchat Configuration==============================</span><br><span class=\"line\">操作系统：Windows-10-10.0.22000-SP0.</span><br><span class=\"line\">python版本：3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]</span><br><span class=\"line\">项目版本：v0.2.9</span><br><span class=\"line\">langchain版本：0.0.353. fastchat版本：0.2.34</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">当前使用的分词器：ChineseRecursiveTextSplitter</span><br><span class=\"line\">当前启动的LLM模型：[&#x27;chatglm2-6b&#x27;] @ cuda</span><br><span class=\"line\">&#123;&#x27;device&#x27;: &#x27;cuda&#x27;,</span><br><span class=\"line\"> &#x27;host&#x27;: &#x27;127.0.0.1&#x27;,</span><br><span class=\"line\"> &#x27;infer_turbo&#x27;: False,</span><br><span class=\"line\"> &#x27;model_path&#x27;: &#x27;D:\\\\llm\\\\chatglm3-6b&#x27;,</span><br><span class=\"line\"> &#x27;model_path_exists&#x27;: True,</span><br><span class=\"line\"> &#x27;port&#x27;: 20002&#125;</span><br><span class=\"line\">当前Embbedings模型： m3e-base @ cuda</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">服务端运行信息：</span><br><span class=\"line\">    OpenAI API Server: http://127.0.0.1:20000/v1</span><br><span class=\"line\">    Chatchat  API  Server: http://127.0.0.1:7861</span><br><span class=\"line\">    Chatchat WEBUI Server: http://127.0.0.1:8501</span><br><span class=\"line\">==============================Langchain-Chatchat Configuration==============================</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">      Welcome to Streamlit!</span><br><span class=\"line\"></span><br><span class=\"line\">      If you’d like to receive helpful onboarding emails, news, offers, promotions,</span><br><span class=\"line\">      and the occasional swag, please enter your email address below. Otherwise,</span><br><span class=\"line\">      leave this field blank.</span><br><span class=\"line\"></span><br><span class=\"line\">      Email:  2024-01-05 14:24:24 | INFO | stdout | INFO:     127.0.0.1:63380 - &quot;GET /v1 HTTP/1.1&quot; 404 Not Found</span><br><span class=\"line\">INFO:     127.0.0.1:63456 - &quot;GET / HTTP/1.1&quot; 307 Temporary Redirect</span><br><span class=\"line\">INFO:     127.0.0.1:63456 - &quot;GET /docs HTTP/1.1&quot; 200 OK</span><br><span class=\"line\">INFO:     127.0.0.1:63456 - &quot;GET /swagger-ui.css HTTP/1.1&quot; 200 OK</span><br><span class=\"line\">INFO:     127.0.0.1:63457 - &quot;GET /swagger-ui-bundle.js HTTP/1.1&quot; 200 OK</span><br><span class=\"line\">INFO:     127.0.0.1:63457 - &quot;GET /openapi.json HTTP/1.1&quot; 200 OK</span><br><span class=\"line\">INFO:     127.0.0.1:63456 - &quot;GET /favicon.png HTTP/1.1&quot; 200 OK</span><br><span class=\"line\">INFO:     127.0.0.1:63645 - &quot;GET /docs HTTP/1.1&quot; 200 OK</span><br><span class=\"line\">INFO:     127.0.0.1:63645 - &quot;GET /swagger-ui.css HTTP/1.1&quot; 200 OK</span><br><span class=\"line\">INFO:     127.0.0.1:63646 - &quot;GET /swagger-ui-bundle.js HTTP/1.1&quot; 200 OK</span><br><span class=\"line\">INFO:     127.0.0.1:63646 - &quot;GET /openapi.json HTTP/1.1&quot; 200 OK</span><br><span class=\"line\">INFO:     127.0.0.1:63646 - &quot;GET /favicon.png HTTP/1.1&quot; 200 OK</span><br><span class=\"line\">INFO:     127.0.0.1:63744 - &quot;POST /chat/chat HTTP/1.1&quot; 200 OK</span><br></pre></td></tr></table></figure>\n\n\n\n<p>运行效果如下：</p>\n<p><img src=\"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/llm/image-20240105165336385.png\" alt=\"image-20240105165336385\"></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>作为一个连门都没入的 java 工程师，前后花了差不多 1 天多时间；主要坑就是 <code>Could not build wheels for jq, which is required to install pyproject.toml-based projects</code> 这个报错让我纠结了很久；官方 issue 关于这个问题也有不少社区反馈：<a href=\"https://github.com/chatchat-space/Langchain-Chatchat/issues?q=jq%E3%80%82%E5%8F%A6%E5%A4%96%E8%BF%98%E6%98%AF%E5%AF%B9%E4%BA%8E%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E4%B8%AD%E7%BB%99%E7%9A%84\">https://github.com/chatchat-space/Langchain-Chatchat/issues?q=jq。另外还是对于官方文档中给的</a> wiki 看的太粗，也导致在某些点上浪费了一些时间。</p>\n<p>通过此次部署，首先是对于大模型本地部署的流程有了一些基本印象和思路；对于如 conda、cuda、pytorch 等相关框架及工具有一些初步认识，还是有一些收获的。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ul>\n<li><a href=\"https://blog.csdn.net/xqdd/article/details/134247532\">https://blog.csdn.net/xqdd/article/details/134247532</a></li>\n<li><a href=\"https://github.com/chatchat-space/Langchain-Chatchat/wiki/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2#%E5%B8%B8%E8%A7%84%E6%A8%A1%E5%BC%8F%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88\">https://github.com/chatchat-space/Langchain-Chatchat/wiki/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2#%E5%B8%B8%E8%A7%84%E6%A8%A1%E5%BC%8F%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88</a></li>\n</ul>\n","text":"基于 ChatGLM 等大语言模型与 Langchain 等应用框架实现，开源、可离线部署的检索增强生成(RAG)大模型知识库项目。 本地环境 品牌：戴尔 op...","permalink":"/post/llm/langchain-chatchat-deploy","photos":[],"count_time":{"symbolsCount":"14k","symbolsTime":"13 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83\"><span class=\"toc-text\">本地环境</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%BD%AF%E4%BB%B6%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85\"><span class=\"toc-text\">软件环境安装</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Anaconda-%E5%AE%89%E8%A3%85\"><span class=\"toc-text\">Anaconda 安装</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#cuda-%E5%AE%89%E8%A3%85\"><span class=\"toc-text\">cuda 安装</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#pytorch-%E5%AE%89%E8%A3%85\"><span class=\"toc-text\">pytorch 安装</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95\"><span class=\"toc-text\">环境测试</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%88%9B%E5%BB%BA-python-%E8%BF%90%E8%A1%8C%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83\"><span class=\"toc-text\">创建 python 运行虚拟环境</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B5%8B%E8%AF%95-pytorch\"><span class=\"toc-text\">测试 pytorch</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Langchain-Chatchat-%E4%B8%8B%E8%BD%BD\"><span class=\"toc-text\">Langchain-Chatchat 下载</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD\"><span class=\"toc-text\">模型下载</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9%E5%92%8C%E6%96%87%E4%BB%B6%E4%BF%AE%E6%94%B9\"><span class=\"toc-text\">配置修改和文件修改</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%90%8D\"><span class=\"toc-text\">批量修改配置文件名</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BF%AE%E6%94%B9-model-config-py-%E6%96%87%E4%BB%B6\"><span class=\"toc-text\">修改 model_config.py 文件</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BF%AE%E6%94%B9-server-config-py\"><span class=\"toc-text\">修改 server_config.py</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%83%A8%E7%BD%B2\"><span class=\"toc-text\">部署</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%9F%A5%E8%AF%86%E5%BA%93%E5%88%9D%E5%A7%8B%E5%8C%96\"><span class=\"toc-text\">知识库初始化</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%90%AF%E5%8A%A8%E9%A1%B9%E7%9B%AE\"><span class=\"toc-text\">启动项目</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%80%BB%E7%BB%93\"><span class=\"toc-text\">总结</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%8F%82%E8%80%83\"><span class=\"toc-text\">参考</span></a></li></ol>","author":{"name":"glmapper","slug":"blog-author","avatar":"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/common/favicon.ico","link":"/","description":"开放，开源，分享，共享","socials":{"github":"https://github.com/glmapper","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/2412872703","zhihu":"","csdn":"","juejin":"https://juejin.cn/user/3227821827961806","customs":{"sofastack":{"icon":"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/common/sofastack.svg","link":"https://www.sofastack.tech/"}}}},"mapped":true,"hidden":false,"prev_post":{"title":"浅谈 JAVA 中的垃圾回收机制","uid":"75a9f38981cbb37e9f3b4b30830b22d2","slug":"jvm/jvm-gc-total-summary","date":"2024-07-01T02:59:14.000Z","updated":"2024-07-05T04:09:05.769Z","comments":true,"path":"api/articles/jvm/jvm-gc-total-summary.json","keywords":null,"cover":[],"text":"在现代编程语言中，垃圾回收机制（Garbage Collection）扮演着至关重要的角色，尤其在 Java 语言中更是如此。Java 作为一门广泛应用于企业级...","permalink":"/post/jvm/jvm-gc-total-summary","photos":[],"count_time":{"symbolsCount":"7.7k","symbolsTime":"7 mins."},"categories":[{"name":"jvm","slug":"jvm","count":6,"path":"api/categories/jvm.json"}],"tags":[{"name":"gc","slug":"gc","count":3,"path":"api/tags/gc.json"},{"name":"jvm","slug":"jvm","count":6,"path":"api/tags/jvm.json"}],"author":{"name":"glmapper","slug":"blog-author","avatar":"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/common/favicon.ico","link":"/","description":"开放，开源，分享，共享","socials":{"github":"https://github.com/glmapper","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/2412872703","zhihu":"","csdn":"","juejin":"https://juejin.cn/user/3227821827961806","customs":{"sofastack":{"icon":"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/common/sofastack.svg","link":"https://www.sofastack.tech/"}}}},"feature":true},"next_post":{"title":"聊一聊 Spring Data JPA 中的那些日常实践","uid":"b4c75674b257351f24efe8fa873c5818","slug":"springboot/spring-boot-data-jpa-practice","date":"2023-11-27T14:58:52.000Z","updated":"2024-07-05T04:09:05.827Z","comments":true,"path":"api/articles/springboot/spring-boot-data-jpa-practice.json","keywords":null,"cover":[],"text":"一直以来，团队在使用 ORM 框架上都是比较随意的，一开始是鼓励大家使用 mybatis，主要是期望团队同学可以自己写写 SQL，不至于写 SQL 手生；但是从...","permalink":"/post/springboot/spring-boot-data-jpa-practice","photos":[],"count_time":{"symbolsCount":"11k","symbolsTime":"10 mins."},"categories":[{"name":"SpringBoot","slug":"SpringBoot","count":17,"path":"api/categories/SpringBoot.json"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","count":17,"path":"api/tags/SpringBoot.json"},{"name":"JPA","slug":"JPA","count":1,"path":"api/tags/JPA.json"}],"author":{"name":"glmapper","slug":"blog-author","avatar":"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/common/favicon.ico","link":"/","description":"开放，开源，分享，共享","socials":{"github":"https://github.com/glmapper","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/2412872703","zhihu":"","csdn":"","juejin":"https://juejin.cn/user/3227821827961806","customs":{"sofastack":{"icon":"https://glmapper-blog.oss-cn-hangzhou.aliyuncs.com/common/sofastack.svg","link":"https://www.sofastack.tech/"}}}}}}